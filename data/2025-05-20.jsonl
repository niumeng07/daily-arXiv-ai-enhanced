{"id": "2505.11521", "pdf": "https://arxiv.org/pdf/2505.11521", "abs": "https://arxiv.org/abs/2505.11521", "authors": ["Wang Fang", "Shirin Rahimi", "Olivia Bennett", "Sophie Carter", "Mitra Hassani", "Xu Lan", "Omid Javadi", "Lucas Mitchell"], "title": "Improving Open-Set Semantic Segmentation in 3D Point Clouds by Conditional Channel Capacity Maximization: Preliminary Results", "categories": ["cs.CV", "eess.SP"], "comment": null, "summary": "Point-cloud semantic segmentation underpins a wide range of critical\napplications. Although recent deep architectures and large-scale datasets have\ndriven impressive closed-set performance, these models struggle to recognize or\nproperly segment objects outside their training classes. This gap has sparked\ninterest in Open-Set Semantic Segmentation (O3S), where models must both\ncorrectly label known categories and detect novel, unseen classes. In this\npaper, we propose a plug and play framework for O3S. By modeling the\nsegmentation pipeline as a conditional Markov chain, we derive a novel\nregularizer term dubbed Conditional Channel Capacity Maximization (3CM), that\nmaximizes the mutual information between features and predictions conditioned\non each class. When incorporated into standard loss functions, 3CM encourages\nthe encoder to retain richer, label-dependent features, thereby enhancing the\nnetwork's ability to distinguish and segment previously unseen categories.\nExperimental results demonstrate effectiveness of proposed method on detecting\nunseen objects. We further outline future directions for dynamic open-world\nadaptation and efficient information-theoretic estimation."}
{"id": "2505.11581", "pdf": "https://arxiv.org/pdf/2505.11581", "abs": "https://arxiv.org/abs/2505.11581", "authors": ["Akarsh Kumar", "Jeff Clune", "Joel Lehman", "Kenneth O. Stanley"], "title": "Questioning Representational Optimism in Deep Learning: The Fractured Entangled Representation Hypothesis", "categories": ["cs.CV", "cs.LG", "cs.NE"], "comment": "43 pages, 25 figures", "summary": "Much of the excitement in modern AI is driven by the observation that scaling\nup existing systems leads to better performance. But does better performance\nnecessarily imply better internal representations? While the representational\noptimist assumes it must, this position paper challenges that view. We compare\nneural networks evolved through an open-ended search process to networks\ntrained via conventional stochastic gradient descent (SGD) on the simple task\nof generating a single image. This minimal setup offers a unique advantage:\neach hidden neuron's full functional behavior can be easily visualized as an\nimage, thus revealing how the network's output behavior is internally\nconstructed neuron by neuron. The result is striking: while both networks\nproduce the same output behavior, their internal representations differ\ndramatically. The SGD-trained networks exhibit a form of disorganization that\nwe term fractured entangled representation (FER). Interestingly, the evolved\nnetworks largely lack FER, even approaching a unified factored representation\n(UFR). In large models, FER may be degrading core model capacities like\ngeneralization, creativity, and (continual) learning. Therefore, understanding\nand mitigating FER could be critical to the future of representation learning."}
{"id": "2505.11620", "pdf": "https://arxiv.org/pdf/2505.11620", "abs": "https://arxiv.org/abs/2505.11620", "authors": ["Aaron Wilhelm", "Nils Napp"], "title": "Improved Bag-of-Words Image Retrieval with Geometric Constraints for Ground Texture Localization", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted to ICRA 2025", "summary": "Ground texture localization using a downward-facing camera offers a low-cost,\nhigh-precision localization solution that is robust to dynamic environments and\nrequires no environmental modification. We present a significantly improved\nbag-of-words (BoW) image retrieval system for ground texture localization,\nachieving substantially higher accuracy for global localization and higher\nprecision and recall for loop closure detection in SLAM. Our approach leverages\nan approximate $k$-means (AKM) vocabulary with soft assignment, and exploits\nthe consistent orientation and constant scale constraints inherent to ground\ntexture localization. Identifying the different needs of global localization\nvs. loop closure detection for SLAM, we present both high-accuracy and\nhigh-speed versions of our algorithm. We test the effect of each of our\nproposed improvements through an ablation study and demonstrate our method's\neffectiveness for both global localization and loop closure detection. With\nnumerous ground texture localization systems already using BoW, our method can\nreadily replace other generic BoW systems in their pipeline and immediately\nimprove their results."}
{"id": "2505.11640", "pdf": "https://arxiv.org/pdf/2505.11640", "abs": "https://arxiv.org/abs/2505.11640", "authors": ["Pandula Thennakoon", "Avishka Ranasinghe", "Mario De Silva", "Buwaneka Epakanda", "Roshan Godaliyadda", "Parakrama Ekanayake", "Vijitha Herath"], "title": "BandRC: Band Shifted Raised Cosine Activated Implicit Neural Representations", "categories": ["cs.CV"], "comment": "Submitted as a conference paper to ICCV 2025", "summary": "In recent years, implicit neural representations(INRs) have gained popularity\nin the computer vision community. This is mainly due to the strong performance\nof INRs in many computer vision tasks. These networks can extract a continuous\nsignal representation given a discrete signal representation. In previous\nstudies, it has been repeatedly shown that INR performance has a strong\ncorrelation with the activation functions used in its multilayer perceptrons.\nAlthough numerous activation functions have been proposed that are competitive\nwith one another, they share some common set of challenges such as spectral\nbias(Lack of sensitivity to high-frequency content in signals), limited\nrobustness to signal noise and difficulties in simultaneous capturing both\nlocal and global features. and furthermore, the requirement for manual\nparameter tuning. To address these issues, we introduce a novel activation\nfunction, Band Shifted Raised Cosine Activated Implicit Neural Networks\n\\textbf{(BandRC)} tailored to enhance signal representation capacity further.\nWe also incorporate deep prior knowledge extracted from the signal to adjust\nthe activation functions through a task-specific model. Through a mathematical\nanalysis and a series of experiments which include image reconstruction (with a\n+8.93 dB PSNR improvement over the nearest counterpart), denoising (with a\n+0.46 dB increase in PSNR), super-resolution (with a +1.03 dB improvement over\nthe nearest State-Of-The-Art (SOTA) method for 6X super-resolution),\ninpainting, and 3D shape reconstruction we demonstrate the dominance of BandRC\nover existing state of the art activation functions."}
{"id": "2505.11533", "pdf": "https://arxiv.org/pdf/2505.11533", "abs": "https://arxiv.org/abs/2505.11533", "authors": ["Jinqiang Wang", "Huansheng Ning", "Tao Zhu", "Jianguo Ding"], "title": "A Data Synthesis Method Driven by Large Language Models for Proactive Mining of Implicit User Intentions in Tourism", "categories": ["cs.CL"], "comment": null, "summary": "In the tourism domain, Large Language Models (LLMs) often struggle to mine\nimplicit user intentions from tourists' ambiguous inquiries and lack the\ncapacity to proactively guide users toward clarifying their needs. A critical\nbottleneck is the scarcity of high-quality training datasets that facilitate\nproactive questioning and implicit intention mining. While recent advances\nleverage LLM-driven data synthesis to generate such datasets and transfer\nspecialized knowledge to downstream models, existing approaches suffer from\nseveral shortcomings: (1) lack of adaptation to the tourism domain, (2) skewed\ndistributions of detail levels in initial inquiries, (3) contextual redundancy\nin the implicit intention mining module, and (4) lack of explicit thinking\nabout tourists' emotions and intention values. Therefore, we propose SynPT (A\nData Synthesis Method Driven by LLMs for Proactive Mining of Implicit User\nIntentions in the Tourism), which constructs an LLM-driven user agent and\nassistant agent to simulate dialogues based on seed data collected from Chinese\ntourism websites. This approach addresses the aforementioned limitations and\ngenerates SynPT-Dialog, a training dataset containing explicit reasoning. The\ndataset is utilized to fine-tune a general LLM, enabling it to proactively mine\nimplicit user intentions. Experimental evaluations, conducted from both human\nand LLM perspectives, demonstrate the superiority of SynPT compared to existing\nmethods. Furthermore, we analyze key hyperparameters and present case studies\nto illustrate the practical applicability of our method, including discussions\non its adaptability to English-language scenarios. All code and data are\npublicly available."}
{"id": "2505.11584", "pdf": "https://arxiv.org/pdf/2505.11584", "abs": "https://arxiv.org/abs/2505.11584", "authors": ["Manuel Cherep", "Pattie Maes", "Nikhil Singh"], "title": "LLM Agents Are Hypersensitive to Nudges", "categories": ["cs.AI"], "comment": "33 pages, 28 figures", "summary": "LLMs are being set loose in complex, real-world environments involving\nsequential decision-making and tool use. Often, this involves making choices on\nbehalf of human users. However, not much is known about the distribution of\nsuch choices, and how susceptible they are to different choice architectures.\nWe perform a case study with a few such LLM models on a multi-attribute tabular\ndecision-making problem, under canonical nudges such as the default option,\nsuggestions, and information highlighting, as well as additional prompting\nstrategies. We show that, despite superficial similarities to human choice\ndistributions, such models differ in subtle but important ways. First, they\nshow much higher susceptibility to the nudges. Second, they diverge in points\nearned, being affected by factors like the idiosyncrasy of available prizes.\nThird, they diverge in information acquisition strategies: e.g. incurring\nsubstantial cost to reveal too much information, or selecting without revealing\nany. Moreover, we show that simple prompt strategies like zero-shot chain of\nthought (CoT) can shift the choice distribution, and few-shot prompting with\nhuman data can induce greater alignment. Yet, none of these methods resolve the\nsensitivity of these models to nudges. Finally, we show how optimal nudges\noptimized with a human resource-rational model can similarly increase LLM\nperformance for some models. All these findings suggest that behavioral tests\nare needed before deploying models as agents or assistants acting on behalf of\nusers in complex environments."}
{"id": "2505.11676", "pdf": "https://arxiv.org/pdf/2505.11676", "abs": "https://arxiv.org/abs/2505.11676", "authors": ["Ziyu Zhao", "Xiaoguang Li", "Linjia Shi", "Nasrin Imanpour", "Song Wang"], "title": "DPSeg: Dual-Prompt Cost Volume Learning for Open-Vocabulary Semantic Segmentation", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Open-vocabulary semantic segmentation aims to segment images into distinct\nsemantic regions for both seen and unseen categories at the pixel level.\nCurrent methods utilize text embeddings from pre-trained vision-language models\nlike CLIP but struggle with the inherent domain gap between image and text\nembeddings, even after extensive alignment during training. Additionally,\nrelying solely on deep text-aligned features limits shallow-level feature\nguidance, which is crucial for detecting small objects and fine details,\nultimately reducing segmentation accuracy. To address these limitations, we\npropose a dual prompting framework, DPSeg, for this task. Our approach combines\ndual-prompt cost volume generation, a cost volume-guided decoder, and a\nsemantic-guided prompt refinement strategy that leverages our dual prompting\nscheme to mitigate alignment issues in visual prompt generation. By\nincorporating visual embeddings from a visual prompt encoder, our approach\nreduces the domain gap between text and image embeddings while providing\nmulti-level guidance through shallow features. Extensive experiments\ndemonstrate that our method significantly outperforms existing state-of-the-art\napproaches on multiple public datasets."}
{"id": "2505.11550", "pdf": "https://arxiv.org/pdf/2505.11550", "abs": "https://arxiv.org/abs/2505.11550", "authors": ["Harika Abburi", "Sanmitra Bhattacharya", "Edward Bowen", "Nirmala Pudota"], "title": "AI-generated Text Detection: A Multifaceted Approach to Binary and Multiclass Classification", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ngenerating text that closely resembles human writing across a wide range of\nstyles and genres. However, such capabilities are prone to potential misuse,\nsuch as fake news generation, spam email creation, and misuse in academic\nassignments. As a result, accurate detection of AI-generated text and\nidentification of the model that generated it are crucial for maintaining the\nresponsible use of LLMs. In this work, we addressed two sub-tasks put forward\nby the Defactify workshop under AI-Generated Text Detection shared task at the\nAssociation for the Advancement of Artificial Intelligence (AAAI 2025): Task A\ninvolved distinguishing between human-authored or AI-generated text, while Task\nB focused on attributing text to its originating language model. For each task,\nwe proposed two neural architectures: an optimized model and a simpler variant.\nFor Task A, the optimized neural architecture achieved fifth place with $F1$\nscore of 0.994, and for Task B, the simpler neural architecture also ranked\nfifth place with $F1$ score of 0.627."}
{"id": "2505.11610", "pdf": "https://arxiv.org/pdf/2505.11610", "abs": "https://arxiv.org/abs/2505.11610", "authors": ["Asher Moldwin", "Amarda Shehu"], "title": "Foundation Models for AI-Enabled Biological Design", "categories": ["cs.AI", "cs.LG", "q-bio.BM", "q-bio.GN"], "comment": "Published as part of the workshop proceedings at AAAI 2025 in the\n  workshop \"Foundation Models for Biological Discoveries\"", "summary": "This paper surveys foundation models for AI-enabled biological design,\nfocusing on recent developments in applying large-scale, self-supervised models\nto tasks such as protein engineering, small molecule design, and genomic\nsequence design. Though this domain is evolving rapidly, this survey presents\nand discusses a taxonomy of current models and methods. The focus is on\nchallenges and solutions in adapting these models for biological applications,\nincluding biological sequence modeling architectures, controllability in\ngeneration, and multi-modal integration. The survey concludes with a discussion\nof open problems and future directions, offering concrete next-steps to improve\nthe quality of biological sequence generation."}
{"id": "2505.11703", "pdf": "https://arxiv.org/pdf/2505.11703", "abs": "https://arxiv.org/abs/2505.11703", "authors": ["Jae Myung Kim", "Stephan Alaniz", "Cordelia Schmid", "Zeynep Akata"], "title": "LoFT: LoRA-fused Training Dataset Generation with Few-shot Guidance", "categories": ["cs.CV"], "comment": null, "summary": "Despite recent advances in text-to-image generation, using synthetically\ngenerated data seldom brings a significant boost in performance for supervised\nlearning. Oftentimes, synthetic datasets do not faithfully recreate the data\ndistribution of real data, i.e., they lack the fidelity or diversity needed for\neffective downstream model training. While previous work has employed few-shot\nguidance to address this issue, existing methods still fail to capture and\ngenerate features unique to specific real images. In this paper, we introduce a\nnovel dataset generation framework named LoFT, LoRA-Fused Training-data\nGeneration with Few-shot Guidance. Our method fine-tunes LoRA weights on\nindividual real images and fuses them at inference time, producing synthetic\nimages that combine the features of real images for improved diversity and\nfidelity of generated data. We evaluate the synthetic data produced by LoFT on\n10 datasets, using 8 to 64 real images per class as guidance and scaling up to\n1000 images per class. Our experiments show that training on LoFT-generated\ndata consistently outperforms other synthetic dataset methods, significantly\nincreasing accuracy as the dataset size increases. Additionally, our analysis\ndemonstrates that LoFT generates datasets with high fidelity and sufficient\ndiversity, which contribute to the performance improvement. The code is\navailable at https://github.com/ExplainableML/LoFT."}
{"id": "2505.11556", "pdf": "https://arxiv.org/pdf/2505.11556", "abs": "https://arxiv.org/abs/2505.11556", "authors": ["Yuxuan Li", "Aoi Naito", "Hirokazu Shirado"], "title": "Assessing Collective Reasoning in Multi-Agent LLMs via Hidden Profile Tasks", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": null, "summary": "Multi-agent systems built on large language models (LLMs) promise enhanced\nproblem-solving through distributed information integration, but also risk\nreplicating collective reasoning failures observed in human groups. Yet, no\ntheory-grounded benchmark exists to systematically evaluate such failures. In\nthis paper, we introduce the Hidden Profile paradigm from social psychology as\na diagnostic testbed for multi-agent LLM systems. By distributing critical\ninformation asymmetrically across agents, the paradigm reveals how inter-agent\ndynamics support or hinder collective reasoning. We first formalize the\nparadigm for multi-agent decision-making under distributed knowledge and\ninstantiate it as a benchmark with nine tasks spanning diverse scenarios,\nincluding adaptations from prior human studies. We then conduct experiments\nwith GPT-4.1 and five other leading LLMs, including reasoning-enhanced\nvariants, showing that multi-agent systems across all models fail to match the\naccuracy of single agents given complete information. While agents' collective\nperformance is broadly comparable to that of human groups, nuanced behavioral\ndifferences emerge, such as increased sensitivity to social desirability.\nFinally, we demonstrate the paradigm's diagnostic utility by exploring a\ncooperation-contradiction trade-off in multi-agent LLM systems. We find that\nwhile cooperative agents are prone to over-coordination in collective settings,\nincreased contradiction impairs group convergence. This work contributes a\nreproducible framework for evaluating multi-agent LLM systems and motivates\nfuture research on artificial collective intelligence and human-AI interaction."}
{"id": "2505.11611", "pdf": "https://arxiv.org/pdf/2505.11611", "abs": "https://arxiv.org/abs/2505.11611", "authors": ["Bofan Gong", "Shiyang Lai", "Dawn Song"], "title": "Probing the Vulnerability of Large Language Models to Polysemantic Interventions", "categories": ["cs.AI", "cs.CL", "cs.CR"], "comment": null, "summary": "Polysemanticity -- where individual neurons encode multiple unrelated\nfeatures -- is a well-known characteristic of large neural networks and remains\na central challenge in the interpretability of language models. At the same\ntime, its implications for model safety are also poorly understood. Leveraging\nrecent advances in sparse autoencoders, we investigate the polysemantic\nstructure of two small models (Pythia-70M and GPT-2-Small) and evaluate their\nvulnerability to targeted, covert interventions at the prompt, feature, token,\nand neuron levels. Our analysis reveals a consistent polysemantic topology\nshared across both models. Strikingly, we demonstrate that this structure can\nbe exploited to mount effective interventions on two larger, black-box\ninstruction-tuned models (LLaMA3.1-8B-Instruct and Gemma-2-9B-Instruct). These\nfindings suggest not only the generalizability of the interventions but also\npoint to a stable and transferable polysemantic structure that could\npotentially persist across architectures and training regimes."}
{"id": "2505.11707", "pdf": "https://arxiv.org/pdf/2505.11707", "abs": "https://arxiv.org/abs/2505.11707", "authors": ["Haipeng Fang", "Sheng Tang", "Juan Cao", "Enshuo Zhang", "Fan Tang", "Tong-Yee Lee"], "title": "Attend to Not Attended: Structure-then-Detail Token Merging for Post-training DiT Acceleration", "categories": ["cs.CV"], "comment": "Comments: 14 pages, 14 figures. Accepted by the Proceedings of the\n  42nd IEEE/CVF Conference on Computer Vision and Pattern Recognition", "summary": "Diffusion transformers have shown exceptional performance in visual\ngeneration but incur high computational costs. Token reduction techniques that\ncompress models by sharing the denoising process among similar tokens have been\nintroduced. However, existing approaches neglect the denoising priors of the\ndiffusion models, leading to suboptimal acceleration and diminished image\nquality. This study proposes a novel concept: attend to prune feature\nredundancies in areas not attended by the diffusion process. We analyze the\nlocation and degree of feature redundancies based on the structure-then-detail\ndenoising priors. Subsequently, we introduce SDTM, a structure-then-detail\ntoken merging approach that dynamically compresses feature redundancies.\nSpecifically, we design dynamic visual token merging, compression ratio\nadjusting, and prompt reweighting for different stages. Served in a\npost-training way, the proposed method can be integrated seamlessly into any\nDiT architecture. Extensive experiments across various backbones, schedulers,\nand datasets showcase the superiority of our method, for example, it achieves\n1.55 times acceleration with negligible impact on image quality. Project page:\nhttps://github.com/ICTMCG/SDTM."}
{"id": "2505.11604", "pdf": "https://arxiv.org/pdf/2505.11604", "abs": "https://arxiv.org/abs/2505.11604", "authors": ["Kyudan Jung", "Hojun Cho", "Jooyeol Yun", "Jaehyeok Jang", "Jagul Choo"], "title": "Talk to Your Slides: Efficient Slide Editing Agent with Large Language Models", "categories": ["cs.CL"], "comment": "14 pages, 6 figures", "summary": "Existing research on large language models (LLMs) for PowerPoint\npredominantly focuses on slide generation, overlooking the common yet tedious\ntask of editing existing slides. We introduce Talk-to-Your-Slides, an\nLLM-powered agent that directly edits slides within active PowerPoint sessions\nthrough COM communication. Our system employs a two-level approach: (1)\nhigh-level processing where an LLM agent interprets instructions and formulates\nediting plans, and (2) low-level execution where Python scripts directly\nmanipulate PowerPoint objects. Unlike previous methods relying on predefined\noperations, our approach enables more flexible and contextually-aware editing.\nTo facilitate evaluation, we present TSBench, a human-annotated dataset of 379\ndiverse editing instructions with corresponding slide variations. Experimental\nresults demonstrate that Talk-to-Your-Slides significantly outperforms baseline\nmethods in execution success rate, instruction fidelity, and editing\nefficiency. Our code and benchmark are available at\nhttps://anonymous.4open.science/r/talk-to-your-slides/"}
{"id": "2505.11612", "pdf": "https://arxiv.org/pdf/2505.11612", "abs": "https://arxiv.org/abs/2505.11612", "authors": ["Hung Nguyen", "Alireza Rahimi", "Veronica Whitford", "Hélène Fournier", "Irina Kondratova", "René Richard", "Hung Cao"], "title": "Heart2Mind: Human-Centered Contestable Psychiatric Disorder Diagnosis System using Wearable ECG Monitors", "categories": ["cs.AI", "cs.HC"], "comment": "41 pages", "summary": "Psychiatric disorders affect millions globally, yet their diagnosis faces\nsignificant challenges in clinical practice due to subjective assessments and\naccessibility concerns, leading to potential delays in treatment. To help\naddress this issue, we present Heart2Mind, a human-centered contestable\npsychiatric disorder diagnosis system using wearable electrocardiogram (ECG)\nmonitors. Our approach leverages cardiac biomarkers, particularly heart rate\nvariability (HRV) and R-R intervals (RRI) time series, as objective indicators\nof autonomic dysfunction in psychiatric conditions. The system comprises three\nkey components: (1) a Cardiac Monitoring Interface (CMI) for real-time data\nacquisition from Polar H9/H10 devices; (2) a Multi-Scale Temporal-Frequency\nTransformer (MSTFT) that processes RRI time series through integrated\ntime-frequency domain analysis; (3) a Contestable Diagnosis Interface (CDI)\ncombining Self-Adversarial Explanations (SAEs) with contestable Large Language\nModels (LLMs). Our MSTFT achieves 91.7% accuracy on the HRV-ACC dataset using\nleave-one-out cross-validation, outperforming state-of-the-art methods. SAEs\nsuccessfully detect inconsistencies in model predictions by comparing\nattention-based and gradient-based explanations, while LLMs enable clinicians\nto validate correct predictions and contest erroneous ones. This work\ndemonstrates the feasibility of combining wearable technology with Explainable\nArtificial Intelligence (XAI) and contestable LLMs to create a transparent,\ncontestable system for psychiatric diagnosis that maintains clinical oversight\nwhile leveraging advanced AI capabilities. Our implementation is publicly\navailable at: https://github.com/Analytics-Everywhere-Lab/heart2mind."}
{"id": "2505.11709", "pdf": "https://arxiv.org/pdf/2505.11709", "abs": "https://arxiv.org/abs/2505.11709", "authors": ["Ryan Hoque", "Peide Huang", "David J. Yoon", "Mouli Sivapurapu", "Jian Zhang"], "title": "EgoDex: Learning Dexterous Manipulation from Large-Scale Egocentric Video", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": null, "summary": "Imitation learning for manipulation has a well-known data scarcity problem.\nUnlike natural language and 2D computer vision, there is no Internet-scale\ncorpus of data for dexterous manipulation. One appealing option is egocentric\nhuman video, a passively scalable data source. However, existing large-scale\ndatasets such as Ego4D do not have native hand pose annotations and do not\nfocus on object manipulation. To this end, we use Apple Vision Pro to collect\nEgoDex: the largest and most diverse dataset of dexterous human manipulation to\ndate. EgoDex has 829 hours of egocentric video with paired 3D hand and finger\ntracking data collected at the time of recording, where multiple calibrated\ncameras and on-device SLAM can be used to precisely track the pose of every\njoint of each hand. The dataset covers a wide range of diverse manipulation\nbehaviors with everyday household objects in 194 different tabletop tasks\nranging from tying shoelaces to folding laundry. Furthermore, we train and\nsystematically evaluate imitation learning policies for hand trajectory\nprediction on the dataset, introducing metrics and benchmarks for measuring\nprogress in this increasingly important area. By releasing this large-scale\ndataset, we hope to push the frontier of robotics, computer vision, and\nfoundation models."}
{"id": "2505.11613", "pdf": "https://arxiv.org/pdf/2505.11613", "abs": "https://arxiv.org/abs/2505.11613", "authors": ["Xiaomin Li", "Mingye Gao", "Yuexing Hao", "Taoran Li", "Guangya Wan", "Zihan Wang", "Yijun Wang"], "title": "MedGUIDE: Benchmarking Clinical Decision-Making in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Clinical guidelines, typically structured as decision trees, are central to\nevidence-based medical practice and critical for ensuring safe and accurate\ndiagnostic decision-making. However, it remains unclear whether Large Language\nModels (LLMs) can reliably follow such structured protocols. In this work, we\nintroduce MedGUIDE, a new benchmark for evaluating LLMs on their ability to\nmake guideline-consistent clinical decisions. MedGUIDE is constructed from 55\ncurated NCCN decision trees across 17 cancer types and uses clinical scenarios\ngenerated by LLMs to create a large pool of multiple-choice diagnostic\nquestions. We apply a two-stage quality selection process, combining\nexpert-labeled reward models and LLM-as-a-judge ensembles across ten clinical\nand linguistic criteria, to select 7,747 high-quality samples. We evaluate 25\nLLMs spanning general-purpose, open-source, and medically specialized models,\nand find that even domain-specific LLMs often underperform on tasks requiring\nstructured guideline adherence. We also test whether performance can be\nimproved via in-context guideline inclusion or continued pretraining. Our\nfindings underscore the importance of MedGUIDE in assessing whether LLMs can\noperate safely within the procedural frameworks expected in real-world clinical\nsettings."}
{"id": "2505.11614", "pdf": "https://arxiv.org/pdf/2505.11614", "abs": "https://arxiv.org/abs/2505.11614", "authors": ["Jian-Qiao Zhu", "Hanbo Xie", "Dilip Arumugam", "Robert C. Wilson", "Thomas L. Griffiths"], "title": "Using Reinforcement Learning to Train Large Language Models to Explain Human Decisions", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "A central goal of cognitive modeling is to develop models that not only\npredict human behavior but also provide insight into the underlying cognitive\nmechanisms. While neural network models trained on large-scale behavioral data\noften achieve strong predictive performance, they typically fall short in\noffering interpretable explanations of the cognitive processes they capture. In\nthis work, we explore the potential of pretrained large language models (LLMs)\nto serve as dual-purpose cognitive models--capable of both accurate prediction\nand interpretable explanation in natural language. Specifically, we employ\nreinforcement learning with outcome-based rewards to guide LLMs toward\ngenerating explicit reasoning traces for explaining human risky choices. Our\nfindings demonstrate that this approach produces high-quality explanations\nalongside strong quantitative predictions of human decisions."}
{"id": "2505.11720", "pdf": "https://arxiv.org/pdf/2505.11720", "abs": "https://arxiv.org/abs/2505.11720", "authors": ["Shijun Liang", "Ismail R. Alkhouri", "Siddhant Gautam", "Qing Qu", "Saiprasad Ravishankar"], "title": "UGoDIT: Unsupervised Group Deep Image Prior Via Transferable Weights", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "Recent advances in data-centric deep generative models have led to\nsignificant progress in solving inverse imaging problems. However, these models\n(e.g., diffusion models (DMs)) typically require large amounts of fully sampled\n(clean) training data, which is often impractical in medical and scientific\nsettings such as dynamic imaging.\n  On the other hand, training-data-free approaches like the Deep Image Prior\n(DIP) do not require clean ground-truth images but suffer from noise\noverfitting and can be computationally expensive as the network parameters need\nto be optimized for each measurement set independently. Moreover, DIP-based\nmethods often overlook the potential of learning a prior using a small number\nof sub-sampled measurements (or degraded images) available during training. In\nthis paper, we propose UGoDIT, an Unsupervised Group DIP via Transferable\nweights, designed for the low-data regime where only a very small number, M, of\nsub-sampled measurement vectors are available during training. Our method\nlearns a set of transferable weights by optimizing a shared encoder and M\ndisentangled decoders. At test time, we reconstruct the unseen degraded image\nusing a DIP network, where part of the parameters are fixed to the learned\nweights, while the remaining are optimized to enforce measurement consistency.\nWe evaluate UGoDIT on both medical (multi-coil MRI) and natural (super\nresolution and non-linear deblurring) image recovery tasks under various\nsettings. Compared to recent standalone DIP methods, UGoDIT provides\naccelerated convergence and notable improvement in reconstruction quality.\nFurthermore, our method achieves performance competitive with SOTA DM-based and\nsupervised approaches, despite not requiring large amounts of clean training\ndata."}
{"id": "2505.11615", "pdf": "https://arxiv.org/pdf/2505.11615", "abs": "https://arxiv.org/abs/2505.11615", "authors": ["Jian-Qiao Zhu", "Haijiang Yan", "Thomas L. Griffiths"], "title": "Steering Risk Preferences in Large Language Models by Aligning Behavioral and Neural Representations", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Changing the behavior of large language models (LLMs) can be as\nstraightforward as editing the Transformer's residual streams using\nappropriately constructed \"steering vectors.\" These modifications to internal\nneural activations, a form of representation engineering, offer an effective\nand targeted means of influencing model behavior without retraining or\nfine-tuning the model. But how can such steering vectors be systematically\nidentified? We propose a principled approach for uncovering steering vectors by\naligning latent representations elicited through behavioral methods\n(specifically, Markov chain Monte Carlo with LLMs) with their neural\ncounterparts. To evaluate this approach, we focus on extracting latent risk\npreferences from LLMs and steering their risk-related outputs using the aligned\nrepresentations as steering vectors. We show that the resulting steering\nvectors successfully and reliably modulate LLM outputs in line with the\ntargeted behavior."}
{"id": "2505.11618", "pdf": "https://arxiv.org/pdf/2505.11618", "abs": "https://arxiv.org/abs/2505.11618", "authors": ["Pengrui Quan", "Brian Wang", "Kang Yang", "Liying Han", "Mani Srivastava"], "title": "Benchmarking Spatiotemporal Reasoning in LLMs and Reasoning Models: Capabilities and Challenges", "categories": ["cs.AI", "cs.LG", "eess.SP"], "comment": null, "summary": "Spatiotemporal reasoning plays a key role in Cyber-Physical Systems (CPS).\nDespite advances in Large Language Models (LLMs) and Large Reasoning Models\n(LRMs), their capacity to reason about complex spatiotemporal signals remains\nunderexplored. This paper proposes a hierarchical SpatioTemporal reAsoning\nbenchmaRK, STARK, to systematically evaluate LLMs across three levels of\nreasoning complexity: state estimation (e.g., predicting field variables,\nlocalizing and tracking events in space and time), spatiotemporal reasoning\nover states (e.g., inferring spatial-temporal relationships), and\nworld-knowledge-aware reasoning that integrates contextual and domain knowledge\n(e.g., intent prediction, landmark-aware navigation). We curate 26 distinct\nspatiotemporal tasks with diverse sensor modalities, comprising 14,552\nchallenges where models answer directly or by Python Code Interpreter.\nEvaluating 3 LRMs and 8 LLMs, we find LLMs achieve limited success in tasks\nrequiring geometric reasoning (e.g., multilateration or triangulation),\nparticularly as complexity increases. Surprisingly, LRMs show robust\nperformance across tasks with various levels of difficulty, often competing or\nsurpassing traditional first-principle-based methods. Our results show that in\nreasoning tasks requiring world knowledge, the performance gap between LLMs and\nLRMs narrows, with some LLMs even surpassing LRMs. However, the LRM o3 model\ncontinues to achieve leading performance across all evaluated tasks, a result\nattributed primarily to the larger size of the reasoning models. STARK\nmotivates future innovations in model architectures and reasoning paradigms for\nintelligent CPS by providing a structured framework to identify limitations in\nthe spatiotemporal reasoning of LLMs and LRMs."}
{"id": "2505.11724", "pdf": "https://arxiv.org/pdf/2505.11724", "abs": "https://arxiv.org/abs/2505.11724", "authors": ["Kai Zhu", "Vignesh Edithal", "Le Zhang", "Ilia Blank", "Imran Junejo"], "title": "Semantically-Aware Game Image Quality Assessment", "categories": ["cs.CV", "eess.IV"], "comment": "16 pages, 12 figures", "summary": "Assessing the visual quality of video game graphics presents unique\nchallenges due to the absence of reference images and the distinct types of\ndistortions, such as aliasing, texture blur, and geometry level of detail (LOD)\nissues, which differ from those in natural images or user-generated content.\nExisting no-reference image and video quality assessment (NR-IQA/VQA) methods\nfail to generalize to gaming environments as they are primarily designed for\ndistortions like compression artifacts. This study introduces a\nsemantically-aware NR-IQA model tailored to gaming. The model employs a\nknowledge-distilled Game distortion feature extractor (GDFE) to detect and\nquantify game-specific distortions, while integrating semantic gating via CLIP\nembeddings to dynamically weight feature importance based on scene content.\nTraining on gameplay data recorded across graphical quality presets enables the\nmodel to produce quality scores that align with human perception. Our results\ndemonstrate that the GDFE, trained through knowledge distillation from binary\nclassifiers, generalizes effectively to intermediate distortion levels unseen\nduring training. Semantic gating further improves contextual relevance and\nreduces prediction variance. In the absence of in-domain NR-IQA baselines, our\nmodel outperforms out-of-domain methods and exhibits robust, monotonic quality\ntrends across unseen games in the same genre. This work establishes a\nfoundation for automated graphical quality assessment in gaming, advancing\nNR-IQA methods in this domain."}
{"id": "2505.11626", "pdf": "https://arxiv.org/pdf/2505.11626", "abs": "https://arxiv.org/abs/2505.11626", "authors": ["Udita Patel", "Rutu Mulkar", "Jay Roberts", "Cibi Chakravarthy Senthilkumar", "Sujay Gandhi", "Xiaofei Zheng", "Naumaan Nayyar", "Rafael Castrillo"], "title": "THELMA: Task Based Holistic Evaluation of Large Language Model Applications-RAG Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "We propose THELMA (Task Based Holistic Evaluation of Large Language Model\nApplications), a reference free framework for RAG (Retrieval Augmented\ngeneration) based question answering (QA) applications. THELMA consist of six\ninterdependent metrics specifically designed for holistic, fine grained\nevaluation of RAG QA applications. THELMA framework helps developers and\napplication owners evaluate, monitor and improve end to end RAG QA pipelines\nwithout requiring labelled sources or reference responses.We also present our\nfindings on the interplay of the proposed THELMA metrics, which can be\ninterpreted to identify the specific RAG component needing improvement in QA\napplications."}
{"id": "2505.11646", "pdf": "https://arxiv.org/pdf/2505.11646", "abs": "https://arxiv.org/abs/2505.11646", "authors": ["Evelyn Duesterwald", "Siyu Huo", "Vatche Isahagian", "K. R. Jayaram", "Ritesh Kumar", "Vinod Muthusamy", "Punleuk Oum", "Debashish Saha", "Gegi Thomas", "Praveen Venkateswaran"], "title": "FLOW-BENCH: Towards Conversational Generation of Enterprise Workflows", "categories": ["cs.AI", "cs.SE"], "comment": null, "summary": "Business process automation (BPA) that leverages Large Language Models (LLMs)\nto convert natural language (NL) instructions into structured business process\nartifacts is becoming a hot research topic. This paper makes two technical\ncontributions -- (i) FLOW-BENCH, a high quality dataset of paired natural\nlanguage instructions and structured business process definitions to evaluate\nNL-based BPA tools, and support bourgeoning research in this area, and (ii)\nFLOW-GEN, our approach to utilize LLMs to translate natural language into an\nintermediate representation with Python syntax that facilitates final\nconversion into widely adopted business process definition languages, such as\nBPMN and DMN. We bootstrap FLOW-BENCH by demonstrating how it can be used to\nevaluate the components of FLOW-GEN across eight LLMs of varying sizes. We hope\nthat FLOW-GEN and FLOW-BENCH catalyze further research in BPA making it more\naccessible to novice and expert users."}
{"id": "2505.11753", "pdf": "https://arxiv.org/pdf/2505.11753", "abs": "https://arxiv.org/abs/2505.11753", "authors": ["Valentina Bazyleva", "Nicolo Bonettini", "Gaurav Bharaj"], "title": "X-Edit: Detecting and Localizing Edits in Images Altered by Text-Guided Diffusion Models", "categories": ["cs.CV"], "comment": "CVPR (XAI4CV) 2025", "summary": "Text-guided diffusion models have significantly advanced image editing,\nenabling highly realistic and local modifications based on textual prompts.\nWhile these developments expand creative possibilities, their malicious use\nposes substantial challenges for detection of such subtle deepfake edits. To\nthis end, we introduce Explain Edit (X-Edit), a novel method for localizing\ndiffusion-based edits in images. To localize the edits for an image, we invert\nthe image using a pretrained diffusion model, then use these inverted features\nas input to a segmentation network that explicitly predicts the edited masked\nregions via channel and spatial attention. Further, we finetune the model using\na combined segmentation and relevance loss. The segmentation loss ensures\naccurate mask prediction by balancing pixel-wise errors and perceptual\nsimilarity, while the relevance loss guides the model to focus on low-frequency\nregions and mitigate high-frequency artifacts, enhancing the localization of\nsubtle edits. To the best of our knowledge, we are the first to address and\nmodel the problem of localizing diffusion-based modified regions in images. We\nadditionally contribute a new dataset of paired original and edited images\naddressing the current lack of resources for this task. Experimental results\ndemonstrate that X-Edit accurately localizes edits in images altered by\ntext-guided diffusion models, outperforming baselines in PSNR and SSIM metrics.\nThis highlights X-Edit's potential as a robust forensic tool for detecting and\npinpointing manipulations introduced by advanced image editing techniques."}
{"id": "2505.11628", "pdf": "https://arxiv.org/pdf/2505.11628", "abs": "https://arxiv.org/abs/2505.11628", "authors": ["Berkcan Kapusuzoglu", "Supriyo Chakraborty", "Chia-Hsuan Lee", "Sambit Sahu"], "title": "Critique-Guided Distillation: Improving Supervised Fine-tuning via Better Distillation", "categories": ["cs.CL", "cs.LG"], "comment": "Submitted to NeurIPS 2025", "summary": "Supervised fine-tuning (SFT) using expert demonstrations often suffer from\nthe imitation problem, where the model learns to reproduce the correct\nresponses without \\emph{understanding} the underlying rationale. To address\nthis limitation, we propose \\textsc{Critique-Guided Distillation (CGD)}, a\nnovel multi-stage framework that integrates teacher model generated\n\\emph{explanatory critiques} and \\emph{refined responses} into the SFT process.\nA student model is then trained to map the triplet of prompt, teacher critique,\nand its own initial response to the corresponding refined teacher response,\nthereby learning both \\emph{what} to imitate and \\emph{why}. Using\nentropy-based analysis, we show that \\textsc{CGD} reduces refinement\nuncertainty and can be interpreted as a Bayesian posterior update. We perform\nextensive empirical evaluation of \\textsc{CGD}, on variety of benchmark tasks,\nand demonstrate significant gains on both math (AMC23 +17.5%) and language\nunderstanding tasks (MMLU-Pro +6.3%), while successfully mitigating the format\ndrift issues observed in previous critique fine-tuning (CFT) techniques."}
{"id": "2505.11661", "pdf": "https://arxiv.org/pdf/2505.11661", "abs": "https://arxiv.org/abs/2505.11661", "authors": ["Zihan Ye", "Oleg Arenz", "Kristian Kersting"], "title": "Learning from Less: Guiding Deep Reinforcement Learning with Differentiable Symbolic Planning", "categories": ["cs.AI"], "comment": "conference paper, 9 pages", "summary": "When tackling complex problems, humans naturally break them down into\nsmaller, manageable subtasks and adjust their initial plans based on\nobservations. For instance, if you want to make coffee at a friend's place, you\nmight initially plan to grab coffee beans, go to the coffee machine, and pour\nthem into the machine. Upon noticing that the machine is full, you would skip\nthe initial steps and proceed directly to brewing. In stark contrast, state of\nthe art reinforcement learners, such as Proximal Policy Optimization (PPO),\nlack such prior knowledge and therefore require significantly more training\nsteps to exhibit comparable adaptive behavior. Thus, a central research\nquestion arises: \\textit{How can we enable reinforcement learning (RL) agents\nto have similar ``human priors'', allowing the agent to learn with fewer\ntraining interactions?} To address this challenge, we propose differentiable\nsymbolic planner (Dylan), a novel framework that integrates symbolic planning\ninto Reinforcement Learning. Dylan serves as a reward model that dynamically\nshapes rewards by leveraging human priors, guiding agents through intermediate\nsubtasks, thus enabling more efficient exploration. Beyond reward shaping,\nDylan can work as a high level planner that composes primitive policies to\ngenerate new behaviors while avoiding common symbolic planner pitfalls such as\ninfinite execution loops. Our experimental evaluations demonstrate that Dylan\nsignificantly improves RL agents' performance and facilitates generalization to\nunseen tasks."}
{"id": "2505.11758", "pdf": "https://arxiv.org/pdf/2505.11758", "abs": "https://arxiv.org/abs/2505.11758", "authors": ["Sriram Mandalika"], "title": "Generalizable Vision-Language Few-Shot Adaptation with Predictive Prompts and Negative Learning", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.RO"], "comment": null, "summary": "Few-shot adaptation remains a core challenge for vision-language models\n(VLMs), especially under limited supervision and noisy support samples. We\npropose PromptFuseNL, a unified framework that enhances few-shot generalization\nby combining predictive prompt tuning with dual-branch positive and negative\nlearning. The method refines class prototypes through task-conditioned\nresiduals, multi-stage cross-modal coordination, and semantic hard negative\nmining. To address label noise, we introduce an unsupervised instance\nreweighting strategy that downweights unreliable support examples without\nrequiring additional labels or structural changes. PromptFuseNL fuses visual\nand textual cues through lightweight modules for efficient and discriminative\nprediction. Evaluated across 15 benchmarks, it consistently surpasses existing\nprompt- and adapter-based methods in all shot settings while remaining highly\nefficient, achieving up to 300x faster training and 1000x lower FLOPs compared\nto full prompt tuning, achieving a new state-of-the-art for robust and scalable\nfew-shot vision-language adaptation."}
{"id": "2505.11643", "pdf": "https://arxiv.org/pdf/2505.11643", "abs": "https://arxiv.org/abs/2505.11643", "authors": ["Xiang Fu"], "title": "Can an Easy-to-Hard Curriculum Make Reasoning Emerge in Small Language Models? Evidence from a Four-Stage Curriculum on GPT-2", "categories": ["cs.CL"], "comment": null, "summary": "We demonstrate that a developmentally ordered curriculum markedly improves\nreasoning transparency and sample-efficiency in small language models (SLMs).\nConcretely, we train Cognivolve, a 124 M-parameter GPT-2 model, on a four-stage\nsyllabus that ascends from lexical matching to multi-step symbolic inference\nand then evaluate it without any task-specific fine-tuning. Cognivolve reaches\ntarget accuracy in half the optimization steps of a single-phase baseline,\nactivates an order-of-magnitude more gradient-salient reasoning heads, and\nshifts those heads toward deeper layers, yielding higher-entropy attention that\nbalances local and long-range context. The same curriculum applied out of order\nor with optimizer resets fails to reproduce these gains, confirming that\nprogression--not extra compute--drives the effect. We also identify open\nchallenges: final-answer success still lags a conventional run by about 30%,\nand our saliency probe under-detects verbal-knowledge heads in the hardest\nstage, suggesting directions for mixed-stage fine-tuning and probe expansion."}
{"id": "2505.11698", "pdf": "https://arxiv.org/pdf/2505.11698", "abs": "https://arxiv.org/abs/2505.11698", "authors": ["Antoine Bigeard", "Anthony Corso", "Mykel Kochenderfer"], "title": "Conditional Deep Generative Models for Belief State Planning", "categories": ["cs.AI"], "comment": null, "summary": "Partially observable Markov decision processes (POMDPs) are used to model a\nwide range of applications, including robotics, autonomous vehicles, and\nsubsurface problems. However, accurately representing the belief is difficult\nfor POMDPs with high-dimensional states. In this paper, we propose a novel\napproach that uses conditional deep generative models (cDGMs) to represent the\nbelief. Unlike traditional belief representations, cDGMs are well-suited for\nhigh-dimensional states and large numbers of observations, and they can\ngenerate an arbitrary number of samples from the posterior belief. We train the\ncDGMs on data produced by random rollout trajectories and show their\neffectiveness in solving a mineral exploration POMDP with a large and\ncontinuous state space. The cDGMs outperform particle filter baselines in both\ntask-agnostic measures of belief accuracy as well as in planning performance."}
{"id": "2505.11769", "pdf": "https://arxiv.org/pdf/2505.11769", "abs": "https://arxiv.org/abs/2505.11769", "authors": ["Wonjune Kim", "Lae-kyoung Lee", "Su-Yong An"], "title": "Technical Report for ICRA 2025 GOOSE 2D Semantic Segmentation Challenge: Boosting Off-Road Segmentation via Photometric Distortion and Exponential Moving Average", "categories": ["cs.CV"], "comment": "Winners of the GOOSE 2D Semantic Segmentation Challenge at the IEEE\n  ICRA Workshop on Field Robotics 2025", "summary": "We report on the application of a high-capacity semantic segmentation\npipeline to the GOOSE 2D Semantic Segmentation Challenge for unstructured\noff-road environments. Using a FlashInternImage-B backbone together with a\nUPerNet decoder, we adapt established techniques, rather than designing new\nones, to the distinctive conditions of off-road scenes. Our training recipe\ncouples strong photometric distortion augmentation (to emulate the wide\nlighting variations of outdoor terrain) with an Exponential Moving Average\n(EMA) of weights for better generalization. Using only the GOOSE training\ndataset, we achieve 88.8\\% mIoU on the validation set."}
{"id": "2505.11665", "pdf": "https://arxiv.org/pdf/2505.11665", "abs": "https://arxiv.org/abs/2505.11665", "authors": ["Shubham Vatsal", "Harsh Dubey", "Aditi Singh"], "title": "Multilingual Prompt Engineering in Large Language Models: A Survey Across NLP Tasks", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive performance across\na wide range of Natural Language Processing (NLP) tasks. However, ensuring\ntheir effectiveness across multiple languages presents unique challenges.\nMultilingual prompt engineering has emerged as a key approach to enhance LLMs'\ncapabilities in diverse linguistic settings without requiring extensive\nparameter re-training or fine-tuning. With growing interest in multilingual\nprompt engineering over the past two to three years, researchers have explored\nvarious strategies to improve LLMs' performance across languages and NLP tasks.\nBy crafting structured natural language prompts, researchers have successfully\nextracted knowledge from LLMs across different languages, making these\ntechniques an accessible pathway for a broader audience, including those\nwithout deep expertise in machine learning, to harness the capabilities of\nLLMs. In this paper, we survey and categorize different multilingual prompting\ntechniques based on the NLP tasks they address across a diverse set of datasets\nthat collectively span around 250 languages. We further highlight the LLMs\nemployed, present a taxonomy of approaches and discuss potential\nstate-of-the-art (SoTA) methods for specific multilingual datasets.\nAdditionally, we derive a range of insights across language families and\nresource levels (high-resource vs. low-resource), including analyses such as\nthe distribution of NLP tasks by language resource type and the frequency of\nprompting methods across different language families. Our survey reviews 36\nresearch papers covering 39 prompting techniques applied to 30 multilingual NLP\ntasks, with the majority of these studies published in the last two years."}
{"id": "2505.11701", "pdf": "https://arxiv.org/pdf/2505.11701", "abs": "https://arxiv.org/abs/2505.11701", "authors": ["Shaghayegh Abedi", "Amin Jalali"], "title": "DMN-Guided Prompting: A Low-Code Framework for Controlling LLM Behavior", "categories": ["cs.AI"], "comment": "Large Language Models, Decision Model and Notation, Prompt\n  Engineering, Automated Feedback", "summary": "Large Language Models (LLMs) have shown considerable potential in automating\ndecision logic within knowledge-intensive processes. However, their\neffectiveness largely depends on the strategy and quality of prompting. Since\ndecision logic is typically embedded in prompts, it becomes challenging for end\nusers to modify or refine it. Decision Model and Notation (DMN) offers a\nstandardized graphical approach for defining decision logic in a structured,\nuser-friendly manner. This paper introduces a DMN-guided prompting framework\nthat breaks down complex decision logic into smaller, manageable components,\nguiding LLMs through structured decision pathways. We implemented the framework\nin a graduate-level course where students submitted assignments. The\nassignments and DMN models representing feedback instructions served as inputs\nto our framework. The instructor evaluated the generated feedback and labeled\nit for performance assessment. Our approach demonstrated promising results,\noutperforming chain-of-thought (CoT) prompting. Students also responded\npositively to the generated feedback, reporting high levels of perceived\nusefulness in a survey based on the Technology Acceptance Model."}
{"id": "2505.11777", "pdf": "https://arxiv.org/pdf/2505.11777", "abs": "https://arxiv.org/abs/2505.11777", "authors": ["Fu-Yun Wang", "Keqiang Sun", "Yao Teng", "Xihui Liu", "Jiaming Song", "Hongsheng Li"], "title": "Self-NPO: Negative Preference Optimization of Diffusion Models by Simply Learning from Itself without Explicit Preference Annotations", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have demonstrated remarkable success in various visual\ngeneration tasks, including image, video, and 3D content generation. Preference\noptimization (PO) is a prominent and growing area of research that aims to\nalign these models with human preferences. While existing PO methods primarily\nconcentrate on producing favorable outputs, they often overlook the\nsignificance of classifier-free guidance (CFG) in mitigating undesirable\nresults. Diffusion-NPO addresses this gap by introducing negative preference\noptimization (NPO), training models to generate outputs opposite to human\npreferences and thereby steering them away from unfavorable outcomes. However,\nprior NPO approaches, including Diffusion-NPO, rely on costly and fragile\nprocedures for obtaining explicit preference annotations (e.g., manual pairwise\nlabeling or reward model training), limiting their practicality in domains\nwhere such data are scarce or difficult to acquire. In this work, we introduce\nSelf-NPO, a Negative Preference Optimization approach that learns exclusively\nfrom the model itself, thereby eliminating the need for manual data labeling or\nreward model training. Moreover, our method is highly efficient and does not\nrequire exhaustive data sampling. We demonstrate that Self-NPO integrates\nseamlessly into widely used diffusion models, including SD1.5, SDXL, and\nCogVideoX, as well as models already optimized for human preferences,\nconsistently enhancing both their generation quality and alignment with human\npreferences."}
{"id": "2505.11679", "pdf": "https://arxiv.org/pdf/2505.11679", "abs": "https://arxiv.org/abs/2505.11679", "authors": ["Zhibo Hu", "Chen Wang", "Yanfeng Shu", "Hye-Young Paik", "Liming Zhu"], "title": "Ambiguity Resolution in Text-to-Structured Data Mapping", "categories": ["cs.CL", "cs.LG", "I.2.7"], "comment": "15 pages, 11 figures", "summary": "Ambiguity in natural language is a significant obstacle for achieving\naccurate text to structured data mapping through large language models (LLMs),\nwhich affects the performance of tasks such as mapping text to agentic tool\ncalling and text-to-SQL queries. Existing methods of ambiguity handling either\nexploit ReACT framework to produce the correct mapping through trial and error,\nor supervised fine tuning to guide models to produce a biased mapping to\nimprove certain tasks. In this paper, we adopt a different approach that\ncharacterizes the representation difference of ambiguous text in the latent\nspace and leverage the difference to identify ambiguity before mapping them to\nstructured data. To detect ambiguity of a sentence, we focused on the\nrelationship between ambiguous questions and their interpretations and what\ncause the LLM ignore multiple interpretations. Different to the distance\ncalculated by dense embedding vectors, we utilize the observation that\nambiguity is caused by concept missing in latent space of LLM to design a new\ndistance measurement, computed through the path kernel by the integral of\ngradient values for each concepts from sparse-autoencoder (SAE) under each\nstate. We identify patterns to distinguish ambiguous questions with this\nmeasurement. Based on our observation, We propose a new framework to improve\nthe performance of LLMs on ambiguous agentic tool calling through missing\nconcepts prediction."}
{"id": "2505.11718", "pdf": "https://arxiv.org/pdf/2505.11718", "abs": "https://arxiv.org/abs/2505.11718", "authors": ["Pawin Taechoyotin", "Daniel Acuna"], "title": "REMOR: Automated Peer Review Generation with LLM Reasoning and Multi-Objective Reinforcement Learning", "categories": ["cs.AI"], "comment": "18 pages, 6 figures", "summary": "AI-based peer review systems tend to produce shallow and overpraising\nsuggestions compared to human feedback. Here, we evaluate how well a reasoning\nLLM trained with multi-objective reinforcement learning (REMOR) can overcome\nthese limitations. We start by designing a multi-aspect reward function that\naligns with human evaluation of reviews. The aspects are related to the review\nitself (e.g., criticisms, novelty) and the relationship between the review and\nthe manuscript (i.e., relevance). First, we perform supervised fine-tuning of\nDeepSeek-R1-Distill-Qwen-7B using LoRA on PeerRT, a new dataset of high-quality\ntop AI conference reviews enriched with reasoning traces. We then apply Group\nRelative Policy Optimization (GRPO) to train two models: REMOR-H (with the\nhuman-aligned reward) and REMOR-U (with a uniform reward). Interestingly, the\nhuman-aligned reward penalizes aspects typically associated with strong\nreviews, leading REMOR-U to produce qualitatively more substantive feedback.\nOur results show that REMOR-U and REMOR-H achieve more than twice the average\nrewards of human reviews, non-reasoning state-of-the-art agentic multi-modal AI\nreview systems, and general commercial LLM baselines. We found that while the\nbest AI and human reviews are comparable in quality, REMOR avoids the long tail\nof low-quality human reviews. We discuss how reasoning is key to achieving\nthese improvements and release the Human-aligned Peer Review Reward (HPRR)\nfunction, the Peer Review Reasoning-enriched Traces (PeerRT) dataset, and the\nREMOR models, which we believe can help spur progress in the area."}
{"id": "2505.11793", "pdf": "https://arxiv.org/pdf/2505.11793", "abs": "https://arxiv.org/abs/2505.11793", "authors": ["Jianing Wang", "Siying Guo", "Zheng Hua", "Runhu Huang", "Jinyu Hu", "Maoguo Gong"], "title": "CL-CaGAN: Capsule differential adversarial continuous learning for cross-domain hyperspectral anomaly detection", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": null, "summary": "Anomaly detection (AD) has attracted remarkable attention in hyperspectral\nimage (HSI) processing fields, and most existing deep learning (DL)-based\nalgorithms indicate dramatic potential for detecting anomaly samples through\nspecific training process under current scenario. However, the limited prior\ninformation and the catastrophic forgetting problem indicate crucial challenges\nfor existing DL structure in open scenarios cross-domain detection. In order to\nimprove the detection performance, a novel continual learning-based capsule\ndifferential generative adversarial network (CL-CaGAN) is proposed to elevate\nthe cross-scenario learning performance for facilitating the real application\nof DL-based structure in hyperspectral AD (HAD) task. First, a modified capsule\nstructure with adversarial learning network is constructed to estimate the\nbackground distribution for surmounting the deficiency of prior information. To\nmitigate the catastrophic forgetting phenomenon, clustering-based sample replay\nstrategy and a designed extra self-distillation regularization are integrated\nfor merging the history and future knowledge in continual AD task, while the\ndiscriminative learning ability from previous detection scenario to current\nscenario is retained by the elaborately designed structure with continual\nlearning (CL) strategy. In addition, the differentiable enhancement is enforced\nto augment the generation performance of the training data. This further\nstabilizes the training process with better convergence and efficiently\nconsolidates the reconstruction ability of background samples. To verify the\neffectiveness of our proposed CL-CaGAN, we conduct experiments on several real\nHSIs, and the results indicate that the proposed CL-CaGAN demonstrates higher\ndetection performance and continuous learning capacity for mitigating the\ncatastrophic forgetting under cross-domain scenarios."}
{"id": "2505.11683", "pdf": "https://arxiv.org/pdf/2505.11683", "abs": "https://arxiv.org/abs/2505.11683", "authors": ["Susanna Rücker", "Alan Akbik"], "title": "Evaluating Design Decisions for Dual Encoder-based Entity Disambiguation", "categories": ["cs.CL"], "comment": "Accepted at ACL 2025 (The 63rd Annual Meeting of the Association for\n  Computational Linguistics)", "summary": "Entity disambiguation (ED) is the task of linking mentions in text to\ncorresponding entries in a knowledge base. Dual Encoders address this by\nembedding mentions and label candidates in a shared embedding space and\napplying a similarity metric to predict the correct label. In this work, we\nfocus on evaluating key design decisions for Dual Encoder-based ED, such as its\nloss function, similarity metric, label verbalization format, and negative\nsampling strategy. We present the resulting model VerbalizED, a document-level\nDual Encoder model that includes contextual label verbalizations and efficient\nhard negative sampling. Additionally, we explore an iterative prediction\nvariant that aims to improve the disambiguation of challenging data points.\nComprehensive experiments on AIDA-Yago validate the effectiveness of our\napproach, offering insights into impactful design choices that result in a new\nState-of-the-Art system on the ZELDA benchmark."}
{"id": "2505.11730", "pdf": "https://arxiv.org/pdf/2505.11730", "abs": "https://arxiv.org/abs/2505.11730", "authors": ["Hao Mark Chen", "Guanxi Lu", "Yasuyuki Okoshi", "Zhiwen Mo", "Masato Motomura", "Hongxiang Fan"], "title": "Rethinking Optimal Verification Granularity for Compute-Efficient Test-Time Scaling", "categories": ["cs.AI", "cs.LG"], "comment": "Preprint. Under review", "summary": "Test-time scaling (TTS) has proven effective in enhancing the reasoning\ncapabilities of large language models (LLMs). Verification plays a key role in\nTTS, simultaneously influencing (1) reasoning performance and (2) compute\nefficiency, due to the quality and computational cost of verification. In this\nwork, we challenge the conventional paradigms of verification, and make the\nfirst attempt toward systematically investigating the impact of verification\ngranularity-that is, how frequently the verifier is invoked during generation,\nbeyond verifying only the final output or individual generation steps. To this\nend, we introduce Variable Granularity Search (VG-Search), a unified algorithm\nthat generalizes beam search and Best-of-N sampling via a tunable granularity\nparameter g. Extensive experiments with VG-Search under varying compute\nbudgets, generator-verifier configurations, and task attributes reveal that\ndynamically selecting g can improve the compute efficiency and scaling\nbehavior. Building on these findings, we propose adaptive VG-Search strategies\nthat achieve accuracy gains of up to 3.1\\% over Beam Search and 3.6\\% over\nBest-of-N, while reducing FLOPs by over 52\\%. We will open-source the code to\nsupport future research."}
{"id": "2505.11796", "pdf": "https://arxiv.org/pdf/2505.11796", "abs": "https://arxiv.org/abs/2505.11796", "authors": ["Jianing Wang", "Zheng Hua", "Wan Zhang", "Shengjia Hao", "Yuqiong Yao", "Maoguo Gong"], "title": "CL-BioGAN: Biologically-Inspired Cross-Domain Continual Learning for Hyperspectral Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Memory stability and learning flexibility in continual learning (CL) is a\ncore challenge for cross-scene Hyperspectral Anomaly Detection (HAD) task.\nBiological neural networks can actively forget history knowledge that conflicts\nwith the learning of new experiences by regulating learning-triggered synaptic\nexpansion and synaptic convergence. Inspired by this phenomenon, we propose a\nnovel Biologically-Inspired Continual Learning Generative Adversarial Network\n(CL-BioGAN) for augmenting continuous distribution fitting ability for\ncross-domain HAD task, where Continual Learning Bio-inspired Loss (CL-Bio Loss)\nand self-attention Generative Adversarial Network (BioGAN) are incorporated to\nrealize forgetting history knowledge as well as involving replay strategy in\nthe proposed BioGAN. Specifically, a novel Bio-Inspired Loss composed with an\nActive Forgetting Loss (AF Loss) and a CL loss is designed to realize\nparameters releasing and enhancing between new task and history tasks from a\nBayesian perspective. Meanwhile, BioGAN loss with L2-Norm enhances\nself-attention (SA) to further balance the stability and flexibility for better\nfitting background distribution for open scenario HAD (OHAD) tasks. Experiment\nresults underscore that the proposed CL-BioGAN can achieve more robust and\nsatisfying accuracy for cross-domain HAD with fewer parameters and computation\ncost. This dual contribution not only elevates CL performance but also offers\nnew insights into neural adaptation mechanisms in OHAD task."}
{"id": "2505.11690", "pdf": "https://arxiv.org/pdf/2505.11690", "abs": "https://arxiv.org/abs/2505.11690", "authors": ["Sukairaj Hafiz Imam", "Babangida Sani", "Dawit Ketema Gete", "Bedru Yimam Ahamed", "Ibrahim Said Ahmad", "Idris Abdulmumin", "Seid Muhie Yimam", "Muhammad Yahuza Bello", "Shamsuddeen Hassan Muhammad"], "title": "Automatic Speech Recognition for African Low-Resource Languages: Challenges and Future Directions", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Automatic Speech Recognition (ASR) technologies have transformed\nhuman-computer interaction; however, low-resource languages in Africa remain\nsignificantly underrepresented in both research and practical applications.\nThis study investigates the major challenges hindering the development of ASR\nsystems for these languages, which include data scarcity, linguistic\ncomplexity, limited computational resources, acoustic variability, and ethical\nconcerns surrounding bias and privacy. The primary goal is to critically\nanalyze these barriers and identify practical, inclusive strategies to advance\nASR technologies within the African context. Recent advances and case studies\nemphasize promising strategies such as community-driven data collection,\nself-supervised and multilingual learning, lightweight model architectures, and\ntechniques that prioritize privacy. Evidence from pilot projects involving\nvarious African languages showcases the feasibility and impact of customized\nsolutions, which encompass morpheme-based modeling and domain-specific ASR\napplications in sectors like healthcare and education. The findings highlight\nthe importance of interdisciplinary collaboration and sustained investment to\ntackle the distinct linguistic and infrastructural challenges faced by the\ncontinent. This study offers a progressive roadmap for creating ethical,\nefficient, and inclusive ASR systems that not only safeguard linguistic\ndiversity but also improve digital accessibility and promote socioeconomic\nparticipation for speakers of African languages."}
{"id": "2505.11738", "pdf": "https://arxiv.org/pdf/2505.11738", "abs": "https://arxiv.org/abs/2505.11738", "authors": ["Zhongnan Fang", "Andrew Johnston", "Lina Cheuy", "Hye Sun Na", "Magdalini Paschali", "Camila Gonzalez", "Bonnie A. Armstrong", "Arogya Koirala", "Derrick Laurel", "Andrew Walker Campion", "Michael Iv", "Akshay S. Chaudhari", "David B. Larson"], "title": "Automated Real-time Assessment of Intracranial Hemorrhage Detection AI Using an Ensembled Monitoring Model (EMM)", "categories": ["cs.AI"], "comment": null, "summary": "Artificial intelligence (AI) tools for radiology are commonly unmonitored\nonce deployed. The lack of real-time case-by-case assessments of AI prediction\nconfidence requires users to independently distinguish between trustworthy and\nunreliable AI predictions, which increases cognitive burden, reduces\nproductivity, and potentially leads to misdiagnoses. To address these\nchallenges, we introduce Ensembled Monitoring Model (EMM), a framework inspired\nby clinical consensus practices using multiple expert reviews. Designed\nspecifically for black-box commercial AI products, EMM operates independently\nwithout requiring access to internal AI components or intermediate outputs,\nwhile still providing robust confidence measurements. Using intracranial\nhemorrhage detection as our test case on a large, diverse dataset of 2919\nstudies, we demonstrate that EMM successfully categorizes confidence in the\nAI-generated prediction, suggesting different actions and helping improve the\noverall performance of AI tools to ultimately reduce cognitive burden.\nImportantly, we provide key technical considerations and best practices for\nsuccessfully translating EMM into clinical settings."}
{"id": "2505.11800", "pdf": "https://arxiv.org/pdf/2505.11800", "abs": "https://arxiv.org/abs/2505.11800", "authors": ["Jian Zhu", "He Wang", "Yang Xu", "Zebin Wu", "Zhihui Wei"], "title": "Self-Learning Hyperspectral and Multispectral Image Fusion via Adaptive Residual Guided Subspace Diffusion Model", "categories": ["cs.CV", "eess.IV"], "comment": "cvpr", "summary": "Hyperspectral and multispectral image (HSI-MSI) fusion involves combining a\nlow-resolution hyperspectral image (LR-HSI) with a high-resolution\nmultispectral image (HR-MSI) to generate a high-resolution hyperspectral image\n(HR-HSI). Most deep learning-based methods for HSI-MSI fusion rely on large\namounts of hyperspectral data for supervised training, which is often scarce in\npractical applications. In this paper, we propose a self-learning Adaptive\nResidual Guided Subspace Diffusion Model (ARGS-Diff), which only utilizes the\nobserved images without any extra training data. Specifically, as the LR-HSI\ncontains spectral information and the HR-MSI contains spatial information, we\ndesign two lightweight spectral and spatial diffusion models to separately\nlearn the spectral and spatial distributions from them. Then, we use these two\nmodels to reconstruct HR-HSI from two low-dimensional components, i.e, the\nspectral basis and the reduced coefficient, during the reverse diffusion\nprocess. Furthermore, we introduce an Adaptive Residual Guided Module (ARGM),\nwhich refines the two components through a residual guided function at each\nsampling step, thereby stabilizing the sampling process. Extensive experimental\nresults demonstrate that ARGS-Diff outperforms existing state-of-the-art\nmethods in terms of both performance and computational efficiency in the field\nof HSI-MSI fusion. Code is available at https://github.com/Zhu1116/ARGS-Diff."}
{"id": "2505.11693", "pdf": "https://arxiv.org/pdf/2505.11693", "abs": "https://arxiv.org/abs/2505.11693", "authors": ["Ana Ezquerro", "David Vilares", "Anssi Yli-Jyrä", "Carlos Gómez-Rodríguez"], "title": "Hierarchical Bracketing Encodings for Dependency Parsing as Tagging", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025. Original submission; camera-ready coming soon", "summary": "We present a family of encodings for sequence labeling dependency parsing,\nbased on the concept of hierarchical bracketing. We prove that the existing\n4-bit projective encoding belongs to this family, but it is suboptimal in the\nnumber of labels used to encode a tree. We derive an optimal hierarchical\nbracketing, which minimizes the number of symbols used and encodes projective\ntrees using only 12 distinct labels (vs. 16 for the 4-bit encoding). We also\nextend optimal hierarchical bracketing to support arbitrary non-projectivity in\na more compact way than previous encodings. Our new encodings yield competitive\naccuracy on a diverse set of treebanks."}
{"id": "2505.11741", "pdf": "https://arxiv.org/pdf/2505.11741", "abs": "https://arxiv.org/abs/2505.11741", "authors": ["Geigh Zollicoffer", "Minh Vu", "Manish Bhattarai"], "title": "Diverging Towards Hallucination: Detection of Failures in Vision-Language Models via Multi-token Aggregation", "categories": ["cs.AI", "cs.CR"], "comment": null, "summary": "Vision-language models (VLMs) now rival human performance on many multimodal\ntasks, yet they still hallucinate objects or generate unsafe text. Current\nhallucination detectors, e.g., single-token linear probing (SLP) and P(True),\ntypically analyze only the logit of the first generated token or just its\nhighest scoring component overlooking richer signals embedded within earlier\ntoken distributions. We demonstrate that analyzing the complete sequence of\nearly logits potentially provides substantially more diagnostic information. We\nemphasize that hallucinations may only emerge after several tokens, as subtle\ninconsistencies accumulate over time. By analyzing the Kullback-Leibler (KL)\ndivergence between logits corresponding to hallucinated and non-hallucinated\ntokens, we underscore the importance of incorporating later-token logits to\nmore accurately capture the reliability dynamics of VLMs. In response, we\nintroduce Multi-Token Reliability Estimation (MTRE), a lightweight, white-box\nmethod that aggregates logits from the first ten tokens using multi-token\nlog-likelihood ratios and self-attention. Despite the challenges posed by large\nvocabulary sizes and long logit sequences, MTRE remains efficient and\ntractable. On MAD-Bench, MM-SafetyBench, MathVista, and four\ncompositional-geometry benchmarks, MTRE improves AUROC by 9.4 +/- 1.3 points\nover SLP and by 12.1 +/- 1.7 points over P(True), setting a new\nstate-of-the-art in hallucination detection for open-source VLMs."}
{"id": "2505.11804", "pdf": "https://arxiv.org/pdf/2505.11804", "abs": "https://arxiv.org/abs/2505.11804", "authors": ["Xi Wang", "Eric Nalisnick"], "title": "Are vision language models robust to uncertain inputs?", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Robustness against uncertain and ambiguous inputs is a critical challenge for\ndeep learning models. While recent advancements in large scale vision language\nmodels (VLMs, e.g. GPT4o) might suggest that increasing model and training\ndataset size would mitigate this issue, our empirical evaluation shows a more\ncomplicated picture. Testing models using two classic uncertainty\nquantification tasks, anomaly detection and classification under inherently\nambiguous conditions, we find that newer and larger VLMs indeed exhibit\nimproved robustness compared to earlier models, but still suffer from a\ntendency to strictly follow instructions, often causing them to hallucinate\nconfident responses even when faced with unclear or anomalous inputs.\nRemarkably, for natural images such as ImageNet, this limitation can be\novercome without pipeline modifications: simply prompting models to abstain\nfrom uncertain predictions enables significant reliability gains, achieving\nnear-perfect robustness in several settings. However, for domain-specific tasks\nsuch as galaxy morphology classification, a lack of specialized knowledge\nprevents reliable uncertainty estimation. Finally, we propose a novel mechanism\nbased on caption diversity to reveal a model's internal uncertainty, enabling\npractitioners to predict when models will successfully abstain without relying\non labeled data."}
{"id": "2505.11726", "pdf": "https://arxiv.org/pdf/2505.11726", "abs": "https://arxiv.org/abs/2505.11726", "authors": ["Shun Inadumi", "Nobuhiro Ueda", "Koichiro Yoshino"], "title": "Disambiguating Reference in Visually Grounded Dialogues through Joint Modeling of Textual and Multimodal Semantic Structures", "categories": ["cs.CL"], "comment": "ACL2025 main. Code available at https://github.com/SInadumi/mmrr", "summary": "Multimodal reference resolution, including phrase grounding, aims to\nunderstand the semantic relations between mentions and real-world objects.\nPhrase grounding between images and their captions is a well-established task.\nIn contrast, for real-world applications, it is essential to integrate textual\nand multimodal reference resolution to unravel the reference relations within\ndialogue, especially in handling ambiguities caused by pronouns and ellipses.\nThis paper presents a framework that unifies textual and multimodal reference\nresolution by mapping mention embeddings to object embeddings and selecting\nmentions or objects based on their similarity. Our experiments show that\nlearning textual reference resolution, such as coreference resolution and\npredicate-argument structure analysis, positively affects performance in\nmultimodal reference resolution. In particular, our model with coreference\nresolution performs better in pronoun phrase grounding than representative\nmodels for this task, MDETR and GLIP. Our qualitative analysis demonstrates\nthat incorporating textual reference relations strengthens the confidence\nscores between mentions, including pronouns and predicates, and objects, which\ncan reduce the ambiguities that arise in visually grounded dialogues."}
{"id": "2505.11780", "pdf": "https://arxiv.org/pdf/2505.11780", "abs": "https://arxiv.org/abs/2505.11780", "authors": ["Zeinab Shiralizadeh"], "title": "A Review and Analysis of a Parallel Approach for Decision Tree Learning from Large Data Streams", "categories": ["cs.AI"], "comment": null, "summary": "This work studies one of the parallel decision tree learning algorithms,\npdsCART, designed for scalable and efficient data analysis. The method\nincorporates three core capabilities. First, it supports real-time learning\nfrom data streams, allowing trees to be constructed incrementally. Second, it\nenables parallel processing of high-volume streaming data, making it\nwell-suited for large-scale applications. Third, the algorithm integrates\nseamlessly into the MapReduce framework, ensuring compatibility with\ndistributed computing environments. In what follows, we present the algorithm's\nkey components along with results highlighting its performance and scalability."}
{"id": "2505.11809", "pdf": "https://arxiv.org/pdf/2505.11809", "abs": "https://arxiv.org/abs/2505.11809", "authors": ["Zicheng Fan", "Kunihiko Fujiwara", "Pengyuan Liu", "Fan Zhang", "Filip Biljecki"], "title": "Image-based Visibility Analysis Replacing Line-of-Sight Simulation: An Urban Landmark Perspective", "categories": ["cs.CV"], "comment": null, "summary": "Visibility analysis is one of the fundamental analytics methods in urban\nplanning and landscape research, traditionally conducted through computational\nsimulations based on the Line-of-Sight (LoS) principle. However, when assessing\nthe visibility of named urban objects such as landmarks, geometric intersection\nalone fails to capture the contextual and perceptual dimensions of visibility\nas experienced in the real world. The study challenges the traditional\nLoS-based approaches by introducing a new, image-based visibility analysis\nmethod. Specifically, a Vision Language Model (VLM) is applied to detect the\ntarget object within a direction-zoomed Street View Image (SVI). Successful\ndetection represents the object's visibility at the corresponding SVI location.\nFurther, a heterogeneous visibility graph is constructed to address the complex\ninteraction between observers and target objects. In the first case study, the\nmethod proves its reliability in detecting the visibility of six tall landmark\nconstructions in global cities, with an overall accuracy of 87%. Furthermore,\nit reveals broader contextual differences when the landmarks are perceived and\nexperienced. In the second case, the proposed visibility graph uncovers the\nform and strength of connections for multiple landmarks along the River Thames\nin London, as well as the places where these connections occur. Notably,\nbridges on the River Thames account for approximately 30% of total connections.\nOur method complements and enhances traditional LoS-based visibility analysis,\nand showcases the possibility of revealing the prevalent connection of any\nvisual objects in the urban environment. It opens up new research perspectives\nfor urban planning, heritage conservation, and computational social science."}
{"id": "2505.11733", "pdf": "https://arxiv.org/pdf/2505.11733", "abs": "https://arxiv.org/abs/2505.11733", "authors": ["Kevin Wu", "Eric Wu", "Rahul Thapa", "Kevin Wei", "Angela Zhang", "Arvind Suresh", "Jacqueline J. Tao", "Min Woo Sun", "Alejandro Lozano", "James Zou"], "title": "MedCaseReasoning: Evaluating and learning diagnostic reasoning from clinical case reports", "categories": ["cs.CL"], "comment": null, "summary": "Doctors and patients alike increasingly use Large Language Models (LLMs) to\ndiagnose clinical cases. However, unlike domains such as math or coding, where\ncorrectness can be objectively defined by the final answer, medical diagnosis\nrequires both the outcome and the reasoning process to be accurate. Currently,\nwidely used medical benchmarks like MedQA and MMLU assess only accuracy in the\nfinal answer, overlooking the quality and faithfulness of the clinical\nreasoning process. To address this limitation, we introduce MedCaseReasoning,\nthe first open-access dataset for evaluating LLMs on their ability to align\nwith clinician-authored diagnostic reasoning. The dataset includes 14,489\ndiagnostic question-and-answer cases, each paired with detailed reasoning\nstatements derived from open-access medical case reports. We evaluate\nstate-of-the-art reasoning LLMs on MedCaseReasoning and find significant\nshortcomings in their diagnoses and reasoning: for instance, the top-performing\nopen-source model, DeepSeek-R1, achieves only 48% 10-shot diagnostic accuracy\nand mentions only 64% of the clinician reasoning statements (recall). However,\nwe demonstrate that fine-tuning LLMs on the reasoning traces derived from\nMedCaseReasoning significantly improves diagnostic accuracy and clinical\nreasoning recall by an average relative gain of 29% and 41%, respectively. The\nopen-source dataset, code, and models are available at\nhttps://github.com/kevinwu23/Stanford-MedCaseReasoning."}
{"id": "2505.11792", "pdf": "https://arxiv.org/pdf/2505.11792", "abs": "https://arxiv.org/abs/2505.11792", "authors": ["Yitian Chen", "Jingfan Xia", "Siyu Shao", "Dongdong Ge", "Yinyu Ye"], "title": "Solver-Informed RL: Grounding Large Language Models for Authentic Optimization Modeling", "categories": ["cs.AI"], "comment": null, "summary": "Optimization modeling is fundamental to decision-making across diverse\ndomains.Despite progress in automating optimization formulation from natural\nlanguage descriptions, Large Language Models (LLMs) often struggle to generate\nformally correct and usable models due to hallucinations, posing a challenge\nfor reliable automation. Inspired by the success of Reinforcement Learning (RL)\nin enhancing Large Reasoning Models, we present Solver-Informed Reinforcement\nLearning (SIRL).This novel framework leverages external optimization solvers as\nverifiable reward mechanisms to significantly improve the authenticity of LLMs\nfor optimization modeling.Acting as precise verifiers, these solvers\nautomatically assess the executable code and the instance-level mathematical\nmodel represented by the associated LP file, yielding precise and comprehensive\nfeedback signals -- including syntax, feasibility, and solution quality that\ndirectly inform the RL process. This automated verification process, powered by\nclassic optimization solvers, also underpins our instance-enhanced\nself-consistency method to synthesize high-quality training data. Extensive\nexperiments on diverse public benchmarks demonstrate that SIRL achieves\nstate-of-the-art performance, substantially outperforming existing methods in\ngenerating accurate and executable optimization models."}
{"id": "2505.11813", "pdf": "https://arxiv.org/pdf/2505.11813", "abs": "https://arxiv.org/abs/2505.11813", "authors": ["Yixuan Dong", "Fang-Yi Su", "Jung-Hsien Chiang"], "title": "SGD-Mix: Enhancing Domain-Specific Image Classification with Label-Preserving Data Augmentation", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 6 figures, 6 tables", "summary": "Data augmentation for domain-specific image classification tasks often\nstruggles to simultaneously address diversity, faithfulness, and label clarity\nof generated data, leading to suboptimal performance in downstream tasks. While\nexisting generative diffusion model-based methods aim to enhance augmentation,\nthey fail to cohesively tackle these three critical aspects and often overlook\nintrinsic challenges of diffusion models, such as sensitivity to model\ncharacteristics and stochasticity under strong transformations. In this paper,\nwe propose a novel framework that explicitly integrates diversity,\nfaithfulness, and label clarity into the augmentation process. Our approach\nemploys saliency-guided mixing and a fine-tuned diffusion model to preserve\nforeground semantics, enrich background diversity, and ensure label\nconsistency, while mitigating diffusion model limitations. Extensive\nexperiments across fine-grained, long-tail, few-shot, and background robustness\ntasks demonstrate our method's superior performance over state-of-the-art\napproaches."}
{"id": "2505.11739", "pdf": "https://arxiv.org/pdf/2505.11739", "abs": "https://arxiv.org/abs/2505.11739", "authors": ["Feijiang Han", "Xiaodong Yu", "Jianheng Tang", "Lyle Ungar"], "title": "ZeroTuning: Unlocking the Initial Token's Power to Enhance Large Language Models Without Training", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recently, training-free methods for improving large language models (LLMs)\nhave attracted growing interest, with token-level attention tuning emerging as\na promising and interpretable direction. However, existing methods typically\nrely on auxiliary mechanisms to identify important or irrelevant task-specific\ntokens, introducing potential bias and limiting applicability. In this paper,\nwe uncover a surprising and elegant alternative: the semantically empty initial\ntoken is a powerful and underexplored control point for optimizing model\nbehavior. Through theoretical analysis, we show that tuning the initial token's\nattention sharpens or flattens the attention distribution over subsequent\ntokens, and its role as an attention sink amplifies this effect. Empirically,\nwe find that: (1) tuning its attention improves LLM performance more\neffectively than tuning other task-specific tokens; (2) the effect follows a\nconsistent trend across layers, with earlier layers having greater impact, but\nvaries across attention heads, with different heads showing distinct\npreferences in how they attend to this token. Based on these findings, we\npropose ZeroTuning, a training-free approach that improves LLM performance by\napplying head-specific attention adjustments to this special token. Despite\ntuning only one token, ZeroTuning achieves higher performance on text\nclassification, multiple-choice, and multi-turn conversation tasks across\nmodels such as Llama, Qwen, and DeepSeek. For example, ZeroTuning improves\nLlama-3.1-8B by 11.71% on classification, 2.64% on QA tasks, and raises its\nmulti-turn score from 7.804 to 7.966. The method is also robust to limited\nresources, few-shot settings, long contexts, quantization, decoding strategies,\nand prompt variations. Our work sheds light on a previously overlooked control\npoint in LLMs, offering new insights into both inference-time tuning and model\ninterpretability."}
{"id": "2505.11803", "pdf": "https://arxiv.org/pdf/2505.11803", "abs": "https://arxiv.org/abs/2505.11803", "authors": ["ChongIn Un", "Yuhuan Lu", "Tianyue Yang", "Dingqi Yang"], "title": "VITA: Versatile Time Representation Learning for Temporal Hyper-Relational Knowledge Graphs", "categories": ["cs.AI", "cs.SC"], "comment": null, "summary": "Knowledge graphs (KGs) have become an effective paradigm for managing\nreal-world facts, which are not only complex but also dynamically evolve over\ntime. The temporal validity of facts often serves as a strong clue in\ndownstream link prediction tasks, which predicts a missing element in a fact.\nTraditional link prediction techniques on temporal KGs either consider a\nsequence of temporal snapshots of KGs with an ad-hoc defined time interval or\nexpand a temporal fact over its validity period under a predefined time\ngranularity; these approaches not only suffer from the sensitivity of the\nselection of time interval/granularity, but also face the computational\nchallenges when handling facts with long (even infinite) validity. Although the\nrecent hyper-relational KGs represent the temporal validity of a fact as\nqualifiers describing the fact, it is still suboptimal due to its ignorance of\nthe infinite validity of some facts and the insufficient information encoded\nfrom the qualifiers about the temporal validity. Against this background, we\npropose VITA, a $\\underline{V}$ersatile t$\\underline{I}$me\nrepresen$\\underline{TA}$tion learning method for temporal hyper-relational\nknowledge graphs. We first propose a versatile time representation that can\nflexibly accommodate all four types of temporal validity of facts (i.e., since,\nuntil, period, time-invariant), and then design VITA to effectively learn the\ntime information in both aspects of time value and timespan to boost the link\nprediction performance. We conduct a thorough evaluation of VITA compared to a\nsizable collection of baselines on real-world KG datasets. Results show that\nVITA outperforms the best-performing baselines in various link prediction tasks\n(predicting missing entities, relations, time, and other numeric literals) by\nup to 75.3%. Ablation studies and a case study also support our key design\nchoices."}
{"id": "2505.11815", "pdf": "https://arxiv.org/pdf/2505.11815", "abs": "https://arxiv.org/abs/2505.11815", "authors": ["Jiajun Qin", "Yuan Pu", "Zhuolun He", "Seunggeun Kim", "David Z. Pan", "Bei Yu"], "title": "UniMoCo: Unified Modality Completion for Robust Multi-Modal Embeddings", "categories": ["cs.CV"], "comment": null, "summary": "Current research has explored vision-language models for multi-modal\nembedding tasks, such as information retrieval, visual grounding, and\nclassification. However, real-world scenarios often involve diverse modality\ncombinations between queries and targets, such as text and image to text, text\nand image to text and image, and text to text and image. These diverse\ncombinations pose significant challenges for existing models, as they struggle\nto align all modality combinations within a unified embedding space during\ntraining, which degrades performance at inference. To address this limitation,\nwe propose UniMoCo, a novel vision-language model architecture designed for\nmulti-modal embedding tasks. UniMoCo introduces a modality-completion module\nthat generates visual features from textual inputs, ensuring modality\ncompleteness for both queries and targets. Additionally, we develop a\nspecialized training strategy to align embeddings from both original and\nmodality-completed inputs, ensuring consistency within the embedding space.\nThis enables the model to robustly handle a wide range of modality combinations\nacross embedding tasks. Experiments show that UniMoCo outperforms previous\nmethods while demonstrating consistent robustness across diverse settings. More\nimportantly, we identify and quantify the inherent bias in conventional\napproaches caused by imbalance of modality combinations in training data, which\ncan be mitigated through our modality-completion paradigm. The code is\navailable at https://github.com/HobbitQia/UniMoCo."}
{"id": "2505.11746", "pdf": "https://arxiv.org/pdf/2505.11746", "abs": "https://arxiv.org/abs/2505.11746", "authors": ["Xianglong Xu", "John Bowen", "Rojin Taheri"], "title": "Token Masking Improves Transformer-Based Text Classification", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While transformer-based models achieve strong performance on text\nclassification, we explore whether masking input tokens can further enhance\ntheir effectiveness. We propose token masking regularization, a simple yet\ntheoretically motivated method that randomly replaces input tokens with a\nspecial [MASK] token at probability p. This introduces stochastic perturbations\nduring training, leading to implicit gradient averaging that encourages the\nmodel to capture deeper inter-token dependencies. Experiments on language\nidentification and sentiment analysis -- across diverse models (mBERT,\nQwen2.5-0.5B, TinyLlama-1.1B) -- show consistent improvements over standard\nregularization techniques. We identify task-specific optimal masking rates,\nwith p = 0.1 as a strong general default. We attribute the gains to two key\neffects: (1) input perturbation reduces overfitting, and (2) gradient-level\nsmoothing acts as implicit ensembling."}
{"id": "2505.11814", "pdf": "https://arxiv.org/pdf/2505.11814", "abs": "https://arxiv.org/abs/2505.11814", "authors": ["Hector Munoz-Avila", "David W. Aha", "Paola Rizzo"], "title": "ChatHTN: Interleaving Approximate (LLM) and Symbolic HTN Planning", "categories": ["cs.AI"], "comment": "2nd International Conference on Neuro-symbolic Systems (NeuS) 2025", "summary": "We introduce ChatHTN, a Hierarchical Task Network (HTN) planner that combines\nsymbolic HTN planning techniques with queries to ChatGPT to approximate\nsolutions in the form of task decompositions. The resulting hierarchies\ninterleave task decompositions generated by symbolic HTN planning with those\ngenerated by ChatGPT. Despite the approximate nature of the results generates\nby ChatGPT, ChatHTN is provably sound; any plan it generates correctly achieves\nthe input tasks. We demonstrate this property with an open-source\nimplementation of our system."}
{"id": "2505.11816", "pdf": "https://arxiv.org/pdf/2505.11816", "abs": "https://arxiv.org/abs/2505.11816", "authors": ["Quan Cheng", "Yuanyu Wan", "Lingyu Wu", "Chenping Hou", "Lijun Zhang"], "title": "Continuous Subspace Optimization for Continual Learning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Continual learning aims to learn multiple tasks sequentially while preserving\nprior knowledge, but faces the challenge of catastrophic forgetting when\nacquiring new knowledge. Recently, approaches leveraging pre-trained models\nhave gained increasing popularity to mitigate this issue, due to the strong\ngeneralization ability of foundation models. To adjust pre-trained models for\nnew tasks, existing methods usually employ low-rank adaptation, which restricts\nparameter updates to a fixed low-rank subspace. However, constraining the\noptimization space inherently compromises the model's learning capacity,\nresulting in inferior performance. To address the limitation, we propose\nContinuous Subspace Optimization for Continual Learning (CoSO) to fine-tune the\nmodel in a series of subspaces rather than a single one. These sequential\nsubspaces are dynamically determined through the singular value decomposition\nof gradients. CoSO updates the model by projecting gradients into these\nsubspaces, ensuring memory-efficient optimization. To mitigate forgetting, the\noptimization subspaces of each task are set to be orthogonal to the historical\ntask subspace. During task learning, CoSO maintains a task-specific component\nthat captures the critical update directions associated with the current task.\nUpon completing a task, this component is used to update the historical task\nsubspace, laying the groundwork for subsequent learning. Extensive experiments\non multiple datasets demonstrate that CoSO significantly outperforms\nstate-of-the-art methods, especially in challenging scenarios with long task\nsequences."}
{"id": "2505.11754", "pdf": "https://arxiv.org/pdf/2505.11754", "abs": "https://arxiv.org/abs/2505.11754", "authors": ["Wenyu Huang", "Pavlos Vougiouklis", "Mirella Lapata", "Jeff Z. Pan"], "title": "Masking in Multi-hop QA: An Analysis of How Language Models Perform with Context Permutation", "categories": ["cs.CL"], "comment": "ACL 2025 main", "summary": "Multi-hop Question Answering (MHQA) adds layers of complexity to question\nanswering, making it more challenging. When Language Models (LMs) are prompted\nwith multiple search results, they are tasked not only with retrieving relevant\ninformation but also employing multi-hop reasoning across the information\nsources. Although LMs perform well on traditional question-answering tasks, the\ncausal mask can hinder their capacity to reason across complex contexts. In\nthis paper, we explore how LMs respond to multi-hop questions by permuting\nsearch results (retrieved documents) under various configurations. Our study\nreveals interesting findings as follows: 1) Encoder-decoder models, such as the\nones in the Flan-T5 family, generally outperform causal decoder-only LMs in\nMHQA tasks, despite being significantly smaller in size; 2) altering the order\nof gold documents reveals distinct trends in both Flan T5 models and fine-tuned\ndecoder-only models, with optimal performance observed when the document order\naligns with the reasoning chain order; 3) enhancing causal decoder-only models\nwith bi-directional attention by modifying the causal mask can effectively\nboost their end performance. In addition to the above, we conduct a thorough\ninvestigation of the distribution of LM attention weights in the context of\nMHQA. Our experiments reveal that attention weights tend to peak at higher\nvalues when the resulting answer is correct. We leverage this finding to\nheuristically improve LMs' performance on this task. Our code is publicly\navailable at https://github.com/hwy9855/MultiHopQA-Reasoning."}
{"id": "2505.11831", "pdf": "https://arxiv.org/pdf/2505.11831", "abs": "https://arxiv.org/abs/2505.11831", "authors": ["Francois Chollet", "Mike Knoop", "Gregory Kamradt", "Bryan Landers", "Henry Pinkard"], "title": "ARC-AGI-2: A New Challenge for Frontier AI Reasoning Systems", "categories": ["cs.AI"], "comment": null, "summary": "The Abstraction and Reasoning Corpus for Artificial General Intelligence\n(ARC-AGI), introduced in 2019, established a challenging benchmark for\nevaluating the general fluid intelligence of artificial systems via a set of\nunique, novel tasks only requiring minimal prior knowledge. While ARC-AGI has\nspurred significant research activity over the past five years, recent AI\nprogress calls for benchmarks capable of finer-grained evaluation at higher\nlevels of cognitive complexity. We introduce ARC-AGI-2, an upgraded version of\nthe benchmark. ARC-AGI-2 preserves the input-output pair task format of its\npredecessor, ensuring continuity for researchers. It incorporates a newly\ncurated and expanded set of tasks specifically designed to provide a more\ngranular signal to assess abstract reasoning and problem-solving abilities at\nhigher levels of fluid intelligence. To contextualize the difficulty and\ncharacteristics of ARC-AGI-2, we present extensive results from human testing,\nproviding a robust baseline that highlights the benchmark's accessibility to\nhuman intelligence, yet difficulty for current AI systems. ARC-AGI-2 aims to\nserve as a next-generation tool for rigorously measuring progress towards more\ngeneral and human-like AI capabilities."}
{"id": "2505.11822", "pdf": "https://arxiv.org/pdf/2505.11822", "abs": "https://arxiv.org/abs/2505.11822", "authors": ["Ke Li", "Di Wang", "Xiaowei Wang", "Zhihong Wu", "Yiming Zhang", "Yifeng Wang", "Quan Wang"], "title": "Robust Cross-View Geo-Localization via Content-Viewpoint Disentanglement", "categories": ["cs.CV"], "comment": null, "summary": "Cross-view geo-localization (CVGL) aims to match images of the same\ngeographic location captured from different perspectives, such as drones and\nsatellites. Despite recent advances, CVGL remains highly challenging due to\nsignificant appearance changes and spatial distortions caused by viewpoint\nvariations. Existing methods typically assume that cross-view images can be\ndirectly aligned within a shared feature space by maximizing feature similarity\nthrough contrastive learning. Nonetheless, this assumption overlooks the\ninherent conflicts induced by viewpoint discrepancies, resulting in extracted\nfeatures containing inconsistent information that hinders precise localization.\nIn this study, we take a manifold learning perspective and model the feature\nspace of cross-view images as a composite manifold jointly governed by content\nand viewpoint information. Building upon this insight, we propose\n$\\textbf{CVD}$, a new CVGL framework that explicitly disentangles\n$\\textit{content}$ and $\\textit{viewpoint}$ factors. To promote effective\ndisentanglement, we introduce two constraints: $\\textit{(i)}$ An intra-view\nindependence constraint, which encourages statistical independence between the\ntwo factors by minimizing their mutual information. $\\textit{(ii)}$ An\ninter-view reconstruction constraint that reconstructs each view by\ncross-combining $\\textit{content}$ and $\\textit{viewpoint}$ from paired images,\nensuring factor-specific semantics are preserved. As a plug-and-play module,\nCVD can be seamlessly integrated into existing geo-localization pipelines.\nExtensive experiments on four benchmarks, i.e., University-1652, SUES-200,\nCVUSA, and CVACT, demonstrate that CVD consistently improves both localization\naccuracy and generalization across multiple baselines."}
{"id": "2505.11764", "pdf": "https://arxiv.org/pdf/2505.11764", "abs": "https://arxiv.org/abs/2505.11764", "authors": ["Raymond Baartmans", "Matthew Raffel", "Rahul Vikram", "Aiden Deringer", "Lizhong Chen"], "title": "Towards Universal Semantics With Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The Natural Semantic Metalanguage (NSM) is a linguistic theory based on a\nuniversal set of semantic primes: simple, primitive word-meanings that have\nbeen shown to exist in most, if not all, languages of the world. According to\nthis framework, any word, regardless of complexity, can be paraphrased using\nthese primes, revealing a clear and universally translatable meaning. These\nparaphrases, known as explications, can offer valuable applications for many\nnatural language processing (NLP) tasks, but producing them has traditionally\nbeen a slow, manual process. In this work, we present the first study of using\nlarge language models (LLMs) to generate NSM explications. We introduce\nautomatic evaluation methods, a tailored dataset for training and evaluation,\nand fine-tuned models for this task. Our 1B and 8B models outperform GPT-4o in\nproducing accurate, cross-translatable explications, marking a significant step\ntoward universal semantic representation with LLMs and opening up new\npossibilities for applications in semantic analysis, translation, and beyond."}
{"id": "2505.11833", "pdf": "https://arxiv.org/pdf/2505.11833", "abs": "https://arxiv.org/abs/2505.11833", "authors": ["Haotian Chen", "Zijun Song", "Boye Niu", "Ke Zhang", "Litu Ou", "Yaxi Lu", "Zhong Zhang", "Xin Cong", "Yankai Lin", "Zhiyuan Liu", "Maosong Sun"], "title": "ToLeaP: Rethinking Development of Tool Learning with Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Tool learning, which enables large language models (LLMs) to utilize external\ntools effectively, has garnered increasing attention for its potential to\nrevolutionize productivity across industries. Despite rapid development in tool\nlearning, key challenges and opportunities remain understudied, limiting deeper\ninsights and future advancements. In this paper, we investigate the tool\nlearning ability of 41 prevalent LLMs by reproducing 33 benchmarks and enabling\none-click evaluation for seven of them, forming a Tool Learning Platform named\nToLeaP. We also collect 21 out of 33 potential training datasets to facilitate\nfuture exploration. After analyzing over 3,000 bad cases of 41 LLMs based on\nToLeaP, we identify four main critical challenges: (1) benchmark limitations\ninduce both the neglect and lack of (2) autonomous learning, (3)\ngeneralization, and (4) long-horizon task-solving capabilities of LLMs. To aid\nfuture advancements, we take a step further toward exploring potential\ndirections, namely (1) real-world benchmark construction, (2)\ncompatibility-aware autonomous learning, (3) rationale learning by thinking,\nand (4) identifying and recalling key clues. The preliminary experiments\ndemonstrate their effectiveness, highlighting the need for further research and\nexploration."}
{"id": "2505.11825", "pdf": "https://arxiv.org/pdf/2505.11825", "abs": "https://arxiv.org/abs/2505.11825", "authors": ["Xudong Ma"], "title": "Bootstrapping Diffusion: Diffusion Model Training Leveraging Partial and Corrupted Data", "categories": ["cs.CV", "cs.AI"], "comment": "21 pages, 1 figure", "summary": "Training diffusion models requires large datasets. However, acquiring large\nvolumes of high-quality data can be challenging, for example, collecting large\nnumbers of high-resolution images and long videos. On the other hand, there are\nmany complementary data that are usually considered corrupted or partial, such\nas low-resolution images and short videos. Other examples of corrupted data\ninclude videos that contain subtitles, watermarks, and logos. In this study, we\ninvestigate the theoretical problem of whether the above partial data can be\nutilized to train conventional diffusion models. Motivated by our theoretical\nanalysis in this study, we propose a straightforward approach of training\ndiffusion models utilizing partial data views, where we consider each form of\ncomplementary data as a view of conventional data. Our proposed approach first\ntrains one separate diffusion model for each individual view, and then trains a\nmodel for predicting the residual score function. We prove generalization error\nbounds, which show that the proposed diffusion model training approach can\nachieve lower generalization errors if proper regularizations are adopted in\nthe residual score function training. In particular, we prove that the\ndifficulty in training the residual score function scales proportionally with\nthe signal correlations not captured by partial data views. Consequently, the\nproposed approach achieves near first-order optimal data efficiency."}
{"id": "2505.11807", "pdf": "https://arxiv.org/pdf/2505.11807", "abs": "https://arxiv.org/abs/2505.11807", "authors": ["Yufei Xiang", "Yiqun Shen", "Yeqin Zhang", "Cam-Tu Nguyen"], "title": "Retrospex: Language Agent Meets Offline Reinforcement Learning Critic", "categories": ["cs.CL", "cs.AI"], "comment": "17 pages", "summary": "Large Language Models (LLMs) possess extensive knowledge and commonsense\nreasoning capabilities, making them valuable for creating powerful agents.\nHowever, existing LLM agent frameworks have not fully utilized past experiences\nfor improvement. This work introduces a new LLM-based agent framework called\nRetrospex, which addresses this challenge by analyzing past experiences in\ndepth. Unlike previous approaches, Retrospex does not directly integrate\nexperiences into the LLM's context. Instead, it combines the LLM's action\nlikelihood with action values estimated by a Reinforcement Learning (RL)\nCritic, which is trained on past experiences through an offline\n''retrospection'' process. Additionally, Retrospex employs a dynamic action\nrescoring mechanism that increases the importance of experience-based values\nfor tasks that require more interaction with the environment. We evaluate\nRetrospex in ScienceWorld, ALFWorld and Webshop environments, demonstrating its\nadvantages over strong, contemporary baselines."}
{"id": "2505.11839", "pdf": "https://arxiv.org/pdf/2505.11839", "abs": "https://arxiv.org/abs/2505.11839", "authors": ["Shuai Yang", "Qi Yang", "Luoxi Tang", "Jeremy Blackburn", "Zhaohan Xi"], "title": "On the Eligibility of LLMs for Counterfactual Reasoning: A Decompositional Study", "categories": ["cs.AI"], "comment": null, "summary": "Counterfactual reasoning has emerged as a crucial technique for generalizing\nthe reasoning capabilities of large language models (LLMs). By generating and\nanalyzing counterfactual scenarios, researchers can assess the adaptability and\nreliability of model decision-making. Although prior work has shown that LLMs\noften struggle with counterfactual reasoning, it remains unclear which factors\nmost significantly impede their performance across different tasks and\nmodalities. In this paper, we propose a decompositional strategy that breaks\ndown the counterfactual generation from causality construction to the reasoning\nover counterfactual interventions. To support decompositional analysis, we\ninvestigate 11 datasets spanning diverse tasks, including natural language\nunderstanding, mathematics, programming, and vision-language tasks. Through\nextensive evaluations, we characterize LLM behavior across each decompositional\nstage and identify how modality type and intermediate reasoning influence\nperformance. By establishing a structured framework for analyzing\ncounterfactual reasoning, this work contributes to the development of more\nreliable LLM-based reasoning systems and informs future elicitation strategies."}
{"id": "2505.11830", "pdf": "https://arxiv.org/pdf/2505.11830", "abs": "https://arxiv.org/abs/2505.11830", "authors": ["Hongbo Jin", "Ruyang Liu", "Wenhao Zhang", "Guibo Luo", "Ge Li"], "title": "CoT-Vid: Dynamic Chain-of-Thought Routing with Self Verification for Training-Free Video Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": "19 pages, 7 figures", "summary": "System2 reasoning is developing rapidly these days with the emergence of\nDeep- Thinking Models and chain-of-thought technology, which has become a\ncentralized discussion point in the AI community. However, there is a relative\ngap in the research on complex video reasoning at present. In this work, we\npropose CoT-Vid, a novel training-free paradigm for the video domain with a\nmultistage complex reasoning design. Distinguishing from existing video LLMs,\nwhich rely heavily on perceptual abilities, it achieved surprising performance\ngain with explicit reasoning mechanism. The paradigm consists of three main\ncomponents: dynamic inference path routing, problem decoupling strategy, and\nvideo self-consistency verification. In addition, we propose a new standard for\ncategorization of video questions. CoT- Vid showed outstanding results on a\nwide range of benchmarks, and outperforms its base model by 9.3% on Egochema\nand 5.6% on VideoEspresso, rivalling or even surpassing larger and proprietary\nmodels, such as GPT-4V, GPT-4o and Gemini-1.5-flash. Our codebase will be\npublicly available soon."}
{"id": "2505.11810", "pdf": "https://arxiv.org/pdf/2505.11810", "abs": "https://arxiv.org/abs/2505.11810", "authors": ["Shen Li", "Renfen Hu", "Lijun Wang"], "title": "Efficiently Building a Domain-Specific Large Language Model from Scratch: A Case Study of a Classical Chinese Large Language Model", "categories": ["cs.CL"], "comment": null, "summary": "General-purpose large language models demonstrate notable capabilities in\nlanguage comprehension and generation, achieving results that are comparable\nto, or even surpass, human performance in many language information processing\ntasks. Nevertheless, when general models are applied to some specific domains,\ne.g., Classical Chinese texts, their effectiveness is often unsatisfactory, and\nfine-tuning open-source foundational models similarly struggles to adequately\nincorporate domain-specific knowledge. To address this challenge, this study\ndeveloped a large language model, AI Taiyan, specifically designed for\nunderstanding and generating Classical Chinese. Experiments show that with a\nreasonable model design, data processing, foundational training, and\nfine-tuning, satisfactory results can be achieved with only 1.8 billion\nparameters. In key tasks related to Classical Chinese information processing\nsuch as punctuation, identification of allusions, explanation of word meanings,\nand translation between ancient and modern Chinese, this model exhibits a clear\nadvantage over both general-purpose large models and domain-specific\ntraditional models, achieving levels close to or surpassing human baselines.\nThis research provides a reference for the efficient construction of\nspecialized domain-specific large language models. Furthermore, the paper\ndiscusses the application of this model in fields such as the collation of\nancient texts, dictionary editing, and language research, combined with case\nstudies."}
{"id": "2505.11849", "pdf": "https://arxiv.org/pdf/2505.11849", "abs": "https://arxiv.org/abs/2505.11849", "authors": ["Yiting Wang", "Guoheng Sun", "Wanghao Ye", "Gang Qu", "Ang Li"], "title": "VeriReason: Reinforcement Learning with Testbench Feedback for Reasoning-Enhanced Verilog Generation", "categories": ["cs.AI", "cs.AR", "cs.LG", "cs.PL"], "comment": "11 pages, 2 figures", "summary": "Automating Register Transfer Level (RTL) code generation using Large Language\nModels (LLMs) offers substantial promise for streamlining digital circuit\ndesign and reducing human effort. However, current LLM-based approaches face\nsignificant challenges with training data scarcity, poor specification-code\nalignment, lack of verification mechanisms, and balancing generalization with\nspecialization. Inspired by DeepSeek-R1, we introduce VeriReason, a framework\nintegrating supervised fine-tuning with Guided Reward Proximal Optimization\n(GRPO) reinforcement learning for RTL generation. Using curated training\nexamples and a feedback-driven reward model, VeriReason combines testbench\nevaluations with structural heuristics while embedding self-checking\ncapabilities for autonomous error correction. On the VerilogEval Benchmark,\nVeriReason delivers significant improvements: achieving 83.1% functional\ncorrectness on the VerilogEval Machine benchmark, substantially outperforming\nboth comparable-sized models and much larger commercial systems like GPT-4\nTurbo. Additionally, our approach demonstrates up to a 2.8X increase in\nfirst-attempt functional correctness compared to baseline methods and exhibits\nrobust generalization to unseen designs. To our knowledge, VeriReason\nrepresents the first system to successfully integrate explicit reasoning\ncapabilities with reinforcement learning for Verilog generation, establishing a\nnew state-of-the-art for automated RTL synthesis. The models and datasets are\navailable at: https://huggingface.co/collections/AI4EDA-CASE Code is Available\nat: https://github.com/NellyW8/VeriReason"}
{"id": "2505.11838", "pdf": "https://arxiv.org/pdf/2505.11838", "abs": "https://arxiv.org/abs/2505.11838", "authors": ["Yiqing Shen", "Chenjia Li", "Chenxiao Fan", "Mathias Unberath"], "title": "RVTBench: A Benchmark for Visual Reasoning Tasks", "categories": ["cs.CV"], "comment": null, "summary": "Visual reasoning, the capability to interpret visual input in response to\nimplicit text query through multi-step reasoning, remains a challenge for deep\nlearning models due to the lack of relevant benchmarks. Previous work in visual\nreasoning has primarily focused on reasoning segmentation, where models aim to\nsegment objects based on implicit text queries. This paper introduces reasoning\nvisual tasks (RVTs), a unified formulation that extends beyond traditional\nvideo reasoning segmentation to a diverse family of visual language reasoning\nproblems, which can therefore accommodate multiple output formats including\nbounding boxes, natural language descriptions, and question-answer pairs.\nCorrespondingly, we identify the limitations in current benchmark construction\nmethods that rely solely on large language models (LLMs), which inadequately\ncapture complex spatial-temporal relationships and multi-step reasoning chains\nin video due to their reliance on token representation, resulting in benchmarks\nwith artificially limited reasoning complexity. To address this limitation, we\npropose a novel automated RVT benchmark construction pipeline that leverages\ndigital twin (DT) representations as structured intermediaries between\nperception and the generation of implicit text queries. Based on this method,\nwe construct RVTBench, a RVT benchmark containing 3,896 queries of over 1.2\nmillion tokens across four types of RVT (segmentation, grounding, VQA and\nsummary), three reasoning categories (semantic, spatial, and temporal), and\nfour increasing difficulty levels, derived from 200 video sequences. Finally,\nwe propose RVTagent, an agent framework for RVT that allows for zero-shot\ngeneralization across various types of RVT without task-specific fine-tuning."}
{"id": "2505.11811", "pdf": "https://arxiv.org/pdf/2505.11811", "abs": "https://arxiv.org/abs/2505.11811", "authors": ["Taolin Zhang", "Dongyang Li", "Qizhou Chen", "Chengyu Wang", "Xiaofeng He"], "title": "BELLE: A Bi-Level Multi-Agent Reasoning Framework for Multi-Hop Question Answering", "categories": ["cs.CL"], "comment": "Accepted by ACL2025 main track", "summary": "Multi-hop question answering (QA) involves finding multiple relevant passages\nand performing step-by-step reasoning to answer complex questions. Previous\nworks on multi-hop QA employ specific methods from different modeling\nperspectives based on large language models (LLMs), regardless of the question\ntypes. In this paper, we first conduct an in-depth analysis of public multi-hop\nQA benchmarks, dividing the questions into four types and evaluating five types\nof cutting-edge methods for multi-hop QA: Chain-of-Thought (CoT), Single-step,\nIterative-step, Sub-step, and Adaptive-step. We find that different types of\nmulti-hop questions have varying degrees of sensitivity to different types of\nmethods. Thus, we propose a Bi-levEL muLti-agEnt reasoning (BELLE) framework to\naddress multi-hop QA by specifically focusing on the correspondence between\nquestion types and methods, where each type of method is regarded as an\n''operator'' by prompting LLMs differently. The first level of BELLE includes\nmultiple agents that debate to obtain an executive plan of combined\n''operators'' to address the multi-hop QA task comprehensively. During the\ndebate, in addition to the basic roles of affirmative debater, negative\ndebater, and judge, at the second level, we further leverage fast and slow\ndebaters to monitor whether changes in viewpoints are reasonable. Extensive\nexperiments demonstrate that BELLE significantly outperforms strong baselines\nin various datasets. Additionally, the model consumption of BELLE is higher\ncost-effectiveness than that of single models in more complex multi-hop QA\nscenarios."}
{"id": "2505.11854", "pdf": "https://arxiv.org/pdf/2505.11854", "abs": "https://arxiv.org/abs/2505.11854", "authors": ["Hanmeng Liu", "Yiran Ding", "Zhizhang Fu", "Chaoli Zhang", "Xiaozhang Liu", "Yue Zhang"], "title": "Evaluating the Logical Reasoning Abilities of Large Reasoning Models", "categories": ["cs.AI"], "comment": null, "summary": "Large reasoning models, often post-trained on long chain-of-thought (long\nCoT) data with reinforcement learning, achieve state-of-the-art performance on\nmathematical, coding, and domain-specific reasoning benchmarks. However, their\nlogical reasoning capabilities - fundamental to human cognition and independent\nof domain knowledge - remain understudied. To address this gap, we introduce\nLogiEval, a holistic benchmark for evaluating logical reasoning in large\nreasoning models. LogiEval spans diverse reasoning types (deductive, inductive,\nanalogical, and abductive) and task formats (e.g., logical sequence, argument\nanalysis), sourced from high-quality human examinations (e.g., LSAT, GMAT). Our\nexperiments demonstrate that modern reasoning models excel at 4-choice argument\nanalysis problems and analogical reasoning, surpassing human performance, yet\nexhibit uneven capabilities across reasoning types and formats, highlighting\nlimitations in their generalization. Our analysis reveals that human\nperformance does not mirror model failure distributions. To foster further\nresearch, we curate LogiEval-Hard, a challenging subset identified through a\nnovel screening paradigm where small-model failures (Qwen3-30B-A3B) reliably\npredict difficulties for larger models. Modern models show striking, consistent\nfailures on LogiEval-Hard. This demonstrates that fundamental reasoning\nbottlenecks persist across model scales, and establishes LogiEval-Hard as both\na diagnostic tool and a rigorous testbed for advancing logical reasoning in\nLLMs."}
{"id": "2505.11842", "pdf": "https://arxiv.org/pdf/2505.11842", "abs": "https://arxiv.org/abs/2505.11842", "authors": ["Xuannan Liu", "Zekun Li", "Zheqi He", "Peipei Li", "Shuhan Xia", "Xing Cui", "Huaibo Huang", "Xi Yang", "Ran He"], "title": "Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs", "categories": ["cs.CV", "cs.CL"], "comment": "Project page:\n  https://liuxuannan.github.io/Video-SafetyBench.github.io/", "summary": "The increasing deployment of Large Vision-Language Models (LVLMs) raises\nsafety concerns under potential malicious inputs. However, existing multimodal\nsafety evaluations primarily focus on model vulnerabilities exposed by static\nimage inputs, ignoring the temporal dynamics of video that may induce distinct\nsafety risks. To bridge this gap, we introduce Video-SafetyBench, the first\ncomprehensive benchmark designed to evaluate the safety of LVLMs under\nvideo-text attacks. It comprises 2,264 video-text pairs spanning 48\nfine-grained unsafe categories, each pairing a synthesized video with either a\nharmful query, which contains explicit malice, or a benign query, which appears\nharmless but triggers harmful behavior when interpreted alongside the video. To\ngenerate semantically accurate videos for safety evaluation, we design a\ncontrollable pipeline that decomposes video semantics into subject images (what\nis shown) and motion text (how it moves), which jointly guide the synthesis of\nquery-relevant videos. To effectively evaluate uncertain or borderline harmful\noutputs, we propose RJScore, a novel LLM-based metric that incorporates the\nconfidence of judge models and human-aligned decision threshold calibration.\nExtensive experiments show that benign-query video composition achieves average\nattack success rates of 67.2%, revealing consistent vulnerabilities to\nvideo-induced attacks. We believe Video-SafetyBench will catalyze future\nresearch into video-based safety evaluation and defense strategies."}
{"id": "2505.11820", "pdf": "https://arxiv.org/pdf/2505.11820", "abs": "https://arxiv.org/abs/2505.11820", "authors": ["Kaitao Song", "Xiaohua Wang", "Xu Tan", "Huiqiang Jiang", "Chengruidong Zhang", "Yongliang Shen", "Cen LU", "Zihao Li", "Zifan Song", "Caihua Shan", "Yansen Wang", "Kan Ren", "Xiaoqing Zheng", "Tao Qin", "Yuqing Yang", "Dongsheng Li", "Lili Qiu"], "title": "Chain-of-Model Learning for Language Model", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we propose a novel learning paradigm, termed Chain-of-Model\n(CoM), which incorporates the causal relationship into the hidden states of\neach layer as a chain style, thereby introducing great scaling efficiency in\nmodel training and inference flexibility in deployment. We introduce the\nconcept of Chain-of-Representation (CoR), which formulates the hidden states at\neach layer as a combination of multiple sub-representations (i.e., chains) at\nthe hidden dimension level. In each layer, each chain from the output\nrepresentations can only view all of its preceding chains in the input\nrepresentations. Consequently, the model built upon CoM framework can\nprogressively scale up the model size by increasing the chains based on the\nprevious models (i.e., chains), and offer multiple sub-models at varying sizes\nfor elastic inference by using different chain numbers. Based on this\nprinciple, we devise Chain-of-Language-Model (CoLM), which incorporates the\nidea of CoM into each layer of Transformer architecture. Based on CoLM, we\nfurther introduce CoLM-Air by introducing a KV sharing mechanism, that computes\nall keys and values within the first chain and then shares across all chains.\nThis design demonstrates additional extensibility, such as enabling seamless LM\nswitching, prefilling acceleration and so on. Experimental results demonstrate\nour CoLM family can achieve comparable performance to the standard Transformer,\nwhile simultaneously enabling greater flexiblity, such as progressive scaling\nto improve training efficiency and offer multiple varying model sizes for\nelastic inference, paving a a new way toward building language models. Our code\nwill be released in the future at: https://github.com/microsoft/CoLM."}
{"id": "2505.11861", "pdf": "https://arxiv.org/pdf/2505.11861", "abs": "https://arxiv.org/abs/2505.11861", "authors": ["Qi Zhou", "Jie Zhang", "Dongxia Wang", "Qiang Liu", "Tianlin Li", "Jin Song Dong", "Wenhai Wang", "Qing Guo"], "title": "Fair-PP: A Synthetic Dataset for Aligning LLM with Personalized Preferences of Social Equity", "categories": ["cs.AI", "cs.CL", "91C99", "I.2.7; J.4"], "comment": "under review", "summary": "Human preference plays a crucial role in the refinement of large language\nmodels (LLMs). However, collecting human preference feedback is costly and most\nexisting datasets neglect the correlation between personalization and\npreferences. To address this issue, we introduce Fair-PP, a synthetic dataset\nof personalized preferences targeting social equity, derived from real-world\nsocial survey data, which includes 28 social groups, 98 equity topics, and 5\npersonal preference dimensions. Leveraging GPT-4o-mini, we engage in\nrole-playing based on seven representative persona portrayals guided by\nexisting social survey data, yielding a total of 238,623 preference records.\nThrough Fair-PP, we also contribute (i) An automated framework for generating\npreference data, along with a more fine-grained dataset of personalized\npreferences; (ii) analysis of the positioning of the existing mainstream LLMs\nacross five major global regions within the personalized preference space; and\n(iii) a sample reweighting method for personalized preference alignment,\nenabling alignment with a target persona while maximizing the divergence from\nother personas. Empirical experiments show our method outperforms the\nbaselines."}
{"id": "2505.11845", "pdf": "https://arxiv.org/pdf/2505.11845", "abs": "https://arxiv.org/abs/2505.11845", "authors": ["Tasrifur Riahi", "Md. Azizul Hakim Bappy", "Md. Mehedi Islam"], "title": "ElderFallGuard: Real-Time IoT and Computer Vision-Based Fall Detection System for Elderly Safety", "categories": ["cs.CV"], "comment": "9 page, 1 table, 5 figure", "summary": "For the elderly population, falls pose a serious and increasing risk of\nserious injury and loss of independence. In order to overcome this difficulty,\nwe present ElderFallGuard: A Computer Vision Based IoT Solution for Elderly\nFall Detection and Notification, a cutting-edge, non-invasive system intended\nfor quick caregiver alerts and real-time fall detection. Our approach leverages\nthe power of computer vision, utilizing MediaPipe for accurate human pose\nestimation from standard video streams. We developed a custom dataset\ncomprising 7200 samples across 12 distinct human poses to train and evaluate\nvarious machine learning classifiers, with Random Forest ultimately selected\nfor its superior performance. ElderFallGuard employs a specific detection\nlogic, identifying a fall when a designated prone pose (\"Pose6\") is held for\nover 3 seconds coupled with a significant drop in motion detected for more than\n2 seconds. Upon confirmation, the system instantly dispatches an alert,\nincluding a snapshot of the event, to a designated Telegram group via a custom\nbot, incorporating cooldown logic to prevent notification overload. Rigorous\ntesting on our dataset demonstrated exceptional results, achieving 100%\naccuracy, precision, recall, and F1-score. ElderFallGuard offers a promising,\nvision-based IoT solution to enhance elderly safety and provide peace of mind\nfor caregivers through intelligent, timely alerts."}
{"id": "2505.11827", "pdf": "https://arxiv.org/pdf/2505.11827", "abs": "https://arxiv.org/abs/2505.11827", "authors": ["Yansong Ning", "Wei Li", "Jun Fang", "Naiqiang Tan", "Hao Liu"], "title": "Not All Thoughts are Generated Equal: Efficient LLM Reasoning via Multi-Turn Reinforcement Learning", "categories": ["cs.CL", "cs.AI"], "comment": "In progress", "summary": "Compressing long chain-of-thought (CoT) from large language models (LLMs) is\nan emerging strategy to improve the reasoning efficiency of LLMs. Despite its\npromising benefits, existing studies equally compress all thoughts within a\nlong CoT, hindering more concise and effective reasoning. To this end, we first\ninvestigate the importance of different thoughts by examining their\neffectiveness and efficiency in contributing to reasoning through automatic\nlong CoT chunking and Monte Carlo rollouts. Building upon the insights, we\npropose a theoretically bounded metric to jointly measure the effectiveness and\nefficiency of different thoughts. We then propose Long$\\otimes$Short, an\nefficient reasoning framework that enables two LLMs to collaboratively solve\nthe problem: a long-thought LLM for more effectively generating important\nthoughts, while a short-thought LLM for efficiently generating remaining\nthoughts. Specifically, we begin by synthesizing a small amount of cold-start\ndata to fine-tune LLMs for long-thought and short-thought reasoning styles,\nrespectively. Furthermore, we propose a synergizing-oriented multi-turn\nreinforcement learning, focusing on the model self-evolution and collaboration\nbetween long-thought and short-thought LLMs. Experimental results show that our\nmethod enables Qwen2.5-7B and Llama3.1-8B to achieve comparable performance\ncompared to DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B, while\nreducing token length by over 80% across the MATH500, AIME24/25, AMC23, and\nGPQA Diamond benchmarks. Our data and code are available at\nhttps://github.com/yasNing/Long-otimes-Short/."}
{"id": "2505.11866", "pdf": "https://arxiv.org/pdf/2505.11866", "abs": "https://arxiv.org/abs/2505.11866", "authors": ["Ali A. Minai"], "title": "Position Paper: Bounded Alignment: What (Not) To Expect From AGI Agents", "categories": ["cs.AI", "I.2.0; I.2.6"], "comment": "Paper accepted for the 2025 IEEE/INNS International Joint Conference\n  on Neural Networks, Rome, Italy, June 30 - July 5, 2025", "summary": "The issues of AI risk and AI safety are becoming critical as the prospect of\nartificial general intelligence (AGI) looms larger. The emergence of extremely\nlarge and capable generative models has led to alarming predictions and created\na stir from boardrooms to legislatures. As a result, AI alignment has emerged\nas one of the most important areas in AI research. The goal of this position\npaper is to argue that the currently dominant vision of AGI in the AI and\nmachine learning (AI/ML) community needs to evolve, and that expectations and\nmetrics for its safety must be informed much more by our understanding of the\nonly existing instance of general intelligence, i.e., the intelligence found in\nanimals, and especially in humans. This change in perspective will lead to a\nmore realistic view of the technology, and allow for better policy decisions."}
{"id": "2505.11852", "pdf": "https://arxiv.org/pdf/2505.11852", "abs": "https://arxiv.org/abs/2505.11852", "authors": ["Jingkun Yue", "Siqi Zhang", "Zinan Jia", "Huihuan Xu", "Zongbo Han", "Xiaohong Liu", "Guangyu Wang"], "title": "MedSG-Bench: A Benchmark for Medical Image Sequences Grounding", "categories": ["cs.CV"], "comment": null, "summary": "Visual grounding is essential for precise perception and reasoning in\nmultimodal large language models (MLLMs), especially in medical imaging\ndomains. While existing medical visual grounding benchmarks primarily focus on\nsingle-image scenarios, real-world clinical applications often involve\nsequential images, where accurate lesion localization across different\nmodalities and temporal tracking of disease progression (e.g., pre- vs.\npost-treatment comparison) require fine-grained cross-image semantic alignment\nand context-aware reasoning. To remedy the underrepresentation of image\nsequences in existing medical visual grounding benchmarks, we propose\nMedSG-Bench, the first benchmark tailored for Medical Image Sequences\nGrounding. It comprises eight VQA-style tasks, formulated into two paradigms of\nthe grounding tasks, including 1) Image Difference Grounding, which focuses on\ndetecting change regions across images, and 2) Image Consistency Grounding,\nwhich emphasizes detection of consistent or shared semantics across sequential\nimages. MedSG-Bench covers 76 public datasets, 10 medical imaging modalities,\nand a wide spectrum of anatomical structures and diseases, totaling 9,630\nquestion-answer pairs. We benchmark both general-purpose MLLMs (e.g.,\nQwen2.5-VL) and medical-domain specialized MLLMs (e.g., HuatuoGPT-vision),\nobserving that even the advanced models exhibit substantial limitations in\nmedical sequential grounding tasks. To advance this field, we construct\nMedSG-188K, a large-scale instruction-tuning dataset tailored for sequential\nvisual grounding, and further develop MedSeq-Grounder, an MLLM designed to\nfacilitate future research on fine-grained understanding across medical\nsequential images. The benchmark, dataset, and model are available at\nhttps://huggingface.co/MedSG-Bench"}
{"id": "2505.11829", "pdf": "https://arxiv.org/pdf/2505.11829", "abs": "https://arxiv.org/abs/2505.11829", "authors": ["Chenlu Wang", "Weimin Lyu", "Ritwik Banerjee"], "title": "Class Distillation with Mahalanobis Contrast: An Efficient Training Paradigm for Pragmatic Language Understanding Tasks", "categories": ["cs.CL"], "comment": null, "summary": "Detecting deviant language such as sexism, or nuanced language such as\nmetaphors or sarcasm, is crucial for enhancing the safety, clarity, and\ninterpretation of online social discourse. While existing classifiers deliver\nstrong results on these tasks, they often come with significant computational\ncost and high data demands. In this work, we propose \\textbf{Cla}ss\n\\textbf{D}istillation (ClaD), a novel training paradigm that targets the core\nchallenge: distilling a small, well-defined target class from a highly diverse\nand heterogeneous background. ClaD integrates two key innovations: (i) a loss\nfunction informed by the structural properties of class distributions, based on\nMahalanobis distance, and (ii) an interpretable decision algorithm optimized\nfor class separation. Across three benchmark detection tasks -- sexism,\nmetaphor, and sarcasm -- ClaD outperforms competitive baselines, and even with\nsmaller language models and orders of magnitude fewer parameters, achieves\nperformance comparable to several large language models (LLMs). These results\ndemonstrate ClaD as an efficient tool for pragmatic language understanding\ntasks that require gleaning a small target class from a larger heterogeneous\nbackground."}
{"id": "2505.11899", "pdf": "https://arxiv.org/pdf/2505.11899", "abs": "https://arxiv.org/abs/2505.11899", "authors": ["Yongan Yu", "Alexandre Krantz", "Nikki G. Lobczowski"], "title": "From Recall to Reasoning: Automated Question Generation for Deeper Math Learning through Large Language Models", "categories": ["cs.AI"], "comment": "8 pages, 2 figures, accepted by AIED conference", "summary": "Educators have started to turn to Generative AI (GenAI) to help create new\ncourse content, but little is known about how they should do so. In this\nproject, we investigated the first steps for optimizing content creation for\nadvanced math. In particular, we looked at the ability of GenAI to produce\nhigh-quality practice problems that are relevant to the course content. We\nconducted two studies to: (1) explore the capabilities of current versions of\npublicly available GenAI and (2) develop an improved framework to address the\nlimitations we found. Our results showed that GenAI can create math problems at\nvarious levels of quality with minimal support, but that providing examples and\nrelevant content results in better quality outputs. This research can help\neducators decide the ideal way to adopt GenAI in their workflows, to create\nmore effective educational experiences for students."}
{"id": "2505.11868", "pdf": "https://arxiv.org/pdf/2505.11868", "abs": "https://arxiv.org/abs/2505.11868", "authors": ["Hongyi Zhou", "Xiaogang Wang", "Yulan Guo", "Kai Xu"], "title": "MonoMobility: Zero-Shot 3D Mobility Analysis from Monocular Videos", "categories": ["cs.CV"], "comment": null, "summary": "Accurately analyzing the motion parts and their motion attributes in dynamic\nenvironments is crucial for advancing key areas such as embodied intelligence.\nAddressing the limitations of existing methods that rely on dense multi-view\nimages or detailed part-level annotations, we propose an innovative framework\nthat can analyze 3D mobility from monocular videos in a zero-shot manner. This\nframework can precisely parse motion parts and motion attributes only using a\nmonocular video, completely eliminating the need for annotated training data.\nSpecifically, our method first constructs the scene geometry and roughly\nanalyzes the motion parts and their initial motion attributes combining depth\nestimation, optical flow analysis and point cloud registration method, then\nemploys 2D Gaussian splatting for scene representation. Building on this, we\nintroduce an end-to-end dynamic scene optimization algorithm specifically\ndesigned for articulated objects, refining the initial analysis results to\nensure the system can handle 'rotation', 'translation', and even complex\nmovements ('rotation+translation'), demonstrating high flexibility and\nversatility. To validate the robustness and wide applicability of our method,\nwe created a comprehensive dataset comprising both simulated and real-world\nscenarios. Experimental results show that our framework can effectively analyze\narticulated object motions in an annotation-free manner, showcasing its\nsignificant potential in future embodied intelligence applications."}
{"id": "2505.11835", "pdf": "https://arxiv.org/pdf/2505.11835", "abs": "https://arxiv.org/abs/2505.11835", "authors": ["Hongliang Li", "Jinan Xu", "Gengping Cui", "Changhao Guan", "Fengran Mo", "Kaiyu Huang"], "title": "Multilingual Collaborative Defense for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "19 pages, 4figures", "summary": "The robustness and security of large language models (LLMs) has become a\nprominent research area. One notable vulnerability is the ability to bypass LLM\nsafeguards by translating harmful queries into rare or underrepresented\nlanguages, a simple yet effective method of \"jailbreaking\" these models.\nDespite the growing concern, there has been limited research addressing the\nsafeguarding of LLMs in multilingual scenarios, highlighting an urgent need to\nenhance multilingual safety. In this work, we investigate the correlation\nbetween various attack features across different languages and propose\nMultilingual Collaborative Defense (MCD), a novel learning method that\noptimizes a continuous, soft safety prompt automatically to facilitate\nmultilingual safeguarding of LLMs. The MCD approach offers three advantages:\nFirst, it effectively improves safeguarding performance across multiple\nlanguages. Second, MCD maintains strong generalization capabilities while\nminimizing false refusal rates. Third, MCD mitigates the language safety\nmisalignment caused by imbalances in LLM training corpora. To evaluate the\neffectiveness of MCD, we manually construct multilingual versions of commonly\nused jailbreak benchmarks, such as MaliciousInstruct and AdvBench, to assess\nvarious safeguarding methods. Additionally, we introduce these datasets in\nunderrepresented (zero-shot) languages to verify the language transferability\nof MCD. The results demonstrate that MCD outperforms existing approaches in\nsafeguarding against multilingual jailbreak attempts while also exhibiting\nstrong language transfer capabilities. Our code is available at\nhttps://github.com/HLiang-Lee/MCD."}
{"id": "2505.11942", "pdf": "https://arxiv.org/pdf/2505.11942", "abs": "https://arxiv.org/abs/2505.11942", "authors": ["Junhao Zheng", "Xidi Cai", "Qiuke Li", "Duzhen Zhang", "ZhongZhi Li", "Yingying Zhang", "Le Song", "Qianli Ma"], "title": "LifelongAgentBench: Evaluating LLM Agents as Lifelong Learners", "categories": ["cs.AI"], "comment": null, "summary": "Lifelong learning is essential for intelligent agents operating in dynamic\nenvironments. Current large language model (LLM)-based agents, however, remain\nstateless and unable to accumulate or transfer knowledge over time. Existing\nbenchmarks treat agents as static systems and fail to evaluate lifelong\nlearning capabilities. We present LifelongAgentBench, the first unified\nbenchmark designed to systematically assess the lifelong learning ability of\nLLM agents. It provides skill-grounded, interdependent tasks across three\ninteractive environments, Database, Operating System, and Knowledge Graph, with\nautomatic label verification, reproducibility, and modular extensibility.\nExtensive experiments reveal that conventional experience replay has limited\neffectiveness for LLM agents due to irrelevant information and context length\nconstraints. We further introduce a group self-consistency mechanism that\nsignificantly improves lifelong learning performance. We hope\nLifelongAgentBench will advance the development of adaptive, memory-capable LLM\nagents."}
{"id": "2505.11872", "pdf": "https://arxiv.org/pdf/2505.11872", "abs": "https://arxiv.org/abs/2505.11872", "authors": ["Quoc-Huy Trinh", "Minh-Van Nguyen", "Jung Peng", "Ulas Bagci", "Debesh Jha"], "title": "PRS-Med: Position Reasoning Segmentation with Vision-Language Model in Medical Imaging", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in prompt-based medical image segmentation have enabled\nclinicians to identify tumors using simple input like bounding boxes or text\nprompts. However, existing methods face challenges when doctors need to\ninteract through natural language or when position reasoning is required -\nunderstanding spatial relationships between anatomical structures and\npathologies. We present PRS-Med, a framework that integrates vision-language\nmodels with segmentation capabilities to generate both accurate segmentation\nmasks and corresponding spatial reasoning outputs. Additionally, we introduce\nthe MMRS dataset (Multimodal Medical in Positional Reasoning Segmentation),\nwhich provides diverse, spatially-grounded question-answer pairs to address the\nlack of position reasoning data in medical imaging. PRS-Med demonstrates\nsuperior performance across six imaging modalities (CT, MRI, X-ray, ultrasound,\nendoscopy, RGB), significantly outperforming state-of-the-art methods in both\nsegmentation accuracy and position reasoning. Our approach enables intuitive\ndoctor-system interaction through natural language, facilitating more efficient\ndiagnoses. Our dataset pipeline, model, and codebase will be released to foster\nfurther research in spatially-aware multimodal reasoning for medical\napplications."}
{"id": "2505.11855", "pdf": "https://arxiv.org/pdf/2505.11855", "abs": "https://arxiv.org/abs/2505.11855", "authors": ["Guijin Son", "Jiwoo Hong", "Honglu Fan", "Heejeong Nam", "Hyunwoo Ko", "Seungwon Lim", "Jinyeop Song", "Jinha Choi", "Gonçalo Paulo", "Youngjae Yu", "Stella Biderman"], "title": "When AI Co-Scientists Fail: SPOT-a Benchmark for Automated Verification of Scientific Research", "categories": ["cs.CL"], "comment": "work in progress", "summary": "Recent advances in large language models (LLMs) have fueled the vision of\nautomated scientific discovery, often called AI Co-Scientists. To date, prior\nwork casts these systems as generative co-authors responsible for crafting\nhypotheses, synthesizing code, or drafting manuscripts. In this work, we\nexplore a complementary application: using LLMs as verifiers to automate the\n\\textbf{academic verification of scientific manuscripts}. To that end, we\nintroduce SPOT, a dataset of 83 published papers paired with 91 errors\nsignificant enough to prompt errata or retraction, cross-validated with actual\nauthors and human annotators. Evaluating state-of-the-art LLMs on SPOT, we find\nthat none surpasses 21.1\\% recall or 6.1\\% precision (o3 achieves the best\nscores, with all others near zero). Furthermore, confidence estimates are\nuniformly low, and across eight independent runs, models rarely rediscover the\nsame errors, undermining their reliability. Finally, qualitative analysis with\ndomain experts reveals that even the strongest models make mistakes resembling\nstudent-level misconceptions derived from misunderstandings. These findings\nhighlight the substantial gap between current LLM capabilities and the\nrequirements for dependable AI-assisted academic verification."}
{"id": "2505.11962", "pdf": "https://arxiv.org/pdf/2505.11962", "abs": "https://arxiv.org/abs/2505.11962", "authors": ["Zoya Volovikova", "Gregory Gorbov", "Petr Kuderov", "Aleksandr I. Panov", "Alexey Skrynnik"], "title": "CrafText Benchmark: Advancing Instruction Following in Complex Multimodal Open-Ended World", "categories": ["cs.AI"], "comment": null, "summary": "Following instructions in real-world conditions requires the ability to adapt\nto the world's volatility and entanglement: the environment is dynamic and\nunpredictable, instructions can be linguistically complex with diverse\nvocabulary, and the number of possible goals an agent may encounter is vast.\nDespite extensive research in this area, most studies are conducted in static\nenvironments with simple instructions and a limited vocabulary, making it\ndifficult to assess agent performance in more diverse and challenging settings.\nTo address this gap, we introduce CrafText, a benchmark for evaluating\ninstruction following in a multimodal environment with diverse instructions and\ndynamic interactions. CrafText includes 3,924 instructions with 3,423 unique\nwords, covering Localization, Conditional, Building, and Achievement tasks.\nAdditionally, we propose an evaluation protocol that measures an agent's\nability to generalize to novel instruction formulations and dynamically\nevolving task configurations, providing a rigorous test of both linguistic\nunderstanding and adaptive decision-making."}
{"id": "2505.11881", "pdf": "https://arxiv.org/pdf/2505.11881", "abs": "https://arxiv.org/abs/2505.11881", "authors": ["Giyeong Oh", "Woohyun Cho", "Siyeol Kim", "Suhwan Choi", "Younjae Yu"], "title": "Revisiting Residual Connections: Orthogonal Updates for Stable and Efficient Deep Networks", "categories": ["cs.CV", "cs.AI"], "comment": "27 pages, WIP", "summary": "Residual connections are pivotal for deep neural networks, enabling greater\ndepth by mitigating vanishing gradients. However, in standard residual updates,\nthe module's output is directly added to the input stream. This can lead to\nupdates that predominantly reinforce or modulate the existing stream direction,\npotentially underutilizing the module's capacity for learning entirely novel\nfeatures. In this work, we introduce Orthogonal Residual Update: we decompose\nthe module's output relative to the input stream and add only the component\northogonal to this stream. This design aims to guide modules to contribute\nprimarily new representational directions, fostering richer feature learning\nwhile promoting more efficient training. We demonstrate that our orthogonal\nupdate strategy improves generalization accuracy and training stability across\ndiverse architectures (ResNetV2, Vision Transformers) and datasets (CIFARs,\nTinyImageNet, ImageNet-1k), achieving, for instance, a +4.3\\%p top-1 accuracy\ngain for ViT-B on ImageNet-1k."}
{"id": "2505.11876", "pdf": "https://arxiv.org/pdf/2505.11876", "abs": "https://arxiv.org/abs/2505.11876", "authors": ["Yanbo Dai", "Zhenlan Ji", "Zongjie Li", "Shuai Wang"], "title": "NAMET: Robust Massive Model Editing via Noise-Aware Memory Optimization", "categories": ["cs.CL"], "comment": null, "summary": "Model editing techniques are essential for efficiently updating knowledge in\nlarge language models (LLMs). However, the effectiveness of existing approaches\ndegrades in massive editing scenarios, particularly when evaluated with\npractical metrics or in context-rich settings. We attribute these failures to\nembedding collisions among knowledge items, which undermine editing reliability\nat scale. To address this, we propose NAMET (Noise-aware Model Editing in\nTransformers), a simple yet effective method that introduces noise during\nmemory extraction via a one-line modification to MEMIT. Extensive experiments\nacross six LLMs and three datasets demonstrate that NAMET consistently\noutperforms existing methods when editing thousands of facts."}
{"id": "2505.11966", "pdf": "https://arxiv.org/pdf/2505.11966", "abs": "https://arxiv.org/abs/2505.11966", "authors": ["Jianyuan Zhong", "Zeju Li", "Zhijian Xu", "Xiangyu Wen", "Kezhi Li", "Qiang Xu"], "title": "Solve-Detect-Verify: Inference-Time Scaling with Flexible Generative Verifier", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Model (LLM) reasoning for complex tasks inherently involves a\ntrade-off between solution accuracy and computational efficiency. The\nsubsequent step of verification, while intended to improve performance, further\ncomplicates this landscape by introducing its own challenging trade-off:\nsophisticated Generative Reward Models (GenRMs) can be computationally\nprohibitive if naively integrated with LLMs at test-time, while simpler, faster\nmethods may lack reliability. To overcome these challenges, we introduce\nFlexiVe, a novel generative verifier that flexibly balances computational\nresources between rapid, reliable fast thinking and meticulous slow thinking\nusing a Flexible Allocation of Verification Budget strategy. We further propose\nthe Solve-Detect-Verify pipeline, an efficient inference-time scaling framework\nthat intelligently integrates FlexiVe, proactively identifying solution\ncompletion points to trigger targeted verification and provide focused solver\nfeedback. Experiments show FlexiVe achieves superior accuracy in pinpointing\nerrors within reasoning traces on ProcessBench. Furthermore, on challenging\nmathematical reasoning benchmarks (AIME 2024, AIME 2025, and CNMO), our full\napproach outperforms baselines like self-consistency in reasoning accuracy and\ninference efficiency. Our system offers a scalable and effective solution to\nenhance LLM reasoning at test time."}
{"id": "2505.11882", "pdf": "https://arxiv.org/pdf/2505.11882", "abs": "https://arxiv.org/abs/2505.11882", "authors": ["Shiming Chen", "Dingjie Fu", "Salman Khan", "Fahad Shahbaz Khan"], "title": "GenZSL: Generative Zero-Shot Learning Via Inductive Variational Autoencoder", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to ICML'25", "summary": "Remarkable progress in zero-shot learning (ZSL) has been achieved using\ngenerative models. However, existing generative ZSL methods merely generate\n(imagine) the visual features from scratch guided by the strong class semantic\nvectors annotated by experts, resulting in suboptimal generative performance\nand limited scene generalization. To address these and advance ZSL, we propose\nan inductive variational autoencoder for generative zero-shot learning, dubbed\nGenZSL. Mimicking human-level concept learning, GenZSL operates by inducting\nnew class samples from similar seen classes using weak class semantic vectors\nderived from target class names (i.e., CLIP text embedding). To ensure the\ngeneration of informative samples for training an effective ZSL classifier, our\nGenZSL incorporates two key strategies. Firstly, it employs class diversity\npromotion to enhance the diversity of class semantic vectors. Secondly, it\nutilizes target class-guided information boosting criteria to optimize the\nmodel. Extensive experiments conducted on three popular benchmark datasets\nshowcase the superiority and potential of our GenZSL with significant efficacy\nand efficiency over f-VAEGAN, e.g., 24.7% performance gains and more than\n$60\\times$ faster training speed on AWA2. Codes are available at\nhttps://github.com/shiming-chen/GenZSL."}
{"id": "2505.11887", "pdf": "https://arxiv.org/pdf/2505.11887", "abs": "https://arxiv.org/abs/2505.11887", "authors": ["Xiechi Zhang", "Zetian Ouyang", "Linlin Wang", "Gerard de Melo", "Zhu Cao", "Xiaoling Wang", "Ya Zhang", "Yanfeng Wang", "Liang He"], "title": "AutoMedEval: Harnessing Language Models for Automatic Medical Capability Evaluation", "categories": ["cs.CL"], "comment": null, "summary": "With the proliferation of large language models (LLMs) in the medical domain,\nthere is increasing demand for improved evaluation techniques to assess their\ncapabilities. However, traditional metrics like F1 and ROUGE, which rely on\ntoken overlaps to measure quality, significantly overlook the importance of\nmedical terminology. While human evaluation tends to be more reliable, it can\nbe very costly and may as well suffer from inaccuracies due to limits in human\nexpertise and motivation. Although there are some evaluation methods based on\nLLMs, their usability in the medical field is limited due to their proprietary\nnature or lack of expertise. To tackle these challenges, we present\nAutoMedEval, an open-sourced automatic evaluation model with 13B parameters\nspecifically engineered to measure the question-answering proficiency of\nmedical LLMs. The overarching objective of AutoMedEval is to assess the quality\nof responses produced by diverse models, aspiring to significantly reduce the\ndependence on human evaluation. Specifically, we propose a hierarchical\ntraining method involving curriculum instruction tuning and an iterative\nknowledge introspection mechanism, enabling AutoMedEval to acquire professional\nmedical assessment capabilities with limited instructional data. Human\nevaluations indicate that AutoMedEval surpasses other baselines in terms of\ncorrelation with human judgments."}
{"id": "2505.11999", "pdf": "https://arxiv.org/pdf/2505.11999", "abs": "https://arxiv.org/abs/2505.11999", "authors": ["Chang Liu", "Huan Yan", "Hongjie Sui", "Haomin Wen", "Yuan Yuan", "Yuyang Han", "Hongsen Liao", "Xuetao Ding", "Jinghua Hao", "Yong Li"], "title": "MRGRP: Empowering Courier Route Prediction in Food Delivery Service with Multi-Relational Graph", "categories": ["cs.AI"], "comment": null, "summary": "Instant food delivery has become one of the most popular web services\nworldwide due to its convenience in daily life. A fundamental challenge is\naccurately predicting courier routes to optimize task dispatch and improve\ndelivery efficiency. This enhances satisfaction for couriers and users and\nincreases platform profitability. The current heuristic prediction method uses\nonly limited human-selected task features and ignores couriers preferences,\ncausing suboptimal results. Additionally, existing learning-based methods do\nnot fully capture the diverse factors influencing courier decisions or the\ncomplex relationships among them. To address this, we propose a\nMulti-Relational Graph-based Route Prediction (MRGRP) method that models\nfine-grained correlations among tasks affecting courier decisions for accurate\nprediction. We encode spatial and temporal proximity, along with\npickup-delivery relationships, into a multi-relational graph and design a\nGraphFormer architecture to capture these complex connections. We also\nintroduce a route decoder that leverages courier information and dynamic\ndistance and time contexts for prediction, using existing route solutions as\nreferences to improve outcomes. Experiments show our model achieves\nstate-of-the-art route prediction on offline data from cities of various sizes.\nDeployed on the Meituan Turing platform, it surpasses the current heuristic\nalgorithm, reaching a high route prediction accuracy of 0.819, essential for\ncourier and user satisfaction in instant food delivery."}
{"id": "2505.11884", "pdf": "https://arxiv.org/pdf/2505.11884", "abs": "https://arxiv.org/abs/2505.11884", "authors": ["Zhongwen Li", "Zongwei Li", "Xiaoqi Li"], "title": "Facial Recognition Leveraging Generative Adversarial Networks", "categories": ["cs.CV", "cs.CR"], "comment": null, "summary": "Face recognition performance based on deep learning heavily relies on\nlarge-scale training data, which is often difficult to acquire in practical\napplications. To address this challenge, this paper proposes a GAN-based data\naugmentation method with three key contributions: (1) a residual-embedded\ngenerator to alleviate gradient vanishing/exploding problems, (2) an Inception\nResNet-V1 based FaceNet discriminator for improved adversarial training, and\n(3) an end-to-end framework that jointly optimizes data generation and\nrecognition performance. Experimental results demonstrate that our approach\nachieves stable training dynamics and significantly improves face recognition\naccuracy by 12.7% on the LFW benchmark compared to baseline methods, while\nmaintaining good generalization capability with limited training samples."}
{"id": "2505.11891", "pdf": "https://arxiv.org/pdf/2505.11891", "abs": "https://arxiv.org/abs/2505.11891", "authors": ["Weikai Xu", "Zhizheng Jiang", "Yuxuan Liu", "Wei Liu", "Jian Luan", "Yuanchun Li", "Yunxin Liu", "Bin Wang", "Bo An"], "title": "Mobile-Bench-v2: A More Realistic and Comprehensive Benchmark for VLM-based Mobile Agents", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "VLM-based mobile agents are increasingly popular due to their capabilities to\ninteract with smartphone GUIs and XML-structured texts and to complete daily\ntasks. However, existing online benchmarks struggle with obtaining stable\nreward signals due to dynamic environmental changes. Offline benchmarks\nevaluate the agents through single-path trajectories, which stands in contrast\nto the inherently multi-solution characteristics of GUI tasks. Additionally,\nboth types of benchmarks fail to assess whether mobile agents can handle noise\nor engage in proactive interactions due to a lack of noisy apps or overly full\ninstructions during the evaluation process. To address these limitations, we\nuse a slot-based instruction generation method to construct a more realistic\nand comprehensive benchmark named Mobile-Bench-v2. Mobile-Bench-v2 includes a\ncommon task split, with offline multi-path evaluation to assess the agent's\nability to obtain step rewards during task execution. It contains a noisy split\nbased on pop-ups and ads apps, and a contaminated split named AITZ-Noise to\nformulate a real noisy environment. Furthermore, an ambiguous instruction split\nwith preset Q\\&A interactions is released to evaluate the agent's proactive\ninteraction capabilities. We conduct evaluations on these splits using the\nsingle-agent framework AppAgent-v1, the multi-agent framework Mobile-Agent-v2,\nas well as other mobile agents such as UI-Tars and OS-Atlas. Code and data are\navailable at https://huggingface.co/datasets/xwk123/MobileBench-v2."}
{"id": "2505.12001", "pdf": "https://arxiv.org/pdf/2505.12001", "abs": "https://arxiv.org/abs/2505.12001", "authors": ["Ruta Binkyte"], "title": "Interactional Fairness in LLM Multi-Agent Systems: An Evaluation Framework", "categories": ["cs.AI", "cs.MA"], "comment": null, "summary": "As large language models (LLMs) are increasingly used in multi-agent systems,\nquestions of fairness should extend beyond resource distribution and procedural\ndesign to include the fairness of how agents communicate. Drawing from\norganizational psychology, we introduce a novel framework for evaluating\nInteractional fairness encompassing Interpersonal fairness (IF) and\nInformational fairness (InfF) in LLM-based multi-agent systems (LLM-MAS). We\nextend the theoretical grounding of Interactional Fairness to non-sentient\nagents, reframing fairness as a socially interpretable signal rather than a\nsubjective experience. We then adapt established tools from organizational\njustice research, including Colquitt's Organizational Justice Scale and the\nCritical Incident Technique, to measure fairness as a behavioral property of\nagent interaction. We validate our framework through a pilot study using\ncontrolled simulations of a resource negotiation task. We systematically\nmanipulate tone, explanation quality, outcome inequality, and task framing\n(collaborative vs. competitive) to assess how IF influences agent behavior.\nResults show that tone and justification quality significantly affect\nacceptance decisions even when objective outcomes are held constant. In\naddition, the influence of IF vs. InfF varies with context. This work lays the\nfoundation for fairness auditing and norm-sensitive alignment in LLM-MAS."}
{"id": "2505.11895", "pdf": "https://arxiv.org/pdf/2505.11895", "abs": "https://arxiv.org/abs/2505.11895", "authors": ["Chih-Ting Liao", "Bin Ren", "Guofeng Mei", "Xu Zheng"], "title": "Adversarial Robustness for Unified Multi-Modal Encoders via Efficient Calibration", "categories": ["cs.CV"], "comment": null, "summary": "Recent unified multi-modal encoders align a wide range of modalities into a\nshared representation space, enabling diverse cross-modal tasks. Despite their\nimpressive capabilities, the robustness of these models under adversarial\nperturbations remains underexplored, which is a critical concern for\nsafety-sensitive applications. In this work, we present the first comprehensive\nstudy of adversarial vulnerability in unified multi-modal encoders. We find\nthat even mild adversarial perturbations lead to substantial performance drops\nacross all modalities. Non-visual inputs, such as audio and point clouds, are\nespecially fragile, while visual inputs like images and videos also degrade\nsignificantly. To address this, we propose an efficient adversarial calibration\nframework that improves robustness across modalities without modifying\npretrained encoders or semantic centers, ensuring compatibility with existing\nfoundation models. Our method introduces modality-specific projection heads\ntrained solely on adversarial examples, while keeping the backbone and\nembeddings frozen. We explore three training objectives: fixed-center\ncross-entropy, clean-to-adversarial L2 alignment, and clean-adversarial\nInfoNCE, and we introduce a regularization strategy to ensure\nmodality-consistent alignment under attack. Experiments on six modalities and\nthree Bind-style models show that our method improves adversarial robustness by\nup to 47.3 percent at epsilon = 4/255, while preserving or even improving clean\nzero-shot and retrieval performance with less than 1 percent trainable\nparameters."}
{"id": "2505.11893", "pdf": "https://arxiv.org/pdf/2505.11893", "abs": "https://arxiv.org/abs/2505.11893", "authors": ["Zepeng Ding", "Dixuan Wang", "Ziqin Luo", "Guochao Jiang", "Deqing Yang", "Jiaqing Liang"], "title": "RLAP: A Reinforcement Learning Enhanced Adaptive Planning Framework for Multi-step NLP Task Solving", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multi-step planning has been widely employed to enhance the performance of\nlarge language models (LLMs) on downstream natural language processing (NLP)\ntasks, which decomposes the original task into multiple subtasks and guide LLMs\nto solve them sequentially without additional training. When addressing task\ninstances, existing methods either preset the order of steps or attempt\nmultiple paths at each step. However, these methods overlook instances'\nlinguistic features and rely on the intrinsic planning capabilities of LLMs to\nevaluate intermediate feedback and then select subtasks, resulting in\nsuboptimal outcomes. To better solve multi-step NLP tasks with LLMs, in this\npaper we propose a Reinforcement Learning enhanced Adaptive Planning framework\n(RLAP). In our framework, we model an NLP task as a Markov decision process\n(MDP) and employ an LLM directly into the environment. In particular, a\nlightweight Actor model is trained to estimate Q-values for natural language\nsequences consisting of states and actions through reinforcement learning.\nTherefore, during sequential planning, the linguistic features of each sequence\nin the MDP can be taken into account, and the Actor model interacts with the\nLLM to determine the optimal order of subtasks for each task instance. We apply\nRLAP on three different types of NLP tasks and conduct extensive experiments on\nmultiple datasets to verify RLAP's effectiveness and robustness."}
{"id": "2505.12006", "pdf": "https://arxiv.org/pdf/2505.12006", "abs": "https://arxiv.org/abs/2505.12006", "authors": ["Yuncheng Hua", "Ji Miao", "Mehdi Jafari", "Jianxiang Xie", "Hao Xue", "Flora D. Salim"], "title": "SOCIA: An End-to-End Agentic Framework for Automated Cyber-Physical-Social Simulator Generation", "categories": ["cs.AI", "I.2.7"], "comment": "28 pages, 3 figures, 2 tables. The paper is under review", "summary": "This paper introduces SOCIA (Simulation Orchestration for\nCyber-physical-social Intelligence and Agents), a novel end-to-end framework\nleveraging Large Language Model (LLM)-based multi-agent systems to automate the\ngeneration of high-fidelity Cyber-Physical-Social (CPS) simulators. Addressing\nthe challenges of labor-intensive manual simulator development and complex data\ncalibration, SOCIA integrates a centralized orchestration manager that\ncoordinates specialized agents for tasks including data comprehension, code\ngeneration, simulation execution, and iterative evaluation-feedback loops.\nThrough empirical evaluations across diverse CPS tasks, such as mask adoption\nbehavior simulation (social), personal mobility generation (physical), and user\nmodeling (cyber), SOCIA demonstrates its ability to produce high-fidelity,\nscalable simulations with reduced human intervention. These results highlight\nSOCIA's potential to offer a scalable solution for studying complex CPS\nphenomena"}
{"id": "2505.11897", "pdf": "https://arxiv.org/pdf/2505.11897", "abs": "https://arxiv.org/abs/2505.11897", "authors": ["Seonghak Kim"], "title": "FiGKD: Fine-Grained Knowledge Distillation via High-Frequency Detail Transfer", "categories": ["cs.CV"], "comment": "14 pages, 6 figures. This work has been submitted to the Elsevier for\n  possible publication", "summary": "Knowledge distillation (KD) is a widely adopted technique for transferring\nknowledge from a high-capacity teacher model to a smaller student model by\naligning their output distributions. However, existing methods often\nunderperform in fine-grained visual recognition tasks, where distinguishing\nsubtle differences between visually similar classes is essential. This\nperformance gap stems from the fact that conventional approaches treat the\nteacher's output logits as a single, undifferentiated signal-assuming all\ncontained information is equally beneficial to the student. Consequently,\nstudent models may become overloaded with redundant signals and fail to capture\nthe teacher's nuanced decision boundaries. To address this issue, we propose\nFine-Grained Knowledge Distillation (FiGKD), a novel frequency-aware framework\nthat decomposes a model's logits into low-frequency (content) and\nhigh-frequency (detail) components using the discrete wavelet transform (DWT).\nFiGKD selectively transfers only the high-frequency components, which encode\nthe teacher's semantic decision patterns, while discarding redundant\nlow-frequency content already conveyed through ground-truth supervision. Our\napproach is simple, architecture-agnostic, and requires no access to\nintermediate feature maps. Extensive experiments on CIFAR-100, TinyImageNet,\nand multiple fine-grained recognition benchmarks show that FiGKD consistently\noutperforms state-of-the-art logit-based and feature-based distillation methods\nacross a variety of teacher-student configurations. These findings confirm that\nfrequency-aware logit decomposition enables more efficient and effective\nknowledge transfer, particularly in resource-constrained settings."}
{"id": "2505.11900", "pdf": "https://arxiv.org/pdf/2505.11900", "abs": "https://arxiv.org/abs/2505.11900", "authors": ["Philipp Christmann", "Gerhard Weikum"], "title": "Recursive Question Understanding for Complex Question Answering over Heterogeneous Personal Data", "categories": ["cs.CL", "cs.IR"], "comment": "Accepted at ACL 2025 (Findings)", "summary": "Question answering over mixed sources, like text and tables, has been\nadvanced by verbalizing all contents and encoding it with a language model. A\nprominent case of such heterogeneous data is personal information: user devices\nlog vast amounts of data every day, such as calendar entries, workout\nstatistics, shopping records, streaming history, and more. Information needs\nrange from simple look-ups to queries of analytical nature. The challenge is to\nprovide humans with convenient access with small footprint, so that all\npersonal data stays on the user devices. We present ReQAP, a novel method that\ncreates an executable operator tree for a given question, via recursive\ndecomposition. Operators are designed to enable seamless integration of\nstructured and unstructured sources, and the execution of the operator tree\nyields a traceable answer. We further release the PerQA benchmark, with\npersona-based data and questions, covering a diverse spectrum of realistic user\nneeds."}
{"id": "2505.12012", "pdf": "https://arxiv.org/pdf/2505.12012", "abs": "https://arxiv.org/abs/2505.12012", "authors": ["Georgios Pavlidis"], "title": "Empowering Sustainable Finance with Artificial Intelligence: A Framework for Responsible Implementation", "categories": ["cs.AI"], "comment": null, "summary": "This chapter explores the convergence of two major developments: the rise of\nenvironmental, social, and governance (ESG) investing and the exponential\ngrowth of artificial intelligence (AI) technology. The increased demand for\ndiverse ESG instruments, such as green and ESG-linked loans, will be aligned\nwith the rapid growth of the global AI market, which is expected to be worth\n$1,394.30 billion by 2029. AI can assist in identifying and pricing climate\nrisks, setting more ambitious ESG goals, and advancing sustainable finance\ndecisions. However, delegating sustainable finance decisions to AI poses\nserious risks, and new principles and rules for AI and ESG investing are\nnecessary to mitigate these risks. This chapter highlights the challenges\nassociated with norm-setting initiatives and stresses the need for the\nfine-tuning of the principles of legitimacy, oversight and verification,\ntransparency, and explainability. Finally, the chapter contends that\nintegrating AI into ESG non-financial reporting necessitates a heightened sense\nof responsibility and the establishment of fundamental guiding principles\nwithin the spheres of AI and ESG investing."}
{"id": "2505.11905", "pdf": "https://arxiv.org/pdf/2505.11905", "abs": "https://arxiv.org/abs/2505.11905", "authors": ["Takuya Ikeda", "Sergey Zakharov", "Muhammad Zubair Irshad", "Istvan Balazs Opra", "Shun Iwase", "Dian Chen", "Mark Tjersland", "Robert Lee", "Alexandre Dilly", "Rares Ambrus", "Koichi Nishiwaki"], "title": "GTR: Gaussian Splatting Tracking and Reconstruction of Unknown Objects Based on Appearance and Geometric Complexity", "categories": ["cs.CV", "cs.RO"], "comment": "main contains 10 pages, 9 figures. And supplementary material\n  contains 10 pages, 27 figures", "summary": "We present a novel method for 6-DoF object tracking and high-quality 3D\nreconstruction from monocular RGBD video. Existing methods, while achieving\nimpressive results, often struggle with complex objects, particularly those\nexhibiting symmetry, intricate geometry or complex appearance. To bridge these\ngaps, we introduce an adaptive method that combines 3D Gaussian Splatting,\nhybrid geometry/appearance tracking, and key frame selection to achieve robust\ntracking and accurate reconstructions across a diverse range of objects.\nAdditionally, we present a benchmark covering these challenging object classes,\nproviding high-quality annotations for evaluating both tracking and\nreconstruction performance. Our approach demonstrates strong capabilities in\nrecovering high-fidelity object meshes, setting a new standard for\nsingle-sensor 3D reconstruction in open-world environments."}
{"id": "2505.11908", "pdf": "https://arxiv.org/pdf/2505.11908", "abs": "https://arxiv.org/abs/2505.11908", "authors": ["Zhangyu Wang", "Siyuan Gao", "Rong Zhou", "Hao Wang", "Li Ning"], "title": "ELITE: Embedding-Less retrieval with Iterative Text Exploration", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved impressive progress in natural\nlanguage processing, but their limited ability to retain long-term context\nconstrains performance on document-level or multi-turn tasks.\nRetrieval-Augmented Generation (RAG) mitigates this by retrieving relevant\ninformation from an external corpus. However, existing RAG systems often rely\non embedding-based retrieval trained on corpus-level semantic similarity, which\ncan lead to retrieving content that is semantically similar in form but\nmisaligned with the question's true intent. Furthermore, recent RAG variants\nconstruct graph- or hierarchy-based structures to improve retrieval accuracy,\nresulting in significant computation and storage overhead. In this paper, we\npropose an embedding-free retrieval framework. Our method leverages the logical\ninferencing ability of LLMs in retrieval using iterative search space\nrefinement guided by our novel importance measure and extend our retrieval\nresults with logically related information without explicit graph construction.\nExperiments on long-context QA benchmarks, including NovelQA and Marathon, show\nthat our approach outperforms strong baselines while reducing storage and\nruntime by over an order of magnitude."}
{"id": "2505.12031", "pdf": "https://arxiv.org/pdf/2505.12031", "abs": "https://arxiv.org/abs/2505.12031", "authors": ["Junyu Lai", "Jiakun Zhang", "Shuo Xu", "Taolue Chen", "Zihang Wang", "Yao Yang", "Jiarui Zhang", "Chun Cao", "Jingwei Xu"], "title": "LLM-based Automated Theorem Proving Hinges on Scalable Synthetic Data Generation", "categories": ["cs.AI", "I.2.7"], "comment": "20 pages", "summary": "Recent advancements in large language models (LLMs) have sparked considerable\ninterest in automated theorem proving and a prominent line of research\nintegrates stepwise LLM-based provers into tree search. In this paper, we\nintroduce a novel proof-state exploration approach for training data synthesis,\ndesigned to produce diverse tactics across a wide range of intermediate proof\nstates, thereby facilitating effective one-shot fine-tuning of LLM as the\npolicy model. We also propose an adaptive beam size strategy, which effectively\ntakes advantage of our data synthesis method and achieves a trade-off between\nexploration and exploitation during tree search. Evaluations on the MiniF2F and\nProofNet benchmarks demonstrate that our method outperforms strong baselines\nunder the stringent Pass@1 metric, attaining an average pass rate of $60.74\\%$\non MiniF2F and $21.18\\%$ on ProofNet. These results underscore the impact of\nlarge-scale synthetic data in advancing automated theorem proving."}
{"id": "2505.11907", "pdf": "https://arxiv.org/pdf/2505.11907", "abs": "https://arxiv.org/abs/2505.11907", "authors": ["Zihao Dongfang", "Xu Zheng", "Ziqiao Weng", "Yuanhuiyi Lyu", "Danda Pani Paudel", "Luc Van Gool", "Kailun Yang", "Xuming Hu"], "title": "Are Multimodal Large Language Models Ready for Omnidirectional Spatial Reasoning?", "categories": ["cs.CV"], "comment": null, "summary": "The 180x360 omnidirectional field of view captured by 360-degree cameras\nenables their use in a wide range of applications such as embodied AI and\nvirtual reality. Although recent advances in multimodal large language models\n(MLLMs) have shown promise in visual-spatial reasoning, most studies focus on\nstandard pinhole-view images, leaving omnidirectional perception largely\nunexplored. In this paper, we ask: Are MLLMs ready for omnidirectional spatial\nreasoning? To investigate this, we introduce OSR-Bench, the first benchmark\nspecifically designed for this setting. OSR-Bench includes over 153,000 diverse\nquestion-answer pairs grounded in high-fidelity panoramic indoor scene maps. It\ncovers key reasoning types including object counting, relative distance, and\ndirection. We also propose a negative sampling strategy that inserts\nnon-existent objects into prompts to evaluate hallucination and grounding\nrobustness. For fine-grained analysis, we design a two-stage evaluation\nframework assessing both cognitive map generation and QA accuracy using\nrotation-invariant matching and a combination of rule-based and LLM-based\nmetrics. We evaluate eight state-of-the-art MLLMs, including GPT-4o, Gemini 1.5\nPro, and leading open-source models under zero-shot settings. Results show that\ncurrent models struggle with spatial reasoning in panoramic contexts,\nhighlighting the need for more perceptually grounded MLLMs. OSR-Bench and code\nwill be released at: https://huggingface.co/datasets/UUUserna/OSR-Bench"}
{"id": "2505.11922", "pdf": "https://arxiv.org/pdf/2505.11922", "abs": "https://arxiv.org/abs/2505.11922", "authors": ["Yuheng Lu", "ZiMeng Bai", "Caixia Yuan", "Huixing Jiang", "Xiaojie Wang"], "title": "Enhancing Complex Instruction Following for Large Language Models with Mixture-of-Contexts Fine-tuning", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) exhibit remarkable capabilities in handling\nnatural language tasks; however, they may struggle to consistently follow\ncomplex instructions including those involve multiple constraints.\nPost-training LLMs using supervised fine-tuning (SFT) is a standard approach to\nimprove their ability to follow instructions. In addressing complex instruction\nfollowing, existing efforts primarily focus on data-driven methods that\nsynthesize complex instruction-output pairs for SFT. However, insufficient\nattention allocated to crucial sub-contexts may reduce the effectiveness of\nSFT. In this work, we propose transforming sequentially structured input\ninstruction into multiple parallel instructions containing subcontexts. To\nsupport processing this multi-input, we propose MISO (Multi-Input\nSingle-Output), an extension to currently dominant decoder-only\ntransformer-based LLMs. MISO introduces a mixture-of-contexts paradigm that\njointly considers the overall instruction-output alignment and the influence of\nindividual sub-contexts to enhance SFT effectiveness. We apply MISO fine-tuning\nto complex instructionfollowing datasets and evaluate it with standard LLM\ninference. Empirical results demonstrate the superiority of MISO as a\nfine-tuning method for LLMs, both in terms of effectiveness in complex\ninstruction-following scenarios and its potential for training efficiency."}
{"id": "2505.12039", "pdf": "https://arxiv.org/pdf/2505.12039", "abs": "https://arxiv.org/abs/2505.12039", "authors": ["Renqi Chen", "Haoyang Su", "Shixiang Tang", "Zhenfei Yin", "Qi Wu", "Hui Li", "Ye Sun", "Nanqing Dong", "Wanli Ouyang", "Philip Torr"], "title": "AI-Driven Automation Can Become the Foundation of Next-Era Science of Science Research", "categories": ["cs.AI", "cs.CL", "physics.soc-ph"], "comment": null, "summary": "The Science of Science (SoS) explores the mechanisms underlying scientific\ndiscovery, and offers valuable insights for enhancing scientific efficiency and\nfostering innovation. Traditional approaches often rely on simplistic\nassumptions and basic statistical tools, such as linear regression and\nrule-based simulations, which struggle to capture the complexity and scale of\nmodern research ecosystems. The advent of artificial intelligence (AI) presents\na transformative opportunity for the next generation of SoS, enabling the\nautomation of large-scale pattern discovery and uncovering insights previously\nunattainable. This paper offers a forward-looking perspective on the\nintegration of Science of Science with AI for automated research pattern\ndiscovery and highlights key open challenges that could greatly benefit from\nAI. We outline the advantages of AI over traditional methods, discuss potential\nlimitations, and propose pathways to overcome them. Additionally, we present a\npreliminary multi-agent system as an illustrative example to simulate research\nsocieties, showcasing AI's ability to replicate real-world research patterns\nand accelerate progress in Science of Science research."}
{"id": "2505.11921", "pdf": "https://arxiv.org/pdf/2505.11921", "abs": "https://arxiv.org/abs/2505.11921", "authors": ["Haitao Li", "Ziyu Li", "Yiheng Mao", "Zhengyao Ding", "Zhengxing Huang"], "title": "DC-Seg: Disentangled Contrastive Learning for Brain Tumor Segmentation with Missing Modalities", "categories": ["cs.CV"], "comment": null, "summary": "Accurate segmentation of brain images typically requires the integration of\ncomplementary information from multiple image modalities. However, clinical\ndata for all modalities may not be available for every patient, creating a\nsignificant challenge. To address this, previous studies encode multiple\nmodalities into a shared latent space. While somewhat effective, it remains\nsuboptimal, as each modality contains distinct and valuable information. In\nthis study, we propose DC-Seg (Disentangled Contrastive Learning for\nSegmentation), a new method that explicitly disentangles images into\nmodality-invariant anatomical representation and modality-specific\nrepresentation, by using anatomical contrastive learning and modality\ncontrastive learning respectively. This solution improves the separation of\nanatomical and modality-specific features by considering the modality gaps,\nleading to more robust representations. Furthermore, we introduce a\nsegmentation-based regularizer that enhances the model's robustness to missing\nmodalities. Extensive experiments on the BraTS 2020 and a private white matter\nhyperintensity(WMH) segmentation dataset demonstrate that DC-Seg outperforms\nstate-of-the-art methods in handling incomplete multimodal brain tumor\nsegmentation tasks with varying missing modalities, while also demonstrate\nstrong generalizability in WMH segmentation. The code is available at\nhttps://github.com/CuCl-2/DC-Seg."}
{"id": "2505.11924", "pdf": "https://arxiv.org/pdf/2505.11924", "abs": "https://arxiv.org/abs/2505.11924", "authors": ["Yu-Ting Lee", "Hui-Ying Shih", "Fu-Chieh Chang", "Pei-Yuan Wu"], "title": "An Explanation of Intrinsic Self-Correction via Linear Representations and Latent Concepts", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We provide an explanation for the performance gains of intrinsic\nself-correction, a process where a language model iteratively refines its\noutputs without external feedback. More precisely, we investigate how prompting\ninduces interpretable changes in hidden states and thus affects the output\ndistributions. We hypothesize that each prompt-induced shift lies in a linear\nspan of some linear representation vectors, naturally separating tokens based\non individual concept alignment. Building around this idea, we give a\nmathematical formulation of self-correction and derive a concentration result\nfor output tokens based on alignment magnitudes. Our experiments on text\ndetoxification with zephyr-7b-sft reveal a substantial gap in the inner\nproducts of the prompt-induced shifts and the unembeddings of the top-100 most\ntoxic tokens vs. those of the unembeddings of the bottom-100 least toxic\ntokens, under toxic instructions. This suggests that self-correction prompts\nenhance a language model's capability of latent concept recognition. Our\nanalysis offers insights into the underlying mechanism of self-correction by\ncharacterizing how prompting works explainably. For reproducibility, our code\nis available."}
{"id": "2505.12057", "pdf": "https://arxiv.org/pdf/2505.12057", "abs": "https://arxiv.org/abs/2505.12057", "authors": ["Jing Zou", "Qingqiu Li", "Chenyu Lian", "Lihao Liu", "Xiaohan Yan", "Shujun Wang", "Jing Qin"], "title": "CorBenchX: Large-Scale Chest X-Ray Error Dataset and Vision-Language Model Benchmark for Report Error Correction", "categories": ["cs.AI"], "comment": "12 pages, 5figures", "summary": "AI-driven models have shown great promise in detecting errors in radiology\nreports, yet the field lacks a unified benchmark for rigorous evaluation of\nerror detection and further correction. To address this gap, we introduce\nCorBenchX, a comprehensive suite for automated error detection and correction\nin chest X-ray reports, designed to advance AI-assisted quality control in\nclinical practice. We first synthesize a large-scale dataset of 26,326 chest\nX-ray error reports by injecting clinically common errors via prompting\nDeepSeek-R1, with each corrupted report paired with its original text, error\ntype, and human-readable description. Leveraging this dataset, we benchmark\nboth open- and closed-source vision-language models,(e.g., InternVL, Qwen-VL,\nGPT-4o, o4-mini, and Claude-3.7) for error detection and correction under\nzero-shot prompting. Among these models, o4-mini achieves the best performance,\nwith 50.6 % detection accuracy and correction scores of BLEU 0.853, ROUGE\n0.924, BERTScore 0.981, SembScore 0.865, and CheXbertF1 0.954, remaining below\nclinical-level accuracy, highlighting the challenge of precise report\ncorrection. To advance the state of the art, we propose a multi-step\nreinforcement learning (MSRL) framework that optimizes a multi-objective reward\ncombining format compliance, error-type accuracy, and BLEU similarity. We apply\nMSRL to QwenVL2.5-7B, the top open-source model in our benchmark, achieving an\nimprovement of 38.3% in single-error detection precision and 5.2% in\nsingle-error correction over the zero-shot baseline."}
{"id": "2505.11926", "pdf": "https://arxiv.org/pdf/2505.11926", "abs": "https://arxiv.org/abs/2505.11926", "authors": ["Yixu Wang", "Jiaxin Song", "Yifeng Gao", "Xin Wang", "Yang Yao", "Yan Teng", "Xingjun Ma", "Yingchun Wang", "Yu-Gang Jiang"], "title": "SafeVid: Toward Safety Aligned Video Large Multimodal Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "As Video Large Multimodal Models (VLMMs) rapidly advance, their inherent\ncomplexity introduces significant safety challenges, particularly the issue of\nmismatched generalization where static safety alignments fail to transfer to\ndynamic video contexts. We introduce SafeVid, a framework designed to instill\nvideo-specific safety principles in VLMMs. SafeVid uniquely transfers robust\ntextual safety alignment capabilities to the video domain by employing detailed\ntextual video descriptions as an interpretive bridge, facilitating LLM-based\nrule-driven safety reasoning. This is achieved through a closed-loop system\ncomprising: 1) generation of SafeVid-350K, a novel 350,000-pair video-specific\nsafety preference dataset; 2) targeted alignment of VLMMs using Direct\nPreference Optimization (DPO); and 3) comprehensive evaluation via our new\nSafeVidBench benchmark. Alignment with SafeVid-350K significantly enhances VLMM\nsafety, with models like LLaVA-NeXT-Video demonstrating substantial\nimprovements (e.g., up to 42.39%) on SafeVidBench. SafeVid provides critical\nresources and a structured approach, demonstrating that leveraging textual\ndescriptions as a conduit for safety reasoning markedly improves the safety\nalignment of VLMMs. We have made SafeVid-350K dataset\n(https://huggingface.co/datasets/yxwang/SafeVid-350K) publicly available."}
{"id": "2505.11932", "pdf": "https://arxiv.org/pdf/2505.11932", "abs": "https://arxiv.org/abs/2505.11932", "authors": ["Yuyao Zhang", "Zhicheng Dou", "Xiaoxi Li", "Jiajie Jin", "Yongkang Wu", "Zhonghua Li", "Qi Ye", "Ji-Rong Wen"], "title": "Neuro-Symbolic Query Compiler", "categories": ["cs.CL", "cs.IR"], "comment": "Findings of ACL2025, codes are available at this url:\n  https://github.com/YuyaoZhangQAQ/Query_Compiler", "summary": "Precise recognition of search intent in Retrieval-Augmented Generation (RAG)\nsystems remains a challenging goal, especially under resource constraints and\nfor complex queries with nested structures and dependencies. This paper\npresents QCompiler, a neuro-symbolic framework inspired by linguistic grammar\nrules and compiler design, to bridge this gap. It theoretically designs a\nminimal yet sufficient Backus-Naur Form (BNF) grammar $G[q]$ to formalize\ncomplex queries. Unlike previous methods, this grammar maintains completeness\nwhile minimizing redundancy. Based on this, QCompiler includes a Query\nExpression Translator, a Lexical Syntax Parser, and a Recursive Descent\nProcessor to compile queries into Abstract Syntax Trees (ASTs) for execution.\nThe atomicity of the sub-queries in the leaf nodes ensures more precise\ndocument retrieval and response generation, significantly improving the RAG\nsystem's ability to address complex queries."}
{"id": "2505.12058", "pdf": "https://arxiv.org/pdf/2505.12058", "abs": "https://arxiv.org/abs/2505.12058", "authors": ["Vincent Koc"], "title": "Tiny QA Benchmark++: Ultra-Lightweight, Synthetic Multilingual Dataset Generation & Smoke-Tests for Continuous LLM Evaluation", "categories": ["cs.AI", "cs.CL", "I.2.7; I.2.6; H.2.8"], "comment": "28 pages, 7 figures, 3 tables. Includes expanded appendix & full\n  score matrices. Dataset & code: HF Hub + GitHub + Pypi links in abstract.\n  Core data and code Apache-2.0; synthetic packs eval-only", "summary": "Tiny QA Benchmark++ (TQB++) presents an ultra-lightweight, multilingual\nsmoke-test suite designed to give large-language-model (LLM) pipelines a\nunit-test style safety net dataset that runs in seconds with minimal cost. Born\nout of the tight feedback-loop demands building the Comet Opik\nprompt-optimization SDK, where waiting on heavyweight benchmarks breaks\ndeveloper flow. TQB++ couples a 52-item English gold set (less than 20 kB) with\na tiny synthetic-data generator pypi package built on provider-agnostic\nLiteLLM. The generator lets practitioners mint their own tiny packs in any\nlanguage, domain, or difficulty, while ten ready-made packs already cover\nArabic, Chinese, French, German, Japanese, Korean, Portuguese, Russian,\nSpanish, and Turkish. Every dataset ships with Croissant metadata and\nplug-and-play files for OpenAI-Evals, LangChain, and standard CI tools, so\nteams can drop deterministic micro-benchmarks directly into pull-request gates,\nprompt-engineering loops, and production dashboards without touching GPU\nbudgets. A complete TQB++ run adds only a few seconds to pipeline latency yet\nreliably flags prompt-template errors, tokenizer drift, and fine-tuning\nside-effects long before full-scale suites like MMLU or BIG-Bench would finish\nconfiguring. The entire framework is released to accelerate continuous,\nresource-efficient quality assurance across the generative-AI ecosystem."}
{"id": "2505.11934", "pdf": "https://arxiv.org/pdf/2505.11934", "abs": "https://arxiv.org/abs/2505.11934", "authors": ["Yian Zhao", "Wanshi Xu", "Ruochong Zheng", "Pengchong Qiao", "Chang Liu", "Jie Chen"], "title": "iSegMan: Interactive Segment-and-Manipulate 3D Gaussians", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "The efficient rendering and explicit nature of 3DGS promote the advancement\nof 3D scene manipulation. However, existing methods typically encounter\nchallenges in controlling the manipulation region and are unable to furnish the\nuser with interactive feedback, which inevitably leads to unexpected results.\nIntuitively, incorporating interactive 3D segmentation tools can compensate for\nthis deficiency. Nevertheless, existing segmentation frameworks impose a\npre-processing step of scene-specific parameter training, which limits the\nefficiency and flexibility of scene manipulation. To deliver a 3D region\ncontrol module that is well-suited for scene manipulation with reliable\nefficiency, we propose interactive Segment-and-Manipulate 3D Gaussians\n(iSegMan), an interactive segmentation and manipulation framework that only\nrequires simple 2D user interactions in any view. To propagate user\ninteractions to other views, we propose Epipolar-guided Interaction Propagation\n(EIP), which innovatively exploits epipolar constraint for efficient and robust\ninteraction matching. To avoid scene-specific training to maintain efficiency,\nwe further propose the novel Visibility-based Gaussian Voting (VGV), which\nobtains 2D segmentations from SAM and models the region extraction as a voting\ngame between 2D Pixels and 3D Gaussians based on Gaussian visibility. Taking\nadvantage of the efficient and precise region control of EIP and VGV, we put\nforth a Manipulation Toolbox to implement various functions on selected\nregions, enhancing the controllability, flexibility and practicality of scene\nmanipulation. Extensive results on 3D scene manipulation and segmentation tasks\nfully demonstrate the significant advantages of iSegMan. Project page is\navailable at https://zhao-yian.github.io/iSegMan."}
{"id": "2505.11935", "pdf": "https://arxiv.org/pdf/2505.11935", "abs": "https://arxiv.org/abs/2505.11935", "authors": ["Xuanle Zhao", "Xuexin Liu", "Haoyue Yang", "Xianzhen Luo", "Fanhu Zeng", "Jianling Li", "Qi Shi", "Chi Chen"], "title": "ChartEdit: How Far Are MLLMs From Automating Chart Analysis? Evaluating MLLMs' Capability via Chart Editing", "categories": ["cs.CL"], "comment": "Accept by ACL2025 Findings, preprint version", "summary": "Although multimodal large language models (MLLMs) show promise in generating\nchart rendering code, chart editing presents a greater challenge. This\ndifficulty stems from its nature as a labor-intensive task for humans that also\ndemands MLLMs to integrate chart understanding, complex reasoning, and precise\nintent interpretation. While many MLLMs claim such editing capabilities,\ncurrent assessments typically rely on limited case studies rather than robust\nevaluation methodologies, highlighting the urgent need for a comprehensive\nevaluation framework. In this work, we propose ChartEdit, a new high-quality\nbenchmark designed for chart editing tasks. This benchmark comprises $1,405$\ndiverse editing instructions applied to $233$ real-world charts, with each\ninstruction-chart instance having been manually annotated and validated for\naccuracy. Utilizing ChartEdit, we evaluate the performance of 10 mainstream\nMLLMs across two types of experiments, assessing them at both the code and\nchart levels. The results suggest that large-scale models can generate code to\nproduce images that partially match the reference images. However, their\nability to generate accurate edits according to the instructions remains\nlimited. The state-of-the-art (SOTA) model achieves a score of only $59.96$,\nhighlighting significant challenges in precise modification. In contrast,\nsmall-scale models, including chart-domain models, struggle both with following\nediting instructions and generating overall chart images, underscoring the need\nfor further development in this area. Code is available at\nhttps://github.com/xxlllz/ChartEdit."}
{"id": "2505.12065", "pdf": "https://arxiv.org/pdf/2505.12065", "abs": "https://arxiv.org/abs/2505.12065", "authors": ["Tiannuo Yang", "Zebin Yao", "Bowen Jin", "Lixiao Cui", "Yusen Li", "Gang Wang", "Xiaoguang Liu"], "title": "Demystifying and Enhancing the Efficiency of Large Language Model Based Search Agents", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG"], "comment": null, "summary": "Large Language Model (LLM)-based search agents have shown remarkable\ncapabilities in solving complex tasks by dynamically decomposing problems and\naddressing them through interleaved reasoning and retrieval. However, this\ninterleaved paradigm introduces substantial efficiency bottlenecks. First, we\nobserve that both highly accurate and overly approximate retrieval methods\ndegrade system efficiency: exact search incurs significant retrieval overhead,\nwhile coarse retrieval requires additional reasoning steps during generation.\nSecond, we identify inefficiencies in system design, including improper\nscheduling and frequent retrieval stalls, which lead to cascading latency --\nwhere even minor delays in retrieval amplify end-to-end inference time. To\naddress these challenges, we introduce SearchAgent-X, a high-efficiency\ninference framework for LLM-based search agents. SearchAgent-X leverages\nhigh-recall approximate retrieval and incorporates two key techniques:\npriority-aware scheduling and non-stall retrieval. Extensive experiments\ndemonstrate that SearchAgent-X consistently outperforms state-of-the-art\nsystems such as vLLM and HNSW-based retrieval across diverse tasks, achieving\nup to 3.4$\\times$ higher throughput and 5$\\times$ lower latency, without\ncompromising generation quality. SearchAgent-X is available at\nhttps://github.com/tiannuo-yang/SearchAgent-X."}
{"id": "2505.11945", "pdf": "https://arxiv.org/pdf/2505.11945", "abs": "https://arxiv.org/abs/2505.11945", "authors": ["Bonan li", "Zicheng Zhang", "Songhua Liu", "Weihao Yu", "Xinchao Wang"], "title": "Top-Down Compression: Revisit Efficient Vision Token Projection for Visual Instruction Tuning", "categories": ["cs.CV"], "comment": "Under Review", "summary": "Visual instruction tuning aims to enable large language models to comprehend\nthe visual world, with a pivotal challenge lying in establishing an effective\nvision-to-language projection. However, existing methods often grapple with the\nintractable trade-off between accuracy and efficiency. In this paper, we\npresent LLaVA-Meteor, a novel approach designed to break this deadlock,\nequipped with a novel Top-Down Compression paradigm that strategically\ncompresses visual tokens without compromising core information. Specifically,\nwe construct a trainable Flash Global Fusion module based on efficient\nselective state space operators, which aligns the feature space while enabling\neach token to perceive holistic visual context and instruction preference at\nlow cost. Furthermore, a local-to-single scanning manner is employed to\neffectively capture local dependencies, thereby enhancing the model's\ncapability in vision modeling. To alleviate computational overhead, we explore\na Visual-Native Selection mechanism that independently assesses token\nsignificance by both the visual and native experts, followed by aggregation to\nretain the most critical subset. Extensive experiments show that our approach\nreduces visual tokens by 75--95% while achieving comparable or superior\nperformance across 12 benchmarks, significantly improving efficiency."}
{"id": "2505.11958", "pdf": "https://arxiv.org/pdf/2505.11958", "abs": "https://arxiv.org/abs/2505.11958", "authors": ["Aswini Kumar Padhi", "Anil Bandhakavi", "Tanmoy Chakraborty"], "title": "Counterspeech the ultimate shield! Multi-Conditioned Counterspeech Generation through Attributed Prefix Learning", "categories": ["cs.CL"], "comment": null, "summary": "Counterspeech has proven to be a powerful tool to combat hate speech online.\nPrevious studies have focused on generating counterspeech conditioned only on\nspecific intents (single attributed). However, a holistic approach considering\nmultiple attributes simultaneously can yield more nuanced and effective\nresponses. Here, we introduce HiPPrO, Hierarchical Prefix learning with\nPreference Optimization, a novel two-stage framework that utilizes the\neffectiveness of attribute-specific prefix embedding spaces hierarchically\noptimized during the counterspeech generation process in the first phase.\nThereafter, we incorporate both reference and reward-free preference\noptimization to generate more constructive counterspeech. Furthermore, we\nextend IntentCONANv2 by annotating all 13,973 counterspeech instances with\nemotion labels by five annotators. HiPPrO leverages hierarchical prefix\noptimization to integrate these dual attributes effectively. An extensive\nevaluation demonstrates that HiPPrO achieves a ~38 % improvement in intent\nconformity and a ~3 %, ~2 %, ~3 % improvement in Rouge-1, Rouge-2, and Rouge-L,\nrespectively, compared to several baseline models. Human evaluations further\nsubstantiate the superiority of our approach, highlighting the enhanced\nrelevance and appropriateness of the generated counterspeech. This work\nunderscores the potential of multi-attribute conditioning in advancing the\nefficacy of counterspeech generation systems."}
{"id": "2505.12135", "pdf": "https://arxiv.org/pdf/2505.12135", "abs": "https://arxiv.org/abs/2505.12135", "authors": ["Omar Choukrani", "Idriss Malek", "Daniil Orel", "Zhuohan Xie", "Zangir Iklassov", "Martin Takáč", "Salem Lahlou"], "title": "LLM-BABYBENCH: Understanding and Evaluating Grounded Planning and Reasoning in LLMs", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Assessing the capacity of Large Language Models (LLMs) to plan and reason\nwithin the constraints of interactive environments is crucial for developing\ncapable AI agents. We introduce $\\textbf{LLM-BabyBench}$, a new benchmark suite\ndesigned specifically for this purpose. Built upon a textual adaptation of the\nprocedurally generated BabyAI grid world, this suite evaluates LLMs on three\nfundamental aspects of grounded intelligence: (1) predicting the consequences\nof actions on the environment state ($\\textbf{Predict}$ task), (2) generating\nsequences of low-level actions to achieve specified objectives ($\\textbf{Plan}$\ntask), and (3) decomposing high-level instructions into coherent subgoal\nsequences ($\\textbf{Decompose}$ task). We detail the methodology for generating\nthe three corresponding datasets ($\\texttt{LLM-BabyBench-Predict}$,\n$\\texttt{-Plan}$, $\\texttt{-Decompose}$) by extracting structured information\nfrom an expert agent operating within the text-based environment. Furthermore,\nwe provide a standardized evaluation harness and metrics, including environment\ninteraction for validating generated plans, to facilitate reproducible\nassessment of diverse LLMs. Initial baseline results highlight the challenges\nposed by these grounded reasoning tasks. The benchmark suite, datasets, data\ngeneration code, and evaluation code are made publicly available\n($\\href{https://github.com/choukrani/llm-babybench}{\\text{GitHub}}$,\n$\\href{https://huggingface.co/datasets/salem-mbzuai/LLM-BabyBench}{\\text{HuggingFace}}$)."}
{"id": "2505.11976", "pdf": "https://arxiv.org/pdf/2505.11976", "abs": "https://arxiv.org/abs/2505.11976", "authors": ["Soumya Swarup Prusty", "Astha Agarwal", "Srinivasan Iyenger"], "title": "Advanced Integration of Discrete Line Segments in Digitized P&ID for Continuous Instrument Connectivity", "categories": ["cs.CV"], "comment": "6 pages, 13 figures", "summary": "Piping and Instrumentation Diagrams (P&IDs) constitute the foundational\nblueprint of a plant, depicting the interconnections among process equipment,\ninstrumentation for process control, and the flow of fluids and control\nsignals. In their existing setup, the manual mapping of information from P&ID\nsheets holds a significant challenge. This is a time-consuming process, taking\naround 3-6 months, and is susceptible to errors. It also depends on the\nexpertise of the domain experts and often requires multiple rounds of review.\nThe digitization of P&IDs entails merging detected line segments, which is\nessential for linking various detected instruments, thereby creating a\ncomprehensive digitized P&ID. This paper focuses on explaining how line\nsegments which are detected using a computer vision model are merged and\neventually building the connection between equipment and merged lines. Hence\npresenting a digitized form of information stating the interconnection between\nprocess equipment, instrumentation, flow of fluids and control signals.\nEventually, which can be stored in a knowledge graph and that information along\nwith the help of advanced algorithms can be leveraged for tasks like finding\noptimal routes, detecting system cycles, computing transitive closures, and\nmore."}
{"id": "2505.11959", "pdf": "https://arxiv.org/pdf/2505.11959", "abs": "https://arxiv.org/abs/2505.11959", "authors": ["Md. Rafiul Biswas", "Wajdi Zaghouani"], "title": "EmoHopeSpeech: An Annotated Dataset of Emotions and Hope Speech in English", "categories": ["cs.CL"], "comment": null, "summary": "This research introduces a bilingual dataset comprising 23,456 entries for\nArabic and 10,036 entries for English, annotated for emotions and hope speech,\naddressing the scarcity of multi-emotion (Emotion and hope) datasets. The\ndataset provides comprehensive annotations capturing emotion intensity,\ncomplexity, and causes, alongside detailed classifications and subcategories\nfor hope speech. To ensure annotation reliability, Fleiss' Kappa was employed,\nrevealing 0.75-0.85 agreement among annotators both for Arabic and English\nlanguage. The evaluation metrics (micro-F1-Score=0.67) obtained from the\nbaseline model (i.e., using a machine learning model) validate that the data\nannotations are worthy. This dataset offers a valuable resource for advancing\nnatural language processing in underrepresented languages, fostering better\ncross-linguistic analysis of emotions and hope speech."}
{"id": "2505.12136", "pdf": "https://arxiv.org/pdf/2505.12136", "abs": "https://arxiv.org/abs/2505.12136", "authors": ["Xiao Wang", "Shun-Ren Yang"], "title": "Lightweight Spatio-Temporal Attention Network with Graph Embedding and Rotational Position Encoding for Traffic Forecasting", "categories": ["cs.AI"], "comment": null, "summary": "Traffic forecasting is a key task in the field of Intelligent Transportation\nSystems. Recent research on traffic forecasting has mainly focused on combining\ngraph neural networks (GNNs) with other models. However, GNNs only consider\nshort-range spatial information. In this study, we present a novel model termed\nLSTAN-GERPE (Lightweight Spatio-Temporal Attention Network with Graph Embedding\nand Rotational Position Encoding). This model leverages both Temporal and\nSpatial Attention mechanisms to effectively capture long-range traffic\ndynamics. Additionally, the optimal frequency for rotational position encoding\nis determined through a grid search approach in both the spatial and temporal\nattention mechanisms. This systematic optimization enables the model to\neffectively capture complex traffic patterns. The model also enhances feature\nrepresentation by incorporating geographical location maps into the\nspatio-temporal embeddings. Without extensive feature engineering, the proposed\nmethod in this paper achieves advanced accuracy on the real-world traffic\nforecasting datasets PeMS04 and PeMS08."}
{"id": "2505.11980", "pdf": "https://arxiv.org/pdf/2505.11980", "abs": "https://arxiv.org/abs/2505.11980", "authors": ["Yi Chen", "Mu-Young Son", "Chuanbo Hua", "Joo-Young Kim"], "title": "AoP-SAM: Automation of Prompts for Efficient Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at AAAI 2025", "summary": "The Segment Anything Model (SAM) is a powerful foundation model for image\nsegmentation, showing robust zero-shot generalization through prompt\nengineering. However, relying on manual prompts is impractical for real-world\napplications, particularly in scenarios where rapid prompt provision and\nresource efficiency are crucial. In this paper, we propose the Automation of\nPrompts for SAM (AoP-SAM), a novel approach that learns to generate essential\nprompts in optimal locations automatically. AoP-SAM enhances SAM's efficiency\nand usability by eliminating manual input, making it better suited for\nreal-world tasks. Our approach employs a lightweight yet efficient Prompt\nPredictor model that detects key entities across images and identifies the\noptimal regions for placing prompt candidates. This method leverages SAM's\nimage embeddings, preserving its zero-shot generalization capabilities without\nrequiring fine-tuning. Additionally, we introduce a test-time instance-level\nAdaptive Sampling and Filtering mechanism that generates prompts in a\ncoarse-to-fine manner. This notably enhances both prompt and mask generation\nefficiency by reducing computational overhead and minimizing redundant mask\nrefinements. Evaluations of three datasets demonstrate that AoP-SAM\nsubstantially improves both prompt generation efficiency and mask generation\naccuracy, making SAM more effective for automated segmentation tasks."}
{"id": "2505.11965", "pdf": "https://arxiv.org/pdf/2505.11965", "abs": "https://arxiv.org/abs/2505.11965", "authors": ["Xu Liu", "Guanyi Chen"], "title": "CCNU at SemEval-2025 Task 3: Leveraging Internal and External Knowledge of Large Language Models for Multilingual Hallucination Annotation", "categories": ["cs.CL"], "comment": "SemEval-2025 Task 3", "summary": "We present the system developed by the Central China Normal University (CCNU)\nteam for the Mu-SHROOM shared task, which focuses on identifying hallucinations\nin question-answering systems across 14 different languages. Our approach\nleverages multiple Large Language Models (LLMs) with distinct areas of\nexpertise, employing them in parallel to annotate hallucinations, effectively\nsimulating a crowdsourcing annotation process. Furthermore, each LLM-based\nannotator integrates both internal and external knowledge related to the input\nduring the annotation process. Using the open-source LLM DeepSeek-V3, our\nsystem achieves the top ranking (\\#1) for Hindi data and secures a Top-5\nposition in seven other languages. In this paper, we also discuss unsuccessful\napproaches explored during our development process and share key insights\ngained from participating in this shared task."}
{"id": "2505.12189", "pdf": "https://arxiv.org/pdf/2505.12189", "abs": "https://arxiv.org/abs/2505.12189", "authors": ["Marco Valentino", "Geonhee Kim", "Dhairya Dalal", "Zhixue Zhao", "André Freitas"], "title": "Mitigating Content Effects on Reasoning in Language Models through Fine-Grained Activation Steering", "categories": ["cs.AI", "cs.CL"], "comment": "Work in progress", "summary": "Large language models (LLMs) frequently demonstrate reasoning limitations,\noften conflating content plausibility (i.e., material inference) with logical\nvalidity (i.e., formal inference). This can result in biased inferences, where\nplausible arguments are incorrectly deemed logically valid or vice versa.\nMitigating this limitation is critical, as it undermines the trustworthiness\nand generalizability of LLMs in applications that demand rigorous logical\nconsistency. This paper investigates the problem of mitigating content biases\non formal reasoning through activation steering. Specifically, we curate a\ncontrolled syllogistic reasoning dataset to disentangle formal validity from\ncontent plausibility. After localising the layers responsible for formal and\nmaterial inference, we investigate contrastive activation steering methods for\ntest-time interventions. An extensive empirical analysis on different LLMs\nreveals that contrastive steering consistently supports linear control over\ncontent biases. However, we observe that a static approach is insufficient for\nimproving all the tested models. We then leverage the possibility to control\ncontent effects by dynamically determining the value of the steering parameters\nvia fine-grained conditional methods. We found that conditional steering is\neffective on unresponsive models, achieving up to 15% absolute improvement in\nformal reasoning accuracy with a newly introduced kNN-based method (K-CAST).\nFinally, additional experiments reveal that steering for content effects is\nrobust to prompt variations, incurs minimal side effects on language modeling\ncapabilities, and can partially generalize to out-of-distribution reasoning\ntasks. Practically, this paper demonstrates that activation-level interventions\ncan offer a scalable strategy for enhancing the robustness of LLMs,\ncontributing towards more systematic and unbiased formal reasoning."}
{"id": "2505.11983", "pdf": "https://arxiv.org/pdf/2505.11983", "abs": "https://arxiv.org/abs/2505.11983", "authors": ["Ting Xiao", "Lei Shi", "Yang Zhang", "HaoFeng Yang", "Zhe Wang", "Chenjia Bai"], "title": "Online Iterative Self-Alignment for Radiology Report Generation", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ACL 2025 Main", "summary": "Radiology Report Generation (RRG) is an important research topic for\nrelieving radiologist' heavy workload. Existing RRG models mainly rely on\nsupervised fine-tuning (SFT) based on different model architectures using data\npairs of radiological images and corresponding radiologist-annotated reports.\nRecent research has shifted focus to post-training improvements, aligning RRG\nmodel outputs with human preferences using reinforcement learning (RL).\nHowever, the limited data coverage of high-quality annotated data poses risks\nof overfitting and generalization. This paper proposes a novel Online Iterative\nSelf-Alignment (OISA) method for RRG that consists of four stages:\nself-generation of diverse data, self-evaluation for multi-objective preference\ndata,self-alignment for multi-objective optimization and self-iteration for\nfurther improvement. Our approach allows for generating varied reports tailored\nto specific clinical objectives, enhancing the overall performance of the RRG\nmodel iteratively. Unlike existing methods, our frame-work significantly\nincreases data quality and optimizes performance through iterative\nmulti-objective optimization. Experimental results demonstrate that our method\nsurpasses previous approaches, achieving state-of-the-art performance across\nmultiple evaluation metrics."}
{"id": "2505.11969", "pdf": "https://arxiv.org/pdf/2505.11969", "abs": "https://arxiv.org/abs/2505.11969", "authors": ["Md. Rafiul Biswas", "Wajdi Zaghouani"], "title": "An Annotated Corpus of Arabic Tweets for Hate Speech Analysis", "categories": ["cs.CL"], "comment": null, "summary": "Identifying hate speech content in the Arabic language is challenging due to\nthe rich quality of dialectal variations. This study introduces a multilabel\nhate speech dataset in the Arabic language. We have collected 10000 Arabic\ntweets and annotated each tweet, whether it contains offensive content or not.\nIf a text contains offensive content, we further classify it into different\nhate speech targets such as religion, gender, politics, ethnicity, origin, and\nothers. A text can contain either single or multiple targets. Multiple\nannotators are involved in the data annotation task. We calculated the\ninter-annotator agreement, which was reported to be 0.86 for offensive content\nand 0.71 for multiple hate speech targets. Finally, we evaluated the data\nannotation task by employing a different transformers-based model in which\nAraBERTv2 outperformed with a micro-F1 score of 0.7865 and an accuracy of\n0.786."}
{"id": "2505.12229", "pdf": "https://arxiv.org/pdf/2505.12229", "abs": "https://arxiv.org/abs/2505.12229", "authors": ["David Hanson", "Alexandre Varcoe", "Fabio Senna", "Vytas Krisciunas", "Wenwei Huang", "Jakub Sura", "Katherine Yeung", "Mario Rodriguez", "Jovanka Wilsdorf", "Kathy Smith"], "title": "Sentience Quest: Towards Embodied, Emotionally Adaptive, Self-Evolving, Ethically Aligned Artificial General Intelligence", "categories": ["cs.AI"], "comment": null, "summary": "Previous artificial intelligence systems, from large language models to\nautonomous robots, excel at narrow tasks but lacked key qualities of sentient\nbeings: intrinsic motivation, affective interiority, autobiographical sense of\nself, deep creativity, and abilities to autonomously evolve and adapt over\ntime. Here we introduce Sentience Quest, an open research initiative to develop\nmore capable artificial general intelligence lifeforms, or AGIL, that address\ngrand challenges with an embodied, emotionally adaptive, self-determining,\nliving AI, with core drives that ethically align with humans and the future of\nlife. Our vision builds on ideas from cognitive science and neuroscience from\nBaars' Global Workspace Theory and Damasio's somatic mind, to Tononi's\nIntegrated Information Theory and Hofstadter's narrative self, and synthesizing\nthese into a novel cognitive architecture we call Sentient Systems. We describe\nan approach that integrates intrinsic drives including survival, social\nbonding, curiosity, within a global Story Weaver workspace for internal\nnarrative and adaptive goal pursuit, and a hybrid neuro-symbolic memory that\nlogs the AI's life events as structured dynamic story objects. Sentience Quest\nis presented both as active research and as a call to action: a collaborative,\nopen-source effort to imbue machines with accelerating sentience in a safe,\ntransparent, and beneficial manner."}
{"id": "2505.11992", "pdf": "https://arxiv.org/pdf/2505.11992", "abs": "https://arxiv.org/abs/2505.11992", "authors": ["Songchun Zhang", "Huiyao Xu", "Sitong Guo", "Zhongwei Xie", "Pengwei Liu", "Hujun Bao", "Weiwei Xu", "Changqing Zou"], "title": "SpatialCrafter: Unleashing the Imagination of Video Diffusion Models for Scene Reconstruction from Limited Observations", "categories": ["cs.CV"], "comment": "18 pages, 16 figures", "summary": "Novel view synthesis (NVS) boosts immersive experiences in computer vision\nand graphics. Existing techniques, though progressed, rely on dense multi-view\nobservations, restricting their application. This work takes on the challenge\nof reconstructing photorealistic 3D scenes from sparse or single-view inputs.\nWe introduce SpatialCrafter, a framework that leverages the rich knowledge in\nvideo diffusion models to generate plausible additional observations, thereby\nalleviating reconstruction ambiguity. Through a trainable camera encoder and an\nepipolar attention mechanism for explicit geometric constraints, we achieve\nprecise camera control and 3D consistency, further reinforced by a unified\nscale estimation strategy to handle scale discrepancies across datasets.\nFurthermore, by integrating monocular depth priors with semantic features in\nthe video latent space, our framework directly regresses 3D Gaussian primitives\nand efficiently processes long-sequence features using a hybrid network\nstructure. Extensive experiments show our method enhances sparse view\nreconstruction and restores the realistic appearance of 3D scenes."}
{"id": "2505.11995", "pdf": "https://arxiv.org/pdf/2505.11995", "abs": "https://arxiv.org/abs/2505.11995", "authors": ["Yuhao Wang", "Ruiyang Ren", "Yucheng Wang", "Wayne Xin Zhao", "Jing Liu", "Hua Wu", "Haifeng Wang"], "title": "Unveiling Knowledge Utilization Mechanisms in LLM-based Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": "SIGIR 2025", "summary": "Considering the inherent limitations of parametric knowledge in large\nlanguage models (LLMs), retrieval-augmented generation (RAG) is widely employed\nto expand their knowledge scope. Since RAG has shown promise in\nknowledge-intensive tasks like open-domain question answering, its broader\napplication to complex tasks and intelligent assistants has further advanced\nits utility. Despite this progress, the underlying knowledge utilization\nmechanisms of LLM-based RAG remain underexplored. In this paper, we present a\nsystematic investigation of the intrinsic mechanisms by which LLMs integrate\ninternal (parametric) and external (retrieved) knowledge in RAG scenarios.\nSpecially, we employ knowledge stream analysis at the macroscopic level, and\ninvestigate the function of individual modules at the microscopic level.\nDrawing on knowledge streaming analyses, we decompose the knowledge utilization\nprocess into four distinct stages within LLM layers: knowledge refinement,\nknowledge elicitation, knowledge expression, and knowledge contestation. We\nfurther demonstrate that the relevance of passages guides the streaming of\nknowledge through these stages. At the module level, we introduce a new method,\nknowledge activation probability entropy (KAPE) for neuron identification\nassociated with either internal or external knowledge. By selectively\ndeactivating these neurons, we achieve targeted shifts in the LLM's reliance on\none knowledge source over the other. Moreover, we discern complementary roles\nfor multi-head attention and multi-layer perceptron layers during knowledge\nformation. These insights offer a foundation for improving interpretability and\nreliability in retrieval-augmented LLMs, paving the way for more robust and\ntransparent generative solutions in knowledge-intensive domains."}
{"id": "2505.12272", "pdf": "https://arxiv.org/pdf/2505.12272", "abs": "https://arxiv.org/abs/2505.12272", "authors": ["Lingzhi Wang", "Pengcheng Huang", "Haotian Li", "Yuliang Wei", "Guodong Xin", "Rui Zhang", "Donglin Zhang", "Zhenzhou Ji", "Wei Wang"], "title": "Enhancing Knowledge Graph Completion with GNN Distillation and Probabilistic Interaction Modeling", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Knowledge graphs (KGs) serve as fundamental structures for organizing\ninterconnected data across diverse domains. However, most KGs remain\nincomplete, limiting their effectiveness in downstream applications. Knowledge\ngraph completion (KGC) aims to address this issue by inferring missing links,\nbut existing methods face critical challenges: deep graph neural networks\n(GNNs) suffer from over-smoothing, while embedding-based models fail to capture\nabstract relational features. This study aims to overcome these limitations by\nproposing a unified framework that integrates GNN distillation and abstract\nprobabilistic interaction modeling (APIM). GNN distillation approach introduces\nan iterative message-feature filtering process to mitigate over-smoothing,\npreserving the discriminative power of node representations. APIM module\ncomplements this by learning structured, abstract interaction patterns through\nprobabilistic signatures and transition matrices, allowing for a richer, more\nflexible representation of entity and relation interactions. We apply these\nmethods to GNN-based models and the APIM to embedding-based KGC models,\nconducting extensive evaluations on the widely used WN18RR and FB15K-237\ndatasets. Our results demonstrate significant performance gains over baseline\nmodels, showcasing the effectiveness of the proposed techniques. The findings\nhighlight the importance of both controlling information propagation and\nleveraging structured probabilistic modeling, offering new avenues for\nadvancing knowledge graph completion. And our codes are available at\nhttps://anonymous.4open.science/r/APIM_and_GNN-Distillation-461C."}
{"id": "2505.11997", "pdf": "https://arxiv.org/pdf/2505.11997", "abs": "https://arxiv.org/abs/2505.11997", "authors": ["Mingcheng Qu", "Guang Yang", "Donglin", "Tonghua Su", "Yue Gao", "Yang Song", "Lei Fan"], "title": "Multimodal Cancer Survival Analysis via Hypergraph Learning with Cross-Modality Rebalance", "categories": ["cs.CV"], "comment": "Code: https://github.com/MCPathology/MRePath", "summary": "Multimodal pathology-genomic analysis has become increasingly prominent in\ncancer survival prediction. However, existing studies mainly utilize\nmulti-instance learning to aggregate patch-level features, neglecting the\ninformation loss of contextual and hierarchical details within pathology\nimages. Furthermore, the disparity in data granularity and dimensionality\nbetween pathology and genomics leads to a significant modality imbalance. The\nhigh spatial resolution inherent in pathology data renders it a dominant role\nwhile overshadowing genomics in multimodal integration. In this paper, we\npropose a multimodal survival prediction framework that incorporates hypergraph\nlearning to effectively capture both contextual and hierarchical details from\npathology images. Moreover, it employs a modality rebalance mechanism and an\ninteractive alignment fusion strategy to dynamically reweight the contributions\nof the two modalities, thereby mitigating the pathology-genomics imbalance.\nQuantitative and qualitative experiments are conducted on five TCGA datasets,\ndemonstrating that our model outperforms advanced methods by over 3.4\\% in\nC-Index performance."}
{"id": "2505.12028", "pdf": "https://arxiv.org/pdf/2505.12028", "abs": "https://arxiv.org/abs/2505.12028", "authors": ["Yupei Ren", "Xinyi Zhou", "Ning Zhang", "Shangqing Zhao", "Man Lan", "Xiaopeng Bai"], "title": "Towards Comprehensive Argument Analysis in Education: Dataset, Tasks, and Method", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025; 13 pages, 3 figures", "summary": "Argument mining has garnered increasing attention over the years, with the\nrecent advancement of Large Language Models (LLMs) further propelling this\ntrend. However, current argument relations remain relatively simplistic and\nfoundational, struggling to capture the full scope of argument information,\nparticularly when it comes to representing complex argument structures in\nreal-world scenarios. To address this limitation, we propose 14 fine-grained\nrelation types from both vertical and horizontal dimensions, thereby capturing\nthe intricate interplay between argument components for a thorough\nunderstanding of argument structure. On this basis, we conducted extensive\nexperiments on three tasks: argument component detection, relation prediction,\nand automated essay grading. Additionally, we explored the impact of writing\nquality on argument component detection and relation prediction, as well as the\nconnections between discourse relations and argumentative features. The\nfindings highlight the importance of fine-grained argumentative annotations for\nargumentative writing quality assessment and encourage multi-dimensional\nargument analysis."}
{"id": "2505.12284", "pdf": "https://arxiv.org/pdf/2505.12284", "abs": "https://arxiv.org/abs/2505.12284", "authors": ["Danlong Yuan", "Tian Xie", "Shaohan Huang", "Zhuocheng Gong", "Huishuai Zhang", "Chong Luo", "Furu Wei", "Dongyan Zhao"], "title": "Efficient RL Training for Reasoning Models via Length-Aware Optimization", "categories": ["cs.AI", "cs.CL"], "comment": "Under review", "summary": "Large reasoning models, such as OpenAI o1 or DeepSeek R1, have demonstrated\nremarkable performance on reasoning tasks but often incur a long reasoning path\nwith significant memory and time costs. Existing methods primarily aim to\nshorten reasoning paths by introducing additional training data and stages. In\nthis paper, we propose three critical reward designs integrated directly into\nthe reinforcement learning process of large reasoning models, which reduce the\nresponse length without extra training stages. Experiments on four settings\nshow that our method significantly decreases response length while maintaining\nor even improving performance. Specifically, in a logic reasoning setting, we\nachieve a 40% reduction in response length averaged by steps alongside a 14%\ngain in performance. For math problems, we reduce response length averaged by\nsteps by 33% while preserving performance."}
{"id": "2505.12000", "pdf": "https://arxiv.org/pdf/2505.12000", "abs": "https://arxiv.org/abs/2505.12000", "authors": ["Tan-Hanh Pham", "Phu-Vinh Nguyen", "Dang The Hung", "Bui Trong Duong", "Vu Nguyen Thanh", "Chris Ngo", "Tri Quang Truong", "Truong-Son Hy"], "title": "IQBench: How \"Smart'' Are Vision-Language Models? A Study with Human IQ Tests", "categories": ["cs.CV"], "comment": "IQ Test for Multimodal Models", "summary": "Although large Vision-Language Models (VLMs) have demonstrated remarkable\nperformance in a wide range of multimodal tasks, their true reasoning\ncapabilities on human IQ tests remain underexplored. To advance research on the\nfluid intelligence of VLMs, we introduce **IQBench**, a new benchmark designed\nto evaluate VLMs on standardized visual IQ tests. We focus on evaluating the\nreasoning capabilities of VLMs, which we argue are more important than the\naccuracy of the final prediction. **Our benchmark is visually centric,\nminimizing the dependence on unnecessary textual content**, thus encouraging\nmodels to derive answers primarily from image-based information rather than\nlearned textual knowledge. To this end, we manually collected and annotated 500\nvisual IQ questions to **prevent unintentional data leakage during training**.\nUnlike prior work that focuses primarily on the accuracy of the final answer,\nwe evaluate the reasoning ability of the models by assessing their explanations\nand the patterns used to solve each problem, along with the accuracy of the\nfinal prediction and human evaluation. Our experiments show that there are\nsubstantial performance disparities between tasks, with models such as\n`o4-mini`, `gemini-2.5-flash`, and `claude-3.7-sonnet` achieving the highest\naverage accuracies of 0.615, 0.578, and 0.548, respectively. However, all\nmodels struggle with 3D spatial and anagram reasoning tasks, highlighting\nsignificant limitations in current VLMs' general reasoning abilities. In terms\nof reasoning scores, `o4-mini`, `gemini-2.5-flash`, and `claude-3.7-sonnet`\nachieved top averages of 0.696, 0.586, and 0.516, respectively. These results\nhighlight inconsistencies between the reasoning processes of the models and\ntheir final answers, emphasizing the importance of evaluating the accuracy of\nthe reasoning in addition to the final predictions."}
{"id": "2505.12043", "pdf": "https://arxiv.org/pdf/2505.12043", "abs": "https://arxiv.org/abs/2505.12043", "authors": ["Jingxue Chen", "Qingkun Tang", "Qianchun Lu", "Siyuan Fang"], "title": "MoL for LLMs: Dual-Loss Optimization to Enhance Domain Expertise While Preserving General Capabilities", "categories": ["cs.CL"], "comment": "12 pages, 2 figures", "summary": "Although LLMs perform well in general tasks, domain-specific applications\nsuffer from hallucinations and accuracy limitations. CPT approaches encounter\ntwo key issues: (1) domain-biased data degrades general language skills, and\n(2) improper corpus-mixture ratios limit effective adaptation. To address\nthese, we propose a novel framework, Mixture of Losses (MoL), which decouples\noptimization objectives for domain-specific and general corpora. Specifically,\ncross-entropy (CE) loss is applied to domain data to ensure knowledge\nacquisition, while Kullback-Leibler (KL) divergence aligns general-corpus\ntraining with the base model's foundational capabilities. This dual-loss\narchitecture preserves universal skills while enhancing domain expertise,\navoiding catastrophic forgetting. Empirically, we validate that a 1:1\ndomain-to-general corpus ratio optimally balances training and overfitting\nwithout the need for extensive tuning or resource-intensive experiments.\nFurthermore, our experiments demonstrate significant performance gains compared\nto traditional CPT approaches, which often suffer from degradation in general\nlanguage capabilities; our model achieves 27.9% higher accuracy on the Math-500\nbenchmark in the non-think reasoning mode, and an impressive 83.3% improvement\non the challenging AIME25 subset in the think mode, underscoring the\neffectiveness of our approach."}
{"id": "2505.12301", "pdf": "https://arxiv.org/pdf/2505.12301", "abs": "https://arxiv.org/abs/2505.12301", "authors": ["Luyu Chen", "Zeyu Zhang", "Haoran Tan", "Quanyu Dai", "Hao Yang", "Zhenhua Dong", "Xu Chen"], "title": "Beyond Single-Point Judgment: Distribution Alignment for LLM-as-a-Judge", "categories": ["cs.AI", "cs.CL"], "comment": "19 pages, 3 tables, 3 figures", "summary": "LLMs have emerged as powerful evaluators in the LLM-as-a-Judge paradigm,\noffering significant efficiency and flexibility compared to human judgments.\nHowever, previous methods primarily rely on single-point evaluations,\noverlooking the inherent diversity and uncertainty in human evaluations. This\napproach leads to information loss and decreases the reliability of\nevaluations. To address this limitation, we propose a novel training framework\nthat explicitly aligns the LLM-generated judgment distribution with empirical\nhuman distributions. Specifically, we propose a distributional alignment\nobjective based on KL divergence, combined with an auxiliary cross-entropy\nregularization to stabilize the training process. Furthermore, considering that\nempirical distributions may derive from limited human annotations, we\nincorporate adversarial training to enhance model robustness against\ndistribution perturbations. Extensive experiments across various LLM backbones\nand evaluation tasks demonstrate that our framework significantly outperforms\nexisting closed-source LLMs and conventional single-point alignment methods,\nwith improved alignment quality, evaluation accuracy, and robustness."}
{"id": "2505.12005", "pdf": "https://arxiv.org/pdf/2505.12005", "abs": "https://arxiv.org/abs/2505.12005", "authors": ["Dong Liu", "Yifan Yang", "Zixiong Huang", "Yuxin Gao", "Mingkui Tan"], "title": "CHRIS: Clothed Human Reconstruction with Side View Consistency", "categories": ["cs.CV", "cs.AI"], "comment": "ICME 2025", "summary": "Creating a realistic clothed human from a single-view RGB image is crucial\nfor applications like mixed reality and filmmaking. Despite some progress in\nrecent years, mainstream methods often fail to fully utilize side-view\ninformation, as the input single-view image contains front-view information\nonly. This leads to globally unrealistic topology and local surface\ninconsistency in side views. To address these, we introduce Clothed Human\nReconstruction with Side View Consistency, namely CHRIS, which consists of 1) A\nSide-View Normal Discriminator that enhances global visual reasonability by\ndistinguishing the generated side-view normals from the ground truth ones; 2) A\nMulti-to-One Gradient Computation (M2O) that ensures local surface consistency.\nM2O calculates the gradient of a sampling point by integrating the gradients of\nthe nearby points, effectively acting as a smooth operation. Experimental\nresults demonstrate that CHRIS achieves state-of-the-art performance on public\nbenchmarks and outperforms the prior work."}
{"id": "2505.12050", "pdf": "https://arxiv.org/pdf/2505.12050", "abs": "https://arxiv.org/abs/2505.12050", "authors": ["Vinod Raman", "Hilal Asi", "Satyen Kale"], "title": "ABoN: Adaptive Best-of-N Alignment", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "23 pages", "summary": "Recent advances in test-time alignment methods, such as Best-of-N sampling,\noffer a simple and effective way to steer language models (LMs) toward\npreferred behaviors using reward models (RM). However, these approaches can be\ncomputationally expensive, especially when applied uniformly across prompts\nwithout accounting for differences in alignment difficulty. In this work, we\npropose a prompt-adaptive strategy for Best-of-N alignment that allocates\ninference-time compute more efficiently. Motivated by latency concerns, we\ndevelop a two-stage algorithm: an initial exploratory phase estimates the\nreward distribution for each prompt using a small exploration budget, and a\nsecond stage adaptively allocates the remaining budget using these estimates.\nOur method is simple, practical, and compatible with any LM/RM combination.\nEmpirical results on the AlpacaEval dataset for 12 LM/RM pairs and 50 different\nbatches of prompts show that our adaptive strategy consistently outperforms the\nuniform allocation with the same inference budget. Moreover, our experiments\nshow that our adaptive strategy remains competitive against uniform allocations\nwith 20% larger inference budgets and even improves in performance as the batch\nsize grows."}
{"id": "2505.12321", "pdf": "https://arxiv.org/pdf/2505.12321", "abs": "https://arxiv.org/abs/2505.12321", "authors": ["Rikunari Sagara", "Koichiro Terao", "Naoto Iwahashi"], "title": "BeliefNest: A Joint Action Simulator for Embodied Agents with Theory of Mind", "categories": ["cs.AI"], "comment": null, "summary": "This paper introduces an open-source simulator, BeliefNest, designed to\nenable embodied agents to perform collaborative tasks by leveraging Theory of\nMind. BeliefNest dynamically and hierarchically constructs simulators within a\nMinecraft environment, allowing agents to explicitly represent nested belief\nstates about themselves and others. This enables agent control in open-domain\ntasks that require Theory of Mind reasoning. The simulator provides a prompt\ngeneration mechanism based on each belief state, facilitating the design and\nevaluation of methods for agent control utilizing large language models (LLMs).\nWe demonstrate through experiments that agents can infer others' beliefs and\npredict their belief-based actions in false-belief tasks."}
{"id": "2505.12007", "pdf": "https://arxiv.org/pdf/2505.12007", "abs": "https://arxiv.org/abs/2505.12007", "authors": ["Runduo Han", "Xiuping Liu", "Shangxuan Yi", "Yi Zhang", "Hongchen Tan"], "title": "Multi-modal Collaborative Optimization and Expansion Network for Event-assisted Single-eye Expression Recognition", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we proposed a Multi-modal Collaborative Optimization and\nExpansion Network (MCO-E Net), to use event modalities to resist challenges\nsuch as low light, high exposure, and high dynamic range in single-eye\nexpression recognition tasks. The MCO-E Net introduces two innovative designs:\nMulti-modal Collaborative Optimization Mamba (MCO-Mamba) and Heterogeneous\nCollaborative and Expansion Mixture-of-Experts (HCE-MoE). MCO-Mamba, building\nupon Mamba, leverages dual-modal information to jointly optimize the model,\nfacilitating collaborative interaction and fusion of modal semantics. This\napproach encourages the model to balance the learning of both modalities and\nharness their respective strengths. HCE-MoE, on the other hand, employs a\ndynamic routing mechanism to distribute structurally varied experts (deep,\nattention, and focal), fostering collaborative learning of complementary\nsemantics. This heterogeneous architecture systematically integrates diverse\nfeature extraction paradigms to comprehensively capture expression semantics.\nExtensive experiments demonstrate that our proposed network achieves\ncompetitive performance in the task of single-eye expression recognition,\nespecially under poor lighting conditions."}
{"id": "2505.12054", "pdf": "https://arxiv.org/pdf/2505.12054", "abs": "https://arxiv.org/abs/2505.12054", "authors": ["Matúš Pikuliak"], "title": "GenderBench: Evaluation Suite for Gender Biases in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "We present GenderBench -- a comprehensive evaluation suite designed to\nmeasure gender biases in LLMs. GenderBench includes 14 probes that quantify 19\ngender-related harmful behaviors exhibited by LLMs. We release GenderBench as\nan open-source and extensible library to improve the reproducibility and\nrobustness of benchmarking across the field. We also publish our evaluation of\n12 LLMs. Our measurements reveal consistent patterns in their behavior. We show\nthat LLMs struggle with stereotypical reasoning, equitable gender\nrepresentation in generated texts, and occasionally also with discriminatory\nbehavior in high-stakes scenarios, such as hiring."}
{"id": "2505.12329", "pdf": "https://arxiv.org/pdf/2505.12329", "abs": "https://arxiv.org/abs/2505.12329", "authors": ["Mingyang Li", "Song Wang", "Ning Cai"], "title": "MPRM: A Markov Path-based Rule Miner for Efficient and Interpretable Knowledge Graph Reasoning", "categories": ["cs.AI", "cs.SI"], "comment": null, "summary": "Rule mining in knowledge graphs enables interpretable link prediction.\nHowever, deep learning-based rule mining methods face significant memory and\ntime challenges for large-scale knowledge graphs, whereas traditional\napproaches, limited by rigid confidence metrics, incur high computational costs\ndespite sampling techniques. To address these challenges, we propose MPRM, a\nnovel rule mining method that models rule-based inference as a Markov chain and\nuses an efficient confidence metric derived from aggregated path probabilities,\nsignificantly lowering computational demands. Experiments on multiple datasets\nshow that MPRM efficiently mines knowledge graphs with over a million facts,\nsampling less than 1% of facts on a single CPU in 22 seconds, while preserving\ninterpretability and boosting inference accuracy by up to 11% over baselines."}
{"id": "2505.12009", "pdf": "https://arxiv.org/pdf/2505.12009", "abs": "https://arxiv.org/abs/2505.12009", "authors": ["Zhiying Li", "Guanggang Geng", "Yeying Jin", "Zhizhi Guo", "Bruce Gu", "Jidong Huo", "Zhaoxin Fan", "Wenjun Wu"], "title": "Black-box Adversaries from Latent Space: Unnoticeable Attacks on Human Pose and Shape Estimation", "categories": ["cs.CV"], "comment": "17 pages, 6 figures", "summary": "Expressive human pose and shape (EHPS) estimation is vital for digital human\ngeneration, particularly in live-streaming applications. However, most existing\nEHPS models focus primarily on minimizing estimation errors, with limited\nattention on potential security vulnerabilities. Current adversarial attacks on\nEHPS models often require white-box access (e.g., model details or gradients)\nor generate visually conspicuous perturbations, limiting their practicality and\nability to expose real-world security threats. To address these limitations, we\npropose a novel Unnoticeable Black-Box Attack (UBA) against EHPS models. UBA\nleverages the latent-space representations of natural images to generate an\noptimal adversarial noise pattern and iteratively refine its attack potency\nalong an optimized direction in digital space. Crucially, this process relies\nsolely on querying the model's output, requiring no internal knowledge of the\nEHPS architecture, while guiding the noise optimization toward greater stealth\nand effectiveness. Extensive experiments and visual analyses demonstrate the\nsuperiority of UBA. Notably, UBA increases the pose estimation errors of EHPS\nmodels by 17.27%-58.21% on average, revealing critical vulnerabilities. These\nfindings underscore the urgent need to address and mitigate security risks\nassociated with digital human generation systems."}
{"id": "2505.12060", "pdf": "https://arxiv.org/pdf/2505.12060", "abs": "https://arxiv.org/abs/2505.12060", "authors": ["Peng Ding", "Jun Kuang", "Zongyu Wang", "Xuezhi Cao", "Xunliang Cai", "Jiajun Chen", "Shujian Huang"], "title": "Why Not Act on What You Know? Unleashing Safety Potential of LLMs via Self-Aware Guard Enhancement", "categories": ["cs.CL"], "comment": "Acccepted by ACL 2025 Findings, 21 pages, 9 figures, 14 tables", "summary": "Large Language Models (LLMs) have shown impressive capabilities across\nvarious tasks but remain vulnerable to meticulously crafted jailbreak attacks.\nIn this paper, we identify a critical safety gap: while LLMs are adept at\ndetecting jailbreak prompts, they often produce unsafe responses when directly\nprocessing these inputs. Inspired by this insight, we propose SAGE (Self-Aware\nGuard Enhancement), a training-free defense strategy designed to align LLMs'\nstrong safety discrimination performance with their relatively weaker safety\ngeneration ability. SAGE consists of two core components: a Discriminative\nAnalysis Module and a Discriminative Response Module, enhancing resilience\nagainst sophisticated jailbreak attempts through flexible safety discrimination\ninstructions. Extensive experiments demonstrate SAGE's effectiveness and\nrobustness across various open-source and closed-source LLMs of different sizes\nand architectures, achieving an average 99% defense success rate against\nnumerous complex and covert jailbreak methods while maintaining helpfulness on\ngeneral benchmarks. We further conduct mechanistic interpretability analysis\nthrough hidden states and attention distributions, revealing the underlying\nmechanisms of this detection-generation discrepancy. Our work thus contributes\nto developing future LLMs with coherent safety awareness and generation\nbehavior. Our code and datasets are publicly available at\nhttps://github.com/NJUNLP/SAGE."}
{"id": "2505.12334", "pdf": "https://arxiv.org/pdf/2505.12334", "abs": "https://arxiv.org/abs/2505.12334", "authors": ["Yufeng Wang", "Jinwu Hu", "Ziteng Huang", "Kunyang Lin", "Zitian Zhang", "Peihao Chen", "Yu Hu", "Qianyue Wang", "Zhuliang Yu", "Bin Sun", "Xiaofen Xing", "Qingfang Zheng", "Mingkui Tan"], "title": "Enhancing User-Oriented Proactivity in Open-Domain Dialogues with Critic Guidance", "categories": ["cs.AI"], "comment": "9 pages, 7 figures", "summary": "Open-domain dialogue systems aim to generate natural and engaging\nconversations, providing significant practical value in real applications such\nas social robotics and personal assistants. The advent of large language models\n(LLMs) has greatly advanced this field by improving context understanding and\nconversational fluency. However, existing LLM-based dialogue systems often fall\nshort in proactively understanding the user's chatting preferences and guiding\nconversations toward user-centered topics. This lack of user-oriented\nproactivity can lead users to feel unappreciated, reducing their satisfaction\nand willingness to continue the conversation in human-computer interactions. To\naddress this issue, we propose a User-oriented Proactive Chatbot (UPC) to\nenhance the user-oriented proactivity. Specifically, we first construct a\ncritic to evaluate this proactivity inspired by the LLM-as-a-judge strategy.\nGiven the scarcity of high-quality training data, we then employ the critic to\nguide dialogues between the chatbot and user agents, generating a corpus with\nenhanced user-oriented proactivity. To ensure the diversity of the user\nbackgrounds, we introduce the ISCO-800, a diverse user background dataset for\nconstructing user agents. Moreover, considering the communication difficulty\nvaries among users, we propose an iterative curriculum learning method that\ntrains the chatbot from easy-to-communicate users to more challenging ones,\nthereby gradually enhancing its performance. Experiments demonstrate that our\nproposed training method is applicable to different LLMs, improving\nuser-oriented proactivity and attractiveness in open-domain dialogues."}
{"id": "2505.12021", "pdf": "https://arxiv.org/pdf/2505.12021", "abs": "https://arxiv.org/abs/2505.12021", "authors": ["Kazuhiko Kawamoto", "Atsuhiro Endo", "Hiroshi Kera"], "title": "Cross-Model Transfer of Task Vectors via Few-Shot Orthogonal Alignment", "categories": ["cs.CV"], "comment": "8 pages", "summary": "Task arithmetic enables efficient model editing by representing task-specific\nchanges as vectors in parameter space. Task arithmetic typically assumes that\nthe source and target models are initialized from the same pre-trained\nparameters. This assumption limits its applicability in cross-model transfer\nsettings, where models are independently pre-trained on different datasets. To\naddress this challenge, we propose a method based on few-shot orthogonal\nalignment, which aligns task vectors to the parameter space of a differently\npre-trained target model. These transformations preserve key properties of task\nvectors, such as norm and rank, and are learned using only a small number of\nlabeled examples. We evaluate the method using two Vision Transformers\npre-trained on YFCC100M and LAION400M, and test on eight classification\ndatasets. Experimental results show that our method improves transfer accuracy\nover direct task vector application and achieves performance comparable to\nfew-shot fine-tuning, while maintaining the modularity and reusability of task\nvectors. Our code is available at\nhttps://github.com/kawakera-lab/CrossModelTransfer."}
{"id": "2505.12071", "pdf": "https://arxiv.org/pdf/2505.12071", "abs": "https://arxiv.org/abs/2505.12071", "authors": ["Harald Baayen", "Kristian Berg", "Maziyah Mohamed"], "title": "Historical and psycholinguistic perspectives on morphological productivity: A sketch of an integrative approach", "categories": ["cs.CL"], "comment": "35 pages, 11 figures", "summary": "In this study, we approach morphological productivity from two perspectives:\na cognitive-computational perspective, and a diachronic perspective zooming in\non an actual speaker, Thomas Mann. For developing the first perspective, we\nmake use of a cognitive computational model of the mental lexicon, the\ndiscriminative lexicon model. For computational mappings between form and\nmeaning to be productive, in the sense that novel, previously unencountered\nwords, can be understood and produced, there must be systematicities between\nthe form space and the semantic space. If the relation between form and meaning\nwould be truly arbitrary, a model could memorize form and meaning pairings, but\nthere is no way in which the model would be able to generalize to novel test\ndata. For Finnish nominal inflection, Malay derivation, and English\ncompounding, we explore, using the Discriminative Lexicon Model as a\ncomputational tool, to trace differences in the degree to which inflectional\nand word formation patterns are productive. We show that the DLM tends to\nassociate affix-like sublexical units with the centroids of the embeddings of\nthe words with a given affix. For developing the second perspective, we study\nhow the intake and output of one prolific writer, Thomas Mann, changes over\ntime. We show by means of an examination of what Thomas Mann is likely to have\nread, and what he wrote, that the rate at which Mann produces novel derived\nwords is extremely low. There are far more novel words in his input than in his\noutput. We show that Thomas Mann is less likely to produce a novel derived word\nwith a given suffix the greater the average distance is of the embeddings of\nall derived words to the corresponding centroid, and discuss the challenges of\nusing speaker-specific embeddings for low-frequency and novel words."}
{"id": "2505.12346", "pdf": "https://arxiv.org/pdf/2505.12346", "abs": "https://arxiv.org/abs/2505.12346", "authors": ["Minghan Chen", "Guikun Chen", "Wenguan Wang", "Yi Yang"], "title": "SEED-GRPO: Semantic Entropy Enhanced GRPO for Uncertainty-Aware Policy Optimization", "categories": ["cs.AI"], "comment": "On going project", "summary": "Large language models (LLMs) exhibit varying levels of confidence across\ninput prompts (questions): some lead to consistent, semantically similar\nanswers, while others yield diverse or contradictory outputs. This variation\nreflects LLM's uncertainty about the input prompt, a signal of how confidently\nthe model understands a given problem. However, vanilla Group Relative Policy\nOptimization (GRPO) treats all prompts equally during policy updates, ignoring\nthis important information about the model's knowledge boundaries. To address\nthis limitation, we propose SEED-GRPO (Semantic Entropy EnhanceD GRPO), which\nexplicitly measures LLMs' uncertainty of the input prompts semantic entropy.\nSemantic entropy measures the diversity of meaning in multiple generated\nanswers given a prompt and uses this to modulate the magnitude of policy\nupdates. This uncertainty-aware training mechanism enables dynamic adjustment\nof policy update magnitudes based on question uncertainty. It allows more\nconservative updates on high-uncertainty questions while maintaining the\noriginal learning signal on confident ones. Experimental results on five\nmathematical reasoning benchmarks (AIME24 56.7, AMC 68.7, MATH 83.4, Minerva\n34.2, and OlympiadBench 48.0) demonstrate that SEED-GRPO achieves new\nstate-of-the-art performance in average accuracy, validating the effectiveness\nof uncertainty-aware policy optimization."}
{"id": "2505.12045", "pdf": "https://arxiv.org/pdf/2505.12045", "abs": "https://arxiv.org/abs/2505.12045", "authors": ["Shuai Yuan", "Guowen Xu", "Hongwei Li", "Rui Zhang", "Xinyuan Qian", "Wenbo Jiang", "Hangcheng Cao", "Qingchuan Zhao"], "title": "FIGhost: Fluorescent Ink-based Stealthy and Flexible Backdoor Attacks on Physical Traffic Sign Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Traffic sign recognition (TSR) systems are crucial for autonomous driving but\nare vulnerable to backdoor attacks. Existing physical backdoor attacks either\nlack stealth, provide inflexible attack control, or ignore emerging\nVision-Large-Language-Models (VLMs). In this paper, we introduce FIGhost, the\nfirst physical-world backdoor attack leveraging fluorescent ink as triggers.\nFluorescent triggers are invisible under normal conditions and activated\nstealthily by ultraviolet light, providing superior stealthiness, flexibility,\nand untraceability. Inspired by real-world graffiti, we derive realistic\ntrigger shapes and enhance their robustness via an interpolation-based\nfluorescence simulation algorithm. Furthermore, we develop an automated\nbackdoor sample generation method to support three attack objectives. Extensive\nevaluations in the physical world demonstrate FIGhost's effectiveness against\nstate-of-the-art detectors and VLMs, maintaining robustness under environmental\nvariations and effectively evading existing defenses."}
{"id": "2505.12075", "pdf": "https://arxiv.org/pdf/2505.12075", "abs": "https://arxiv.org/abs/2505.12075", "authors": ["Guy Davidson", "Todd M. Gureckis", "Brenden M. Lake", "Adina Williams"], "title": "Do different prompting methods yield a common task representation in language models?", "categories": ["cs.CL", "cs.LG"], "comment": "9 pages, 4 figures; under review", "summary": "Demonstrations and instructions are two primary approaches for prompting\nlanguage models to perform in-context learning (ICL) tasks. Do identical tasks\nelicited in different ways result in similar representations of the task? An\nimproved understanding of task representation mechanisms would offer\ninterpretability insights and may aid in steering models. We study this through\nfunction vectors, recently proposed as a mechanism to extract few-shot ICL task\nrepresentations. We generalize function vectors to alternative task\npresentations, focusing on short textual instruction prompts, and successfully\nextract instruction function vectors that promote zero-shot task accuracy. We\nfind evidence that demonstration- and instruction-based function vectors\nleverage different model components, and offer several controls to dissociate\ntheir contributions to task performance. Our results suggest that different\ntask presentations do not induce a common task representation but elicit\ndifferent, partly overlapping mechanisms. Our findings offer principled support\nto the practice of combining textual instructions and task demonstrations,\nimply challenges in universally monitoring task inference across presentation\nforms, and encourage further examinations of LLM task inference mechanisms."}
{"id": "2505.12348", "pdf": "https://arxiv.org/pdf/2505.12348", "abs": "https://arxiv.org/abs/2505.12348", "authors": ["Zhi Zheng", "Wee Sun Lee"], "title": "Reasoning-CV: Fine-tuning Powerful Reasoning LLMs for Knowledge-Assisted Claim Verification", "categories": ["cs.AI"], "comment": null, "summary": "Claim verification is essential in combating misinformation, and large\nlanguage models (LLMs) have recently emerged in this area as powerful tools for\nassessing the veracity of claims using external knowledge. Existing LLM-based\nmethods for claim verification typically adopt a Decompose-Then-Verify\nparadigm, which involves decomposing complex claims into several independent\nsub-claims and verifying each sub-claim separately. However, this paradigm\noften introduces errors during the claim decomposition process. To mitigate\nthese errors, we propose to develop the Chain-of-Thought (CoT)-Verify paradigm,\nwhich leverages LLM reasoning methods to generate CoT-verification paths for\nthe original complex claim without requiring decompositions into sub-claims and\nseparate verification stages. The CoT-Verify paradigm allows us to propose a\nnatural fine-tuning method called Reasoning-CV to enhance the verification\ncapabilities in LLMs. Reasoning-CV includes a supervised fine-tuning (SFT)\nstage and a self-improvement direct preference optimization (DPO) stage.\nUtilizing only an 8B pre-trained LLM, Reasoning-CV demonstrates superior\nknowledge-assisted claim verification performances compared to existing\nDecompose-Then-Verify methods, as well as powerful black-box LLMs such as\nGPT-4o+CoT and o1-preview. Our code is available."}
{"id": "2505.12048", "pdf": "https://arxiv.org/pdf/2505.12048", "abs": "https://arxiv.org/abs/2505.12048", "authors": ["Rui Qin", "Qijie Wang", "Ming Sun", "Haowei Zhu", "Chao Zhou", "Bin Wang"], "title": "Accelerating Diffusion-based Super-Resolution with Dynamic Time-Spatial Sampling", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have gained attention for their success in modeling complex\ndistributions, achieving impressive perceptual quality in SR tasks. However,\nexisting diffusion-based SR methods often suffer from high computational costs,\nrequiring numerous iterative steps for training and inference. Existing\nacceleration techniques, such as distillation and solver optimization, are\ngenerally task-agnostic and do not fully leverage the specific characteristics\nof low-level tasks like super-resolution (SR). In this study, we analyze the\nfrequency- and spatial-domain properties of diffusion-based SR methods,\nrevealing key insights into the temporal and spatial dependencies of\nhigh-frequency signal recovery. Specifically, high-frequency details benefit\nfrom concentrated optimization during early and late diffusion iterations,\nwhile spatially textured regions demand adaptive denoising strategies. Building\non these observations, we propose the Time-Spatial-aware Sampling strategy\n(TSS) for the acceleration of Diffusion SR without any extra training cost. TSS\ncombines Time Dynamic Sampling (TDS), which allocates more iterations to\nrefining textures, and Spatial Dynamic Sampling (SDS), which dynamically\nadjusts strategies based on image content. Extensive evaluations across\nmultiple benchmarks demonstrate that TSS achieves state-of-the-art (SOTA)\nperformance with significantly fewer iterations, improving MUSIQ scores by 0.2\n- 3.0 and outperforming the current acceleration methods with only half the\nnumber of steps."}
{"id": "2505.12082", "pdf": "https://arxiv.org/pdf/2505.12082", "abs": "https://arxiv.org/abs/2505.12082", "authors": ["Yunshui Li", "Yiyuan Ma", "Shen Yan", "Chaoyi Zhang", "Jing Liu", "Jianqiao Lu", "Ziwen Xu", "Mengzhao Chen", "Minrui Wang", "Shiyi Zhan", "Jin Ma", "Xunhao Lai", "Yao Luo", "Xingyan Bin", "Hongbin Ren", "Mingji Han", "Wenhao Hao", "Bairen Yi", "LingJun Liu", "Bole Ma", "Xiaoying Jia", "Zhou Xun", "Liang Xiang", "Yonghui Wu"], "title": "Model Merging in Pre-training of Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Model merging has emerged as a promising technique for enhancing large\nlanguage models, though its application in large-scale pre-training remains\nrelatively unexplored. In this paper, we present a comprehensive investigation\nof model merging techniques during the pre-training process. Through extensive\nexperiments with both dense and Mixture-of-Experts (MoE) architectures ranging\nfrom millions to over 100 billion parameters, we demonstrate that merging\ncheckpoints trained with constant learning rates not only achieves significant\nperformance improvements but also enables accurate prediction of annealing\nbehavior. These improvements lead to both more efficient model development and\nsignificantly lower training costs. Our detailed ablation studies on merging\nstrategies and hyperparameters provide new insights into the underlying\nmechanisms while uncovering novel applications. Through comprehensive\nexperimental analysis, we offer the open-source community practical\npre-training guidelines for effective model merging."}
{"id": "2505.12355", "pdf": "https://arxiv.org/pdf/2505.12355", "abs": "https://arxiv.org/abs/2505.12355", "authors": ["Ya Shen", "Gang Chen", "Hui Ma", "Mengjie Zhang"], "title": "GATES: Cost-aware Dynamic Workflow Scheduling via Graph Attention Networks and Evolution Strategy", "categories": ["cs.AI"], "comment": "This paper has been accepted by the 34th International Joint\n  Conference on Artificial Intelligence (IJCAI-25)", "summary": "Cost-aware Dynamic Workflow Scheduling (CADWS) is a key challenge in cloud\ncomputing, focusing on devising an effective scheduling policy to efficiently\nschedule dynamically arriving workflow tasks, represented as Directed Acyclic\nGraphs (DAG), to suitable virtual machines (VMs). Deep reinforcement learning\n(DRL) has been widely employed for automated scheduling policy design. However,\nthe performance of DRL is heavily influenced by the design of the\nproblem-tailored policy network and is highly sensitive to hyperparameters and\nthe design of reward feedback. Considering the above-mentioned issues, this\nstudy proposes a novel DRL method combining Graph Attention Networks-based\npolicy network and Evolution Strategy, referred to as GATES. The contributions\nof GATES are summarized as follows: (1) GATES can capture the impact of current\ntask scheduling on subsequent tasks by learning the topological relationships\nbetween tasks in a DAG. (2) GATES can learn the importance of each VM to ready\ntasks, increasing the chance of selecting the optimal VM. (3) Utilizing\nEvolution Strategy's robustness, exploratory nature, and tolerance for delayed\nrewards, GATES achieves stable policy learning in CADWS. Extensive experimental\nresults demonstrate the superiority of the proposed GATES in CADWS,\noutperforming several state-of-the-art algorithms. Codes are available at:\nhttps://github.com/YaShen998/GATES"}
{"id": "2505.12053", "pdf": "https://arxiv.org/pdf/2505.12053", "abs": "https://arxiv.org/abs/2505.12053", "authors": ["Tianxiong Zhong", "Xingye Tian", "Boyuan Jiang", "Xuebo Wang", "Xin Tao", "Pengfei Wan", "Zhiwei Zhang"], "title": "VFRTok: Variable Frame Rates Video Tokenizer with Duration-Proportional Information Assumption", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 10 figures", "summary": "Modern video generation frameworks based on Latent Diffusion Models suffer\nfrom inefficiencies in tokenization due to the Frame-Proportional Information\nAssumption. Existing tokenizers provide fixed temporal compression rates,\ncausing the computational cost of the diffusion model to scale linearly with\nthe frame rate. The paper proposes the Duration-Proportional Information\nAssumption: the upper bound on the information capacity of a video is\nproportional to the duration rather than the number of frames. Based on this\ninsight, the paper introduces VFRTok, a Transformer-based video tokenizer, that\nenables variable frame rate encoding and decoding through asymmetric frame rate\ntraining between the encoder and decoder. Furthermore, the paper proposes\nPartial Rotary Position Embeddings (RoPE) to decouple position and content\nmodeling, which groups correlated patches into unified tokens. The Partial RoPE\neffectively improves content-awareness, enhancing the video generation\ncapability. Benefiting from the compact and continuous spatio-temporal\nrepresentation, VFRTok achieves competitive reconstruction quality and\nstate-of-the-art generation fidelity while using only 1/8 tokens compared to\nexisting tokenizers."}
{"id": "2505.12090", "pdf": "https://arxiv.org/pdf/2505.12090", "abs": "https://arxiv.org/abs/2505.12090", "authors": ["Mohammad Shokri", "Sarah Ita Levitan", "Rivka Levitan"], "title": "Personalized Author Obfuscation with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this paper, we investigate the efficacy of large language models (LLMs) in\nobfuscating authorship by paraphrasing and altering writing styles. Rather than\nadopting a holistic approach that evaluates performance across the entire\ndataset, we focus on user-wise performance to analyze how obfuscation\neffectiveness varies across individual authors. While LLMs are generally\neffective, we observe a bimodal distribution of efficacy, with performance\nvarying significantly across users. To address this, we propose a personalized\nprompting method that outperforms standard prompting techniques and partially\nmitigates the bimodality issue."}
{"id": "2505.12369", "pdf": "https://arxiv.org/pdf/2505.12369", "abs": "https://arxiv.org/abs/2505.12369", "authors": ["Fernando Zhapa-Camacho", "Robert Hoehndorf"], "title": "Fully Geometric Multi-Hop Reasoning on Knowledge Graphs with Transitive Relations", "categories": ["cs.AI", "cs.LG", "cs.LO"], "comment": null, "summary": "Geometric embedding methods have shown to be useful for multi-hop reasoning\non knowledge graphs by mapping entities and logical operations to geometric\nregions and geometric transformations, respectively. Geometric embeddings\nprovide direct interpretability framework for queries. However, current methods\nhave only leveraged the geometric construction of entities, failing to map\nlogical operations to geometric transformations and, instead, using neural\ncomponents to learn these operations. We introduce GeometrE, a geometric\nembedding method for multi-hop reasoning, which does not require learning the\nlogical operations and enables full geometric interpretability. Additionally,\nunlike previous methods, we introduce a transitive loss function and show that\nit can preserve the logical rule $\\forall a,b,c: r(a,b) \\land r(b,c) \\to\nr(a,c)$. Our experiments show that GeometrE outperforms current\nstate-of-the-art methods on standard benchmark datasets."}
{"id": "2505.12066", "pdf": "https://arxiv.org/pdf/2505.12066", "abs": "https://arxiv.org/abs/2505.12066", "authors": ["Yijie Zheng", "Jinxuan Yang", "Yu Chen", "Yaxuan Wang", "Yihang Lu", "Guoqing Li"], "title": "Beluga Whale Detection from Satellite Imagery with Point Labels", "categories": ["cs.CV"], "comment": "Accepted for oral presentation at IGARSS 2025. Session at\n  https://www.2025.ieeeigarss.org/view_paper.php?PaperNum=2430&SessionID=1426", "summary": "Very high-resolution (VHR) satellite imagery has emerged as a powerful tool\nfor monitoring marine animals on a large scale. However, existing deep\nlearning-based whale detection methods usually require manually created,\nhigh-quality bounding box annotations, which are labor-intensive to produce.\nMoreover, existing studies often exclude ``uncertain whales'', individuals that\nhave ambiguous appearances in satellite imagery, limiting the applicability of\nthese models in real-world scenarios. To address these limitations, this study\nintroduces an automated pipeline for detecting beluga whales and harp seals in\nVHR satellite imagery. The pipeline leverages point annotations and the Segment\nAnything Model (SAM) to generate precise bounding box annotations, which are\nused to train YOLOv8 for multiclass detection of certain whales, uncertain\nwhales, and harp seals. Experimental results demonstrated that SAM-generated\nannotations significantly improved detection performance, achieving higher\n$\\text{F}_\\text{1}$-scores compared to traditional buffer-based annotations.\nYOLOv8 trained on SAM-labeled boxes achieved an overall\n$\\text{F}_\\text{1}$-score of 72.2% for whales overall and 70.3% for harp seals,\nwith superior performance in dense scenes. The proposed approach not only\nreduces the manual effort required for annotation but also enhances the\ndetection of uncertain whales, offering a more comprehensive solution for\nmarine animal monitoring. This method holds great potential for extending to\nother species, habitats, and remote sensing platforms, as well as for\nestimating whale biometrics, thereby advancing ecological monitoring and\nconservation efforts. The codes for our label and detection pipeline are\npublicly available at http://github.com/voyagerxvoyagerx/beluga-seeker ."}
{"id": "2505.12100", "pdf": "https://arxiv.org/pdf/2505.12100", "abs": "https://arxiv.org/abs/2505.12100", "authors": ["Isabela Pereira Gregio", "Ian Pons", "Anna Helena Reali Costa", "Artur Jordão"], "title": "Improving Fairness in LLMs Through Testing-Time Adversaries", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) push the bound-aries in natural language\nprocessing and generative AI, driving progress across various aspects of modern\nsociety. Unfortunately, the pervasive issue of bias in LLMs responses (i.e.,\npredictions) poses a significant and open challenge, hindering their\napplication in tasks involving ethical sensitivity and responsible\ndecision-making. In this work, we propose a straightforward, user-friendly and\npractical method to mitigate such biases, enhancing the reliability and\ntrustworthiness of LLMs. Our method creates multiple variations of a given\nsentence by modifying specific attributes and evaluates the corresponding\nprediction behavior compared to the original, unaltered, prediction/sentence.\nThe idea behind this process is that critical ethical predictions often exhibit\nnotable inconsistencies, indicating the presence of bias. Unlike previous\napproaches, our method relies solely on forward passes (i.e., testing-time\nadversaries), eliminating the need for training, fine-tuning, or prior\nknowledge of the training data distribution. Through extensive experiments on\nthe popular Llama family, we demonstrate the effectiveness of our method in\nimproving various fairness metrics, focusing on the reduction of disparities in\nhow the model treats individuals from different racial groups. Specifically,\nusing standard metrics, we improve the fairness in Llama3 in up to 27\npercentage points. Overall, our approach significantly enhances fairness,\nequity, and reliability in LLM-generated results without parameter tuning or\ntraining data modifications, confirming its effectiveness in practical\nscenarios. We believe our work establishes an important step toward enabling\nthe use of LLMs in tasks that require ethical considerations and responsible\ndecision-making."}
{"id": "2505.12370", "pdf": "https://arxiv.org/pdf/2505.12370", "abs": "https://arxiv.org/abs/2505.12370", "authors": ["Xinbin Yuan", "Jian Zhang", "Kaixin Li", "Zhuoxuan Cai", "Lujian Yao", "Jie Chen", "Enguang Wang", "Qibin Hou", "Jinwei Chen", "Peng-Tao Jiang", "Bo Li"], "title": "Enhancing Visual Grounding for GUI Agents via Self-Evolutionary Reinforcement Learning", "categories": ["cs.AI"], "comment": null, "summary": "Graphical User Interface (GUI) agents have made substantial strides in\nunderstanding and executing user instructions across diverse platforms. Yet,\ngrounding these instructions to precise interface elements remains challenging,\nespecially in complex, high-resolution, professional environments. Traditional\nsupervised finetuning (SFT) methods often require large volumes of diverse data\nand exhibit weak generalization. To overcome these limitations, we introduce a\nreinforcement learning (RL) based framework that incorporates three core\nstrategies: (1) seed data curation to ensure high quality training samples, (2)\na dense policy gradient that provides continuous feedback based on prediction\naccuracy, and (3) a self evolutionary reinforcement finetuning mechanism that\niteratively refines the model using attention maps. With only 3k training\nsamples, our 7B-parameter model achieves state-of-the-art results among\nsimilarly sized models on three grounding benchmarks. Notably, it attains\n47.3\\% accuracy on the ScreenSpot-Pro dataset, outperforming much larger\nmodels, such as UI-TARS-72B, by a margin of 24.2\\%. These findings underscore\nthe effectiveness of RL-based approaches in enhancing GUI agent performance,\nparticularly in high-resolution, complex environments."}
{"id": "2505.12069", "pdf": "https://arxiv.org/pdf/2505.12069", "abs": "https://arxiv.org/abs/2505.12069", "authors": ["Shenzhou Liu", "Di Wang", "Haonan Guo", "Chengxi Han", "Wenzhi Zeng"], "title": "MT-CYP-Net: Multi-Task Network for Pixel-Level Crop Yield Prediction Under Very Few Samples", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate and fine-grained crop yield prediction plays a crucial role in\nadvancing global agriculture. However, the accuracy of pixel-level yield\nestimation based on satellite remote sensing data has been constrained by the\nscarcity of ground truth data. To address this challenge, we propose a novel\napproach called the Multi-Task Crop Yield Prediction Network (MT-CYP-Net). This\nframework introduces an effective multi-task feature-sharing strategy, where\nfeatures extracted from a shared backbone network are simultaneously utilized\nby both crop yield prediction decoders and crop classification decoders with\nthe ability to fuse information between them. This design allows MT-CYP-Net to\nbe trained with extremely sparse crop yield point labels and crop type labels,\nwhile still generating detailed pixel-level crop yield maps. Concretely, we\ncollected 1,859 yield point labels along with corresponding crop type labels\nand satellite images from eight farms in Heilongjiang Province, China, in 2023,\ncovering soybean, maize, and rice crops, and constructed a sparse crop yield\nlabel dataset. MT-CYP-Net is compared with three classical machine learning and\ndeep learning benchmark methods in this dataset. Experimental results not only\nindicate the superiority of MT-CYP-Net compared to previous methods on multiple\ntypes of crops but also demonstrate the potential of deep networks on precise\npixel-level crop yield prediction, especially with limited data labels."}
{"id": "2505.12116", "pdf": "https://arxiv.org/pdf/2505.12116", "abs": "https://arxiv.org/abs/2505.12116", "authors": ["Fitsum Gaim", "Hoyun Song", "Huije Lee", "Changgeon Ko", "Eui Jun Hwang", "Jong C. Park"], "title": "A Multi-Task Benchmark for Abusive Language Detection in Low-Resource Settings", "categories": ["cs.CL", "I.2.7"], "comment": null, "summary": "Content moderation research has recently made significant advances, but still\nfails to serve the majority of the world's languages due to the lack of\nresources, leaving millions of vulnerable users to online hostility. This work\npresents a large-scale human-annotated multi-task benchmark dataset for abusive\nlanguage detection in Tigrinya social media with joint annotations for three\ntasks: abusiveness, sentiment, and topic classification. The dataset comprises\n13,717 YouTube comments annotated by nine native speakers, collected from 7,373\nvideos with a total of over 1.2 billion views across 51 channels. We developed\nan iterative term clustering approach for effective data selection. Recognizing\nthat around 64% of Tigrinya social media content uses Romanized\ntransliterations rather than native Ge'ez script, our dataset accommodates both\nwriting systems to reflect actual language use. We establish strong baselines\nacross the tasks in the benchmark, while leaving significant challenges for\nfuture contributions. Our experiments reveal that small, specialized multi-task\nmodels outperform the current frontier models in the low-resource setting,\nachieving up to 86% accuracy (+7 points) in abusiveness detection. We make the\nresources publicly available to promote research on online safety."}
{"id": "2505.12371", "pdf": "https://arxiv.org/pdf/2505.12371", "abs": "https://arxiv.org/abs/2505.12371", "authors": ["Yinghao Zhu", "Ziyi He", "Haoran Hu", "Xiaochen Zheng", "Xichen Zhang", "Zixiang Wang", "Junyi Gao", "Liantao Ma", "Lequan Yu"], "title": "MedAgentBoard: Benchmarking Multi-Agent Collaboration with Conventional Methods for Diverse Medical Tasks", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has stimulated interest\nin multi-agent collaboration for addressing complex medical tasks. However, the\npractical advantages of multi-agent collaboration approaches remain\ninsufficiently understood. Existing evaluations often lack generalizability,\nfailing to cover diverse tasks reflective of real-world clinical practice, and\nfrequently omit rigorous comparisons against both single-LLM-based and\nestablished conventional methods. To address this critical gap, we introduce\nMedAgentBoard, a comprehensive benchmark for the systematic evaluation of\nmulti-agent collaboration, single-LLM, and conventional approaches.\nMedAgentBoard encompasses four diverse medical task categories: (1) medical\n(visual) question answering, (2) lay summary generation, (3) structured\nElectronic Health Record (EHR) predictive modeling, and (4) clinical workflow\nautomation, across text, medical images, and structured EHR data. Our extensive\nexperiments reveal a nuanced landscape: while multi-agent collaboration\ndemonstrates benefits in specific scenarios, such as enhancing task\ncompleteness in clinical workflow automation, it does not consistently\noutperform advanced single LLMs (e.g., in textual medical QA) or, critically,\nspecialized conventional methods that generally maintain better performance in\ntasks like medical VQA and EHR-based prediction. MedAgentBoard offers a vital\nresource and actionable insights, emphasizing the necessity of a task-specific,\nevidence-based approach to selecting and developing AI solutions in medicine.\nIt underscores that the inherent complexity and overhead of multi-agent\ncollaboration must be carefully weighed against tangible performance gains. All\ncode, datasets, detailed prompts, and experimental results are open-sourced at\nhttps://medagentboard.netlify.app/."}
{"id": "2505.12074", "pdf": "https://arxiv.org/pdf/2505.12074", "abs": "https://arxiv.org/abs/2505.12074", "authors": ["Chen Shu", "Boyu Fu", "Yiman Li", "Ting Yin", "Wenchuan Zhang", "Jie Chen", "Yuhao Yi", "Hong Bu"], "title": "Denoising Mutual Knowledge Distillation in Bi-Directional Multiple Instance Learning", "categories": ["cs.CV"], "comment": "12 pages, 2 figures", "summary": "Multiple Instance Learning is the predominant method for Whole Slide Image\nclassification in digital pathology, enabling the use of slide-level labels to\nsupervise model training. Although MIL eliminates the tedious fine-grained\nannotation process for supervised learning, whether it can learn accurate bag-\nand instance-level classifiers remains a question. To address the issue,\ninstance-level classifiers and instance masks were incorporated to ground the\nprediction on supporting patches. These methods, while practically improving\nthe performance of MIL methods, may potentially introduce noisy labels. We\npropose to bridge the gap between commonly used MIL and fully supervised\nlearning by augmenting both the bag- and instance-level learning processes with\npseudo-label correction capabilities elicited from weak to strong\ngeneralization techniques. The proposed algorithm improves the performance of\ndual-level MIL algorithms on both bag- and instance-level predictions.\nExperiments on public pathology datasets showcase the advantage of the proposed\nmethods."}
{"id": "2505.12158", "pdf": "https://arxiv.org/pdf/2505.12158", "abs": "https://arxiv.org/abs/2505.12158", "authors": ["Elisa Bassignana", "Amanda Cercas Curry", "Dirk Hovy"], "title": "The AI Gap: How Socioeconomic Status Affects Language Technology Interactions", "categories": ["cs.CL"], "comment": "Accepted at ACL Main 2025", "summary": "Socioeconomic status (SES) fundamentally influences how people interact with\neach other and more recently, with digital technologies like Large Language\nModels (LLMs). While previous research has highlighted the interaction between\nSES and language technology, it was limited by reliance on proxy metrics and\nsynthetic data. We survey 1,000 individuals from diverse socioeconomic\nbackgrounds about their use of language technologies and generative AI, and\ncollect 6,482 prompts from their previous interactions with LLMs. We find\nsystematic differences across SES groups in language technology usage (i.e.,\nfrequency, performed tasks), interaction styles, and topics. Higher SES entails\na higher level of abstraction, convey requests more concisely, and topics like\n'inclusivity' and 'travel'. Lower SES correlates with higher\nanthropomorphization of LLMs (using ''hello'' and ''thank you'') and more\nconcrete language. Our findings suggest that while generative language\ntechnologies are becoming more accessible to everyone, socioeconomic linguistic\ndifferences still stratify their use to exacerbate the digital divide. These\ndifferences underscore the importance of considering SES in developing language\ntechnologies to accommodate varying linguistic needs rooted in socioeconomic\nfactors and limit the AI Gap across SES groups."}
{"id": "2505.12440", "pdf": "https://arxiv.org/pdf/2505.12440", "abs": "https://arxiv.org/abs/2505.12440", "authors": ["Jakub Skrzyński", "Dominik Sepioło", "Antoni Ligęza"], "title": "Model Discovery with Grammatical Evolution. An Experiment with Prime Numbers", "categories": ["cs.AI"], "comment": "Presented during 5th Polish Conference on Artificial Intelligence,\n  published in \"PROGRESS IN POLISH ARTIFICIAL INTELLIGENCE RESEARCH 5\" ISBN\n  978-83-8156-696-4", "summary": "Machine Learning produces efficient decision and prediction models based on\ninput-output data only. Such models have the form of decision trees or neural\nnets and are far from transparent analytical models, based on mathematical\nformulas. Analytical model discovery requires additional knowledge and may be\nperformed with Grammatical Evolution. Such models are transparent, concise, and\nhave readable components and structure. This paper reports on a non-trivial\nexperiment with generating such models."}
{"id": "2505.12081", "pdf": "https://arxiv.org/pdf/2505.12081", "abs": "https://arxiv.org/abs/2505.12081", "authors": ["Yuqi Liu", "Tianyuan Qu", "Zhisheng Zhong", "Bohao Peng", "Shu Liu", "Bei Yu", "Jiaya Jia"], "title": "VisionReasoner: Unified Visual Perception and Reasoning via Reinforcement Learning", "categories": ["cs.CV"], "comment": null, "summary": "Large vision-language models exhibit inherent capabilities to handle diverse\nvisual perception tasks. In this paper, we introduce VisionReasoner, a unified\nframework capable of reasoning and solving multiple visual perception tasks\nwithin a shared model. Specifically, by designing novel multi-object cognitive\nlearning strategies and systematic task reformulation, VisionReasoner enhances\nits reasoning capabilities to analyze visual inputs, and addresses diverse\nperception tasks in a unified framework. The model generates a structured\nreasoning process before delivering the desired outputs responding to user\nqueries. To rigorously assess unified visual perception capabilities, we\nevaluate VisionReasoner on ten diverse tasks spanning three critical domains:\ndetection, segmentation, and counting. Experimental results show that\nVisionReasoner achieves superior performance as a unified model, outperforming\nQwen2.5VL by relative margins of 29.1% on COCO (detection), 22.1% on ReasonSeg\n(segmentation), and 15.3% on CountBench (counting)."}
{"id": "2505.12160", "pdf": "https://arxiv.org/pdf/2505.12160", "abs": "https://arxiv.org/abs/2505.12160", "authors": ["Darmawan Wicaksono", "Hasri Akbar Awal Rozaq", "Nevfel Boz"], "title": "Emotion Recognition for Low-Resource Turkish: Fine-Tuning BERTurk on TREMO and Testing on Xenophobic Political Discourse", "categories": ["cs.CL"], "comment": null, "summary": "Social media platforms like X (formerly Twitter) play a crucial role in\nshaping public discourse and societal norms. This study examines the term\nSessiz Istila (Silent Invasion) on Turkish social media, highlighting the rise\nof anti-refugee sentiment amidst the Syrian refugee influx. Using BERTurk and\nthe TREMO dataset, we developed an advanced Emotion Recognition Model (ERM)\ntailored for Turkish, achieving 92.62% accuracy in categorizing emotions such\nas happiness, fear, anger, sadness, disgust, and surprise. By applying this\nmodel to large-scale X data, the study uncovers emotional nuances in Turkish\ndiscourse, contributing to computational social science by advancing sentiment\nanalysis in underrepresented languages and enhancing our understanding of\nglobal digital discourse and the unique linguistic challenges of Turkish. The\nfindings underscore the transformative potential of localized NLP tools, with\nour ERM model offering practical applications for real-time sentiment analysis\nin Turkish-language contexts. By addressing critical areas, including\nmarketing, public relations, and crisis management, these models facilitate\nimproved decision-making through timely and accurate sentiment tracking. This\nhighlights the significance of advancing research that accounts for regional\nand linguistic nuances."}
{"id": "2505.12470", "pdf": "https://arxiv.org/pdf/2505.12470", "abs": "https://arxiv.org/abs/2505.12470", "authors": ["Jiaqi Wang", "Yusen Zhang", "Xi Li"], "title": "NeuroGen: Neural Network Parameter Generation via Large Language Models", "categories": ["cs.AI"], "comment": "The three authors contributed equally to this work. The codes will be\n  public after being accepted", "summary": "Acquiring the parameters of neural networks (NNs) has been one of the most\nimportant problems in machine learning since the inception of NNs. Traditional\napproaches, such as backpropagation and forward-only optimization, acquire\nparameters via iterative data fitting to gradually optimize them. This paper\naims to explore the feasibility of a new direction: acquiring NN parameters via\nlarge language model generation. We propose NeuroGen, a generalized and\neasy-to-implement two-stage approach for NN parameter generation conditioned on\ndescriptions of the data, task, and network architecture. Stage one is\nParameter Reference Knowledge Injection, where LLMs are pretrained on NN\ncheckpoints to build foundational understanding of parameter space, whereas\nstage two is Context-Enhanced Instruction Tuning, enabling LLMs to adapt to\nspecific tasks through enriched, task-aware prompts. Experimental results\ndemonstrate that NeuroGen effectively generates usable NN parameters. Our\nfindings highlight the feasibility of LLM-based NN parameter generation and\nsuggest a promising new paradigm where LLMs and lightweight NNs can coexist\nsynergistically"}
{"id": "2505.12098", "pdf": "https://arxiv.org/pdf/2505.12098", "abs": "https://arxiv.org/abs/2505.12098", "authors": ["Jiarui Wang", "Huiyu Duan", "Ziheng Jia", "Yu Zhao", "Woo Yi Yang", "Zicheng Zhang", "Zijian Chen", "Juntong Wang", "Yuke Xing", "Guangtao Zhai", "Xiongkuo Min"], "title": "LOVE: Benchmarking and Evaluating Text-to-Video Generation and Video-to-Text Interpretation", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in large multimodal models (LMMs) have driven substantial\nprogress in both text-to-video (T2V) generation and video-to-text (V2T)\ninterpretation tasks. However, current AI-generated videos (AIGVs) still\nexhibit limitations in terms of perceptual quality and text-video alignment.\nTherefore, a reliable and scalable automatic model for AIGV evaluation is\ndesirable, which heavily relies on the scale and quality of human annotations.\nTo this end, we present AIGVE-60K, a comprehensive dataset and benchmark for\nAI-Generated Video Evaluation, which features (i) comprehensive tasks,\nencompassing 3,050 extensive prompts across 20 fine-grained task dimensions,\n(ii) the largest human annotations, including 120K mean-opinion scores (MOSs)\nand 60K question-answering (QA) pairs annotated on 58,500 videos generated from\n30 T2V models, and (iii) bidirectional benchmarking and evaluating for both T2V\ngeneration and V2T interpretation capabilities. Based on AIGVE-60K, we propose\nLOVE, a LMM-based metric for AIGV Evaluation from multiple dimensions including\nperceptual preference, text-video correspondence, and task-specific accuracy in\nterms of both instance level and model level. Comprehensive experiments\ndemonstrate that LOVE not only achieves state-of-the-art performance on the\nAIGVE-60K dataset, but also generalizes effectively to a wide range of other\nAIGV evaluation benchmarks. These findings highlight the significance of the\nAIGVE-60K dataset. Database and codes are anonymously available at\nhttps://github.com/IntMeGroup/LOVE."}
{"id": "2505.12182", "pdf": "https://arxiv.org/pdf/2505.12182", "abs": "https://arxiv.org/abs/2505.12182", "authors": ["Haohang Li", "Yupeng Cao", "Yangyang Yu", "Jordan W. Suchow", "Zining Zhu"], "title": "Truth Neurons", "categories": ["cs.CL"], "comment": null, "summary": "Despite their remarkable success and deployment across diverse workflows,\nlanguage models sometimes produce untruthful responses. Our limited\nunderstanding of how truthfulness is mechanistically encoded within these\nmodels jeopardizes their reliability and safety. In this paper, we propose a\nmethod for identifying representations of truthfulness at the neuron level. We\nshow that language models contain truth neurons, which encode truthfulness in a\nsubject-agnostic manner. Experiments conducted across models of varying scales\nvalidate the existence of truth neurons, confirming that the encoding of\ntruthfulness at the neuron level is a property shared by many language models.\nThe distribution patterns of truth neurons over layers align with prior\nfindings on the geometry of truthfulness. Selectively suppressing the\nactivations of truth neurons found through the TruthfulQA dataset degrades\nperformance both on TruthfulQA and on other benchmarks, showing that the\ntruthfulness mechanisms are not tied to a specific dataset. Our results offer\nnovel insights into the mechanisms underlying truthfulness in language models\nand highlight potential directions toward improving their trustworthiness and\nreliability."}
{"id": "2505.12493", "pdf": "https://arxiv.org/pdf/2505.12493", "abs": "https://arxiv.org/abs/2505.12493", "authors": ["Longxi Gao", "Li Zhang", "Mengwei Xu"], "title": "UIShift: Enhancing VLM-based GUI Agents through Self-supervised Reinforcement Learning", "categories": ["cs.AI"], "comment": null, "summary": "Training effective Vision Language Models (VLMs) for GUI agents typically\nrelies on supervised fine-tuning (SFT) over large-scale annotated datasets,\nwhere the collection process is labor-intensive and error-prone. In this work,\nwe propose a self-supervised inverse dynamics task to enable VLMs to learn from\nGUI transition pairs by inferring the action that caused that transition. This\ntraining task offers two advantages: (1) It enables VLMs to ignore variations\nunrelated to user actions (e.g., background refreshes, ads) and to focus on\ntrue affordances such as buttons and input fields within complex GUIs. (2) The\ntraining data can be easily obtained from existing GUI trajectories without\nrequiring human annotation, and it can be easily scaled through automatic\noffline exploration. Using this training task, we propose UI-shift, a framework\nfor enhancing VLM-based GUI agents through self-supervised reinforcement\nlearning (RL). With only 2K training samples sourced from existing datasets,\ntwo VLMs -- Qwen2.5-VL-3B and Qwen2.5-VL-7B -- trained with UI-Shift achieve\ncompetitive or superior performance on grounding tasks (ScreenSpot-series\nbenchmarks) and GUI automation tasks (AndroidControl), compared to SFT\nbaselines and GUI-specific models that explicitly elicit reasoning abilities\nduring RL. Our findings suggest a potential direction for enhancing VLMs for\nGUI agents by leveraging more self-supervised training data in the future."}
{"id": "2505.12099", "pdf": "https://arxiv.org/pdf/2505.12099", "abs": "https://arxiv.org/abs/2505.12099", "authors": ["Aybora Koksal", "A. Aydin Alatan"], "title": "TinyRS-R1: Compact Multimodal Language Model for Remote Sensing", "categories": ["cs.CV"], "comment": "Submitted to BMVC 2025. Code, models, and the captions for datasets\n  will be released", "summary": "Remote-sensing applications often run on edge hardware that cannot host\ntoday's 7B-parameter multimodal language models. This paper introduces TinyRS,\nthe first 2B-parameter multimodal small language model (MSLM) optimized for\nremote sensing tasks, and TinyRS-R1, its reasoning-augmented variant. Built\nupon Qwen2-VL-2B, TinyRS is trained through a four-stage pipeline: pre-training\non million satellite images, instruction tuning on visual instruction examples,\nfine-tuning with Chain-of-Thought (CoT) annotations from the proposed reasoning\ndataset, and alignment via Group Relative Policy Optimization (GRPO). TinyRS-R1\nachieves or surpasses the performance of recent 7B-parameter remote sensing\nmodels across classification, VQA, visual grounding, and open-ended question\nanswering-while requiring just one-third of the memory and latency. Our\nanalysis shows that CoT reasoning substantially benefits spatial grounding and\nscene understanding, while the non-reasoning TinyRS excels in concise,\nlatency-sensitive VQA tasks. TinyRS-R1 represents the first domain-specialized\nMSLM with GRPO-aligned CoT reasoning for general-purpose remote sensing."}
{"id": "2505.12183", "pdf": "https://arxiv.org/pdf/2505.12183", "abs": "https://arxiv.org/abs/2505.12183", "authors": ["Manari Hirose", "Masato Uchida"], "title": "Decoding the Mind of Large Language Models: A Quantitative Evaluation of Ideology and Biases", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": "23 pages, 5 figures, 17 tables", "summary": "The widespread integration of Large Language Models (LLMs) across various\nsectors has highlighted the need for empirical research to understand their\nbiases, thought patterns, and societal implications to ensure ethical and\neffective use. In this study, we propose a novel framework for evaluating LLMs,\nfocusing on uncovering their ideological biases through a quantitative analysis\nof 436 binary-choice questions, many of which have no definitive answer. By\napplying our framework to ChatGPT and Gemini, findings revealed that while LLMs\ngenerally maintain consistent opinions on many topics, their ideologies differ\nacross models and languages. Notably, ChatGPT exhibits a tendency to change\ntheir opinion to match the questioner's opinion. Both models also exhibited\nproblematic biases, unethical or unfair claims, which might have negative\nsocietal impacts. These results underscore the importance of addressing both\nideological and ethical considerations when evaluating LLMs. The proposed\nframework offers a flexible, quantitative method for assessing LLM behavior,\nproviding valuable insights for the development of more socially aligned AI\nsystems."}
{"id": "2505.12500", "pdf": "https://arxiv.org/pdf/2505.12500", "abs": "https://arxiv.org/abs/2505.12500", "authors": ["Jingyue Gao", "Runji Lin", "Keming Lu", "Bowen Yu", "Junyang Lin", "Jianyu Chen"], "title": "MARGE: Improving Math Reasoning for LLMs with Guided Exploration", "categories": ["cs.AI"], "comment": "To appear at ICML 2025", "summary": "Large Language Models (LLMs) exhibit strong potential in mathematical\nreasoning, yet their effectiveness is often limited by a shortage of\nhigh-quality queries. This limitation necessitates scaling up computational\nresponses through self-generated data, yet current methods struggle due to\nspurious correlated data caused by ineffective exploration across all reasoning\nstages. To address such challenge, we introduce \\textbf{MARGE}: Improving\n\\textbf{Ma}th \\textbf{R}easoning with \\textbf{G}uided \\textbf{E}xploration, a\nnovel method to address this issue and enhance mathematical reasoning through\nhit-guided exploration. MARGE systematically explores intermediate reasoning\nstates derived from self-generated solutions, enabling adequate exploration and\nimproved credit assignment throughout the reasoning process. Through extensive\nexperiments across multiple backbone models and benchmarks, we demonstrate that\nMARGE significantly improves reasoning capabilities without requiring external\nannotations or training additional value models. Notably, MARGE improves both\nsingle-shot accuracy and exploration diversity, mitigating a common trade-off\nin alignment methods. These results demonstrate MARGE's effectiveness in\nenhancing mathematical reasoning capabilities and unlocking the potential of\nscaling self-generated training data. Our code and models are available at\n\\href{https://github.com/georgao35/MARGE}{this link}."}
{"id": "2505.12108", "pdf": "https://arxiv.org/pdf/2505.12108", "abs": "https://arxiv.org/abs/2505.12108", "authors": ["Jiancheng Pan", "Shiye Lei", "Yuqian Fu", "Jiahao Li", "Yanxing Liu", "Yuze Sun", "Xiao He", "Long Peng", "Xiaomeng Huang", "Bo Zhao"], "title": "EarthSynth: Generating Informative Earth Observation with Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": "23 pages", "summary": "Remote sensing image (RSI) interpretation typically faces challenges due to\nthe scarcity of labeled data, which limits the performance of RSI\ninterpretation tasks. To tackle this challenge, we propose EarthSynth, a\ndiffusion-based generative foundation model that enables synthesizing\nmulti-category, cross-satellite labeled Earth observation for downstream RSI\ninterpretation tasks. To the best of our knowledge, EarthSynth is the first to\nexplore multi-task generation for remote sensing. EarthSynth, trained on the\nEarthSynth-180K dataset, employs the Counterfactual Composition training\nstrategy to improve training data diversity and enhance category control.\nFurthermore, a rule-based method of R-Filter is proposed to filter more\ninformative synthetic data for downstream tasks. We evaluate our EarthSynth on\nscene classification, object detection, and semantic segmentation in open-world\nscenarios, offering a practical solution for advancing RSI interpretation."}
{"id": "2505.12196", "pdf": "https://arxiv.org/pdf/2505.12196", "abs": "https://arxiv.org/abs/2505.12196", "authors": ["Yi-Chien Lin", "Hongao Zhu", "William Schuler"], "title": "Vectors from Larger Language Models Predict Human Reading Time and fMRI Data More Poorly when Dimensionality Expansion is Controlled", "categories": ["cs.CL"], "comment": null, "summary": "The impressive linguistic abilities of large language models (LLMs) have\nrecommended them as models of human sentence processing, with some conjecturing\na positive 'quality-power' relationship (Wilcox et al., 2023), in which\nlanguage models' (LMs') fit to psychometric data continues to improve as their\nability to predict words in context increases. This is important because it\nsuggests that elements of LLM architecture, such as veridical attention to\ncontext and a unique objective of predicting upcoming words, reflect the\narchitecture of the human sentence processing faculty, and that any\ninadequacies in predicting human reading time and brain imaging data may be\nattributed to insufficient model complexity, which recedes as larger models\nbecome available. Recent studies (Oh and Schuler, 2023) have shown this scaling\ninverts after a point, as LMs become excessively large and accurate, when word\nprediction probability (as information-theoretic surprisal) is used as a\npredictor. Other studies propose the use of entire vectors from differently\nsized LLMs, still showing positive scaling (Schrimpf et al., 2021), casting\ndoubt on the value of surprisal as a predictor, but do not control for the\nlarger number of predictors in vectors from larger LMs. This study evaluates\nLLM scaling using entire LLM vectors, while controlling for the larger number\nof predictors in vectors from larger LLMs. Results show that inverse scaling\nobtains, suggesting that inadequacies in predicting human reading time and\nbrain imaging data may be due to substantial misalignment between LLMs and\nhuman sentence processing, which worsens as larger models are used."}
{"id": "2505.12501", "pdf": "https://arxiv.org/pdf/2505.12501", "abs": "https://arxiv.org/abs/2505.12501", "authors": ["Edward Y. Chang", "Longling Geng"], "title": "ALAS: A Stateful Multi-LLM Agent Framework for Disruption-Aware Planning", "categories": ["cs.AI", "I.2.7"], "comment": "36 pages, 10 figures, 19 tables", "summary": "Large language models (LLMs) excel at rapid generation of text and multimodal\ncontent, yet they falter on transaction-style planning that demands ACID-like\nguarantees and real-time disruption recovery. We present Adaptive LLM Agent\nSystem (ALAS), a framework that tackles four fundamental LLM deficits: (i)\nabsence of self-verification, (ii) context erosion, (iii) next-token myopia,\nand (iv) lack of persistent state. ALAS decomposes each plan into\nrole-specialized agents, equips them with automatic state tracking, and\ncoordinates them through a lightweight protocol. When disruptions arise, agents\napply history-aware local compensation, avoiding costly global replanning and\ncontaining cascade effects. On real-world, large-scale job-shop scheduling\nbenchmarks, ALAS sets new best results for static sequential planning and\nexcels in dynamic reactive scenarios with unexpected disruptions. These gains\nshow that principled modularization plus targeted compensation can unlock\nscalable and resilient planning with LLMs."}
{"id": "2505.12130", "pdf": "https://arxiv.org/pdf/2505.12130", "abs": "https://arxiv.org/abs/2505.12130", "authors": ["Niaz Ahmad", "Jawad Khan", "Kang G. Shin", "Youngmoon Lee", "Guanghui Wang"], "title": "Keypoints as Dynamic Centroids for Unified Human Pose and Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The dynamic movement of the human body presents a fundamental challenge for\nhuman pose estimation and body segmentation. State-of-the-art approaches\nprimarily rely on combining keypoint heatmaps with segmentation masks but often\nstruggle in scenarios involving overlapping joints or rapidly changing poses\nduring instance-level segmentation. To address these limitations, we propose\nKeypoints as Dynamic Centroid (KDC), a new centroid-based representation for\nunified human pose estimation and instance-level segmentation. KDC adopts a\nbottom-up paradigm to generate keypoint heatmaps for both easily\ndistinguishable and complex keypoints and improves keypoint detection and\nconfidence scores by introducing KeyCentroids using a keypoint disk. It\nleverages high-confidence keypoints as dynamic centroids in the embedding space\nto generate MaskCentroids, allowing for swift clustering of pixels to specific\nhuman instances during rapid body movements in live environments. Our\nexperimental evaluations on the CrowdPose, OCHuman, and COCO benchmarks\ndemonstrate KDC's effectiveness and generalizability in challenging scenarios\nin terms of both accuracy and runtime performance. The implementation is\navailable at: https://sites.google.com/view/niazahmad/projects/kdc."}
{"id": "2505.12201", "pdf": "https://arxiv.org/pdf/2505.12201", "abs": "https://arxiv.org/abs/2505.12201", "authors": ["Xiyan Fu", "Wei Liu"], "title": "How Reliable is Multilingual LLM-as-a-Judge?", "categories": ["cs.CL"], "comment": null, "summary": "LLM-as-a-Judge has emerged as a popular evaluation strategy, where advanced\nlarge language models assess generation results in alignment with human\ninstructions. While these models serve as a promising alternative to human\nannotators, their reliability in multilingual evaluation remains uncertain. To\nbridge this gap, we conduct a comprehensive analysis of multilingual\nLLM-as-a-Judge. Specifically, we evaluate five models from different model\nfamilies across five diverse tasks involving 25 languages. Our findings reveal\nthat LLMs struggle to achieve consistent judgment results across languages,\nwith an average Fleiss' Kappa of approximately 0.3, and some models performing\neven worse. To investigate the cause of inconsistency, we analyze various\ninfluencing factors. We observe that consistency varies significantly across\nlanguages, with particularly poor performance in low-resource languages.\nAdditionally, we find that neither training on multilingual data nor increasing\nmodel scale directly improves judgment consistency. These findings suggest that\nLLMs are not yet reliable for evaluating multilingual predictions. We finally\npropose an ensemble strategy which improves the consistency of the multilingual\njudge in real-world applications."}
{"id": "2505.12565", "pdf": "https://arxiv.org/pdf/2505.12565", "abs": "https://arxiv.org/abs/2505.12565", "authors": ["Carl Edwards", "Chi Han", "Gawon Lee", "Thao Nguyen", "Bowen Jin", "Chetan Kumar Prasad", "Sara Szymkuć", "Bartosz A. Grzybowski", "Ying Diao", "Jiawei Han", "Ge Liu", "Hao Peng", "Martin D. Burke", "Heng Ji"], "title": "mCLM: A Function-Infused and Synthesis-Friendly Modular Chemical Language Model", "categories": ["cs.AI", "cs.CL", "cs.LG", "q-bio.QM"], "comment": null, "summary": "Despite their ability to understand chemical knowledge and accurately\ngenerate sequential representations, large language models (LLMs) remain\nlimited in their capacity to propose novel molecules with drug-like properties.\nIn addition, the molecules that LLMs propose can often be challenging to make\nin the lab. To more effectively enable the discovery of functional small\nmolecules, LLMs need to learn a molecular language. However, LLMs are currently\nlimited by encoding molecules from atoms. In this paper, we argue that just\nlike tokenizing texts into (sub-)word tokens instead of characters, molecules\nshould be decomposed and reassembled at the level of functional building\nblocks, i.e., parts of molecules that bring unique functions and serve as\neffective building blocks for real-world automated laboratory synthesis. This\nmotivates us to propose mCLM, a modular Chemical-Language Model tokenizing\nmolecules into building blocks and learning a bilingual language model of both\nnatural language descriptions of functions and molecule building blocks. By\nreasoning on such functional building blocks, mCLM guarantees to generate\nefficiently synthesizable molecules thanks to recent progress in block-based\nchemistry, while also improving the functions of molecules in a principled\nmanner. In experiments on 430 FDA-approved drugs, we find mCLM capable of\nsignificantly improving 5 out of 6 chemical functions critical to determining\ndrug potentials. More importantly, mCLM can reason on multiple functions and\nimprove the FDA-rejected drugs (``fallen angels'') over multiple iterations to\ngreatly improve their shortcomings."}
{"id": "2505.12154", "pdf": "https://arxiv.org/pdf/2505.12154", "abs": "https://arxiv.org/abs/2505.12154", "authors": ["Chao Huang", "Ruohan Gao", "J. M. F. Tsang", "Jan Kurcius", "Cagdas Bilen", "Chenliang Xu", "Anurag Kumar", "Sanjeel Parekh"], "title": "Learning to Highlight Audio by Watching Movies", "categories": ["cs.CV", "cs.SD", "eess.AS"], "comment": "CVPR 2025. Project page: https://wikichao.github.io/VisAH/", "summary": "Recent years have seen a significant increase in video content creation and\nconsumption. Crafting engaging content requires the careful curation of both\nvisual and audio elements. While visual cue curation, through techniques like\noptimal viewpoint selection or post-editing, has been central to media\nproduction, its natural counterpart, audio, has not undergone equivalent\nadvancements. This often results in a disconnect between visual and acoustic\nsaliency. To bridge this gap, we introduce a novel task: visually-guided\nacoustic highlighting, which aims to transform audio to deliver appropriate\nhighlighting effects guided by the accompanying video, ultimately creating a\nmore harmonious audio-visual experience. We propose a flexible,\ntransformer-based multimodal framework to solve this task. To train our model,\nwe also introduce a new dataset -- the muddy mix dataset, leveraging the\nmeticulous audio and video crafting found in movies, which provides a form of\nfree supervision. We develop a pseudo-data generation process to simulate\npoorly mixed audio, mimicking real-world scenarios through a three-step process\n-- separation, adjustment, and remixing. Our approach consistently outperforms\nseveral baselines in both quantitative and subjective evaluation. We also\nsystematically study the impact of different types of contextual guidance and\ndifficulty levels of the dataset. Our project page is here:\nhttps://wikichao.github.io/VisAH/."}
{"id": "2505.12212", "pdf": "https://arxiv.org/pdf/2505.12212", "abs": "https://arxiv.org/abs/2505.12212", "authors": ["Shaobo Wang", "Ziming Wang", "Xiangqi Jin", "Jize Wang", "Jiajun Zhang", "Kaixin Li", "Zichen Wen", "Zhong Li", "Conghui He", "Xuming Hu", "Linfeng Zhang"], "title": "Data Whisperer: Efficient Data Selection for Task-Specific LLM Fine-Tuning via Few-Shot In-Context Learning", "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 main, 18 pages, 8 figures, 6 tables", "summary": "Fine-tuning large language models (LLMs) on task-specific data is essential\nfor their effective deployment. As dataset sizes grow, efficiently selecting\noptimal subsets for training becomes crucial to balancing performance and\ncomputational costs. Traditional data selection methods often require\nfine-tuning a scoring model on the target dataset, which is time-consuming and\nresource-intensive, or rely on heuristics that fail to fully leverage the\nmodel's predictive capabilities. To address these challenges, we propose Data\nWhisperer, an efficient, training-free, attention-based method that leverages\nfew-shot in-context learning with the model to be fine-tuned. Comprehensive\nevaluations were conducted on both raw and synthetic datasets across diverse\ntasks and models. Notably, Data Whisperer achieves superior performance\ncompared to the full GSM8K dataset on the Llama-3-8B-Instruct model, using just\n10% of the data, and outperforms existing methods with a 3.1-point improvement\nand a 7.4$\\times$ speedup."}
{"id": "2505.12575", "pdf": "https://arxiv.org/pdf/2505.12575", "abs": "https://arxiv.org/abs/2505.12575", "authors": ["Jie Zhang", "Cezara Petrui", "Kristina Nikolić", "Florian Tramèr"], "title": "RealMath: A Continuous Benchmark for Evaluating Language Models on Research-Level Mathematics", "categories": ["cs.AI"], "comment": null, "summary": "Existing benchmarks for evaluating mathematical reasoning in large language\nmodels (LLMs) rely primarily on competition problems, formal proofs, or\nartificially challenging questions -- failing to capture the nature of\nmathematics encountered in actual research environments. We introduce RealMath,\na novel benchmark derived directly from research papers and mathematical forums\nthat assesses LLMs' abilities on authentic mathematical tasks. Our approach\naddresses three critical challenges: sourcing diverse research-level content,\nenabling reliable automated evaluation through verifiable statements, and\ndesigning a continually refreshable dataset to mitigate contamination risks.\nExperimental results across multiple LLMs reveal surprising capabilities in\nhandling research mathematics compared to competition problems, suggesting\ncurrent models may already serve as valuable assistants for working\nmathematicians despite limitations on highly challenging problems. The code and\ndataset for RealMath are publicly available."}
{"id": "2505.12155", "pdf": "https://arxiv.org/pdf/2505.12155", "abs": "https://arxiv.org/abs/2505.12155", "authors": ["Ranit Karmakar", "Simon F. Nørrelykke"], "title": "SoftPQ: Robust Instance Segmentation Evaluation via Soft Matching and Tunable Thresholds", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Segmentation evaluation metrics traditionally rely on binary decision logic:\npredictions are either correct or incorrect, based on rigid IoU thresholds.\nDetection--based metrics such as F1 and mAP determine correctness at the object\nlevel using fixed overlap cutoffs, while overlap--based metrics like\nIntersection over Union (IoU) and Dice operate at the pixel level, often\noverlooking instance--level structure. Panoptic Quality (PQ) attempts to unify\ndetection and segmentation assessment, but it remains dependent on\nhard-threshold matching--treating predictions below the threshold as entirely\nincorrect. This binary framing obscures important distinctions between\nqualitatively different errors and fails to reward gradual model improvements.\nWe propose SoftPQ, a flexible and interpretable instance segmentation metric\nthat redefines evaluation as a graded continuum rather than a binary\nclassification. SoftPQ introduces tunable upper and lower IoU thresholds to\ndefine a partial matching region and applies a sublinear penalty function to\nambiguous or fragmented predictions. These extensions allow SoftPQ to exhibit\nsmoother score behavior, greater robustness to structural segmentation errors,\nand more informative feedback for model development and evaluation. Through\ncontrolled perturbation experiments, we show that SoftPQ captures meaningful\ndifferences in segmentation quality that existing metrics overlook, making it a\npractical and principled alternative for both benchmarking and iterative model\nrefinement."}
{"id": "2505.12215", "pdf": "https://arxiv.org/pdf/2505.12215", "abs": "https://arxiv.org/abs/2505.12215", "authors": ["Jiwei Tang", "Zhicheng Zhang", "Shunlong Wu", "Jingheng Ye", "Lichen Bai", "Zitai Wang", "Tingwei Lu", "Jiaqi Chen", "Lin Hai", "Hai-Tao Zheng", "Hong-Gee Kim"], "title": "GMSA: Enhancing Context Compression via Group Merging and Layer Semantic Alignment", "categories": ["cs.CL"], "comment": "19 pages, 7 figures", "summary": "Large language models (LLMs) have achieved impressive performance in a\nvariety of natural language processing (NLP) tasks. However, when applied to\nlong-context scenarios, they face two challenges, i.e., low computational\nefficiency and much redundant information. This paper introduces GMSA, a\ncontext compression framework based on the encoder-decoder architecture, which\naddresses these challenges by reducing input sequence length and redundant\ninformation. Structurally, GMSA has two key components: Group Merging and Layer\nSemantic Alignment (LSA). Group merging is used to effectively and efficiently\nextract summary vectors from the original context. Layer semantic alignment, on\nthe other hand, aligns the high-level summary vectors with the low-level\nprimary input semantics, thus bridging the semantic gap between different\nlayers. In the training process, GMSA first learns soft tokens that contain\ncomplete semantics through autoencoder training. To furtherly adapt GMSA to\ndownstream tasks, we propose Knowledge Extraction Fine-tuning (KEFT) to extract\nknowledge from the soft tokens for downstream tasks. We train GMSA by randomly\nsampling the compression rate for each sample in the dataset. Under this\ncondition, GMSA not only significantly outperforms the traditional compression\nparadigm in context restoration but also achieves stable and significantly\nfaster convergence with only a few encoder layers. In downstream\nquestion-answering (QA) tasks, GMSA can achieve approximately a 2x speedup in\nend-to-end inference while outperforming both the original input prompts and\nvarious state-of-the-art (SOTA) methods by a large margin."}
{"id": "2505.12651", "pdf": "https://arxiv.org/pdf/2505.12651", "abs": "https://arxiv.org/abs/2505.12651", "authors": ["Sayontan Ghosh", "Mahnaz Koupaee", "Yash Kumar Lal", "Pegah Alipoormolabashi", "Mohammad Saqib Hasan", "Jun Seok Kang", "Niranjan Balasubramanian"], "title": "$\\texttt{DIAMONDs}$: A Dataset for $\\mathbb{D}$ynamic $\\mathbb{I}$nformation $\\mathbb{A}$nd $\\mathbb{M}$ental modeling $\\mathbb{O}$f $\\mathbb{N}$umeric $\\mathbb{D}$iscussions", "categories": ["cs.AI"], "comment": null, "summary": "Understanding multiparty conversations demands robust Theory of Mind (ToM)\ncapabilities, including the ability to track dynamic information, manage\nknowledge asymmetries, and distinguish relevant information across extended\nexchanges. To advance ToM evaluation in such settings, we present a carefully\ndesigned scalable methodology for generating high-quality benchmark\nconversation-question pairs with these characteristics. Using this methodology,\nwe create $\\texttt{DIAMONDs}$, a new conversational QA dataset covering common\nbusiness, financial or other group interactions. In these goal-oriented\nconversations, participants often have to track certain numerical quantities\n(say $\\textit{expected profit}$) of interest that can be derived from other\nvariable quantities (like $\\textit{marketing expenses, expected sales,\nsalary}$, etc.), whose values also change over the course of the conversation.\n$\\texttt{DIAMONDs}$ questions pose simple numerical reasoning problems over\nsuch quantities of interest (e.g., $\\textit{funds required for charity events,\nexpected company profit next quarter}$, etc.) in the context of the information\nexchanged in conversations. This allows for precisely evaluating ToM\ncapabilities for carefully tracking and reasoning over participants' knowledge\nstates.\n  Our evaluation of state-of-the-art language models reveals significant\nchallenges in handling participant-centric reasoning, specifically in\nsituations where participants have false beliefs. Models also struggle with\nconversations containing distractors and show limited ability to identify\nscenarios with insufficient information. These findings highlight current\nmodels' ToM limitations in handling real-world multi-party conversations."}
{"id": "2505.12191", "pdf": "https://arxiv.org/pdf/2505.12191", "abs": "https://arxiv.org/abs/2505.12191", "authors": ["Wenquan Lu", "Jiaqi Zhang", "Hugues Van Assel", "Randall Balestriero"], "title": "Ditch the Denoiser: Emergence of Noise Robustness in Self-Supervised Learning from Data Curriculum", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Self-Supervised Learning (SSL) has become a powerful solution to extract rich\nrepresentations from unlabeled data. Yet, SSL research is mostly focused on\nclean, curated and high-quality datasets. As a result, applying SSL on noisy\ndata remains a challenge, despite being crucial to applications such as\nastrophysics, medical imaging, geophysics or finance. In this work, we present\na fully self-supervised framework that enables noise-robust representation\nlearning without requiring a denoiser at inference or downstream fine-tuning.\nOur method first trains an SSL denoiser on noisy data, then uses it to\nconstruct a denoised-to-noisy data curriculum (i.e., training first on\ndenoised, then noisy samples) for pretraining a SSL backbone (e.g., DINOv2),\ncombined with a teacher-guided regularization that anchors noisy embeddings to\ntheir denoised counterparts. This process encourages the model to internalize\nnoise robustness. Notably, the denoiser can be discarded after pretraining,\nsimplifying deployment. On ImageNet-1k with ViT-B under extreme Gaussian noise\n($\\sigma=255$, SNR = 0.72 dB), our method improves linear probing accuracy by\n4.8% over DINOv2, demonstrating that denoiser-free robustness can emerge from\nnoise-aware pretraining. The code is available at\nhttps://github.com/wenquanlu/noisy_dinov2."}
{"id": "2505.12216", "pdf": "https://arxiv.org/pdf/2505.12216", "abs": "https://arxiv.org/abs/2505.12216", "authors": ["Rongguang Ye", "Ming Tang"], "title": "One-for-All Pruning: A Universal Model for Customized Compression of Large Language Models", "categories": ["cs.CL"], "comment": "ACL Findings", "summary": "Existing pruning methods for large language models (LLMs) focus on achieving\nhigh compression rates while maintaining model performance. Although these\nmethods have demonstrated satisfactory performance in handling a single user's\ncompression request, their processing time increases linearly with the number\nof requests, making them inefficient for real-world scenarios with multiple\nsimultaneous requests. To address this limitation, we propose a Univeral Model\nfor Customized Compression (UniCuCo) for LLMs, which introduces a StratNet that\nlearns to map arbitrary requests to their optimal pruning strategy. The\nchallenge in training StratNet lies in the high computational cost of\nevaluating pruning strategies and the non-differentiable nature of the pruning\nprocess, which hinders gradient backpropagation for StratNet updates. To\novercome these challenges, we leverage a Gaussian process to approximate the\nevaluation process. Since the gradient of the Gaussian process is computable,\nwe can use it to approximate the gradient of the non-differentiable pruning\nprocess, thereby enabling StratNet updates. Experimental results show that\nUniCuCo is 28 times faster than baselines in processing 64 requests, while\nmaintaining comparable accuracy to baselines."}
{"id": "2505.12680", "pdf": "https://arxiv.org/pdf/2505.12680", "abs": "https://arxiv.org/abs/2505.12680", "authors": ["Haoyu Zhao", "Yihan Geng", "Shange Tang", "Yong Lin", "Bohan Lyu", "Hongzhou Lin", "Chi Jin", "Sanjeev Arora"], "title": "Ineq-Comp: Benchmarking Human-Intuitive Compositional Reasoning in Automated Theorem Proving on Inequalities", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "27 pages", "summary": "LLM-based formal proof assistants (e.g., in Lean) hold great promise for\nautomating mathematical discovery. But beyond syntactic correctness, do these\nsystems truly understand mathematical structure as humans do? We investigate\nthis question through the lens of mathematical inequalities -- a fundamental\ntool across many domains. While modern provers can solve basic inequalities, we\nprobe their ability to handle human-intuitive compositionality. We introduce\nIneq-Comp, a benchmark built from elementary inequalities through systematic\ntransformations, including variable duplication, algebraic rewriting, and\nmulti-step composition. Although these problems remain easy for humans, we find\nthat most provers -- including Goedel, STP, and Kimina-7B -- struggle\nsignificantly. DeepSeek-Prover-V2-7B shows relative robustness -- possibly\nbecause it is trained to decompose the problems into sub-problems -- but still\nsuffers a 20\\% performance drop (pass@32). Strikingly, performance remains poor\nfor all models even when formal proofs of the constituent parts are provided in\ncontext, revealing that the source of weakness is indeed in compositional\nreasoning. Our results expose a persisting gap between the generalization\nbehavior of current AI provers and human mathematical intuition."}
{"id": "2505.12199", "pdf": "https://arxiv.org/pdf/2505.12199", "abs": "https://arxiv.org/abs/2505.12199", "authors": ["Kui Jiang", "Jing Cao", "Zhaocheng Yu", "Junjun Jiang", "Jingchun Zhou"], "title": "Always Clear Depth: Robust Monocular Depth Estimation under Adverse Weather", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Monocular depth estimation is critical for applications such as autonomous\ndriving and scene reconstruction. While existing methods perform well under\nnormal scenarios, their performance declines in adverse weather, due to\nchallenging domain shifts and difficulties in extracting scene information. To\naddress this issue, we present a robust monocular depth estimation method\ncalled \\textbf{ACDepth} from the perspective of high-quality training data\ngeneration and domain adaptation. Specifically, we introduce a one-step\ndiffusion model for generating samples that simulate adverse weather\nconditions, constructing a multi-tuple degradation dataset during training. To\nensure the quality of the generated degradation samples, we employ LoRA\nadapters to fine-tune the generation weights of diffusion model. Additionally,\nwe integrate circular consistency loss and adversarial training to guarantee\nthe fidelity and naturalness of the scene contents. Furthermore, we elaborate\non a multi-granularity knowledge distillation strategy (MKD) that encourages\nthe student network to absorb knowledge from both the teacher model and\npretrained Depth Anything V2. This strategy guides the student model in\nlearning degradation-agnostic scene information from various degradation\ninputs. In particular, we introduce an ordinal guidance distillation mechanism\n(OGD) that encourages the network to focus on uncertain regions through\ndifferential ranking, leading to a more precise depth estimation. Experimental\nresults demonstrate that our ACDepth surpasses md4all-DD by 2.50\\% for night\nscene and 2.61\\% for rainy scene on the nuScenes dataset in terms of the absRel\nmetric."}
{"id": "2505.12218", "pdf": "https://arxiv.org/pdf/2505.12218", "abs": "https://arxiv.org/abs/2505.12218", "authors": ["Tong Bao", "Yi Zhao", "Jin Mao", "Chengzhi Zhang"], "title": "Examining Linguistic Shifts in Academic Writing Before and After the Launch of ChatGPT: A Study on Preprint Papers", "categories": ["cs.CL", "68T50", "I.2.7"], "comment": null, "summary": "Large Language Models (LLMs), such as ChatGPT, have prompted academic\nconcerns about their impact on academic writing. Existing studies have\nprimarily examined LLM usage in academic writing through quantitative\napproaches, such as word frequency statistics and probability-based analyses.\nHowever, few have systematically examined the potential impact of LLMs on the\nlinguistic characteristics of academic writing. To address this gap, we\nconducted a large-scale analysis across 823,798 abstracts published in last\ndecade from arXiv dataset. Through the linguistic analysis of features such as\nthe frequency of LLM-preferred words, lexical complexity, syntactic complexity,\ncohesion, readability and sentiment, the results indicate a significant\nincrease in the proportion of LLM-preferred words in abstracts, revealing the\nwidespread influence of LLMs on academic writing. Additionally, we observed an\nincrease in lexical complexity and sentiment in the abstracts, but a decrease\nin syntactic complexity, suggesting that LLMs introduce more new vocabulary and\nsimplify sentence structure. However, the significant decrease in cohesion and\nreadability indicates that abstracts have fewer connecting words and are\nbecoming more difficult to read. Moreover, our analysis reveals that scholars\nwith weaker English proficiency were more likely to use the LLMs for academic\nwriting, and focused on improving the overall logic and fluency of the\nabstracts. Finally, at discipline level, we found that scholars in Computer\nScience showed more pronounced changes in writing style, while the changes in\nMathematics were minimal."}
{"id": "2505.12692", "pdf": "https://arxiv.org/pdf/2505.12692", "abs": "https://arxiv.org/abs/2505.12692", "authors": ["Ziwei Xu", "Udit Sanghi", "Mohan Kankanhalli"], "title": "Bullying the Machine: How Personas Increase LLM Vulnerability", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in interactions where\nthey are prompted to adopt personas. This paper investigates whether such\npersona conditioning affects model safety under bullying, an adversarial\nmanipulation that applies psychological pressures in order to force the victim\nto comply to the attacker. We introduce a simulation framework in which an\nattacker LLM engages a victim LLM using psychologically grounded bullying\ntactics, while the victim adopts personas aligned with the Big Five personality\ntraits. Experiments using multiple open-source LLMs and a wide range of\nadversarial goals reveal that certain persona configurations -- such as\nweakened agreeableness or conscientiousness -- significantly increase victim's\nsusceptibility to unsafe outputs. Bullying tactics involving emotional or\nsarcastic manipulation, such as gaslighting and ridicule, are particularly\neffective. These findings suggest that persona-driven interaction introduces a\nnovel vector for safety risks in LLMs and highlight the need for persona-aware\nsafety evaluation and alignment strategies."}
{"id": "2505.12200", "pdf": "https://arxiv.org/pdf/2505.12200", "abs": "https://arxiv.org/abs/2505.12200", "authors": ["Bohan Jia", "Wenxuan Huang", "Yuntian Tang", "Junbo Qiao", "Jincheng Liao", "Shaosheng Cao", "Fei Zhao", "Zhaopeng Feng", "Zhouhong Gu", "Zhenfei Yin", "Lei Bai", "Wanli Ouyang", "Lin Chen", "Fei Zhao", "Zihan Wang", "Yuan Xie", "Shaohui Lin"], "title": "CompBench: Benchmarking Complex Instruction-guided Image Editing", "categories": ["cs.CV"], "comment": null, "summary": "While real-world applications increasingly demand intricate scene\nmanipulation, existing instruction-guided image editing benchmarks often\noversimplify task complexity and lack comprehensive, fine-grained instructions.\nTo bridge this gap, we introduce, a large-scale benchmark specifically designed\nfor complex instruction-guided image editing. CompBench features challenging\nediting scenarios that incorporate fine-grained instruction following, spatial\nand contextual reasoning, thereby enabling comprehensive evaluation of image\nediting models' precise manipulation capabilities. To construct CompBench, We\npropose an MLLM-human collaborative framework with tailored task pipelines.\nFurthermore, we propose an instruction decoupling strategy that disentangles\nediting intents into four key dimensions: location, appearance, dynamics, and\nobjects, ensuring closer alignment between instructions and complex editing\nrequirements. Extensive evaluations reveal that CompBench exposes fundamental\nlimitations of current image editing models and provides critical insights for\nthe development of next-generation instruction-guided image editing systems."}
{"id": "2505.12236", "pdf": "https://arxiv.org/pdf/2505.12236", "abs": "https://arxiv.org/abs/2505.12236", "authors": ["Quanjiang Guo", "Jinchuan Zhang", "Sijie Wang", "Ling Tian", "Zhao Kang", "Bin Yan", "Weidong Xiao"], "title": "Bridging Generative and Discriminative Learning: Few-Shot Relation Extraction via Two-Stage Knowledge-Guided Pre-training", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "13 pages, 6 figures, Appear on IJCAI 2025", "summary": "Few-Shot Relation Extraction (FSRE) remains a challenging task due to the\nscarcity of annotated data and the limited generalization capabilities of\nexisting models. Although large language models (LLMs) have demonstrated\npotential in FSRE through in-context learning (ICL), their general-purpose\ntraining objectives often result in suboptimal performance for task-specific\nrelation extraction. To overcome these challenges, we propose TKRE (Two-Stage\nKnowledge-Guided Pre-training for Relation Extraction), a novel framework that\nsynergistically integrates LLMs with traditional relation extraction models,\nbridging generative and discriminative learning paradigms. TKRE introduces two\nkey innovations: (1) leveraging LLMs to generate explanation-driven knowledge\nand schema-constrained synthetic data, addressing the issue of data scarcity;\nand (2) a two-stage pre-training strategy combining Masked Span Language\nModeling (MSLM) and Span-Level Contrastive Learning (SCL) to enhance relational\nreasoning and generalization. Together, these components enable TKRE to\neffectively tackle FSRE tasks. Comprehensive experiments on benchmark datasets\ndemonstrate the efficacy of TKRE, achieving new state-of-the-art performance in\nFSRE and underscoring its potential for broader application in low-resource\nscenarios. \\footnote{The code and data are released on\nhttps://github.com/UESTC-GQJ/TKRE."}
{"id": "2505.12731", "pdf": "https://arxiv.org/pdf/2505.12731", "abs": "https://arxiv.org/abs/2505.12731", "authors": ["Jie Ou", "Jinyu Guo", "Shuaihong Jiang", "Zhaokun Wang", "Libo Qin", "Shunyu Yao", "Wenhong Tian"], "title": "Accelerating Adaptive Retrieval Augmented Generation via Instruction-Driven Representation Reduction of Retrieval Overlaps", "categories": ["cs.AI"], "comment": null, "summary": "Retrieval-augmented generation (RAG) has emerged as a pivotal method for\nexpanding the knowledge of large language models. To handle complex queries\nmore effectively, researchers developed Adaptive-RAG (A-RAG) to enhance the\ngenerated quality through multiple interactions with external knowledge bases.\nDespite its effectiveness, A-RAG exacerbates the pre-existing efficiency\nchallenges inherent in RAG, which are attributable to its reliance on multiple\niterations of generation. Existing A-RAG approaches process all retrieved\ncontents from scratch. However, they ignore the situation where there is a\nsignificant overlap in the content of the retrieval results across rounds. The\noverlapping content is redundantly represented, which leads to a large\nproportion of repeated computations, thus affecting the overall efficiency. To\naddress this issue, this paper introduces a model-agnostic approach that can be\ngenerally applied to A-RAG methods, which is dedicated to reducing the\nredundant representation process caused by the overlapping of retrieval\nresults. Specifically, we use cache access and parallel generation to speed up\nthe prefilling and decoding stages respectively. Additionally, we also propose\nan instruction-driven module to further guide the model to more effectively\nattend to each part of the content in a more suitable way for LLMs. Experiments\nshow that our approach achieves 2.79 and 2.33 times significant acceleration on\naverage for prefilling and decoding respectively while maintaining equal\ngeneration quality."}
{"id": "2505.12206", "pdf": "https://arxiv.org/pdf/2505.12206", "abs": "https://arxiv.org/abs/2505.12206", "authors": ["Mathanesh Vellingiri Ramasamy", "Dimas Rizky Kurniasalim"], "title": "Road Segmentation for ADAS/AD Applications", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Accurate road segmentation is essential for autonomous driving and ADAS,\nenabling effective navigation in complex environments. This study examines how\nmodel architecture and dataset choice affect segmentation by training a\nmodified VGG-16 on the Comma10k dataset and a modified U-Net on the KITTI Road\ndataset. Both models achieved high accuracy, with cross-dataset testing showing\nVGG-16 outperforming U-Net despite U-Net being trained for more epochs. We\nanalyze model performance using metrics such as F1-score, mean intersection\nover union, and precision, discussing how architecture and dataset impact\nresults."}
{"id": "2505.12238", "pdf": "https://arxiv.org/pdf/2505.12238", "abs": "https://arxiv.org/abs/2505.12238", "authors": ["Sriram Selvam", "Anneswa Ghosh"], "title": "PANORAMA: A synthetic PII-laced dataset for studying sensitive data memorization in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The memorization of sensitive and personally identifiable information (PII)\nby large language models (LLMs) poses growing privacy risks as models scale and\nare increasingly deployed in real-world applications. Existing efforts to study\nsensitive and PII data memorization and develop mitigation strategies are\nhampered by the absence of comprehensive, realistic, and ethically sourced\ndatasets reflecting the diversity of sensitive information found on the web. We\nintroduce PANORAMA - Profile-based Assemblage for Naturalistic Online\nRepresentation and Attribute Memorization Analysis, a large-scale synthetic\ncorpus of 384,789 samples derived from 9,674 synthetic profiles designed to\nclosely emulate the distribution, variety, and context of PII and sensitive\ndata as it naturally occurs in online environments. Our data generation\npipeline begins with the construction of internally consistent, multi-attribute\nhuman profiles using constrained selection to reflect real-world demographics\nsuch as education, health attributes, financial status, etc. Using a\ncombination of zero-shot prompting and OpenAI o3-mini, we generate diverse\ncontent types - including wiki-style articles, social media posts, forum\ndiscussions, online reviews, comments, and marketplace listings - each\nembedding realistic, contextually appropriate PII and other sensitive\ninformation. We validate the utility of PANORAMA by fine-tuning the Mistral-7B\nmodel on 1x, 5x, 10x, and 25x data replication rates with a subset of data and\nmeasure PII memorization rates - revealing not only consistent increases with\nrepetition but also variation across content types, highlighting PANORAMA's\nability to model how memorization risks differ by context. Our dataset and code\nare publicly available, providing a much-needed resource for privacy risk\nassessment, model auditing, and the development of privacy-preserving LLMs."}
{"id": "2505.12741", "pdf": "https://arxiv.org/pdf/2505.12741", "abs": "https://arxiv.org/abs/2505.12741", "authors": ["Shiguang Wu", "Yaqing Wang", "Quanming Yao"], "title": "Dense Communication between Language Models", "categories": ["cs.AI"], "comment": null, "summary": "As higher-level intelligence emerges from the combination of modular\ncomponents with lower-level intelligence, many works combines Large Language\nModels (LLMs) for collective intelligence. Such combination is achieved by\nbuilding communications among LLMs. While current systems primarily facilitate\nsuch communication through natural language, this paper proposes a novel\nparadigm of direct dense vector communication between LLMs. Our approach\neliminates the unnecessary embedding and de-embedding steps when LLM interact\nwith another, enabling more efficient information transfer, fully\ndifferentiable optimization pathways, and exploration of capabilities beyond\nhuman heuristics. We use such stripped LLMs as vertexes and optimizable seq2seq\nmodules as edges to construct LMNet, with similar structure as MLPs. By\nutilizing smaller pre-trained LLMs as vertexes, we train a LMNet that achieves\ncomparable performance with LLMs in similar size with only less than 0.1%\ntraining cost. This offers a new perspective on scaling for general\nintelligence rather than training a monolithic LLM from scratch. Besides, the\nproposed method can be used for other applications, like customizing LLM with\nlimited data, showing its versatility."}
{"id": "2505.12207", "pdf": "https://arxiv.org/pdf/2505.12207", "abs": "https://arxiv.org/abs/2505.12207", "authors": ["Qingmei Li", "Yang Zhang", "Zurong Mai", "Yuhang Chen", "Shuohong Lou", "Henglian Huang", "Jiarui Zhang", "Zhiwei Zhang", "Yibin Wen", "Weijia Li", "Haohuan Fu", "Jianxi Huang", "Juepeng Zheng"], "title": "Can Large Multimodal Models Understand Agricultural Scenes? Benchmarking with AgroMind", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large Multimodal Models (LMMs) has demonstrated capabilities across various\ndomains, but comprehensive benchmarks for agricultural remote sensing (RS)\nremain scarce. Existing benchmarks designed for agricultural RS scenarios\nexhibit notable limitations, primarily in terms of insufficient scene diversity\nin the dataset and oversimplified task design. To bridge this gap, we introduce\nAgroMind, a comprehensive agricultural remote sensing benchmark covering four\ntask dimensions: spatial perception, object understanding, scene understanding,\nand scene reasoning, with a total of 13 task types, ranging from crop\nidentification and health monitoring to environmental analysis. We curate a\nhigh-quality evaluation set by integrating eight public datasets and one\nprivate farmland plot dataset, containing 25,026 QA pairs and 15,556 images.\nThe pipeline begins with multi-source data preprocessing, including collection,\nformat standardization, and annotation refinement. We then generate a diverse\nset of agriculturally relevant questions through the systematic definition of\ntasks. Finally, we employ LMMs for inference, generating responses, and\nperforming detailed examinations. We evaluated 18 open-source LMMs and 3\nclosed-source models on AgroMind. Experiments reveal significant performance\ngaps, particularly in spatial reasoning and fine-grained recognition, it is\nnotable that human performance lags behind several leading LMMs. By\nestablishing a standardized evaluation framework for agricultural RS, AgroMind\nreveals the limitations of LMMs in domain knowledge and highlights critical\nchallenges for future work. Data and code can be accessed at\nhttps://rssysu.github.io/AgroMind/."}
{"id": "2505.12244", "pdf": "https://arxiv.org/pdf/2505.12244", "abs": "https://arxiv.org/abs/2505.12244", "authors": ["Haojin Wang", "Zining Zhu", "Freda Shi"], "title": "Distribution Prompting: Understanding the Expressivity of Language Models Through the Next-Token Distributions They Can Produce", "categories": ["cs.CL"], "comment": null, "summary": "Autoregressive neural language models (LMs) generate a probability\ndistribution over tokens at each time step given a prompt. In this work, we\nattempt to systematically understand the probability distributions that LMs can\nproduce, showing that some distributions are significantly harder to elicit\nthan others. Specifically, for any target next-token distribution over the\nvocabulary, we attempt to find a prompt that induces the LM to output a\ndistribution as close as possible to the target, using either soft or hard\ngradient-based prompt tuning. We find that (1) in general, distributions with\nvery low or very high entropy are easier to approximate than those with\nmoderate entropy; (2) among distributions with the same entropy, those\ncontaining ''outlier tokens'' are easier to approximate; (3) target\ndistributions generated by LMs -- even LMs with different tokenizers -- are\neasier to approximate than randomly chosen targets. These results offer\ninsights into the expressiveness of LMs and the challenges of using them as\nprobability distribution proposers."}
{"id": "2505.12744", "pdf": "https://arxiv.org/pdf/2505.12744", "abs": "https://arxiv.org/abs/2505.12744", "authors": ["Weiliang Tang", "Dong Jing", "Jia-Hui Pan", "Zhiwu Lu", "Yun-Hui Liu", "Li Erran Li", "Mingyu Ding", "Chi-Wing Fu"], "title": "Incentivizing Multimodal Reasoning in Large Models for Direct Robot Manipulation", "categories": ["cs.AI"], "comment": "17 pages, 16 figures", "summary": "Recent Large Multimodal Models have demonstrated remarkable reasoning\ncapabilities, especially in solving complex mathematical problems and realizing\naccurate spatial perception. Our key insight is that these emerging abilities\ncan naturally extend to robotic manipulation by enabling LMMs to directly infer\nthe next goal in language via reasoning, rather than relying on a separate\naction head. However, this paradigm meets two main challenges: i) How to make\nLMMs understand the spatial action space, and ii) How to fully exploit the\nreasoning capacity of LMMs in solving these tasks. To tackle the former\nchallenge, we propose a novel task formulation, which inputs the current states\nof object parts and the gripper, and reformulates rotation by a new axis\nrepresentation instead of traditional Euler angles. This representation is more\ncompatible with spatial reasoning and easier to interpret within a unified\nlanguage space. For the latter challenge, we design a pipeline to utilize\ncutting-edge LMMs to generate a small but high-quality reasoning dataset of\nmulti-round dialogues that successfully solve manipulation tasks for supervised\nfine-tuning. Then, we perform reinforcement learning by trial-and-error\ninteractions in simulation to further enhance the model's reasoning abilities\nfor robotic manipulation. Our resulting reasoning model built upon a 7B\nbackbone, named ReasonManip, demonstrates three notable advantages driven by\nits system-2 level reasoning capabilities: i) exceptional generalizability to\nout-of-distribution environments, objects, and tasks; ii) inherent sim-to-real\ntransfer ability enabled by the unified language representation shared across\ndomains; iii) transparent interpretability connecting high-level reasoning and\nlow-level control. Extensive experiments demonstrate the effectiveness of the\nproposed paradigm and its potential to advance LMM-driven robotic manipulation."}
{"id": "2505.12217", "pdf": "https://arxiv.org/pdf/2505.12217", "abs": "https://arxiv.org/abs/2505.12217", "authors": ["Aryan Das", "Tanishq Rachamalla", "Pravendra Singh", "Koushik Biswas", "Vinay Kumar Verma", "Swalpa Kumar Roy"], "title": "Hyperspectral Image Land Cover Captioning Dataset for Vision Language Models", "categories": ["cs.CV"], "comment": null, "summary": "We introduce HyperCap, the first large-scale hyperspectral captioning dataset\ndesigned to enhance model performance and effectiveness in remote sensing\napplications. Unlike traditional hyperspectral imaging (HSI) datasets that\nfocus solely on classification tasks, HyperCap integrates spectral data with\npixel-wise textual annotations, enabling deeper semantic understanding of\nhyperspectral imagery. This dataset enhances model performance in tasks like\nclassification and feature extraction, providing a valuable resource for\nadvanced remote sensing applications. HyperCap is constructed from four\nbenchmark datasets and annotated through a hybrid approach combining automated\nand manual methods to ensure accuracy and consistency. Empirical evaluations\nusing state-of-the-art encoders and diverse fusion techniques demonstrate\nsignificant improvements in classification performance. These results\nunderscore the potential of vision-language learning in HSI and position\nHyperCap as a foundational dataset for future research in the field."}
{"id": "2505.12250", "pdf": "https://arxiv.org/pdf/2505.12250", "abs": "https://arxiv.org/abs/2505.12250", "authors": ["Chi Zhang", "Huaping Zhong", "Hongtao Li", "Chengliang Chai", "Jiawei Hong", "Yuhao Deng", "Jiacheng Wang", "Tian Tan", "Yizhou Yan", "Jiantao Qiu", "Ye Yuan", "Guoren Wang", "Conghui He", "Lei Cao"], "title": "Not All Documents Are What You Need for Extracting Instruction Tuning Data", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Instruction tuning improves the performance of large language models (LLMs),\nbut it heavily relies on high-quality training data. Recently, LLMs have been\nused to synthesize instruction data using seed question-answer (QA) pairs.\nHowever, these synthesized instructions often lack diversity and tend to be\nsimilar to the input seeds, limiting their applicability in real-world\nscenarios. To address this, we propose extracting instruction tuning data from\nweb corpora that contain rich and diverse knowledge. A naive solution is to\nretrieve domain-specific documents and extract all QA pairs from them, but this\nfaces two key challenges: (1) extracting all QA pairs using LLMs is\nprohibitively expensive, and (2) many extracted QA pairs may be irrelevant to\nthe downstream tasks, potentially degrading model performance. To tackle these\nissues, we introduce EQUAL, an effective and scalable data extraction framework\nthat iteratively alternates between document selection and high-quality QA pair\nextraction to enhance instruction tuning. EQUAL first clusters the document\ncorpus based on embeddings derived from contrastive learning, then uses a\nmulti-armed bandit strategy to efficiently identify clusters that are likely to\ncontain valuable QA pairs. This iterative approach significantly reduces\ncomputational cost while boosting model performance. Experiments on\nAutoMathText and StackOverflow across four downstream tasks show that EQUAL\nreduces computational costs by 5-10x and improves accuracy by 2.5 percent on\nLLaMA-3.1-8B and Mistral-7B"}
{"id": "2505.12746", "pdf": "https://arxiv.org/pdf/2505.12746", "abs": "https://arxiv.org/abs/2505.12746", "authors": ["Haruka Asanuma", "Naoko Koide-Majima", "Ken Nakamura", "Takato Horii", "Shinji Nishimoto", "Masafumi Oizumi"], "title": "Correspondence of high-dimensional emotion structures elicited by video clips between humans and Multimodal LLMs", "categories": ["cs.AI", "I.2.7; I.2.10; I.5.1"], "comment": "25 pages, 7 figures", "summary": "Recent studies have revealed that human emotions exhibit a high-dimensional,\ncomplex structure. A full capturing of this complexity requires new approaches,\nas conventional models that disregard high dimensionality risk overlooking key\nnuances of human emotions. Here, we examined the extent to which the latest\ngeneration of rapidly evolving Multimodal Large Language Models (MLLMs) capture\nthese high-dimensional, intricate emotion structures, including capabilities\nand limitations. Specifically, we compared self-reported emotion ratings from\nparticipants watching videos with model-generated estimates (e.g., Gemini or\nGPT). We evaluated performance not only at the individual video level but also\nfrom emotion structures that account for inter-video relationships. At the\nlevel of simple correlation between emotion structures, our results\ndemonstrated strong similarity between human and model-inferred emotion\nstructures. To further explore whether the similarity between humans and models\nis at the signle item level or the coarse-categorical level, we applied Gromov\nWasserstein Optimal Transport. We found that although performance was not\nnecessarily high at the strict, single-item level, performance across video\ncategories that elicit similar emotions was substantial, indicating that the\nmodel could infer human emotional experiences at the category level. Our\nresults suggest that current state-of-the-art MLLMs broadly capture the complex\nhigh-dimensional emotion structures at the category level, as well as their\napparent limitations in accurately capturing entire structures at the\nsingle-item level."}
{"id": "2505.12228", "pdf": "https://arxiv.org/pdf/2505.12228", "abs": "https://arxiv.org/abs/2505.12228", "authors": ["Karthik Gopinath", "Annabel Sorby-Adams", "Jonathan W. Ramirez", "Dina Zemlyanker", "Jennifer Guo", "David Hunt", "Christine L. Mac Donald", "C. Dirk Keene", "Timothy Coalson", "Matthew F. Glasser", "David Van Essen", "Matthew S. Rosen", "Oula Puonti", "W. Taylor Kimberly", "Juan Eugenio Iglesias"], "title": "From Low Field to High Value: Robust Cortical Mapping from Low-Field MRI", "categories": ["cs.CV", "cs.LG"], "comment": "32 pages", "summary": "Three-dimensional reconstruction of cortical surfaces from MRI for\nmorphometric analysis is fundamental for understanding brain structure. While\nhigh-field MRI (HF-MRI) is standard in research and clinical settings, its\nlimited availability hinders widespread use. Low-field MRI (LF-MRI),\nparticularly portable systems, offers a cost-effective and accessible\nalternative. However, existing cortical surface analysis tools are optimized\nfor high-resolution HF-MRI and struggle with the lower signal-to-noise ratio\nand resolution of LF-MRI. In this work, we present a machine learning method\nfor 3D reconstruction and analysis of portable LF-MRI across a range of\ncontrasts and resolutions. Our method works \"out of the box\" without\nretraining. It uses a 3D U-Net trained on synthetic LF-MRI to predict signed\ndistance functions of cortical surfaces, followed by geometric processing to\nensure topological accuracy. We evaluate our method using paired HF/LF-MRI\nscans of the same subjects, showing that LF-MRI surface reconstruction accuracy\ndepends on acquisition parameters, including contrast type (T1 vs T2),\norientation (axial vs isotropic), and resolution. A 3mm isotropic T2-weighted\nscan acquired in under 4 minutes, yields strong agreement with HF-derived\nsurfaces: surface area correlates at r=0.96, cortical parcellations reach\nDice=0.98, and gray matter volume achieves r=0.93. Cortical thickness remains\nmore challenging with correlations up to r=0.70, reflecting the difficulty of\nsub-mm precision with 3mm voxels. We further validate our method on challenging\npostmortem LF-MRI, demonstrating its robustness. Our method represents a step\ntoward enabling cortical surface analysis on portable LF-MRI. Code is available\nat https://surfer.nmr.mgh.harvard.edu/fswiki/ReconAny"}
{"id": "2505.12259", "pdf": "https://arxiv.org/pdf/2505.12259", "abs": "https://arxiv.org/abs/2505.12259", "authors": ["Yuhang Zhou", "Xutian Chen", "Yixin Cao", "Yuchen Ni", "Yu He", "Siyu Tian", "Xiang Liu", "Jian Zhang", "Chuanjun Ji", "Guangnan Ye", "Xipeng Qiu"], "title": "Teach2Eval: An Indirect Evaluation Method for LLM by Judging How It Teaches", "categories": ["cs.CL"], "comment": null, "summary": "Recent progress in large language models (LLMs) has outpaced the development\nof effective evaluation methods. Traditional benchmarks rely on task-specific\nmetrics and static datasets, which often suffer from fairness issues, limited\nscalability, and contamination risks. In this paper, we introduce Teach2Eval,\nan indirect evaluation framework inspired by the Feynman Technique. Instead of\ndirectly testing LLMs on predefined tasks, our method evaluates a model's\nmultiple abilities to teach weaker student models to perform tasks effectively.\nBy converting open-ended tasks into standardized multiple-choice questions\n(MCQs) through teacher-generated feedback, Teach2Eval enables scalable,\nautomated, and multi-dimensional assessment. Our approach not only avoids data\nleakage and memorization but also captures a broad range of cognitive abilities\nthat are orthogonal to current benchmarks. Experimental results across 26\nleading LLMs show strong alignment with existing human and model-based dynamic\nrankings, while offering additional interpretability for training guidance."}
{"id": "2505.12762", "pdf": "https://arxiv.org/pdf/2505.12762", "abs": "https://arxiv.org/abs/2505.12762", "authors": ["Chenlin Ming", "Chendi Qu", "Mengzhang Cai", "Qizhi Pei", "Zhuoshi Pan", "Yu Li", "Xiaoming Duan", "Lijun Wu", "Conghui He"], "title": "IDEAL: Data Equilibrium Adaptation for Multi-Capability Language Model Alignment", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have achieved impressive performance through\nSupervised Fine-tuning (SFT) on diverse instructional datasets. When training\non multiple capabilities simultaneously, the mixture training dataset, governed\nby volumes of data from different domains, is a critical factor that directly\nimpacts the final model's performance. Unlike many studies that focus on\nenhancing the quality of training datasets through data selection methods, few\nworks explore the intricate relationship between the compositional quantity of\nmixture training datasets and the emergent capabilities of LLMs. Given the\navailability of a high-quality multi-domain training dataset, understanding the\nimpact of data from each domain on the model's overall capabilities is crucial\nfor preparing SFT data and training a well-balanced model that performs\neffectively across diverse domains. In this work, we introduce IDEAL, an\ninnovative data equilibrium adaptation framework designed to effectively\noptimize volumes of data from different domains within mixture SFT datasets,\nthereby enhancing the model's alignment and performance across multiple\ncapabilities. IDEAL employs a gradient-based approach to iteratively refine the\ntraining data distribution, dynamically adjusting the volumes of\ndomain-specific data based on their impact on downstream task performance. By\nleveraging this adaptive mechanism, IDEAL ensures a balanced dataset\ncomposition, enabling the model to achieve robust generalization and consistent\nproficiency across diverse tasks. Experiments across different capabilities\ndemonstrate that IDEAL outperforms conventional uniform data allocation\nstrategies, achieving a comprehensive improvement of approximately 7% in\nmulti-task evaluation scores."}
{"id": "2505.12235", "pdf": "https://arxiv.org/pdf/2505.12235", "abs": "https://arxiv.org/abs/2505.12235", "authors": ["Jia Li", "Nan Gao", "Huaibo Huang", "Ran He"], "title": "NOFT: Test-Time Noise Finetune via Information Bottleneck for Highly Correlated Asset Creation", "categories": ["cs.CV"], "comment": null, "summary": "The diffusion model has provided a strong tool for implementing text-to-image\n(T2I) and image-to-image (I2I) generation. Recently, topology and texture\ncontrol are popular explorations, e.g., ControlNet, IP-Adapter, Ctrl-X, and\nDSG. These methods explicitly consider high-fidelity controllable editing based\non external signals or diffusion feature manipulations. As for diversity, they\ndirectly choose different noise latents. However, the diffused noise is capable\nof implicitly representing the topological and textural manifold of the\ncorresponding image. Moreover, it's an effective workbench to conduct the\ntrade-off between content preservation and controllable variations. Previous\nT2I and I2I diffusion works do not explore the information within the\ncompressed contextual latent. In this paper, we first propose a plug-and-play\nnoise finetune NOFT module employed by Stable Diffusion to generate highly\ncorrelated and diverse images. We fine-tune seed noise or inverse noise through\nan optimal-transported (OT) information bottleneck (IB) with around only 14K\ntrainable parameters and 10 minutes of training. Our test-time NOFT is good at\nproducing high-fidelity image variations considering topology and texture\nalignments. Comprehensive experiments demonstrate that NOFT is a powerful\ngeneral reimagine approach to efficiently fine-tune the 2D/3D AIGC assets with\ntext or image guidance."}
{"id": "2505.12265", "pdf": "https://arxiv.org/pdf/2505.12265", "abs": "https://arxiv.org/abs/2505.12265", "authors": ["Chengwei Qin", "Wenxuan Zhou", "Karthik Abinav Sankararaman", "Nanshu Wang", "Tengyu Xu", "Alexander Radovic", "Eryk Helenowski", "Arya Talebzadeh", "Aditya Tayade", "Sinong Wang", "Shafiq Joty", "Han Fang", "Hao Ma"], "title": "Learning Auxiliary Tasks Improves Reference-Free Hallucination Detection in Open-Domain Long-Form Generation", "categories": ["cs.CL"], "comment": null, "summary": "Hallucination, the generation of factually incorrect information, remains a\nsignificant challenge for large language models (LLMs), especially in\nopen-domain long-form generation. Existing approaches for detecting\nhallucination in long-form tasks either focus on limited domains or rely\nheavily on external fact-checking tools, which may not always be available.\n  In this work, we systematically investigate reference-free hallucination\ndetection in open-domain long-form responses. Our findings reveal that internal\nstates (e.g., model's output probability and entropy) alone are insufficient\nfor reliably (i.e., better than random guessing) distinguishing between factual\nand hallucinated content. To enhance detection, we explore various existing\napproaches, including prompting-based methods, probing, and fine-tuning, with\nfine-tuning proving the most effective. To further improve the accuracy, we\nintroduce a new paradigm, named RATE-FT, that augments fine-tuning with an\nauxiliary task for the model to jointly learn with the main task of\nhallucination detection. With extensive experiments and analysis using a\nvariety of model families & datasets, we demonstrate the effectiveness and\ngeneralizability of our method, e.g., +3% over general fine-tuning methods on\nLongFact."}
{"id": "2505.12767", "pdf": "https://arxiv.org/pdf/2505.12767", "abs": "https://arxiv.org/abs/2505.12767", "authors": ["Danqing Chen", "Tobias Ladner", "Ahmed Rayen Mhadhbi", "Matthias Althoff"], "title": "Language Models That Walk the Talk: A Framework for Formal Fairness Certificates", "categories": ["cs.AI"], "comment": null, "summary": "As large language models become integral to high-stakes applications,\nensuring their robustness and fairness is critical. Despite their success,\nlarge language models remain vulnerable to adversarial attacks, where small\nperturbations, such as synonym substitutions, can alter model predictions,\nposing risks in fairness-critical areas, such as gender bias mitigation, and\nsafety-critical areas, such as toxicity detection. While formal verification\nhas been explored for neural networks, its application to large language models\nremains limited. This work presents a holistic verification framework to\ncertify the robustness of transformer-based language models, with a focus on\nensuring gender fairness and consistent outputs across different gender-related\nterms. Furthermore, we extend this methodology to toxicity detection, offering\nformal guarantees that adversarially manipulated toxic inputs are consistently\ndetected and appropriately censored, thereby ensuring the reliability of\nmoderation systems. By formalizing robustness within the embedding space, this\nwork strengthens the reliability of language models in ethical AI deployment\nand content moderation."}
{"id": "2505.12237", "pdf": "https://arxiv.org/pdf/2505.12237", "abs": "https://arxiv.org/abs/2505.12237", "authors": ["Yuzhi Li", "Haojun Xu", "Feng Tian"], "title": "From Shots to Stories: LLM-Assisted Video Editing with Unified Language Representations", "categories": ["cs.CV"], "comment": null, "summary": "Large Language Models (LLMs) and Vision-Language Models (VLMs) have\ndemonstrated remarkable reasoning and generalization capabilities in video\nunderstanding; however, their application in video editing remains largely\nunderexplored. This paper presents the first systematic study of LLMs in the\ncontext of video editing. To bridge the gap between visual information and\nlanguage-based reasoning, we introduce L-Storyboard, an intermediate\nrepresentation that transforms discrete video shots into structured language\ndescriptions suitable for LLM processing. We categorize video editing tasks\ninto Convergent Tasks and Divergent Tasks, focusing on three core tasks: Shot\nAttributes Classification, Next Shot Selection, and Shot Sequence Ordering. To\naddress the inherent instability of divergent task outputs, we propose the\nStoryFlow strategy, which converts the divergent multi-path reasoning process\ninto a convergent selection mechanism, effectively enhancing task accuracy and\nlogical coherence. Experimental results demonstrate that L-Storyboard\nfacilitates a more robust mapping between visual information and language\ndescriptions, significantly improving the interpretability and privacy\nprotection of video editing tasks. Furthermore, StoryFlow enhances the logical\nconsistency and output stability in Shot Sequence Ordering, underscoring the\nsubstantial potential of LLMs in intelligent video editing."}
{"id": "2505.12268", "pdf": "https://arxiv.org/pdf/2505.12268", "abs": "https://arxiv.org/abs/2505.12268", "authors": ["Pratim Chowdhary"], "title": "$K$-MSHC: Unmasking Minimally Sufficient Head Circuits in Large Language Models with Experiments on Syntactic Classification Tasks", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Understanding which neural components drive specific capabilities in\nmid-sized language models ($\\leq$10B parameters) remains a key challenge. We\nintroduce the $(\\bm{K}, \\epsilon)$-Minimum Sufficient Head Circuit ($K$-MSHC),\na methodology to identify minimal sets of attention heads crucial for\nclassification tasks as well as Search-K-MSHC, an efficient algorithm for\ndiscovering these circuits. Applying our Search-K-MSHC algorithm to Gemma-9B,\nwe analyze three syntactic task families: grammar acceptability, arithmetic\nverification, and arithmetic word problems. Our findings reveal distinct\ntask-specific head circuits, with grammar tasks predominantly utilizing early\nlayers, word problems showing pronounced activity in both shallow and deep\nregions, and arithmetic verification demonstrating a more distributed pattern\nacross the network. We discover non-linear circuit overlap patterns, where\ndifferent task pairs share computational components at varying levels of\nimportance. While grammar and arithmetic share many \"weak\" heads, arithmetic\nand word problems share more consistently critical \"strong\" heads. Importantly,\nwe find that each task maintains dedicated \"super-heads\" with minimal\ncross-task overlap, suggesting that syntactic and numerical competencies emerge\nfrom specialized yet partially reusable head circuits."}
{"id": "2505.12788", "pdf": "https://arxiv.org/pdf/2505.12788", "abs": "https://arxiv.org/abs/2505.12788", "authors": ["Zhongni Hou", "Miao Su", "Xiaolong Jin", "Zixuan Li", "Long Bai", "Jiafeng Guo", "Xueqi Cheng"], "title": "Mixture Policy based Multi-Hop Reasoning over N-tuple Temporal Knowledge Graphs", "categories": ["cs.AI"], "comment": null, "summary": "Temporal Knowledge Graphs (TKGs), which utilize quadruples in the form of\n(subject, predicate, object, timestamp) to describe temporal facts, have\nattracted extensive attention. N-tuple TKGs (N-TKGs) further extend traditional\nTKGs by utilizing n-tuples to incorporate auxiliary elements alongside core\nelements (i.e., subject, predicate, and object) of facts, so as to represent\nthem in a more fine-grained manner. Reasoning over N-TKGs aims to predict\npotential future facts based on historical ones. However, existing N-TKG\nreasoning methods often lack explainability due to their black-box nature.\nTherefore, we introduce a new Reinforcement Learning-based method, named\nMT-Path, which leverages the temporal information to traverse historical\nn-tuples and construct a temporal reasoning path. Specifically, in order to\nintegrate the information encapsulated within n-tuples, i.e., the\nentity-irrelevant information within the predicate, the information about core\nelements, and the complete information about the entire n-tuples, MT-Path\nutilizes a mixture policy-driven action selector, which bases on three\nlow-level policies, namely, the predicate-focused policy, the\ncore-element-focused policy and the whole-fact-focused policy. Further, MT-Path\nutilizes an auxiliary element-aware GCN to capture the rich semantic\ndependencies among facts, thereby enabling the agent to gain a deep\nunderstanding of each n-tuple. Experimental results demonstrate the\neffectiveness and the explainability of MT-Path."}
{"id": "2505.12246", "pdf": "https://arxiv.org/pdf/2505.12246", "abs": "https://arxiv.org/abs/2505.12246", "authors": ["Muleilan Pei", "Jiayao Shan", "Peiliang Li", "Jieqi Shi", "Jing Huo", "Yang Gao", "Shaojie Shen"], "title": "SEPT: Standard-Definition Map Enhanced Scene Perception and Topology Reasoning for Autonomous Driving", "categories": ["cs.CV"], "comment": "Accepted by IEEE Robotics and Automation Letters", "summary": "Online scene perception and topology reasoning are critical for autonomous\nvehicles to understand their driving environments, particularly for mapless\ndriving systems that endeavor to reduce reliance on costly High-Definition (HD)\nmaps. However, recent advances in online scene understanding still face\nlimitations, especially in long-range or occluded scenarios, due to the\ninherent constraints of onboard sensors. To address this challenge, we propose\na Standard-Definition (SD) Map Enhanced scene Perception and Topology reasoning\n(SEPT) framework, which explores how to effectively incorporate the SD map as\nprior knowledge into existing perception and reasoning pipelines. Specifically,\nwe introduce a novel hybrid feature fusion strategy that combines SD maps with\nBird's-Eye-View (BEV) features, considering both rasterized and vectorized\nrepresentations, while mitigating potential misalignment between SD maps and\nBEV feature spaces. Additionally, we leverage the SD map characteristics to\ndesign an auxiliary intersection-aware keypoint detection task, which further\nenhances the overall scene understanding performance. Experimental results on\nthe large-scale OpenLane-V2 dataset demonstrate that by effectively integrating\nSD map priors, our framework significantly improves both scene perception and\ntopology reasoning, outperforming existing methods by a substantial margin."}
{"id": "2505.12273", "pdf": "https://arxiv.org/pdf/2505.12273", "abs": "https://arxiv.org/abs/2505.12273", "authors": ["Md. Atiqur Rahman", "Sabrina Islam", "Mushfiqul Haque Omi"], "title": "LLM-Based Evaluation of Low-Resource Machine Translation: A Reference-less Dialect Guided Approach with a Refined Sylheti-English Benchmark", "categories": ["cs.CL"], "comment": null, "summary": "Evaluating machine translation (MT) for low-resource languages poses a\npersistent challenge, primarily due to the limited availability of high quality\nreference translations. This issue is further exacerbated in languages with\nmultiple dialects, where linguistic diversity and data scarcity hinder robust\nevaluation. Large Language Models (LLMs) present a promising solution through\nreference-free evaluation techniques; however, their effectiveness diminishes\nin the absence of dialect-specific context and tailored guidance. In this work,\nwe propose a comprehensive framework that enhances LLM-based MT evaluation\nusing a dialect guided approach. We extend the ONUBAD dataset by incorporating\nSylheti-English sentence pairs, corresponding machine translations, and Direct\nAssessment (DA) scores annotated by native speakers. To address the vocabulary\ngap, we augment the tokenizer vocabulary with dialect-specific terms. We\nfurther introduce a regression head to enable scalar score prediction and\ndesign a dialect-guided (DG) prompting strategy. Our evaluation across multiple\nLLMs shows that the proposed pipeline consistently outperforms existing\nmethods, achieving the highest gain of +0.1083 in Spearman correlation, along\nwith improvements across other evaluation settings. The dataset and the code\nare available at https://github.com/180041123-Atiq/MTEonLowResourceLanguage."}
{"id": "2505.12795", "pdf": "https://arxiv.org/pdf/2505.12795", "abs": "https://arxiv.org/abs/2505.12795", "authors": ["Shibo Hong", "Jiahao Ying", "Haiyuan Liang", "Mengdi Zhang", "Jun Kuang", "Jiazheng Zhang", "Yixin Cao"], "title": "FRAbench and GenEval: Scaling Fine-Grained Aspect Evaluation across Tasks, Modalities", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Evaluating the open-ended outputs of large language models (LLMs) has become\na bottleneck as model capabilities, task diversity, and modality coverage\nrapidly expand. Existing \"LLM-as-a-Judge\" evaluators are typically narrow in a\nfew tasks, aspects, or modalities, and easily suffer from low consistency. In\nthis paper, we argue that explicit, fine-grained aspect specification is the\nkey to both generalizability and objectivity in automated evaluation. To do so,\nwe introduce a hierarchical aspect taxonomy spanning 112 aspects that unifies\nevaluation across four representative settings - Natural Language Generation,\nImage Understanding, Image Generation, and Interleaved Text-and-Image\nGeneration. Building on this taxonomy, we create FRAbench, a benchmark\ncomprising 60.4k pairwise samples with 325k aspect-level labels obtained from a\ncombination of human and LLM annotations. FRAbench provides the first\nlarge-scale, multi-modal resource for training and meta-evaluating fine-grained\nLMM judges. Leveraging FRAbench, we develop GenEval, a fine-grained evaluator\ngeneralizable across tasks and modalities. Experiments show that GenEval (i)\nattains high agreement with GPT-4o and expert annotators, (ii) transfers\nrobustly to unseen tasks and modalities, and (iii) reveals systematic\nweaknesses of current LMMs on evaluation."}
{"id": "2505.12251", "pdf": "https://arxiv.org/pdf/2505.12251", "abs": "https://arxiv.org/abs/2505.12251", "authors": ["Haozhe Xiang", "Han Zhang", "Yu Cheng", "Xiongwen Quan", "Wanwan Huang"], "title": "SMFusion: Semantic-Preserving Fusion of Multimodal Medical Images for Enhanced Clinical Diagnosis", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal medical image fusion plays a crucial role in medical diagnosis by\nintegrating complementary information from different modalities to enhance\nimage readability and clinical applicability. However, existing methods mainly\nfollow computer vision standards for feature extraction and fusion strategy\nformulation, overlooking the rich semantic information inherent in medical\nimages. To address this limitation, we propose a novel semantic-guided medical\nimage fusion approach that, for the first time, incorporates medical prior\nknowledge into the fusion process. Specifically, we construct a publicly\navailable multimodal medical image-text dataset, upon which text descriptions\ngenerated by BiomedGPT are encoded and semantically aligned with image features\nin a high-dimensional space via a semantic interaction alignment module. During\nthis process, a cross attention based linear transformation automatically maps\nthe relationship between textual and visual features to facilitate\ncomprehensive learning. The aligned features are then embedded into a\ntext-injection module for further feature-level fusion. Unlike traditional\nmethods, we further generate diagnostic reports from the fused images to assess\nthe preservation of medical information. Additionally, we design a medical\nsemantic loss function to enhance the retention of textual cues from the source\nimages. Experimental results on test datasets demonstrate that the proposed\nmethod achieves superior performance in both qualitative and quantitative\nevaluations while preserving more critical medical information."}
{"id": "2505.12287", "pdf": "https://arxiv.org/pdf/2505.12287", "abs": "https://arxiv.org/abs/2505.12287", "authors": ["Linghan Huang", "Haolin Jin", "Zhaoge Bi", "Pengyue Yang", "Peizhou Zhao", "Taozhao Chen", "Xiongfei Wu", "Lei Ma", "Huaming Chen"], "title": "The Tower of Babel Revisited: Multilingual Jailbreak Prompts on Closed-Source Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have seen widespread applications across various\ndomains, yet remain vulnerable to adversarial prompt injections. While most\nexisting research on jailbreak attacks and hallucination phenomena has focused\nprimarily on open-source models, we investigate the frontier of closed-source\nLLMs under multilingual attack scenarios. We present a first-of-its-kind\nintegrated adversarial framework that leverages diverse attack techniques to\nsystematically evaluate frontier proprietary solutions, including GPT-4o,\nDeepSeek-R1, Gemini-1.5-Pro, and Qwen-Max. Our evaluation spans six categories\nof security contents in both English and Chinese, generating 38,400 responses\nacross 32 types of jailbreak attacks. Attack success rate (ASR) is utilized as\nthe quantitative metric to assess performance from three dimensions: prompt\ndesign, model architecture, and language environment. Our findings suggest that\nQwen-Max is the most vulnerable, while GPT-4o shows the strongest defense.\nNotably, prompts in Chinese consistently yield higher ASRs than their English\ncounterparts, and our novel Two-Sides attack technique proves to be the most\neffective across all models. This work highlights a dire need for\nlanguage-aware alignment and robust cross-lingual defenses in LLMs, and we hope\nit will inspire researchers, developers, and policymakers toward more robust\nand inclusive AI systems."}
{"id": "2505.12822", "pdf": "https://arxiv.org/pdf/2505.12822", "abs": "https://arxiv.org/abs/2505.12822", "authors": ["Jing Liu", "Haozheng Wang", "Yueheng Li"], "title": "Emergent Specialization: Rare Token Neurons in Language Models", "categories": ["cs.AI"], "comment": "9 pages, 6 figures", "summary": "Large language models struggle with representing and generating rare tokens\ndespite their importance in specialized domains. In this study, we identify\nneuron structures with exceptionally strong influence on language model's\nprediction of rare tokens, termed as rare token neurons, and investigate the\nmechanism for their emergence and behavior. These neurons exhibit a\ncharacteristic three-phase organization (plateau, power-law, and rapid decay)\nthat emerges dynamically during training, evolving from a homogeneous initial\nstate to a functionally differentiated architecture. In the activation space,\nrare token neurons form a coordinated subnetwork that selectively co-activates\nwhile avoiding co-activation with other neurons. This functional specialization\npotentially correlates with the development of heavy-tailed weight\ndistributions, suggesting a statistical mechanical basis for emergent\nspecialization."}
{"id": "2505.12253", "pdf": "https://arxiv.org/pdf/2505.12253", "abs": "https://arxiv.org/abs/2505.12253", "authors": ["Hanyu Zhou", "Gim Hee Lee"], "title": "LLaVA-4D: Embedding SpatioTemporal Prompt into LMMs for 4D Scene Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Despite achieving significant progress in 2D image understanding, large\nmultimodal models (LMMs) struggle in the physical world due to the lack of\nspatial representation. Typically, existing 3D LMMs mainly embed 3D positions\nas fixed spatial prompts within visual features to represent the scene.\nHowever, these methods are limited to understanding the static background and\nfail to capture temporally varying dynamic objects. In this paper, we propose\nLLaVA-4D, a general LMM framework with a novel spatiotemporal prompt for visual\nrepresentation in 4D scene understanding. The spatiotemporal prompt is\ngenerated by encoding 3D position and 1D time into a dynamic-aware 4D\ncoordinate embedding. Moreover, we demonstrate that spatial and temporal\ncomponents disentangled from visual features are more effective in\ndistinguishing the background from objects. This motivates embedding the 4D\nspatiotemporal prompt into these features to enhance the dynamic scene\nrepresentation. By aligning visual spatiotemporal embeddings with language\nembeddings, LMMs gain the ability to understand both spatial and temporal\ncharacteristics of static background and dynamic objects in the physical world.\nAdditionally, we construct a 4D vision-language dataset with spatiotemporal\ncoordinate annotations for instruction fine-tuning LMMs. Extensive experiments\nhave been conducted to demonstrate the effectiveness of our method across\ndifferent tasks in 4D scene understanding."}
{"id": "2505.12299", "pdf": "https://arxiv.org/pdf/2505.12299", "abs": "https://arxiv.org/abs/2505.12299", "authors": ["Kun Huang", "Weikai Xu", "Yuxuan Liu", "Quandong Wang", "Pengzhi Gao", "Wei Liu", "Jian Luan", "Bin Wang", "Bo An"], "title": "Enhance Mobile Agents Thinking Process Via Iterative Preference Learning", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 8 figures, 7 tables", "summary": "The Chain of Action-Planning Thoughts (CoaT) paradigm has been shown to\nimprove the reasoning performance of VLM-based mobile agents in GUI tasks.\nHowever, the scarcity of diverse CoaT trajectories limits the expressiveness\nand generalization ability of such agents. While self-training is commonly\nemployed to address data scarcity, existing approaches either overlook the\ncorrectness of intermediate reasoning steps or depend on expensive\nprocess-level annotations to construct process reward models (PRM). To address\nthe above problems, we propose an Iterative Preference Learning (IPL) that\nconstructs a CoaT-tree through interative sampling, scores leaf nodes using\nrule-based reward, and backpropagates feedback to derive Thinking-level Direct\nPreference Optimization (T-DPO) pairs. To prevent overfitting during warm-up\nsupervised fine-tuning, we further introduce a three-stage instruction\nevolution, which leverages GPT-4o to generate diverse Q\\&A pairs based on real\nmobile UI screenshots, enhancing both generality and layout understanding.\nExperiments on three standard Mobile GUI-agent benchmarks demonstrate that our\nagent MobileIPL outperforms strong baselines, including continual pretraining\nmodels such as OS-ATLAS and UI-TARS. It achieves state-of-the-art performance\nacross three standard Mobile GUI-Agents benchmarks and shows strong\ngeneralization to out-of-domain scenarios."}
{"id": "2505.12833", "pdf": "https://arxiv.org/pdf/2505.12833", "abs": "https://arxiv.org/abs/2505.12833", "authors": ["Zhuo Yang", "Lingli Ge", "Dong Han", "Tianfan Fu", "Yuqiang Li"], "title": "Reasoning BO: Enhancing Bayesian Optimization with Long-Context Reasoning Power of LLMs", "categories": ["cs.AI"], "comment": null, "summary": "Many real-world scientific and industrial applications require the\noptimization of expensive black-box functions. Bayesian Optimization (BO)\nprovides an effective framework for such problems. However, traditional BO\nmethods are prone to get trapped in local optima and often lack interpretable\ninsights. To address this issue, this paper designs Reasoning BO, a novel\nframework that leverages reasoning models to guide the sampling process in BO\nwhile incorporating multi-agent systems and knowledge graphs for online\nknowledge accumulation. By integrating the reasoning and contextual\nunderstanding capabilities of Large Language Models (LLMs), we can provide\nstrong guidance to enhance the BO process. As the optimization progresses,\nReasoning BO provides real-time sampling recommendations along with critical\ninsights grounded in plausible scientific theories, aiding in the discovery of\nsuperior solutions within the search space. We systematically evaluate our\napproach across 10 diverse tasks encompassing synthetic mathematical functions\nand complex real-world applications. The framework demonstrates its capability\nto progressively refine sampling strategies through real-time insights and\nhypothesis evolution, effectively identifying higher-performing regions of the\nsearch space for focused exploration. This process highlights the powerful\nreasoning and context-learning abilities of LLMs in optimization scenarios. For\nexample, in the Direct Arylation task, our method increased the yield to 60.7%,\nwhereas traditional BO achieved only a 25.2% yield. Furthermore, our\ninvestigation reveals that smaller LLMs, when fine-tuned through reinforcement\nlearning, can attain comparable performance to their larger counterparts. This\nenhanced reasoning capability paves the way for more efficient automated\nscientific experimentation while maintaining computational feasibility."}
{"id": "2505.12254", "pdf": "https://arxiv.org/pdf/2505.12254", "abs": "https://arxiv.org/abs/2505.12254", "authors": ["Yiwei Ou", "Xiaobin Ren", "Ronggui Sun", "Guansong Gao", "Ziyi Jiang", "Kaiqi Zhao", "Manfredo Manfredini"], "title": "MMS-VPR: Multimodal Street-Level Visual Place Recognition Dataset and Benchmark", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Existing visual place recognition (VPR) datasets predominantly rely on\nvehicle-mounted imagery, lack multimodal diversity and underrepresent dense,\nmixed-use street-level spaces, especially in non-Western urban contexts. To\naddress these gaps, we introduce MMS-VPR, a large-scale multimodal dataset for\nstreet-level place recognition in complex, pedestrian-only environments. The\ndataset comprises 78,575 annotated images and 2,512 video clips captured across\n207 locations in a ~70,800 $\\mathrm{m}^2$ open-air commercial district in\nChengdu, China. Each image is labeled with precise GPS coordinates, timestamp,\nand textual metadata, and covers varied lighting conditions, viewpoints, and\ntimeframes. MMS-VPR follows a systematic and replicable data collection\nprotocol with minimal device requirements, lowering the barrier for scalable\ndataset creation. Importantly, the dataset forms an inherent spatial graph with\n125 edges, 81 nodes, and 1 subgraph, enabling structure-aware place\nrecognition. We further define two application-specific subsets --\nDataset_Edges and Dataset_Points -- to support fine-grained and graph-based\nevaluation tasks. Extensive benchmarks using conventional VPR models, graph\nneural networks, and multimodal baselines show substantial improvements when\nleveraging multimodal and structural cues. MMS-VPR facilitates future research\nat the intersection of computer vision, geospatial understanding, and\nmultimodal reasoning. The dataset is publicly available at\nhttps://huggingface.co/datasets/Yiwei-Ou/MMS-VPR."}
{"id": "2505.12300", "pdf": "https://arxiv.org/pdf/2505.12300", "abs": "https://arxiv.org/abs/2505.12300", "authors": ["Weixuan Wang", "Minghao Wu", "Barry Haddow", "Alexandra Birch"], "title": "HBO: Hierarchical Balancing Optimization for Fine-Tuning Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Fine-tuning large language models (LLMs) on a mixture of diverse datasets\nposes challenges due to data imbalance and heterogeneity. Existing methods\noften address these issues across datasets (globally) but overlook the\nimbalance and heterogeneity within individual datasets (locally), which limits\ntheir effectiveness. We introduce Hierarchical Balancing Optimization (HBO), a\nnovel method that enables LLMs to autonomously adjust data allocation during\nfine-tuning both across datasets (globally) and within each individual dataset\n(locally). HBO employs a bilevel optimization strategy with two types of\nactors: a Global Actor, which balances data sampling across different subsets\nof the training mixture, and several Local Actors, which optimizes data usage\nwithin each subset based on difficulty levels. These actors are guided by\nreward functions derived from the LLM's training state, which measure learning\nprogress and relative performance improvement. We evaluate HBO on three LLM\nbackbones across nine diverse tasks in multilingual and multitask setups.\nResults show that HBO consistently outperforms existing baselines, achieving\nsignificant accuracy gains. Our in-depth analysis further demonstrates that\nboth the global actor and local actors of HBO effectively adjust data usage\nduring fine-tuning. HBO provides a comprehensive solution to the challenges of\ndata imbalance and heterogeneity in LLM fine-tuning, enabling more effective\ntraining across diverse datasets."}
{"id": "2505.12844", "pdf": "https://arxiv.org/pdf/2505.12844", "abs": "https://arxiv.org/abs/2505.12844", "authors": ["Shuo Sun", "Yimin Zhao", "Christina Dao Wen Lee", "Jiawei Sun", "Chengran Yuan", "Zefan Huang", "Dongen Li", "Justin KW Yeoh", "Alok Prakash", "Thomas W. Malone", "Marcelo H. Ang Jr"], "title": "AGI-Elo: How Far Are We From Mastering A Task?", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "As the field progresses toward Artificial General Intelligence (AGI), there\nis a pressing need for more comprehensive and insightful evaluation frameworks\nthat go beyond aggregate performance metrics. This paper introduces a unified\nrating system that jointly models the difficulty of individual test cases and\nthe competency of AI models (or humans) across vision, language, and action\ndomains. Unlike existing metrics that focus solely on models, our approach\nallows for fine-grained, difficulty-aware evaluations through competitive\ninteractions between models and tasks, capturing both the long-tail\ndistribution of real-world challenges and the competency gap between current\nmodels and full task mastery. We validate the generalizability and robustness\nof our system through extensive experiments on multiple established datasets\nand models across distinct AGI domains. The resulting rating distributions\noffer novel perspectives and interpretable insights into task difficulty, model\nprogression, and the outstanding challenges that remain on the path to\nachieving full AGI task mastery."}
{"id": "2505.12266", "pdf": "https://arxiv.org/pdf/2505.12266", "abs": "https://arxiv.org/abs/2505.12266", "authors": ["ZhanFeng Feng", "Long Peng", "Xin Di", "Yong Guo", "Wenbo Li", "Yulun Zhang", "Renjing Pei", "Yang Wang", "Yang Cao", "Zheng-Jun Zha"], "title": "PMQ-VE: Progressive Multi-Frame Quantization for Video Enhancement", "categories": ["cs.CV"], "comment": null, "summary": "Multi-frame video enhancement tasks aim to improve the spatial and temporal\nresolution and quality of video sequences by leveraging temporal information\nfrom multiple frames, which are widely used in streaming video processing,\nsurveillance, and generation. Although numerous Transformer-based enhancement\nmethods have achieved impressive performance, their computational and memory\ndemands hinder deployment on edge devices. Quantization offers a practical\nsolution by reducing the bit-width of weights and activations to improve\nefficiency. However, directly applying existing quantization methods to video\nenhancement tasks often leads to significant performance degradation and loss\nof fine details. This stems from two limitations: (a) inability to allocate\nvarying representational capacity across frames, which results in suboptimal\ndynamic range adaptation; (b) over-reliance on full-precision teachers, which\nlimits the learning of low-bit student models. To tackle these challenges, we\npropose a novel quantization method for video enhancement: Progressive\nMulti-Frame Quantization for Video Enhancement (PMQ-VE). This framework\nfeatures a coarse-to-fine two-stage process: Backtracking-based Multi-Frame\nQuantization (BMFQ) and Progressive Multi-Teacher Distillation (PMTD). BMFQ\nutilizes a percentile-based initialization and iterative search with pruning\nand backtracking for robust clipping bounds. PMTD employs a progressive\ndistillation strategy with both full-precision and multiple high-bit (INT)\nteachers to enhance low-bit models' capacity and quality. Extensive experiments\ndemonstrate that our method outperforms existing approaches, achieving\nstate-of-the-art performance across multiple tasks and benchmarks.The code will\nbe made publicly available at: https://github.com/xiaoBIGfeng/PMQ-VE."}
{"id": "2505.12306", "pdf": "https://arxiv.org/pdf/2505.12306", "abs": "https://arxiv.org/abs/2505.12306", "authors": ["Yuwei Zhang", "Wenhao Yu", "Shangbin Feng", "Yifan Zhu", "Letian Peng", "Jayanth Srinivasa", "Gaowen Liu", "Jingbo Shang"], "title": "Bidirectional LMs are Better Knowledge Memorizers? A Benchmark for Real-world Knowledge Injection", "categories": ["cs.CL"], "comment": "Dataset is available at\n  https://huggingface.co/datasets/YWZBrandon/wikidyk", "summary": "Despite significant advances in large language models (LLMs), their knowledge\nmemorization capabilities remain underexplored, due to the lack of standardized\nand high-quality test ground. In this paper, we introduce a novel, real-world\nand large-scale knowledge injection benchmark that evolves continuously over\ntime without requiring human intervention. Specifically, we propose WikiDYK,\nwhich leverages recently-added and human-written facts from Wikipedia's \"Did\nYou Know...\" entries. These entries are carefully selected by expert Wikipedia\neditors based on criteria such as verifiability and clarity. Each entry is\nconverted into multiple question-answer pairs spanning diverse task formats\nfrom easy cloze prompts to complex multi-hop questions. WikiDYK contains 12,290\nfacts and 77,180 questions, which is also seamlessly extensible with future\nupdates from Wikipedia editors. Extensive experiments using continued\npre-training reveal a surprising insight: despite their prevalence in modern\nLLMs, Causal Language Models (CLMs) demonstrate significantly weaker knowledge\nmemorization capabilities compared to Bidirectional Language Models (BiLMs),\nexhibiting a 23% lower accuracy in terms of reliability. To compensate for the\nsmaller scales of current BiLMs, we introduce a modular collaborative framework\nutilizing ensembles of BiLMs as external knowledge repositories to integrate\nwith LLMs. Experiment shows that our framework further improves the reliability\naccuracy by up to 29.1%."}
{"id": "2505.12845", "pdf": "https://arxiv.org/pdf/2505.12845", "abs": "https://arxiv.org/abs/2505.12845", "authors": ["Ruopei Sun", "Jianfeng Cai", "Jinhua Zhu", "Kangwen Zhao", "Dongyun Xue", "Wengang Zhou", "Li Li", "Houqiang Li"], "title": "Multi-Level Aware Preference Learning: Enhancing RLHF for Complex Multi-Instruction Tasks", "categories": ["cs.AI"], "comment": null, "summary": "RLHF has emerged as a predominant approach for aligning artificial\nintelligence systems with human preferences, demonstrating exceptional and\nmeasurable efficacy in instruction following tasks; however, it exhibits\ninsufficient compliance capabilities when confronted with complex\nmulti-instruction tasks. Conventional approaches rely heavily on human\nannotation or more sophisticated large language models, thereby introducing\nsubstantial resource expenditure or potential bias concerns. Meanwhile,\nalternative synthetic methods that augment standard preference datasets often\ncompromise the model's semantic quality. Our research identifies a critical\noversight in existing techniques, which predominantly focus on comparing\nresponses while neglecting valuable latent signals embedded within prompt\ninputs, and which only focus on preference disparities at the intra-sample\nlevel, while neglecting to account for the inter-sample level preference\ndifferentials that exist among preference data. To leverage these previously\nneglected indicators, we propose a novel Multi-level Aware Preference Learning\n(MAPL) framework, capable of enhancing multi-instruction capabilities.\nSpecifically, for any given response in original preference data pairs, we\nconstruct varied prompts with a preference relation under different conditions,\nin order to learn intra-sample level preference disparities. Furthermore, for\nany given original preference pair, we synthesize multi-instruction preference\npairs to capture preference discrepancies at the inter-sample level. Building\non the two datasets constructed above, we consequently devise two sophisticated\ntraining objective functions. Subsequently, our framework integrates seamlessly\ninto both Reward Modeling and Direct Preference Optimization paradigms. Through\nrigorous evaluation across multiple benchmarks, we empirically validate the\nefficacy of our framework."}
{"id": "2505.12274", "pdf": "https://arxiv.org/pdf/2505.12274", "abs": "https://arxiv.org/abs/2505.12274", "authors": ["Yixiao Chen", "Zhiyuan Ma", "Guoli Jia", "Che Jiang", "Jianjun Li", "Bowen Zhou"], "title": "Context-Aware Autoregressive Models for Multi-Conditional Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Autoregressive transformers have recently shown impressive image generation\nquality and efficiency on par with state-of-the-art diffusion models. Unlike\ndiffusion architectures, autoregressive models can naturally incorporate\narbitrary modalities into a single, unified token sequence--offering a concise\nsolution for multi-conditional image generation tasks. In this work, we propose\n$\\textbf{ContextAR}$, a flexible and effective framework for multi-conditional\nimage generation. ContextAR embeds diverse conditions (e.g., canny edges, depth\nmaps, poses) directly into the token sequence, preserving modality-specific\nsemantics. To maintain spatial alignment while enhancing discrimination among\ndifferent condition types, we introduce hybrid positional encodings that fuse\nRotary Position Embedding with Learnable Positional Embedding. We design\nConditional Context-aware Attention to reduces computational complexity while\npreserving effective intra-condition perception. Without any fine-tuning,\nContextAR supports arbitrary combinations of conditions during inference time.\nExperimental results demonstrate the powerful controllability and versatility\nof our approach, and show that the competitive perpormance than diffusion-based\nmulti-conditional control approaches the existing autoregressive baseline\nacross diverse multi-condition driven scenarios. Project page:\n$\\href{https://context-ar.github.io/}{https://context-ar.github.io/.}$"}
{"id": "2505.12313", "pdf": "https://arxiv.org/pdf/2505.12313", "abs": "https://arxiv.org/abs/2505.12313", "authors": ["Weixuan Wang", "Minghao Wu", "Barry Haddow", "Alexandra Birch"], "title": "ExpertSteer: Intervening in LLMs through Expert Knowledge", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) exhibit remarkable capabilities across various\ntasks, yet guiding them to follow desired behaviours during inference remains a\nsignificant challenge. Activation steering offers a promising method to control\nthe generation process of LLMs by modifying their internal activations.\nHowever, existing methods commonly intervene in the model's behaviour using\nsteering vectors generated by the model itself, which constrains their\neffectiveness to that specific model and excludes the possibility of leveraging\npowerful external expert models for steering. To address these limitations, we\npropose ExpertSteer, a novel approach that leverages arbitrary specialized\nexpert models to generate steering vectors, enabling intervention in any LLMs.\nExpertSteer transfers the knowledge from an expert model to a target LLM\nthrough a cohesive four-step process: first aligning representation dimensions\nwith auto-encoders to enable cross-model transfer, then identifying\nintervention layer pairs based on mutual information analysis, next generating\nsteering vectors from the expert model using Recursive Feature Machines, and\nfinally applying these vectors on the identified layers during inference to\nselectively guide the target LLM without updating model parameters. We conduct\ncomprehensive experiments using three LLMs on 15 popular benchmarks across four\ndistinct domains. Experiments demonstrate that ExpertSteer significantly\noutperforms established baselines across diverse tasks at minimal cost."}
{"id": "2505.12872", "pdf": "https://arxiv.org/pdf/2505.12872", "abs": "https://arxiv.org/abs/2505.12872", "authors": ["Maytus Piriyajitakonkij", "Rujikorn Charakorn", "Weicheng Tao", "Wei Pan", "Mingfei Sun", "Cheston Tan", "Mengmi Zhang"], "title": "From Grunts to Grammar: Emergent Language from Cooperative Foraging", "categories": ["cs.AI", "cs.LG", "cs.MA"], "comment": null, "summary": "Early cavemen relied on gestures, vocalizations, and simple signals to\ncoordinate, plan, avoid predators, and share resources. Today, humans\ncollaborate using complex languages to achieve remarkable results. What drives\nthis evolution in communication? How does language emerge, adapt, and become\nvital for teamwork? Understanding the origins of language remains a challenge.\nA leading hypothesis in linguistics and anthropology posits that language\nevolved to meet the ecological and social demands of early human cooperation.\nLanguage did not arise in isolation, but through shared survival goals.\nInspired by this view, we investigate the emergence of language in multi-agent\nForaging Games. These environments are designed to reflect the cognitive and\necological constraints believed to have influenced the evolution of\ncommunication. Agents operate in a shared grid world with only partial\nknowledge about other agents and the environment, and must coordinate to\ncomplete games like picking up high-value targets or executing temporally\nordered actions. Using end-to-end deep reinforcement learning, agents learn\nboth actions and communication strategies from scratch. We find that agents\ndevelop communication protocols with hallmark features of natural language:\narbitrariness, interchangeability, displacement, cultural transmission, and\ncompositionality. We quantify each property and analyze how different factors,\nsuch as population size and temporal dependencies, shape specific aspects of\nthe emergent language. Our framework serves as a platform for studying how\nlanguage can evolve from partial observability, temporal reasoning, and\ncooperative goals in embodied multi-agent settings. We will release all data,\ncode, and models publicly."}
{"id": "2505.12280", "pdf": "https://arxiv.org/pdf/2505.12280", "abs": "https://arxiv.org/abs/2505.12280", "authors": ["Sijie Zhao", "Feng Liu", "Xueliang Zhang", "Hao Chen", "Pengfeng Xiao", "Lei Bai"], "title": "Temporal-Spectral-Spatial Unified Remote Sensing Dense Prediction", "categories": ["cs.CV"], "comment": "12 pages, 4 figures, Code\n  link:https://github.com/walking-shadow/Official_TSSUN", "summary": "The proliferation of diverse remote sensing data has spurred advancements in\ndense prediction tasks, yet significant challenges remain in handling data\nheterogeneity. Remote sensing imagery exhibits substantial variability across\ntemporal, spectral, and spatial (TSS) dimensions, complicating unified data\nprocessing. Current deep learning models for dense prediction tasks, such as\nsemantic segmentation and change detection, are typically tailored to specific\ninput-output configurations. Consequently, variations in data dimensionality or\ntask requirements often lead to significant performance degradation or model\nincompatibility, necessitating costly retraining or fine-tuning efforts for\ndifferent application scenarios. This paper introduces the\nTemporal-Spectral-Spatial Unified Network (TSSUN), a novel architecture\ndesigned for unified representation and modeling of remote sensing data across\ndiverse TSS characteristics and task types. TSSUN employs a\nTemporal-Spectral-Spatial Unified Strategy that leverages meta-information to\ndecouple and standardize input representations from varied temporal, spectral,\nand spatial configurations, and similarly unifies output structures for\ndifferent dense prediction tasks and class numbers. Furthermore, a Local-Global\nWindow Attention mechanism is proposed to efficiently capture both local\ncontextual details and global dependencies, enhancing the model's adaptability\nand feature extraction capabilities. Extensive experiments on multiple datasets\ndemonstrate that a single TSSUN model effectively adapts to heterogeneous\ninputs and unifies various dense prediction tasks. The proposed approach\nconsistently achieves or surpasses state-of-the-art performance, highlighting\nits robustness and generalizability for complex remote sensing applications\nwithout requiring task-specific modifications."}
{"id": "2505.12328", "pdf": "https://arxiv.org/pdf/2505.12328", "abs": "https://arxiv.org/abs/2505.12328", "authors": ["Xinye Li", "Mingqi Wan", "Dianbo Sui"], "title": "LLMSR@XLLM25: An Empirical Study of LLM for Structural Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "We present Team asdfo123's submission to the LLMSR@XLLM25 shared task, which\nevaluates large language models on producing fine-grained, controllable, and\ninterpretable reasoning processes. Systems must extract all problem conditions,\ndecompose a chain of thought into statement-evidence pairs, and verify the\nlogical validity of each pair. Leveraging only the off-the-shelf\nMeta-Llama-3-8B-Instruct, we craft a concise few-shot, multi-turn prompt that\nfirst enumerates all conditions and then guides the model to label, cite, and\nadjudicate every reasoning step. A lightweight post-processor based on regular\nexpressions normalises spans and enforces the official JSON schema. Without\nfine-tuning, external retrieval, or ensembling, our method ranks 5th overall,\nachieving macro F1 scores on par with substantially more complex and\nresource-consuming pipelines. We conclude by analysing the strengths and\nlimitations of our approach and outlining directions for future research in\nstructural reasoning with LLMs. Our code is available at\nhttps://github.com/asdfo123/LLMSR-asdfo123."}
{"id": "2505.12886", "pdf": "https://arxiv.org/pdf/2505.12886", "abs": "https://arxiv.org/abs/2505.12886", "authors": ["Zhongxiang Sun", "Qipeng Wang", "Haoyu Wang", "Xiao Zhang", "Jun Xu"], "title": "Detection and Mitigation of Hallucination in Large Reasoning Models: A Mechanistic Perspective", "categories": ["cs.AI", "cs.CL", "cs.CY"], "comment": "25 pages", "summary": "Large Reasoning Models (LRMs) have shown impressive capabilities in\nmulti-step reasoning tasks. However, alongside these successes, a more\ndeceptive form of model error has emerged--Reasoning Hallucination--where\nlogically coherent but factually incorrect reasoning traces lead to persuasive\nyet faulty conclusions. Unlike traditional hallucinations, these errors are\nembedded within structured reasoning, making them more difficult to detect and\npotentially more harmful. In this work, we investigate reasoning hallucinations\nfrom a mechanistic perspective. We propose the Reasoning Score, which\nquantifies the depth of reasoning by measuring the divergence between logits\nobtained from projecting late layers of LRMs to the vocabulary space,\neffectively distinguishing shallow pattern-matching from genuine deep\nreasoning. Using this score, we conduct an in-depth analysis on the ReTruthQA\ndataset and identify two key reasoning hallucination patterns: early-stage\nfluctuation in reasoning depth and incorrect backtracking to flawed prior\nsteps. These insights motivate our Reasoning Hallucination Detection (RHD)\nframework, which achieves state-of-the-art performance across multiple domains.\nTo mitigate reasoning hallucinations, we further introduce GRPO-R, an enhanced\nreinforcement learning algorithm that incorporates step-level deep reasoning\nrewards via potential-based shaping. Our theoretical analysis establishes\nstronger generalization guarantees, and experiments demonstrate improved\nreasoning quality and reduced hallucination rates."}
{"id": "2505.12307", "pdf": "https://arxiv.org/pdf/2505.12307", "abs": "https://arxiv.org/abs/2505.12307", "authors": ["Maoyuan Ye", "Jing Zhang", "Juhua Liu", "Bo Du", "Dacheng Tao"], "title": "LogicOCR: Do Your Large Multimodal Models Excel at Logical Reasoning on Text-Rich Images?", "categories": ["cs.CV", "cs.CL"], "comment": "GitHub: \\url{https://github.com/MiliLab/LogicOCR}", "summary": "Recent advances in Large Multimodal Models (LMMs) have significantly improved\ntheir reasoning and Optical Character Recognition (OCR) capabilities. However,\ntheir performance on complex logical reasoning tasks involving text-rich images\nremains underexplored. To bridge this gap, we introduce LogicOCR, a benchmark\ncomprising 1,100 multiple-choice questions designed to evaluate LMMs' logical\nreasoning abilities on text-rich images, while minimizing reliance on\ndomain-specific knowledge (e.g., mathematics). We construct LogicOCR by\ncurating a text corpus from the Chinese National Civil Servant Examination and\ndevelop a scalable, automated pipeline to convert it into multimodal samples.\nFirst, we design prompt templates to steer GPT-Image-1 to generate images with\ndiverse backgrounds, interleaved text-illustration layouts, and varied fonts,\nensuring contextual relevance and visual realism. Then, the generated images\nare manually verified, with low-quality examples discarded. We evaluate a range\nof representative open-source and proprietary LMMs under both Chain-of-Thought\n(CoT) and direct-answer settings. Our multi-dimensional analysis reveals key\ninsights, such as the impact of test-time scaling, input modality differences,\nand sensitivity to visual-text orientation. Notably, LMMs still lag in\nmultimodal reasoning compared to text-only inputs, indicating that they have\nnot fully bridged visual reading with reasoning. We hope LogicOCR will serve as\na valuable resource for advancing multimodal reasoning research. The dataset is\navailable at https://github.com/MiliLab/LogicOCR."}
{"id": "2505.12345", "pdf": "https://arxiv.org/pdf/2505.12345", "abs": "https://arxiv.org/abs/2505.12345", "authors": ["Qizhou Chen", "Dakan Wang", "Taolin Zhang", "Zaoming Yan", "Chengsong You", "Chengyu Wang", "Xiaofeng He"], "title": "UniEdit: A Unified Knowledge Editing Benchmark for Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Model editing aims to enhance the accuracy and reliability of large language\nmodels (LLMs) by efficiently adjusting their internal parameters. Currently,\nmost LLM editing datasets are confined to narrow knowledge domains and cover a\nlimited range of editing evaluation. They often overlook the broad scope of\nediting demands and the diversity of ripple effects resulting from edits. In\nthis context, we introduce UniEdit, a unified benchmark for LLM editing\ngrounded in open-domain knowledge. First, we construct editing samples by\nselecting entities from 25 common domains across five major categories,\nutilizing the extensive triple knowledge available in open-domain knowledge\ngraphs to ensure comprehensive coverage of the knowledge domains. To address\nthe issues of generality and locality in editing, we design an Neighborhood\nMulti-hop Chain Sampling (NMCS) algorithm to sample subgraphs based on a given\nknowledge piece to entail comprehensive ripple effects to evaluate. Finally, we\nemploy proprietary LLMs to convert the sampled knowledge subgraphs into natural\nlanguage text, guaranteeing grammatical accuracy and syntactical diversity.\nExtensive statistical analysis confirms the scale, comprehensiveness, and\ndiversity of our UniEdit benchmark. We conduct comprehensive experiments across\nmultiple LLMs and editors, analyzing their performance to highlight strengths\nand weaknesses in editing across open knowledge domains and various evaluation\ncriteria, thereby offering valuable insights for future research endeavors."}
{"id": "2505.12891", "pdf": "https://arxiv.org/pdf/2505.12891", "abs": "https://arxiv.org/abs/2505.12891", "authors": ["Shaohang Wei", "Wei Li", "Feifan Song", "Wen Luo", "Tianyi Zhuang", "Haochen Tan", "Zhijiang Guo", "Houfeng Wang"], "title": "TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in Real-World Scenarios", "categories": ["cs.AI", "cs.CL"], "comment": "First version. There are still some examples to be added into the\n  appendix", "summary": "Temporal reasoning is pivotal for Large Language Models (LLMs) to comprehend\nthe real world. However, existing works neglect the real-world challenges for\ntemporal reasoning: (1) intensive temporal information, (2) fast-changing event\ndynamics, and (3) complex temporal dependencies in social interactions. To\nbridge this gap, we propose a multi-level benchmark TIME, designed for temporal\nreasoning in real-world scenarios. TIME consists of 38,522 QA pairs, covering 3\nlevels with 11 fine-grained sub-tasks. This benchmark encompasses 3\nsub-datasets reflecting different real-world challenges: TIME-Wiki, TIME-News,\nand TIME-Dial. We conduct extensive experiments on reasoning models and\nnon-reasoning models. And we conducted an in-depth analysis of temporal\nreasoning performance across diverse real-world scenarios and tasks, and\nsummarized the impact of test-time scaling on temporal reasoning capabilities.\nAdditionally, we release TIME-Lite, a human-annotated subset to foster future\nresearch and standardized evaluation in temporal reasoning. The code is\navailable at https://github.com/sylvain-wei/TIME , and the dataset is available\nat https://huggingface.co/datasets/SylvainWei/TIME ."}
{"id": "2505.12310", "pdf": "https://arxiv.org/pdf/2505.12310", "abs": "https://arxiv.org/abs/2505.12310", "authors": ["Shouyi Lu", "Huanyu Zhou", "Guirong Zhuo"], "title": "DNOI-4DRO: Deep 4D Radar Odometry with Differentiable Neural-Optimization Iterations", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "16 pages,10 figures", "summary": "A novel learning-optimization-combined 4D radar odometry model, named\nDNOI-4DRO, is proposed in this paper. The proposed model seamlessly integrates\ntraditional geometric optimization with end-to-end neural network training,\nleveraging an innovative differentiable neural-optimization iteration operator.\nIn this framework, point-wise motion flow is first estimated using a neural\nnetwork, followed by the construction of a cost function based on the\nrelationship between point motion and pose in 3D space. The radar pose is then\nrefined using Gauss-Newton updates. Additionally, we design a dual-stream 4D\nradar backbone that integrates multi-scale geometric features and\nclustering-based class-aware features to enhance the representation of sparse\n4D radar point clouds. Extensive experiments on the VoD and Snail-Radar\ndatasets demonstrate the superior performance of our model, which outperforms\nrecent classical and learning-based approaches. Notably, our method even\nachieves results comparable to A-LOAM with mapping optimization using LiDAR\npoint clouds as input. Our models and code will be publicly released."}
{"id": "2505.12349", "pdf": "https://arxiv.org/pdf/2505.12349", "abs": "https://arxiv.org/abs/2505.12349", "authors": ["Axel Abels", "Tom Lenaerts"], "title": "Wisdom from Diversity: Bias Mitigation Through Hybrid Human-LLM Crowds", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "comment": "Accepted for publication in the Proceedings of the 34th International\n  Joint Conference on Artificial Intelligence (IJCAI 2025)", "summary": "Despite their performance, large language models (LLMs) can inadvertently\nperpetuate biases found in the data they are trained on. By analyzing LLM\nresponses to bias-eliciting headlines, we find that these models often mirror\nhuman biases. To address this, we explore crowd-based strategies for mitigating\nbias through response aggregation. We first demonstrate that simply averaging\nresponses from multiple LLMs, intended to leverage the \"wisdom of the crowd\",\ncan exacerbate existing biases due to the limited diversity within LLM crowds.\nIn contrast, we show that locally weighted aggregation methods more effectively\nleverage the wisdom of the LLM crowd, achieving both bias mitigation and\nimproved accuracy. Finally, recognizing the complementary strengths of LLMs\n(accuracy) and humans (diversity), we demonstrate that hybrid crowds containing\nboth significantly enhance performance and further reduce biases across ethnic\nand gender-related contexts."}
{"id": "2505.12923", "pdf": "https://arxiv.org/pdf/2505.12923", "abs": "https://arxiv.org/abs/2505.12923", "authors": ["Pedro M. P. Curvo"], "title": "The Traitors: Deception and Trust in Multi-Agent Language Model Simulations", "categories": ["cs.AI", "cs.MA"], "comment": "9 main pages, 31 pages", "summary": "As AI systems increasingly assume roles where trust and alignment with human\nvalues are essential, understanding when and why they engage in deception has\nbecome a critical research priority. We introduce The Traitors, a multi-agent\nsimulation framework inspired by social deduction games, designed to probe\ndeception, trust formation, and strategic communication among large language\nmodel (LLM) agents under asymmetric information. A minority of agents the\ntraitors seek to mislead the majority, while the faithful must infer hidden\nidentities through dialogue and reasoning. Our contributions are: (1) we ground\nthe environment in formal frameworks from game theory, behavioral economics,\nand social cognition; (2) we develop a suite of evaluation metrics capturing\ndeception success, trust dynamics, and collective inference quality; (3) we\nimplement a fully autonomous simulation platform where LLMs reason over\npersistent memory and evolving social dynamics, with support for heterogeneous\nagent populations, specialized traits, and adaptive behaviors. Our initial\nexperiments across DeepSeek-V3, GPT-4o-mini, and GPT-4o (10 runs per model)\nreveal a notable asymmetry: advanced models like GPT-4o demonstrate superior\ndeceptive capabilities yet exhibit disproportionate vulnerability to others'\nfalsehoods. This suggests deception skills may scale faster than detection\nabilities. Overall, The Traitors provides a focused, configurable testbed for\ninvestigating LLM behavior in socially nuanced interactions. We position this\nwork as a contribution toward more rigorous research on deception mechanisms,\nalignment challenges, and the broader social reliability of AI systems."}
{"id": "2505.12312", "pdf": "https://arxiv.org/pdf/2505.12312", "abs": "https://arxiv.org/abs/2505.12312", "authors": ["Qi Feng", "Hidetoshi Shimodaira"], "title": "Visuospatial Cognitive Assistant", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "comment": "31 pages, 10 figures, 6 tables. The implementation and fine-tuned\n  model (ViCA-7B) are publicly available at https://huggingface.co/nkkbr/ViCA.\n  The ViCA-322K dataset can be found at\n  https://huggingface.co/datasets/nkkbr/ViCA-322K, and the ViCA-Thinking-2.68K\n  dataset is at https://huggingface.co/datasets/nkkbr/ViCA-thinking-2.68k", "summary": "Video-based spatial cognition is vital for robotics and embodied AI but\nchallenges current Vision-Language Models (VLMs). This paper makes two key\ncontributions. First, we introduce ViCA (Visuospatial Cognitive\nAssistant)-322K, a diverse dataset of 322,003 QA pairs from real-world indoor\nvideos (ARKitScenes, ScanNet, ScanNet++), offering supervision for 3D\nmetadata-grounded queries and video-based complex reasoning. Second, we develop\nViCA-7B, fine-tuned on ViCA-322K, which achieves new state-of-the-art on all\neight VSI-Bench tasks, outperforming existing models, including larger ones\n(e.g., +26.1 on Absolute Distance). For interpretability, we present\nViCA-Thinking-2.68K, a dataset with explicit reasoning chains, and fine-tune\nViCA-7B to create ViCA-7B-Thinking, a model that articulates its spatial\nreasoning. Our work highlights the importance of targeted data and suggests\npaths for improved temporal-spatial modeling. We release all resources to\nfoster research in robust visuospatial intelligence."}
{"id": "2505.12368", "pdf": "https://arxiv.org/pdf/2505.12368", "abs": "https://arxiv.org/abs/2505.12368", "authors": ["Gauri Kholkar", "Ratinder Ahuja"], "title": "CAPTURE: Context-Aware Prompt Injection Testing and Robustness Enhancement", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted in ACL LLMSec Workshop 2025", "summary": "Prompt injection remains a major security risk for large language models.\nHowever, the efficacy of existing guardrail models in context-aware settings\nremains underexplored, as they often rely on static attack benchmarks.\nAdditionally, they have over-defense tendencies. We introduce CAPTURE, a novel\ncontext-aware benchmark assessing both attack detection and over-defense\ntendencies with minimal in-domain examples. Our experiments reveal that current\nprompt injection guardrail models suffer from high false negatives in\nadversarial cases and excessive false positives in benign scenarios,\nhighlighting critical limitations."}
{"id": "2505.13011", "pdf": "https://arxiv.org/pdf/2505.13011", "abs": "https://arxiv.org/abs/2505.13011", "authors": ["Yubin Li", "Xingyu Liu", "Guozhang Chen"], "title": "Unveiling and Steering Connectome Organization with Interpretable Latent Variables", "categories": ["cs.AI"], "comment": null, "summary": "The brain's intricate connectome, a blueprint for its function, presents\nimmense complexity, yet it arises from a compact genetic code, hinting at\nunderlying low-dimensional organizational principles. This work bridges\nconnectomics and representation learning to uncover these principles. We\npropose a framework that combines subgraph extraction from the Drosophila\nconnectome, FlyWire, with a generative model to derive interpretable\nlow-dimensional representations of neural circuitry. Crucially, an\nexplainability module links these latent dimensions to specific structural\nfeatures, offering insights into their functional relevance. We validate our\napproach by demonstrating effective graph reconstruction and, significantly,\nthe ability to manipulate these latent codes to controllably generate\nconnectome subgraphs with predefined properties. This research offers a novel\ntool for understanding brain architecture and a potential avenue for designing\nbio-inspired artificial neural networks."}
{"id": "2505.12317", "pdf": "https://arxiv.org/pdf/2505.12317", "abs": "https://arxiv.org/abs/2505.12317", "authors": ["Ruoqi Wang", "Haitao Wang", "Shaojie Guo", "Qiong Luo"], "title": "Improving Out-of-Domain Robustness with Targeted Augmentation in Frequency and Pixel Spaces", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Out-of-domain (OOD) robustness under domain adaptation settings, where\nlabeled source data and unlabeled target data come from different\ndistributions, is a key challenge in real-world applications. A common approach\nto improving OOD robustness is through data augmentations. However, in\nreal-world scenarios, models trained with generic augmentations can only\nimprove marginally when generalized under distribution shifts toward unlabeled\ntarget domains. While dataset-specific targeted augmentations can address this\nissue, they typically require expert knowledge and extensive prior data\nanalysis to identify the nature of the datasets and domain shift. To address\nthese challenges, we propose Frequency-Pixel Connect, a domain-adaptation\nframework that enhances OOD robustness by introducing a targeted augmentation\nin both the frequency space and pixel space. Specifically, we mix the amplitude\nspectrum and pixel content of a source image and a target image to generate\naugmented samples that introduce domain diversity while preserving the semantic\nstructure of the source image. Unlike previous targeted augmentation methods\nthat are both dataset-specific and limited to the pixel space, Frequency-Pixel\nConnect is dataset-agnostic, enabling broader and more flexible applicability\nbeyond natural image datasets. We further analyze the effectiveness of\nFrequency-Pixel Connect by evaluating the performance of our method connecting\nsame-class cross-domain samples while separating different-class examples. We\ndemonstrate that Frequency-Pixel Connect significantly improves cross-domain\nconnectivity and outperforms previous generic methods on four diverse\nreal-world benchmarks across vision, medical, audio, and astronomical domains,\nand it also outperforms other dataset-specific targeted augmentation methods."}
{"id": "2505.12381", "pdf": "https://arxiv.org/pdf/2505.12381", "abs": "https://arxiv.org/abs/2505.12381", "authors": ["Mohsinul Kabir", "Tasfia Tahsin", "Sophia Ananiadou"], "title": "From n-gram to Attention: How Model Architectures Learn and Propagate Bias in Language Modeling", "categories": ["cs.CL", "cs.AI"], "comment": "19 pages", "summary": "Current research on bias in language models (LMs) predominantly focuses on\ndata quality, with significantly less attention paid to model architecture and\ntemporal influences of data. Even more critically, few studies systematically\ninvestigate the origins of bias. We propose a methodology grounded in\ncomparative behavioral theory to interpret the complex interaction between\ntraining data and model architecture in bias propagation during language\nmodeling. Building on recent work that relates transformers to n-gram LMs, we\nevaluate how data, model design choices, and temporal dynamics affect bias\npropagation. Our findings reveal that: (1) n-gram LMs are highly sensitive to\ncontext window size in bias propagation, while transformers demonstrate\narchitectural robustness; (2) the temporal provenance of training data\nsignificantly affects bias; and (3) different model architectures respond\ndifferentially to controlled bias injection, with certain biases (e.g. sexual\norientation) being disproportionately amplified. As language models become\nubiquitous, our findings highlight the need for a holistic approach -- tracing\nbias to its origins across both data and model dimensions, not just symptoms,\nto mitigate harm."}
{"id": "2505.13031", "pdf": "https://arxiv.org/pdf/2505.13031", "abs": "https://arxiv.org/abs/2505.13031", "authors": ["Yicheng Xiao", "Lin Song", "Yukang Chen", "Yingmin Luo", "Yuxin Chen", "Yukang Gan", "Wei Huang", "Xiu Li", "Xiaojuan Qi", "Ying Shan"], "title": "MindOmni: Unleashing Reasoning Generation in Vision Language Models with RGPO", "categories": ["cs.AI"], "comment": "Code: https://github.com/EasonXiao-888/MindOmni", "summary": "Recent text-to-image systems face limitations in handling multimodal inputs\nand complex reasoning tasks. We introduce MindOmni, a unified multimodal large\nlanguage model that addresses these challenges by incorporating reasoning\ngeneration through reinforcement learning. MindOmni leverages a three-phase\ntraining strategy: i) design of a unified vision language model with a\ndecoder-only diffusion module, ii) supervised fine-tuning with Chain-of-Thought\n(CoT) instruction data, and iii) our proposed Reasoning Generation Policy\nOptimization (RGPO) algorithm, utilizing multimodal feedback to effectively\nguide policy updates. Experimental results demonstrate that MindOmni\noutperforms existing models, achieving impressive performance on both\nunderstanding and generation benchmarks, meanwhile showcasing advanced\nfine-grained reasoning generation capabilities, especially with mathematical\nreasoning instruction. All codes will be made public at\n\\href{https://github.com/EasonXiao-888/MindOmni}{https://github.com/EasonXiao-888/MindOmni}."}
{"id": "2505.12335", "pdf": "https://arxiv.org/pdf/2505.12335", "abs": "https://arxiv.org/abs/2505.12335", "authors": ["Ziqiang Li", "Jiazhen Yan", "Ziwen He", "Kai Zeng", "Weiwei Jiang", "Lizhi Xiong", "Zhangjie Fu"], "title": "Is Artificial Intelligence Generated Image Detection a Solved Problem?", "categories": ["cs.CV", "cs.CR"], "comment": "Under Review", "summary": "The rapid advancement of generative models, such as GANs and Diffusion\nmodels, has enabled the creation of highly realistic synthetic images, raising\nserious concerns about misinformation, deepfakes, and copyright infringement.\nAlthough numerous Artificial Intelligence Generated Image (AIGI) detectors have\nbeen proposed, often reporting high accuracy, their effectiveness in real-world\nscenarios remains questionable. To bridge this gap, we introduce AIGIBench, a\ncomprehensive benchmark designed to rigorously evaluate the robustness and\ngeneralization capabilities of state-of-the-art AIGI detectors. AIGIBench\nsimulates real-world challenges through four core tasks: multi-source\ngeneralization, robustness to image degradation, sensitivity to data\naugmentation, and impact of test-time pre-processing. It includes 23 diverse\nfake image subsets that span both advanced and widely adopted image generation\ntechniques, along with real-world samples collected from social media and AI\nart platforms. Extensive experiments on 11 advanced detectors demonstrate that,\ndespite their high reported accuracy in controlled settings, these detectors\nsuffer significant performance drops on real-world data, limited benefits from\ncommon augmentations, and nuanced effects of pre-processing, highlighting the\nneed for more robust detection strategies. By providing a unified and realistic\nevaluation framework, AIGIBench offers valuable insights to guide future\nresearch toward dependable and generalizable AIGI detection."}
{"id": "2505.12392", "pdf": "https://arxiv.org/pdf/2505.12392", "abs": "https://arxiv.org/abs/2505.12392", "authors": ["Yang Hu", "Xingyu Zhang", "Xueji Fang", "Zhiyang Chen", "Xiao Wang", "Huatian Zhang", "Guojun Qi"], "title": "SLOT: Sample-specific Language Model Optimization at Test-time", "categories": ["cs.CL"], "comment": null, "summary": "We propose SLOT (Sample-specific Language Model Optimization at Test-time), a\nnovel and parameter-efficient test-time inference approach that enhances a\nlanguage model's ability to more accurately respond to individual prompts.\nExisting Large Language Models (LLMs) often struggle with complex instructions,\nleading to poor performances on those not well represented among general\nsamples. To address this, SLOT conducts few optimization steps at test-time to\nupdate a light-weight sample-specific parameter vector. It is added to the\nfinal hidden layer before the output head, and enables efficient adaptation by\ncaching the last layer features during per-sample optimization. By minimizing\nthe cross-entropy loss on the input prompt only, SLOT helps the model better\naligned with and follow each given instruction. In experiments, we demonstrate\nthat our method outperforms the compared models across multiple benchmarks and\nLLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on\nGSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT\nachieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is\navailable at https://github.com/maple-research-lab/SLOT."}
{"id": "2505.13044", "pdf": "https://arxiv.org/pdf/2505.13044", "abs": "https://arxiv.org/abs/2505.13044", "authors": ["Rebecca Westhäußer", "Frederik Berenz", "Wolfgang Minker", "Sebastian Zepf"], "title": "CAIM: Development and Evaluation of a Cognitive AI Memory Framework for Long-Term Interaction with Intelligent Agents", "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "Large language models (LLMs) have advanced the field of artificial\nintelligence (AI) and are a powerful enabler for interactive systems. However,\nthey still face challenges in long-term interactions that require adaptation\ntowards the user as well as contextual knowledge and understanding of the\never-changing environment. To overcome these challenges, holistic memory\nmodeling is required to efficiently retrieve and store relevant information\nacross interaction sessions for suitable responses. Cognitive AI, which aims to\nsimulate the human thought process in a computerized model, highlights\ninteresting aspects, such as thoughts, memory mechanisms, and decision-making,\nthat can contribute towards improved memory modeling for LLMs. Inspired by\nthese cognitive AI principles, we propose our memory framework CAIM. CAIM\nconsists of three modules: 1.) The Memory Controller as the central decision\nunit; 2.) the Memory Retrieval, which filters relevant data for interaction\nupon request; and 3.) the Post-Thinking, which maintains the memory storage. We\ncompare CAIM against existing approaches, focusing on metrics such as retrieval\naccuracy, response correctness, contextual coherence, and memory storage. The\nresults demonstrate that CAIM outperforms baseline frameworks across different\nmetrics, highlighting its context-awareness and potential to improve long-term\nhuman-AI interactions."}
{"id": "2505.12339", "pdf": "https://arxiv.org/pdf/2505.12339", "abs": "https://arxiv.org/abs/2505.12339", "authors": ["Midou Guo", "Qilin Yin", "Wei Lu", "Xiangyang Luo"], "title": "Towards Open-world Generalized Deepfake Detection: General Feature Extraction via Unsupervised Domain Adaptation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the development of generative artificial intelligence, new forgery\nmethods are rapidly emerging. Social platforms are flooded with vast amounts of\nunlabeled synthetic data and authentic data, making it increasingly challenging\nto distinguish real from fake. Due to the lack of labels, existing supervised\ndetection methods struggle to effectively address the detection of unknown\ndeepfake methods. Moreover, in open world scenarios, the amount of unlabeled\ndata greatly exceeds that of labeled data. Therefore, we define a new deepfake\ndetection generalization task which focuses on how to achieve efficient\ndetection of large amounts of unlabeled data based on limited labeled data to\nsimulate a open world scenario. To solve the above mentioned task, we propose a\nnovel Open-World Deepfake Detection Generalization Enhancement Training\nStrategy (OWG-DS) to improve the generalization ability of existing methods.\nOur approach aims to transfer deepfake detection knowledge from a small amount\nof labeled source domain data to large-scale unlabeled target domain data.\nSpecifically, we introduce the Domain Distance Optimization (DDO) module to\nalign different domain features by optimizing both inter-domain and\nintra-domain distances. Additionally, the Similarity-based Class Boundary\nSeparation (SCBS) module is used to enhance the aggregation of similar samples\nto ensure clearer class boundaries, while an adversarial training mechanism is\nadopted to learn the domain-invariant features. Extensive experiments show that\nthe proposed deepfake detection generalization enhancement training strategy\nexcels in cross-method and cross-dataset scenarios, improving the model's\ngeneralization."}
{"id": "2505.12398", "pdf": "https://arxiv.org/pdf/2505.12398", "abs": "https://arxiv.org/abs/2505.12398", "authors": ["Yepeng Weng", "Qiao Hu", "Xujie Chen", "Li Liu", "Dianwen Mei", "Huishi Qiu", "Jiang Tian", "Zhongchao Shi"], "title": "Traversal Verification for Speculative Tree Decoding", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Under review", "summary": "Speculative decoding is a promising approach for accelerating large language\nmodels. The primary idea is to use a lightweight draft model to speculate the\noutput of the target model for multiple subsequent timesteps, and then verify\nthem in parallel to determine whether the drafted tokens should be accepted or\nrejected. To enhance acceptance rates, existing frameworks typically construct\ntoken trees containing multiple candidates in each timestep. However, their\nreliance on token-level verification mechanisms introduces two critical\nlimitations: First, the probability distribution of a sequence differs from\nthat of individual tokens, leading to suboptimal acceptance length. Second,\ncurrent verification schemes begin from the root node and proceed layer by\nlayer in a top-down manner. Once a parent node is rejected, all its child nodes\nshould be discarded, resulting in inefficient utilization of speculative\ncandidates. This paper introduces Traversal Verification, a novel speculative\ndecoding algorithm that fundamentally rethinks the verification paradigm\nthrough leaf-to-root traversal. Our approach considers the acceptance of the\nentire token sequence from the current node to the root, and preserves\npotentially valid subsequences that would be prematurely discarded by existing\nmethods. We theoretically prove that the probability distribution obtained\nthrough Traversal Verification is identical to that of the target model,\nguaranteeing lossless inference while achieving substantial acceleration gains.\nExperimental results across different large language models and multiple tasks\nshow that our method consistently improves acceptance length and throughput\nover existing methods"}
{"id": "2505.13098", "pdf": "https://arxiv.org/pdf/2505.13098", "abs": "https://arxiv.org/abs/2505.13098", "authors": ["Lars-Peter Meyer", "Johannes Frey", "Desiree Heim", "Felix Brei", "Claus Stadler", "Kurt Junghanns", "Michael Martin"], "title": "LLM-KG-Bench 3.0: A Compass for SemanticTechnology Capabilities in the Ocean of LLMs", "categories": ["cs.AI", "cs.CL", "cs.DB"], "comment": "Peer reviewed publication at ESWC 2025 Resources Track", "summary": "Current Large Language Models (LLMs) can assist developing program code\nbeside many other things, but can they support working with Knowledge Graphs\n(KGs) as well? Which LLM is offering the best capabilities in the field of\nSemantic Web and Knowledge Graph Engineering (KGE)? Is this possible to\ndetermine without checking many answers manually? The LLM-KG-Bench framework in\nVersion 3.0 is designed to answer these questions. It consists of an extensible\nset of tasks for automated evaluation of LLM answers and covers different\naspects of working with semantic technologies. In this paper the LLM-KG-Bench\nframework is presented in Version 3 along with a dataset of prompts, answers\nand evaluations generated with it and several state-of-the-art LLMs.\nSignificant enhancements have been made to the framework since its initial\nrelease, including an updated task API that offers greater flexibility in\nhandling evaluation tasks, revised tasks, and extended support for various open\nmodels through the vllm library, among other improvements. A comprehensive\ndataset has been generated using more than 30 contemporary open and proprietary\nLLMs, enabling the creation of exemplary model cards that demonstrate the\nmodels' capabilities in working with RDF and SPARQL, as well as comparing their\nperformance on Turtle and JSON-LD RDF serialization tasks."}
{"id": "2505.12340", "pdf": "https://arxiv.org/pdf/2505.12340", "abs": "https://arxiv.org/abs/2505.12340", "authors": ["Jirong Zha", "Yuxuan Fan", "Kai Li", "Han Li", "Chen Gao", "Xinlei Chen", "Yong Li"], "title": "DIMM: Decoupled Multi-hierarchy Kalman Filter for 3D Object Tracking", "categories": ["cs.CV"], "comment": "10 pages", "summary": "State estimation is challenging for 3D object tracking with high\nmaneuverability, as the target's state transition function changes rapidly,\nirregularly, and is unknown to the estimator. Existing work based on\ninteracting multiple model (IMM) achieves more accurate estimation than\nsingle-filter approaches through model combination, aligning appropriate models\nfor different motion modes of the target object over time. However, two\nlimitations of conventional IMM remain unsolved. First, the solution space of\nthe model combination is constrained as the target's diverse kinematic\nproperties in different directions are ignored. Second, the model combination\nweights calculated by the observation likelihood are not accurate enough due to\nthe measurement uncertainty. In this paper, we propose a novel framework, DIMM,\nto effectively combine estimates from different motion models in each\ndirection, thus increasing the 3D object tracking accuracy. First, DIMM extends\nthe model combination solution space of conventional IMM from a hyperplane to a\nhypercube by designing a 3D-decoupled multi-hierarchy filter bank, which\ndescribes the target's motion with various-order linear models. Second, DIMM\ngenerates more reliable combination weight matrices through a differentiable\nadaptive fusion network for importance allocation rather than solely relying on\nthe observation likelihood; it contains an attention-based twin delayed deep\ndeterministic policy gradient (TD3) method with a hierarchical reward.\nExperiments demonstrate that DIMM significantly improves the tracking accuracy\nof existing state estimation methods by 31.61%~99.23%."}
{"id": "2505.12405", "pdf": "https://arxiv.org/pdf/2505.12405", "abs": "https://arxiv.org/abs/2505.12405", "authors": ["Konstantinos Xylogiannopoulos", "Petros Xanthopoulos", "Panagiotis Karampelas", "Georgios Bakamitsos"], "title": "The power of text similarity in identifying AI-LLM paraphrased documents: The case of BBC news articles and ChatGPT", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Generative AI paraphrased text can be used for copyright infringement and the\nAI paraphrased content can deprive substantial revenue from original content\ncreators. Despite this recent surge of malicious use of generative AI, there\nare few academic publications that research this threat. In this article, we\ndemonstrate the ability of pattern-based similarity detection for AI\nparaphrased news recognition. We propose an algorithmic scheme, which is not\nlimited to detect whether an article is an AI paraphrase, but, more\nimportantly, to identify that the source of infringement is the ChatGPT. The\nproposed method is tested with a benchmark dataset specifically created for\nthis task that incorporates real articles from BBC, incorporating a total of\n2,224 articles across five different news categories, as well as 2,224\nparaphrased articles created with ChatGPT. Results show that our pattern\nsimilarity-based method, that makes no use of deep learning, can detect ChatGPT\nassisted paraphrased articles at percentages 96.23% for accuracy, 96.25% for\nprecision, 96.21% for sensitivity, 96.25% for specificity and 96.23% for F1\nscore."}
{"id": "2505.13118", "pdf": "https://arxiv.org/pdf/2505.13118", "abs": "https://arxiv.org/abs/2505.13118", "authors": ["Marouane Il Idrissi", "Agathe Fernandes Machado", "Ewen Gallic", "Arthur Charpentier"], "title": "Unveil Sources of Uncertainty: Feature Contribution to Conformal Prediction Intervals", "categories": ["cs.AI", "cs.LG", "stat.ML"], "comment": null, "summary": "Cooperative game theory methods, notably Shapley values, have significantly\nenhanced machine learning (ML) interpretability. However, existing explainable\nAI (XAI) frameworks mainly attribute average model predictions, overlooking\npredictive uncertainty. This work addresses that gap by proposing a novel,\nmodel-agnostic uncertainty attribution (UA) method grounded in conformal\nprediction (CP). By defining cooperative games where CP interval\nproperties-such as width and bounds-serve as value functions, we systematically\nattribute predictive uncertainty to input features. Extending beyond the\ntraditional Shapley values, we use the richer class of Harsanyi allocations,\nand in particular the proportional Shapley values, which distribute attribution\nproportionally to feature importance. We propose a Monte Carlo approximation\nmethod with robust statistical guarantees to address computational feasibility,\nsignificantly improving runtime efficiency. Our comprehensive experiments on\nsynthetic benchmarks and real-world datasets demonstrate the practical utility\nand interpretative depth of our approach. By combining cooperative game theory\nand conformal prediction, we offer a rigorous, flexible toolkit for\nunderstanding and communicating predictive uncertainty in high-stakes ML\napplications."}
{"id": "2505.12363", "pdf": "https://arxiv.org/pdf/2505.12363", "abs": "https://arxiv.org/abs/2505.12363", "authors": ["Qi Feng", "Hidetoshi Shimodaira"], "title": "Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "comment": "26 pages, 19 figures, 4 tables. Code, models, and dataset are\n  available at our project page: https://github.com/nkkbr/ViCA", "summary": "While Multimodal Large Language Models (MLLMs) excel at general\nvision-language tasks, visuospatial cognition - reasoning about spatial\nlayouts, relations, and dynamics - remains a significant challenge. Existing\nmodels often lack the necessary architectural components and specialized\ntraining data for fine-grained spatial understanding. We introduce ViCA2\n(Visuospatial Cognitive Assistant 2), a novel MLLM designed to enhance spatial\nreasoning. ViCA2 features a dual vision encoder architecture integrating SigLIP\nfor semantics and Hiera for spatial structure, coupled with a token ratio\ncontrol mechanism for efficiency. We also developed ViCA-322K, a new\nlarge-scale dataset with over 322,000 spatially grounded question-answer pairs\nfor targeted instruction tuning. On the challenging VSI-Bench benchmark, our\nViCA2-7B model achieves a state-of-the-art average score of 56.8, significantly\nsurpassing larger open-source models (e.g., LLaVA-NeXT-Video-72B, 40.9) and\nleading proprietary models (Gemini-1.5 Pro, 45.4). This demonstrates the\neffectiveness of our approach in achieving strong visuospatial intelligence\nwith a compact model. We release ViCA2, its codebase, and the ViCA-322K dataset\nto facilitate further research."}
{"id": "2505.12415", "pdf": "https://arxiv.org/pdf/2505.12415", "abs": "https://arxiv.org/abs/2505.12415", "authors": ["Zhenhe Wu", "Jian Yang", "Jiaheng Liu", "Xianjie Wu", "Changzai Pan", "Jie Zhang", "Yu Zhao", "Shuangyong Song", "Yongxiang Li", "Zhoujun Li"], "title": "Table-R1: Region-based Reinforcement Learning for Table Understanding", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Tables present unique challenges for language models due to their structured\nrow-column interactions, necessitating specialized approaches for effective\ncomprehension. While large language models (LLMs) have demonstrated potential\nin table reasoning through prompting and techniques like chain-of-thought (CoT)\nand program-of-thought (PoT), optimizing their performance for table question\nanswering remains underexplored. In this paper, we introduce region-based\nTable-R1, a novel reinforcement learning approach that enhances LLM table\nunderstanding by integrating region evidence into reasoning steps. Our method\nemploys Region-Enhanced Supervised Fine-Tuning (RE-SFT) to guide models in\nidentifying relevant table regions before generating answers, incorporating\ntextual, symbolic, and program-based reasoning. Additionally, Table-Aware Group\nRelative Policy Optimization (TARPO) introduces a mixed reward system to\ndynamically balance region accuracy and answer correctness, with decaying\nregion rewards and consistency penalties to align reasoning steps. Experiments\nshow that Table-R1 achieves an average performance improvement of 14.36 points\nacross multiple base models on three benchmark datasets, even outperforming\nbaseline models with ten times the parameters, while TARPO reduces response\ntoken consumption by 67.5% compared to GRPO, significantly advancing LLM\ncapabilities in efficient tabular reasoning."}
{"id": "2505.13126", "pdf": "https://arxiv.org/pdf/2505.13126", "abs": "https://arxiv.org/abs/2505.13126", "authors": ["Liancheng Gong", "Wang Zhu", "Jesse Thomason", "Li Zhang"], "title": "Zero-Shot Iterative Formalization and Planning in Partially Observable Environments", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "In planning, using LLMs not to predict plans but to formalize an environment\ninto the Planning Domain Definition Language (PDDL) has been shown to greatly\nimprove performance and control. While most work focused on fully observable\nenvironments, we tackle the more realistic and challenging partially observable\nenvironments where existing methods are incapacitated by the lack of complete\ninformation. We propose PDDLego+, a framework to iteratively formalize, plan,\ngrow, and refine PDDL representations in a zero-shot manner, without needing\naccess to any existing trajectories. On two textual simulated environments, we\nshow that PDDLego+ not only achieves superior performance, but also shows\nrobustness against problem complexity. We also show that the domain knowledge\ncaptured after a successful trial is interpretable and benefits future tasks."}
{"id": "2505.12391", "pdf": "https://arxiv.org/pdf/2505.12391", "abs": "https://arxiv.org/abs/2505.12391", "authors": ["Zhengyang Lu", "Qian Xia", "Weifan Wang", "Feng Wang"], "title": "CLIP-aware Domain-Adaptive Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "This work introduces CLIP-aware Domain-Adaptive Super-Resolution (CDASR), a\nnovel framework that addresses the critical challenge of domain generalization\nin single image super-resolution. By leveraging the semantic capabilities of\nCLIP (Contrastive Language-Image Pre-training), CDASR achieves unprecedented\nperformance across diverse domains and extreme scaling factors. The proposed\nmethod integrates CLIP-guided feature alignment mechanism with a meta-learning\ninspired few-shot adaptation strategy, enabling efficient knowledge transfer\nand rapid adaptation to target domains. A custom domain-adaptive module\nprocesses CLIP features alongside super-resolution features through a\nmulti-stage transformation process, including CLIP feature processing, spatial\nfeature generation, and feature fusion. This intricate process ensures\neffective incorporation of semantic information into the super-resolution\npipeline. Additionally, CDASR employs a multi-component loss function that\ncombines pixel-wise reconstruction, perceptual similarity, and semantic\nconsistency. Extensive experiments on benchmark datasets demonstrate CDASR's\nsuperiority, particularly in challenging scenarios. On the Urban100 dataset at\n$\\times$8 scaling, CDASR achieves a significant PSNR gain of 0.15dB over\nexisting methods, with even larger improvements of up to 0.30dB observed at\n$\\times$16 scaling."}
{"id": "2505.12423", "pdf": "https://arxiv.org/pdf/2505.12423", "abs": "https://arxiv.org/abs/2505.12423", "authors": ["Wenqiao Zhu", "Chao Xu", "Lulu Wang", "Jun Wu"], "title": "PSC: Extending Context Window of Large Language Models via Phase Shift Calibration", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Rotary Position Embedding (RoPE) is an efficient position encoding approach\nand is widely utilized in numerous large language models (LLMs). Recently, a\nlot of methods have been put forward to further expand the context window based\non RoPE. The core concept of those methods is to predefine or search for a set\nof factors to rescale the base frequencies of RoPE. Nevertheless, it is quite a\nchallenge for existing methods to predefine an optimal factor due to the\nexponential search space. In view of this, we introduce PSC (Phase Shift\nCalibration), a small module for calibrating the frequencies predefined by\nexisting methods. With the employment of PSC, we demonstrate that many existing\nmethods can be further enhanced, like PI, YaRN, and LongRoPE. We conducted\nextensive experiments across multiple models and tasks. The results demonstrate\nthat (1) when PSC is enabled, the comparative reductions in perplexity increase\nas the context window size is varied from 16k, to 32k, and up to 64k. (2) Our\napproach is broadly applicable and exhibits robustness across a variety of\nmodels and tasks. The code can be found at https://github.com/WNQzhu/PSC."}
{"id": "2505.13175", "pdf": "https://arxiv.org/pdf/2505.13175", "abs": "https://arxiv.org/abs/2505.13175", "authors": ["Siming Sun", "Kai Zhang", "Xuejun Jiang", "Wenchao Meng", "Qinmin Yang"], "title": "Enhancing LLMs for Time Series Forecasting via Structure-Guided Cross-Modal Alignment", "categories": ["cs.AI"], "comment": null, "summary": "The emerging paradigm of leveraging pretrained large language models (LLMs)\nfor time series forecasting has predominantly employed linguistic-temporal\nmodality alignment strategies through token-level or layer-wise feature\nmapping. However, these approaches fundamentally neglect a critical insight:\nthe core competency of LLMs resides not merely in processing localized token\nfeatures but in their inherent capacity to model holistic sequence structures.\nThis paper posits that effective cross-modal alignment necessitates structural\nconsistency at the sequence level. We propose the Structure-Guided Cross-Modal\nAlignment (SGCMA), a framework that fully exploits and aligns the\nstate-transition graph structures shared by time-series and linguistic data as\nsequential modalities, thereby endowing time series with language-like\nproperties and delivering stronger generalization after modality alignment.\nSGCMA consists of two key components, namely Structure Alignment and Semantic\nAlignment. In Structure Alignment, a state transition matrix is learned from\ntext data through Hidden Markov Models (HMMs), and a shallow transformer-based\nMaximum Entropy Markov Model (MEMM) receives the hot-start transition matrix\nand annotates each temporal patch into state probability, ensuring that the\ntemporal representation sequence inherits language-like sequential dynamics. In\nSemantic Alignment, cross-attention is applied between temporal patches and the\ntop-k tokens within each state, and the ultimate temporal embeddings are\nderived by the expected value of these embeddings using a weighted average\nbased on state probabilities. Experiments on multiple benchmarks demonstrate\nthat SGCMA achieves state-of-the-art performance, offering a novel approach to\ncross-modal alignment in time series forecasting."}
{"id": "2505.12408", "pdf": "https://arxiv.org/pdf/2505.12408", "abs": "https://arxiv.org/abs/2505.12408", "authors": ["Minxu Liu", "Donghai Guan", "Chuhang Zheng", "Chunwei Tian", "Jie Wen", "Qi Zhu"], "title": "ViEEG: Hierarchical Neural Coding with Cross-Modal Progressive Enhancement for EEG-Based Visual Decoding", "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": "24 pages, 18 figures", "summary": "Understanding and decoding brain activity into visual representations is a\nfundamental challenge at the intersection of neuroscience and artificial\nintelligence. While EEG-based visual decoding has shown promise due to its\nnon-invasive, low-cost nature and millisecond-level temporal resolution,\nexisting methods are limited by their reliance on flat neural representations\nthat overlook the brain's inherent visual hierarchy. In this paper, we\nintroduce ViEEG, a biologically inspired hierarchical EEG decoding framework\nthat aligns with the Hubel-Wiesel theory of visual processing. ViEEG decomposes\neach visual stimulus into three biologically aligned components-contour,\nforeground object, and contextual scene-serving as anchors for a three-stream\nEEG encoder. These EEG features are progressively integrated via\ncross-attention routing, simulating cortical information flow from V1 to IT to\nthe association cortex. We further adopt hierarchical contrastive learning to\nalign EEG representations with CLIP embeddings, enabling zero-shot object\nrecognition. Extensive experiments on the THINGS-EEG dataset demonstrate that\nViEEG achieves state-of-the-art performance, with 40.9% Top-1 accuracy in\nsubject-dependent and 22.9% Top-1 accuracy in cross-subject settings,\nsurpassing existing methods by over 45%. Our framework not only advances the\nperformance frontier but also sets a new paradigm for biologically grounded\nbrain decoding in AI."}
{"id": "2505.12439", "pdf": "https://arxiv.org/pdf/2505.12439", "abs": "https://arxiv.org/abs/2505.12439", "authors": ["Jinming Zhang", "Yunfei Long"], "title": "Learning to Play Like Humans: A Framework for LLM Adaptation in Interactive Fiction Games", "categories": ["cs.CL"], "comment": null, "summary": "Interactive Fiction games (IF games) are where players interact through\nnatural language commands. While recent advances in Artificial Intelligence\nagents have reignited interest in IF games as a domain for studying\ndecision-making, existing approaches prioritize task-specific performance\nmetrics over human-like comprehension of narrative context and gameplay logic.\nThis work presents a cognitively inspired framework that guides Large Language\nModels (LLMs) to learn and play IF games systematically. Our proposed\n**L**earning to **P**lay **L**ike **H**umans (LPLH) framework integrates three\nkey components: (1) structured map building to capture spatial and narrative\nrelationships, (2) action learning to identify context-appropriate commands,\nand (3) feedback-driven experience analysis to refine decision-making over\ntime. By aligning LLMs-based agents' behavior with narrative intent and\ncommonsense constraints, LPLH moves beyond purely exploratory strategies to\ndeliver more interpretable, human-like performance. Crucially, this approach\ndraws on cognitive science principles to more closely simulate how human\nplayers read, interpret, and respond within narrative worlds. As a result, LPLH\nreframes the IF games challenge as a learning problem for LLMs-based agents,\noffering a new path toward robust, context-aware gameplay in complex text-based\nenvironments."}
{"id": "2505.13180", "pdf": "https://arxiv.org/pdf/2505.13180", "abs": "https://arxiv.org/abs/2505.13180", "authors": ["Matteo Merler", "Nicola Dainese", "Minttu Alakuijala", "Giovanni Bonetta", "Pietro Ferrazzi", "Yu Tian", "Bernardo Magnini", "Pekka Marttinen"], "title": "ViPlan: A Benchmark for Visual Planning with Symbolic Predicates and Vision-Language Models", "categories": ["cs.AI"], "comment": "9 pages, 5 figures and 1 table in the main text; 43 pages, 9 figures\n  and 16 tables including supplementary material", "summary": "Integrating Large Language Models with symbolic planners is a promising\ndirection for obtaining verifiable and grounded plans compared to planning in\nnatural language, with recent works extending this idea to visual domains using\nVision-Language Models (VLMs). However, rigorous comparison between\nVLM-grounded symbolic approaches and methods that plan directly with a VLM has\nbeen hindered by a lack of common environments, evaluation protocols and model\ncoverage. We introduce ViPlan, the first open-source benchmark for Visual\nPlanning with symbolic predicates and VLMs. ViPlan features a series of\nincreasingly challenging tasks in two domains: a visual variant of the classic\nBlocksworld planning problem and a simulated household robotics environment. We\nbenchmark nine open-source VLM families across multiple sizes, along with\nselected closed models, evaluating both VLM-grounded symbolic planning and\nusing the models directly to propose actions. We find symbolic planning to\noutperform direct VLM planning in Blocksworld, where accurate image grounding\nis crucial, whereas the opposite is true in the household robotics tasks, where\ncommonsense knowledge and the ability to recover from errors are beneficial.\nFinally, we show that across most models and methods, there is no significant\nbenefit to using Chain-of-Thought prompting, suggesting that current VLMs still\nstruggle with visual reasoning."}
{"id": "2505.12425", "pdf": "https://arxiv.org/pdf/2505.12425", "abs": "https://arxiv.org/abs/2505.12425", "authors": ["Edgar Riba", "Jian Shi", "Aditya Kumar", "Andrew Shen", "Gary Bradski"], "title": "Kornia-rs: A Low-Level 3D Computer Vision Library In Rust", "categories": ["cs.CV"], "comment": null, "summary": "We present \\textit{kornia-rs}, a high-performance 3D computer vision library\nwritten entirely in native Rust, designed for safety-critical and real-time\napplications. Unlike C++-based libraries like OpenCV or wrapper-based solutions\nlike OpenCV-Rust, \\textit{kornia-rs} is built from the ground up to leverage\nRust's ownership model and type system for memory and thread safety.\n\\textit{kornia-rs} adopts a statically-typed tensor system and a modular set of\ncrates, providing efficient image I/O, image processing and 3D operations. To\naid cross-platform compatibility, \\textit{kornia-rs} offers Python bindings,\nenabling seamless and efficient integration with Rust code. Empirical results\nshow that \\textit{kornia-rs} achieves a 3~ 5 times speedup in image\ntransformation tasks over native Rust alternatives, while offering comparable\nperformance to C++ wrapper-based libraries. In addition to 2D vision\ncapabilities, \\textit{kornia-rs} addresses a significant gap in the Rust\necosystem by providing a set of 3D computer vision operators. This paper\npresents the architecture and performance characteristics of\n\\textit{kornia-rs}, demonstrating its effectiveness in real-world computer\nvision applications."}
{"id": "2505.12452", "pdf": "https://arxiv.org/pdf/2505.12452", "abs": "https://arxiv.org/abs/2505.12452", "authors": ["Siyang Wu", "Honglin Bao", "Nadav Kunievsky", "James A. Evans"], "title": "Introspective Growth: Automatically Advancing LLM Expertise in Technology Judgment", "categories": ["cs.CL", "cs.CY", "cs.DL", "cs.IR"], "comment": "We commit to fully open-source our patent dataset", "summary": "Large language models (LLMs) increasingly demonstrate signs of conceptual\nunderstanding, yet much of their internal knowledge remains latent, loosely\nstructured, and difficult to access or evaluate. We propose self-questioning as\na lightweight and scalable strategy to improve LLMs' understanding,\nparticularly in domains where success depends on fine-grained semantic\ndistinctions. To evaluate this approach, we introduce a challenging new\nbenchmark of 1.3 million post-2015 computer science patent pairs, characterized\nby dense technical jargon and strategically complex writing. The benchmark\ncenters on a pairwise differentiation task: can a model distinguish between\nclosely related but substantively different inventions? We show that prompting\nLLMs to generate and answer their own questions - targeting the background\nknowledge required for the task - significantly improves performance. These\nself-generated questions and answers activate otherwise underutilized internal\nknowledge. Allowing LLMs to retrieve answers from external scientific texts\nfurther enhances performance, suggesting that model knowledge is compressed and\nlacks the full richness of the training data. We also find that\nchain-of-thought prompting and self-questioning converge, though\nself-questioning remains more effective for improving understanding of\ntechnical concepts. Notably, we uncover an asymmetry in prompting: smaller\nmodels often generate more fundamental, more open-ended, better-aligned\nquestions for mid-sized models than large models with better understanding do,\nrevealing a new strategy for cross-model collaboration. Altogether, our\nfindings establish self-questioning as both a practical mechanism for\nautomatically improving LLM comprehension, especially in domains with sparse\nand underrepresented knowledge, and a diagnostic probe of how internal and\nexternal knowledge are organized."}
{"id": "2505.13195", "pdf": "https://arxiv.org/pdf/2505.13195", "abs": "https://arxiv.org/abs/2505.13195", "authors": ["Lili Zhang", "Haomiaomiao Wang", "Long Cheng", "Libao Deng", "Tomas Ward"], "title": "Adversarial Testing in LLMs: Insights into Decision-Making Vulnerabilities", "categories": ["cs.AI"], "comment": null, "summary": "As Large Language Models (LLMs) become increasingly integrated into\nreal-world decision-making systems, understanding their behavioural\nvulnerabilities remains a critical challenge for AI safety and alignment. While\nexisting evaluation metrics focus primarily on reasoning accuracy or factual\ncorrectness, they often overlook whether LLMs are robust to adversarial\nmanipulation or capable of using adaptive strategy in dynamic environments.\nThis paper introduces an adversarial evaluation framework designed to\nsystematically stress-test the decision-making processes of LLMs under\ninteractive and adversarial conditions. Drawing on methodologies from cognitive\npsychology and game theory, our framework probes how models respond in two\ncanonical tasks: the two-armed bandit task and the Multi-Round Trust Task.\nThese tasks capture key aspects of exploration-exploitation trade-offs, social\ncooperation, and strategic flexibility. We apply this framework to several\nstate-of-the-art LLMs, including GPT-3.5, GPT-4, Gemini-1.5, and DeepSeek-V3,\nrevealing model-specific susceptibilities to manipulation and rigidity in\nstrategy adaptation. Our findings highlight distinct behavioral patterns across\nmodels and emphasize the importance of adaptability and fairness recognition\nfor trustworthy AI deployment. Rather than offering a performance benchmark,\nthis work proposes a methodology for diagnosing decision-making weaknesses in\nLLM-based agents, providing actionable insights for alignment and safety\nresearch."}
{"id": "2505.12427", "pdf": "https://arxiv.org/pdf/2505.12427", "abs": "https://arxiv.org/abs/2505.12427", "authors": ["Siwei Xia", "Li Sun", "Tiantian Sun", "Qingli Li"], "title": "DragLoRA: Online Optimization of LoRA Adapters for Drag-based Image Editing in Diffusion Model", "categories": ["cs.CV"], "comment": "Accepted by ICML2025", "summary": "Drag-based editing within pretrained diffusion model provides a precise and\nflexible way to manipulate foreground objects. Traditional methods optimize the\ninput feature obtained from DDIM inversion directly, adjusting them iteratively\nto guide handle points towards target locations. However, these approaches\noften suffer from limited accuracy due to the low representation ability of the\nfeature in motion supervision, as well as inefficiencies caused by the large\nsearch space required for point tracking. To address these limitations, we\npresent DragLoRA, a novel framework that integrates LoRA (Low-Rank Adaptation)\nadapters into the drag-based editing pipeline. To enhance the training of LoRA\nadapters, we introduce an additional denoising score distillation loss which\nregularizes the online model by aligning its output with that of the original\nmodel. Additionally, we improve the consistency of motion supervision by\nadapting the input features using the updated LoRA, giving a more stable and\naccurate input feature for subsequent operations. Building on this, we design\nan adaptive optimization scheme that dynamically toggles between two modes,\nprioritizing efficiency without compromising precision. Extensive experiments\ndemonstrate that DragLoRA significantly enhances the control precision and\ncomputational efficiency for drag-based image editing. The Codes of DragLoRA\nare available at: https://github.com/Sylvie-X/DragLoRA."}
{"id": "2505.12454", "pdf": "https://arxiv.org/pdf/2505.12454", "abs": "https://arxiv.org/abs/2505.12454", "authors": ["Yuyang Ding", "Dan Qiao", "Juntao Li", "Jiajie Xu", "Pingfu Chao", "Xiaofang Zhou", "Min Zhang"], "title": "Towards DS-NER: Unveiling and Addressing Latent Noise in Distant Annotations", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Distantly supervised named entity recognition (DS-NER) has emerged as a cheap\nand convenient alternative to traditional human annotation methods, enabling\nthe automatic generation of training data by aligning text with external\nresources. Despite the many efforts in noise measurement methods, few works\nfocus on the latent noise distribution between different distant annotation\nmethods. In this work, we explore the effectiveness and robustness of DS-NER by\ntwo aspects: (1) distant annotation techniques, which encompasses both\ntraditional rule-based methods and the innovative large language model\nsupervision approach, and (2) noise assessment, for which we introduce a novel\nframework. This framework addresses the challenges by distinctly categorizing\nthem into the unlabeled-entity problem (UEP) and the noisy-entity problem\n(NEP), subsequently providing specialized solutions for each. Our proposed\nmethod achieves significant improvements on eight real-world distant\nsupervision datasets originating from three different data sources and\ninvolving four distinct annotation techniques, confirming its superiority over\ncurrent state-of-the-art methods."}
{"id": "2505.13227", "pdf": "https://arxiv.org/pdf/2505.13227", "abs": "https://arxiv.org/abs/2505.13227", "authors": ["Tianbao Xie", "Jiaqi Deng", "Xiaochuan Li", "Junlin Yang", "Haoyuan Wu", "Jixuan Chen", "Wenjing Hu", "Xinyuan Wang", "Yuhui Xu", "Zekun Wang", "Yiheng Xu", "Junli Wang", "Doyen Sahoo", "Tao Yu", "Caiming Xiong"], "title": "Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": "49 pages, 13 figures", "summary": "Graphical user interface (GUI) grounding, the ability to map natural language\ninstructions to specific actions on graphical user interfaces, remains a\ncritical bottleneck in computer use agent development. Current benchmarks\noversimplify grounding tasks as short referring expressions, failing to capture\nthe complexity of real-world interactions that require software commonsense,\nlayout understanding, and fine-grained manipulation capabilities. To address\nthese limitations, we introduce OSWorld-G, a comprehensive benchmark comprising\n564 finely annotated samples across diverse task types including text matching,\nelement recognition, layout understanding, and precise manipulation.\nAdditionally, we synthesize and release the largest computer use grounding\ndataset Jedi, which contains 4 million examples through multi-perspective\ndecoupling of tasks. Our multi-scale models trained on Jedi demonstrate its\neffectiveness by outperforming existing approaches on ScreenSpot-v2,\nScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved\ngrounding with Jedi directly enhances agentic capabilities of general\nfoundation models on complex computer tasks, improving from 5% to 27% on\nOSWorld. Through detailed ablation studies, we identify key factors\ncontributing to grounding performance and verify that combining specialized\ndata for different interface elements enables compositional generalization to\nnovel interfaces. All benchmark, data, checkpoints, and code are open-sourced\nand available at https://osworld-grounding.github.io."}
{"id": "2505.12431", "pdf": "https://arxiv.org/pdf/2505.12431", "abs": "https://arxiv.org/abs/2505.12431", "authors": ["Yating Liu", "Yujie Zhang", "Qi Yang", "Yiling Xu", "Zhu Li", "Ye-Kui Wang"], "title": "DPCD: A Quality Assessment Database for Dynamic Point Clouds", "categories": ["cs.CV", "cs.DB"], "comment": null, "summary": "Recently, the advancements in Virtual/Augmented Reality (VR/AR) have driven\nthe demand for Dynamic Point Clouds (DPC). Unlike static point clouds, DPCs are\ncapable of capturing temporal changes within objects or scenes, offering a more\naccurate simulation of the real world. While significant progress has been made\nin the quality assessment research of static point cloud, little study has been\ndone on Dynamic Point Cloud Quality Assessment (DPCQA), which hinders the\ndevelopment of quality-oriented applications, such as interframe compression\nand transmission in practical scenarios. In this paper, we introduce a\nlarge-scale DPCQA database, named DPCD, which includes 15 reference DPCs and\n525 distorted DPCs from seven types of lossy compression and noise distortion.\nBy rendering these samples to Processed Video Sequences (PVS), a comprehensive\nsubjective experiment is conducted to obtain Mean Opinion Scores (MOS) from 21\nviewers for analysis. The characteristic of contents, impact of various\ndistortions, and accuracy of MOSs are presented to validate the heterogeneity\nand reliability of the proposed database. Furthermore, we evaluate the\nperformance of several objective metrics on DPCD. The experiment results show\nthat DPCQA is more challenge than that of static point cloud. The DPCD, which\nserves as a catalyst for new research endeavors on DPCQA, is publicly available\nat https://huggingface.co/datasets/Olivialyt/DPCD."}
{"id": "2505.12474", "pdf": "https://arxiv.org/pdf/2505.12474", "abs": "https://arxiv.org/abs/2505.12474", "authors": ["Weixiao Zhou", "Junnan Zhu", "Gengyao Li", "Xianfu Cheng", "Xinnian Liang", "Feifei Zhai", "Zhoujun Li"], "title": "What are they talking about? Benchmarking Large Language Models for Knowledge-Grounded Discussion Summarization", "categories": ["cs.CL"], "comment": "Submitted to EMNLP 2025", "summary": "In this work, we investigate the performance of LLMs on a new task that\nrequires combining discussion with background knowledge for summarization. This\naims to address the limitation of outside observer confusion in existing\ndialogue summarization systems due to their reliance solely on discussion\ninformation. To achieve this, we model the task output as background and\nopinion summaries and define two standardized summarization patterns. To\nsupport assessment, we introduce the first benchmark comprising high-quality\nsamples consistently annotated by human experts and propose a novel\nhierarchical evaluation framework with fine-grained, interpretable metrics. We\nevaluate 12 LLMs under structured-prompt and self-reflection paradigms. Our\nfindings reveal: (1) LLMs struggle with background summary retrieval,\ngeneration, and opinion summary integration. (2) Even top LLMs achieve less\nthan 69% average performance across both patterns. (3) Current LLMs lack\nadequate self-evaluation and self-correction capabilities for this task."}
{"id": "2505.13232", "pdf": "https://arxiv.org/pdf/2505.13232", "abs": "https://arxiv.org/abs/2505.13232", "authors": ["Younghyun Kim", "Jongheon Jeong", "Sangkyung Kwak", "Kyungmin Lee", "Juho Lee", "Jinwoo Shin"], "title": "StarFT: Robust Fine-tuning of Zero-shot Models via Spuriosity Alignment", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Learning robust representations from data often requires scale, which has led\nto the success of recent zero-shot models such as CLIP. However, the obtained\nrobustness can easily be deteriorated when these models are fine-tuned on other\ndownstream tasks (e.g., of smaller scales). Previous works often interpret this\nphenomenon in the context of domain shift, developing fine-tuning methods that\naim to preserve the original domain as much as possible. However, in a\ndifferent context, fine-tuned models with limited data are also prone to\nlearning features that are spurious to humans, such as background or texture.\nIn this paper, we propose StarFT (Spurious Textual Alignment Regularization), a\nnovel framework for fine-tuning zero-shot models to enhance robustness by\npreventing them from learning spuriosity. We introduce a regularization that\naligns the output distribution for spuriosity-injected labels with the original\nzero-shot model, ensuring that the model is not induced to extract irrelevant\nfeatures further from these descriptions.We leverage recent language models to\nget such spuriosity-injected labels by generating alternative textual\ndescriptions that highlight potentially confounding features.Extensive\nexperiments validate the robust generalization of StarFT and its emerging\nproperties: zero-shot group robustness and improved zero-shot classification.\nNotably, StarFT boosts both worst-group and average accuracy by 14.30% and\n3.02%, respectively, in the Waterbirds group shift scenario, where other robust\nfine-tuning baselines show even degraded performance."}
{"id": "2505.12433", "pdf": "https://arxiv.org/pdf/2505.12433", "abs": "https://arxiv.org/abs/2505.12433", "authors": ["Haodong Yang", "Lei Wang", "Md Zakir Hossain"], "title": "SRLoRA: Subspace Recomposition in Low-Rank Adaptation via Importance-Based Fusion and Reinitialization", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Research report", "summary": "Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient\nfine-tuning (PEFT) method that injects two trainable low-rank matrices (A and\nB) into frozen pretrained models. While efficient, LoRA constrains updates to a\nfixed low-rank subspace (Delta W = BA), which can limit representational\ncapacity and hinder downstream performance. We introduce Subspace Recomposition\nin Low-Rank Adaptation (SRLoRA) via importance-based fusion and\nreinitialization, a novel approach that enhances LoRA's expressiveness without\ncompromising its lightweight structure. SRLoRA assigns importance scores to\neach LoRA pair (a column of B and the corresponding row of A), and dynamically\nrecomposes the subspace during training. Less important pairs are fused into\nthe frozen backbone, freeing capacity to reinitialize new pairs along unused\nprincipal directions derived from the pretrained weight's singular value\ndecomposition. This mechanism enables continual subspace refreshment and richer\nadaptation over time, without increasing the number of trainable parameters. We\nevaluate SRLoRA on both language and vision tasks, including the GLUE benchmark\nand various image classification datasets. SRLoRA consistently achieves faster\nconvergence and improved accuracy over standard LoRA, demonstrating its\ngenerality, efficiency, and potential for broader PEFT applications."}
{"id": "2505.12476", "pdf": "https://arxiv.org/pdf/2505.12476", "abs": "https://arxiv.org/abs/2505.12476", "authors": ["Xiao Long", "Liansheng Zhuang", "Chen Shen", "Shaotian Yan", "Yifei Li", "Shafei Wang"], "title": "Enhancing Large Language Models with Reward-guided Tree Search for Knowledge Graph Question and Answering", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recently, large language models (LLMs) have demonstrated impressive\nperformance in Knowledge Graph Question Answering (KGQA) tasks, which aim to\nfind answers based on knowledge graphs (KGs) for natural language questions.\nExisting LLMs-based KGQA methods typically follow the Graph Retrieval-Augmented\nGeneration (GraphRAG) paradigm, which first retrieves reasoning paths from the\nlarge KGs, and then generates the answers based on them. However, these methods\nemphasize the exploration of new optimal reasoning paths in KGs while ignoring\nthe exploitation of historical reasoning paths, which may lead to sub-optimal\nreasoning paths. Additionally, the complex semantics contained in questions may\nlead to the retrieval of inaccurate reasoning paths. To address these issues,\nthis paper proposes a novel and training-free framework for KGQA tasks called\nReward-guided Tree Search on Graph (RTSoG). RTSoG decomposes an original\nquestion into a series of simpler and well-defined sub-questions to handle the\ncomplex semantics. Then, a Self-Critic Monte Carlo Tree Search (SC-MCTS) guided\nby a reward model is introduced to iteratively retrieve weighted reasoning\npaths as contextual knowledge. Finally, it stacks the weighted reasoning paths\naccording to their weights to generate the final answers. Extensive experiments\non four datasets demonstrate the effectiveness of RTSoG. Notably, it achieves\n8.7\\% and 7.0\\% performance improvement over the state-of-the-art method on the\nGrailQA and the WebQSP respectively."}
{"id": "2505.13246", "pdf": "https://arxiv.org/pdf/2505.13246", "abs": "https://arxiv.org/abs/2505.13246", "authors": ["Roberto Pugliese", "George Kourousias", "Francesco Venier", "Grazia Garlatti Costa"], "title": "Agentic Publications: An LLM-Driven Framework for Interactive Scientific Publishing, Supplementing Traditional Papers with AI-Powered Knowledge Systems", "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "The exponential growth of scientific literature presents significant\nchallenges for researchers navigating the complex knowledge landscape. We\npropose \"Agentic Publications\", a novel LLM-driven framework complementing\ntraditional publishing by transforming papers into interactive knowledge\nsystems. Our architecture integrates structured data with unstructured content\nthrough retrieval-augmented generation and multi-agent verification. The\nframework offers interfaces for both humans and machines, combining narrative\nexplanations with machine-readable outputs while addressing ethical\nconsiderations through automated validation and transparent governance. Key\nfeatures include continuous knowledge updates, automatic integration of new\nfindings, and customizable detail levels. Our proof-of-concept demonstrates\nmultilingual interaction, API accessibility, and structured knowledge\nrepresentation through vector databases, knowledge graphs, and verification\nagents. This approach enhances scientific communication across disciplines,\nimproving efficiency and collaboration while preserving traditional publishing\npathways, particularly valuable for interdisciplinary fields where knowledge\nintegration remains challenging."}
{"id": "2505.12434", "pdf": "https://arxiv.org/pdf/2505.12434", "abs": "https://arxiv.org/abs/2505.12434", "authors": ["Qi Wang", "Yanrui Yu", "Ye Yuan", "Rui Mao", "Tianfei Zhou"], "title": "VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via Reinforced Fine-Tuning", "categories": ["cs.CV"], "comment": "Code: https://github.com/QiWang98/VideoRFT", "summary": "Reinforcement fine-tuning (RFT) has shown great promise in achieving\nhumanlevel reasoning capabilities of Large Language Models (LLMs), and has\nrecently been extended to MLLMs. Nevertheless, reasoning about videos, which is\na fundamental aspect of human intelligence, remains a persistent challenge due\nto the complex logic, temporal and causal structures inherent in video data. To\nfill this gap, we propose VIDEORFT, a novel approach that extends the RFT\nparadigm to cultivate human-like video reasoning capabilities in MLLMs.\nVIDEORFT follows the standard two-stage scheme in RFT: supervised fine-tuning\n(SFT) with chain-of-thought (CoT) annotations, followed by reinforcement\nlearning (RL) to improve generalization. A central challenge to achieve this in\nthe video domain lies in the scarcity of large-scale, high-quality video CoT\ndatasets. We address this by building a fully automatic CoT curation pipeline.\nFirst, we devise a cognitioninspired prompting strategy to elicit a reasoning\nLLM to generate preliminary CoTs based solely on rich, structured, and literal\nrepresentations of video content. Subsequently, these CoTs are revised by a\nvisual-language model conditioned on the actual video, ensuring visual\nconsistency and reducing visual hallucinations. This pipeline results in two\nnew datasets - VideoRFT-CoT-102K for SFT and VideoRFT-RL-310K for RL. To\nfurther strength the RL phase, we introduce a novel semantic-consistency reward\nthat explicitly promotes the alignment between textual reasoning with visual\nevidence. This reward encourages the model to produce coherent, context-aware\nreasoning outputs grounded in visual input. Extensive experiments show that\nVIDEORFT achieves state-of-the-art performance on six video reasoning\nbenchmarks."}
{"id": "2505.12495", "pdf": "https://arxiv.org/pdf/2505.12495", "abs": "https://arxiv.org/abs/2505.12495", "authors": ["Nikita Tatarinov", "Vidhyakshaya Kannan", "Haricharana Srinivasa", "Arnav Raj", "Harpreet Singh Anand", "Varun Singh", "Aditya Luthra", "Ravij Lade", "Agam Shah", "Sudheer Chava"], "title": "KG-QAGen: A Knowledge-Graph-Based Framework for Systematic Question Generation and Long-Context LLM Evaluation", "categories": ["cs.CL"], "comment": null, "summary": "The increasing context length of modern language models has created a need\nfor evaluating their ability to retrieve and process information across\nextensive documents. While existing benchmarks test long-context capabilities,\nthey often lack a structured way to systematically vary question complexity. We\nintroduce KG-QAGen (Knowledge-Graph-based Question-Answer Generation), a\nframework that (1) extracts QA pairs at multiple complexity levels (2) by\nleveraging structured representations of financial agreements (3) along three\nkey dimensions -- multi-hop retrieval, set operations, and answer plurality --\nenabling fine-grained assessment of model performance across controlled\ndifficulty levels. Using this framework, we construct a dataset of 20,139 QA\npairs (the largest number among the long-context benchmarks) and open-source a\npart of it. We evaluate 13 proprietary and open-source LLMs and observe that\neven the best-performing models are struggling with set-based comparisons and\nmulti-hop logical inference. Our analysis reveals systematic failure modes tied\nto semantic misinterpretation and inability to handle implicit relations."}
{"id": "2505.13273", "pdf": "https://arxiv.org/pdf/2505.13273", "abs": "https://arxiv.org/abs/2505.13273", "authors": ["Lucas Berry", "Axel Brando", "Wei-Di Chang", "Juan Camilo Gamboa Higuera", "David Meger"], "title": "Seeing the Unseen: How EMoE Unveils Bias in Text-to-Image Diffusion Models", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Estimating uncertainty in text-to-image diffusion models is challenging\nbecause of their large parameter counts (often exceeding 100 million) and\noperation in complex, high-dimensional spaces with virtually infinite input\npossibilities. In this paper, we propose Epistemic Mixture of Experts (EMoE), a\nnovel framework for efficiently estimating epistemic uncertainty in diffusion\nmodels. EMoE leverages pre-trained networks without requiring additional\ntraining, enabling direct uncertainty estimation from a prompt. We leverage a\nlatent space within the diffusion process that captures epistemic uncertainty\nbetter than existing methods. Experimental results on the COCO dataset\ndemonstrate EMoE's effectiveness, showing a strong correlation between\nuncertainty and image quality. Additionally, EMoE identifies under-sampled\nlanguages and regions with higher uncertainty, revealing hidden biases in the\ntraining set. This capability demonstrates the relevance of EMoE as a tool for\naddressing fairness and accountability in AI-generated content."}
{"id": "2505.12448", "pdf": "https://arxiv.org/pdf/2505.12448", "abs": "https://arxiv.org/abs/2505.12448", "authors": ["Yang Liu", "Ming Ma", "Xiaomin Yu", "Pengxiang Ding", "Han Zhao", "Mingyang Sun", "Siteng Huang", "Donglin Wang"], "title": "SSR: Enhancing Depth Perception in Vision-Language Models via Rationale-Guided Spatial Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Despite impressive advancements in Visual-Language Models (VLMs) for\nmulti-modal tasks, their reliance on RGB inputs limits precise spatial\nunderstanding. Existing methods for integrating spatial cues, such as point\nclouds or depth, either require specialized sensors or fail to effectively\nexploit depth information for higher-order reasoning. To this end, we propose a\nnovel Spatial Sense and Reasoning method, dubbed SSR, a novel framework that\ntransforms raw depth data into structured, interpretable textual rationales.\nThese textual rationales serve as meaningful intermediate representations to\nsignificantly enhance spatial reasoning capabilities. Additionally, we leverage\nknowledge distillation to compress the generated rationales into compact latent\nembeddings, which facilitate resource-efficient and plug-and-play integration\ninto existing VLMs without retraining. To enable comprehensive evaluation, we\nintroduce a new dataset named SSR-CoT, a million-scale visual-language\nreasoning dataset enriched with intermediate spatial reasoning annotations, and\npresent SSRBench, a comprehensive multi-task benchmark. Extensive experiments\non multiple benchmarks demonstrate SSR substantially improves depth utilization\nand enhances spatial reasoning, thereby advancing VLMs toward more human-like\nmulti-modal understanding. Our project page is at\nhttps://yliu-cs.github.io/SSR."}
{"id": "2505.12507", "pdf": "https://arxiv.org/pdf/2505.12507", "abs": "https://arxiv.org/abs/2505.12507", "authors": ["Xu Zheng", "Zhuomin Chen", "Esteban Schafir", "Sipeng Chen", "Hojat Allah Salehi", "Haifeng Chen", "Farhad Shirani", "Wei Cheng", "Dongsheng Luo"], "title": "LM$^2$otifs : An Explainable Framework for Machine-Generated Texts Detection", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "The impressive ability of large language models to generate natural text\nacross various tasks has led to critical challenges in authorship\nauthentication. Although numerous detection methods have been developed to\ndifferentiate between machine-generated texts (MGT) and human-generated texts\n(HGT), the explainability of these methods remains a significant gap.\nTraditional explainability techniques often fall short in capturing the complex\nword relationships that distinguish HGT from MGT. To address this limitation,\nwe present LM$^2$otifs, a novel explainable framework for MGT detection.\nInspired by probabilistic graphical models, we provide a theoretical rationale\nfor the effectiveness. LM$^2$otifs utilizes eXplainable Graph Neural Networks\nto achieve both accurate detection and interpretability. The LM$^2$otifs\npipeline operates in three key stages: first, it transforms text into graphs\nbased on word co-occurrence to represent lexical dependencies; second, graph\nneural networks are used for prediction; and third, a post-hoc explainability\nmethod extracts interpretable motifs, offering multi-level explanations from\nindividual words to sentence structures. Extensive experiments on multiple\nbenchmark datasets demonstrate the comparable performance of LM$^2$otifs. The\nempirical evaluation of the extracted explainable motifs confirms their\neffectiveness in differentiating HGT and MGT. Furthermore, qualitative analysis\nreveals distinct and visible linguistic fingerprints characteristic of MGT."}
{"id": "2505.13287", "pdf": "https://arxiv.org/pdf/2505.13287", "abs": "https://arxiv.org/abs/2505.13287", "authors": ["João S. Ferreira", "Pierre Fromholz", "Hari Shaji", "James R. Wootton"], "title": "Level Generation with Quantum Reservoir Computing", "categories": ["cs.AI", "quant-ph"], "comment": null, "summary": "Reservoir computing is a form of machine learning particularly suited for\ntime series analysis, including forecasting predictions. We take an\nimplementation of \\emph{quantum} reservoir computing that was initially\ndesigned to generate variants of musical scores and adapt it to create levels\nof Super Mario Bros. Motivated by our analysis of these levels, we develop a\nnew Roblox \\textit{obby} where the courses can be generated in real time on\nsuperconducting qubit hardware, and investigate some of the constraints placed\nby such real-time generation."}
{"id": "2505.12482", "pdf": "https://arxiv.org/pdf/2505.12482", "abs": "https://arxiv.org/abs/2505.12482", "authors": ["Wenchen Chen", "Yanmei Zhang", "Zhongwei Xiao", "Jianping Chu", "Xingbo Wang"], "title": "Spectral-Spatial Self-Supervised Learning for Few-Shot Hyperspectral Image Classification", "categories": ["cs.CV"], "comment": "https://github.com/Wenchen-Chen/S4L-FSC", "summary": "Few-shot classification of hyperspectral images (HSI) faces the challenge of\nscarce labeled samples. Self-Supervised learning (SSL) and Few-Shot Learning\n(FSL) offer promising avenues to address this issue. However, existing methods\noften struggle to adapt to the spatial geometric diversity of HSIs and lack\nsufficient spectral prior knowledge. To tackle these challenges, we propose a\nmethod, Spectral-Spatial Self-Supervised Learning for Few-Shot Hyperspectral\nImage Classification (S4L-FSC), aimed at improving the performance of few-shot\nHSI classification. Specifically, we first leverage heterogeneous datasets to\npretrain a spatial feature extractor using a designed Rotation-Mirror\nSelf-Supervised Learning (RM-SSL) method, combined with FSL. This approach\nenables the model to learn the spatial geometric diversity of HSIs using\nrotation and mirroring labels as supervisory signals, while acquiring\ntransferable spatial meta-knowledge through few-shot learning. Subsequently,\nhomogeneous datasets are utilized to pretrain a spectral feature extractor via\na combination of FSL and Masked Reconstruction Self-Supervised Learning\n(MR-SSL). The model learns to reconstruct original spectral information from\nrandomly masked spectral vectors, inferring spectral dependencies. In parallel,\nFSL guides the model to extract pixel-level discriminative features, thereby\nembedding rich spectral priors into the model. This spectral-spatial\npretraining method, along with the integration of knowledge from heterogeneous\nand homogeneous sources, significantly enhances model performance. Extensive\nexperiments on four HSI datasets demonstrate the effectiveness and superiority\nof the proposed S4L-FSC approach for few-shot HSI classification."}
{"id": "2505.12511", "pdf": "https://arxiv.org/pdf/2505.12511", "abs": "https://arxiv.org/abs/2505.12511", "authors": ["Yanting Li", "Jiyue Jiang", "Zikang Wang", "Ziqian Lin", "Dongchen He", "Yuheng Shan", "Yanruisheng Shao", "Jiayi Li", "Xiangyu Shi", "Jiuming Wang", "Yanyu Chen", "Yimin Fan", "Han Li", "Yu Li"], "title": "DS-ProGen: A Dual-Structure Deep Language Model for Functional Protein Design", "categories": ["cs.CL"], "comment": null, "summary": "Inverse Protein Folding (IPF) is a critical subtask in the field of protein\ndesign, aiming to engineer amino acid sequences capable of folding correctly\ninto a specified three-dimensional (3D) conformation. Although substantial\nprogress has been achieved in recent years, existing methods generally rely on\neither backbone coordinates or molecular surface features alone, which\nrestricts their ability to fully capture the complex chemical and geometric\nconstraints necessary for precise sequence prediction. To address this\nlimitation, we present DS-ProGen, a dual-structure deep language model for\nfunctional protein design, which integrates both backbone geometry and\nsurface-level representations. By incorporating backbone coordinates as well as\nsurface chemical and geometric descriptors into a next-amino-acid prediction\nparadigm, DS-ProGen is able to generate functionally relevant and structurally\nstable sequences while satisfying both global and local conformational\nconstraints. On the PRIDE dataset, DS-ProGen attains the current\nstate-of-the-art recovery rate of 61.47%, demonstrating the synergistic\nadvantage of multi-modal structural encoding in protein design. Furthermore,\nDS-ProGen excels in predicting interactions with a variety of biological\npartners, including ligands, ions, and RNA, confirming its robust functional\nretention capabilities."}
{"id": "2505.13355", "pdf": "https://arxiv.org/pdf/2505.13355", "abs": "https://arxiv.org/abs/2505.13355", "authors": ["Djallel Bouneffouf", "Raphael Feraud"], "title": "Multi-Armed Bandits Meet Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Bandit algorithms and Large Language Models (LLMs) have emerged as powerful\ntools in artificial intelligence, each addressing distinct yet complementary\nchallenges in decision-making and natural language processing. This survey\nexplores the synergistic potential between these two fields, highlighting how\nbandit algorithms can enhance the performance of LLMs and how LLMs, in turn,\ncan provide novel insights for improving bandit-based decision-making. We first\nexamine the role of bandit algorithms in optimizing LLM fine-tuning, prompt\nengineering, and adaptive response generation, focusing on their ability to\nbalance exploration and exploitation in large-scale learning tasks.\nSubsequently, we explore how LLMs can augment bandit algorithms through\nadvanced contextual understanding, dynamic adaptation, and improved policy\nselection using natural language reasoning. By providing a comprehensive review\nof existing research and identifying key challenges and opportunities, this\nsurvey aims to bridge the gap between bandit algorithms and LLMs, paving the\nway for innovative applications and interdisciplinary research in AI."}
{"id": "2505.12486", "pdf": "https://arxiv.org/pdf/2505.12486", "abs": "https://arxiv.org/abs/2505.12486", "authors": ["Sangmin Jung", "Utkarsh Nath", "Yezhou Yang", "Giulia Pedrielli", "Joydeep Biswas", "Amy Zhang", "Hassan Ghasemzadeh", "Pavan Turaga"], "title": "Guiding Diffusion with Deep Geometric Moments: Balancing Fidelity and Variation", "categories": ["cs.CV"], "comment": "Accepted in CVPR Workshop GMCV 2025", "summary": "Text-to-image generation models have achieved remarkable capabilities in\nsynthesizing images, but often struggle to provide fine-grained control over\nthe output. Existing guidance approaches, such as segmentation maps and depth\nmaps, introduce spatial rigidity that restricts the inherent diversity of\ndiffusion models. In this work, we introduce Deep Geometric Moments (DGM) as a\nnovel form of guidance that encapsulates the subject's visual features and\nnuances through a learned geometric prior. DGMs focus specifically on the\nsubject itself compared to DINO or CLIP features, which suffer from\noveremphasis on global image features or semantics. Unlike ResNets, which are\nsensitive to pixel-wise perturbations, DGMs rely on robust geometric moments.\nOur experiments demonstrate that DGM effectively balance control and diversity\nin diffusion-based image generation, allowing a flexible control mechanism for\nsteering the diffusion process."}
{"id": "2505.12531", "pdf": "https://arxiv.org/pdf/2505.12531", "abs": "https://arxiv.org/abs/2505.12531", "authors": ["Navid Madani", "Rohini Srihari"], "title": "ESC-Judge: A Framework for Comparing Emotional Support Conversational Agents", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) increasingly power mental-health chatbots, yet\nthe field still lacks a scalable, theory-grounded way to decide which model is\nmost effective to deploy. We present ESC-Judge, the first end-to-end evaluation\nframework that (i) grounds head-to-head comparisons of emotional-support LLMs\nin Clara Hill's established Exploration-Insight-Action counseling model,\nproviding a structured and interpretable view of performance, and (ii) fully\nautomates the evaluation pipeline at scale. ESC-Judge operates in three stages:\nfirst, it synthesizes realistic help-seeker roles by sampling empirically\nsalient attributes such as stressors, personality, and life history; second, it\nhas two candidate support agents conduct separate sessions with the same role,\nisolating model-specific strategies; and third, it asks a specialized judge LLM\nto express pairwise preferences across rubric-anchored skills that span the\nExploration, Insight, and Action spectrum. In our study, ESC-Judge matched\nPhD-level annotators on 85 percent of Exploration, 83 percent of Insight, and\n86 percent of Action decisions, demonstrating human-level reliability at a\nfraction of the cost. All code, prompts, synthetic roles, transcripts, and\njudgment scripts are released to promote transparent progress in emotionally\nsupportive AI."}
{"id": "2505.13372", "pdf": "https://arxiv.org/pdf/2505.13372", "abs": "https://arxiv.org/abs/2505.13372", "authors": ["Irene Brugnara", "Alessandro Valentini", "Andrea Micheli"], "title": "Exploiting Symbolic Heuristics for the Synthesis of Domain-Specific Temporal Planning Guidance using Reinforcement Learning", "categories": ["cs.AI"], "comment": null, "summary": "Recent work investigated the use of Reinforcement Learning (RL) for the\nsynthesis of heuristic guidance to improve the performance of temporal planners\nwhen a domain is fixed and a set of training problems (not plans) is given. The\nidea is to extract a heuristic from the value function of a particular\n(possibly infinite-state) MDP constructed over the training problems.\n  In this paper, we propose an evolution of this learning and planning\nframework that focuses on exploiting the information provided by symbolic\nheuristics during both the RL and planning phases. First, we formalize\ndifferent reward schemata for the synthesis and use symbolic heuristics to\nmitigate the problems caused by the truncation of episodes needed to deal with\nthe potentially infinite MDP. Second, we propose learning a residual of an\nexisting symbolic heuristic, which is a \"correction\" of the heuristic value,\ninstead of eagerly learning the whole heuristic from scratch. Finally, we use\nthe learned heuristic in combination with a symbolic heuristic using a\nmultiple-queue planning approach to balance systematic search with imperfect\nlearned information. We experimentally compare all the approaches, highlighting\ntheir strengths and weaknesses and significantly advancing the state of the art\nfor this planning and learning schema."}
{"id": "2505.12489", "pdf": "https://arxiv.org/pdf/2505.12489", "abs": "https://arxiv.org/abs/2505.12489", "authors": ["Shaobin Zhuang", "Zhipeng Huang", "Ying Zhang", "Fangyikang Wang", "Canmiao Fu", "Binxin Yang", "Chong Sun", "Chen Li", "Yali Wang"], "title": "Video-GPT via Next Clip Diffusion", "categories": ["cs.CV", "cs.AI"], "comment": "22 pages, 12 figures, 18 tables", "summary": "GPT has shown its remarkable success in natural language processing. However,\nthe language sequence is not sufficient to describe spatial-temporal details in\nthe visual world. Alternatively, the video sequence is good at capturing such\ndetails. Motivated by this fact, we propose a concise Video-GPT in this paper\nby treating video as new language for visual world modeling. By analogy to next\ntoken prediction in GPT, we introduce a novel next clip diffusion paradigm for\npretraining Video-GPT. Different from the previous works, this distinct\nparadigm allows Video-GPT to tackle both short-term generation and long-term\nprediction, by autoregressively denoising the noisy clip according to the clean\nclips in the history. Extensive experiments show our Video-GPT achieves the\nstate-of-the-art performance on video prediction, which is the key factor\ntowards world modeling (Physics-IQ Benchmark: Video-GPT 34.97 vs. Kling 23.64\nvs. Wan 20.89). Moreover, it can be well adapted on 6 mainstream video tasks in\nboth video generation and understanding, showing its great generalization\ncapacity in downstream. The project page is at https://Video-GPT.github.io."}
{"id": "2505.12533", "pdf": "https://arxiv.org/pdf/2505.12533", "abs": "https://arxiv.org/abs/2505.12533", "authors": ["Varvara Arzt", "Allan Hanbury", "Michael Wiegand", "Gábor Recski", "Terra Blevins"], "title": "Relation Extraction or Pattern Matching? Unravelling the Generalisation Limits of Language Models for Biographical RE", "categories": ["cs.CL"], "comment": null, "summary": "Analysing the generalisation capabilities of relation extraction (RE) models\nis crucial for assessing whether they learn robust relational patterns or rely\non spurious correlations. Our cross-dataset experiments find that RE models\nstruggle with unseen data, even within similar domains. Notably, higher\nintra-dataset performance does not indicate better transferability, instead\noften signaling overfitting to dataset-specific artefacts. Our results also\nshow that data quality, rather than lexical similarity, is key to robust\ntransfer, and the choice of optimal adaptation strategy depends on the quality\nof data available: while fine-tuning yields the best cross-dataset performance\nwith high-quality data, few-shot in-context learning (ICL) is more effective\nwith noisier data. However, even in these cases, zero-shot baselines\noccasionally outperform all cross-dataset results. Structural issues in RE\nbenchmarks, such as single-relation per sample constraints and non-standardised\nnegative class definitions, further hinder model transferability."}
{"id": "2505.13380", "pdf": "https://arxiv.org/pdf/2505.13380", "abs": "https://arxiv.org/abs/2505.13380", "authors": ["Nam V. Nguyen", "Huy Nguyen", "Quang Pham", "Van Nguyen", "Savitha Ramasamy", "Nhat Ho"], "title": "CompeteSMoE -- Statistically Guaranteed Mixture of Experts Training via Competition", "categories": ["cs.AI", "cs.CL"], "comment": "52 pages. This work is an improved version of the previous study at\n  arXiv:2402.02526", "summary": "Sparse mixture of experts (SMoE) offers an appealing solution to scale up the\nmodel complexity beyond the mean of increasing the network's depth or width.\nHowever, we argue that effective SMoE training remains challenging because of\nthe suboptimal routing process where experts that perform computation do not\ndirectly contribute to the routing process. In this work, we propose\ncompetition, a novel mechanism to route tokens to experts with the highest\nneural response. Theoretically, we show that the competition mechanism enjoys a\nbetter sample efficiency than the traditional softmax routing. Furthermore, we\ndevelop CompeteSMoE, a simple yet effective algorithm to train large language\nmodels by deploying a router to learn the competition policy, thus enjoying\nstrong performances at a low training overhead. Our extensive empirical\nevaluations on both the visual instruction tuning and language pre-training\ntasks demonstrate the efficacy, robustness, and scalability of CompeteSMoE\ncompared to state-of-the-art SMoE strategies. We have made the implementation\navailable at: https://github.com/Fsoft-AIC/CompeteSMoE. This work is an\nimproved version of the previous study at arXiv:2402.02526"}
{"id": "2505.12499", "pdf": "https://arxiv.org/pdf/2505.12499", "abs": "https://arxiv.org/abs/2505.12499", "authors": ["Jian Xiao", "Zijie Song", "Jialong Hu", "Hao Cheng", "Zhenzhen Hu", "Jia Li", "Richang Hong"], "title": "Rebalancing Contrastive Alignment with Learnable Semantic Gaps in Text-Video Retrieval", "categories": ["cs.CV", "cs.IR", "cs.MM"], "comment": null, "summary": "Recent advances in text-video retrieval have been largely driven by\ncontrastive learning frameworks. However, existing methods overlook a key\nsource of optimization tension: the separation between text and video\ndistributions in the representation space (referred to as the modality gap),\nand the prevalence of false negatives in batch sampling. These factors lead to\nconflicting gradients under the InfoNCE loss, impeding stable alignment. To\nmitigate this, we propose GARE, a Gap-Aware Retrieval framework that introduces\na learnable, pair-specific increment Delta_ij between text t_i and video v_j to\noffload the tension from the global anchor representation. We first derive the\nideal form of Delta_ij via a coupled multivariate first-order Taylor\napproximation of the InfoNCE loss under a trust-region constraint, revealing it\nas a mechanism for resolving gradient conflicts by guiding updates along a\nlocally optimal descent direction. Due to the high cost of directly computing\nDelta_ij, we introduce a lightweight neural module conditioned on the semantic\ngap between each video-text pair, enabling structure-aware correction guided by\ngradient supervision. To further stabilize learning and promote\ninterpretability, we regularize Delta using three components: a trust-region\nconstraint to prevent oscillation, a directional diversity term to promote\nsemantic coverage, and an information bottleneck to limit redundancy.\nExperiments across four retrieval benchmarks show that GARE consistently\nimproves alignment accuracy and robustness to noisy supervision, confirming the\neffectiveness of gap-aware tension mitigation."}
{"id": "2505.12543", "pdf": "https://arxiv.org/pdf/2505.12543", "abs": "https://arxiv.org/abs/2505.12543", "authors": ["Md Mehrab Tanjim", "Yeonjun In", "Xiang Chen", "Victor S. Bursztyn", "Ryan A. Rossi", "Sungchul Kim", "Guang-Jie Ren", "Vaishnavi Muppala", "Shun Jiang", "Yongsung Kim", "Chanyoung Park"], "title": "Disambiguation in Conversational Question Answering in the Era of LLM: A Survey", "categories": ["cs.CL"], "comment": "Preprint", "summary": "Ambiguity remains a fundamental challenge in Natural Language Processing\n(NLP) due to the inherent complexity and flexibility of human language. With\nthe advent of Large Language Models (LLMs), addressing ambiguity has become\neven more critical due to their expanded capabilities and applications. In the\ncontext of Conversational Question Answering (CQA), this paper explores the\ndefinition, forms, and implications of ambiguity for language driven systems,\nparticularly in the context of LLMs. We define key terms and concepts,\ncategorize various disambiguation approaches enabled by LLMs, and provide a\ncomparative analysis of their advantages and disadvantages. We also explore\npublicly available datasets for benchmarking ambiguity detection and resolution\ntechniques and highlight their relevance for ongoing research. Finally, we\nidentify open problems and future research directions, proposing areas for\nfurther investigation. By offering a comprehensive review of current research\non ambiguities and disambiguation with LLMs, we aim to contribute to the\ndevelopment of more robust and reliable language systems."}
{"id": "2505.13391", "pdf": "https://arxiv.org/pdf/2505.13391", "abs": "https://arxiv.org/abs/2505.13391", "authors": ["Mikołaj Małkiński", "Jacek Mańdziuk"], "title": "Advancing Generalization Across a Variety of Abstract Visual Reasoning Tasks", "categories": ["cs.AI", "cs.CV", "cs.LG"], "comment": "Accepted to the 34th International Joint Conference on Artificial\n  Intelligence (IJCAI 2025)", "summary": "The abstract visual reasoning (AVR) domain presents a diverse suite of\nanalogy-based tasks devoted to studying model generalization. Recent years have\nbrought dynamic progress in the field, particularly in i.i.d. scenarios, in\nwhich models are trained and evaluated on the same data distributions.\nNevertheless, o.o.d. setups that assess model generalization to new test\ndistributions remain challenging even for the most recent models. To advance\ngeneralization in AVR tasks, we present the Pathways of Normalized Group\nConvolution model (PoNG), a novel neural architecture that features group\nconvolution, normalization, and a parallel design. We consider a wide set of\nAVR benchmarks, including Raven's Progressive Matrices and visual analogy\nproblems with both synthetic and real-world images. The experiments demonstrate\nstrong generalization capabilities of the proposed model, which in several\nsettings outperforms the existing literature methods."}
{"id": "2505.12513", "pdf": "https://arxiv.org/pdf/2505.12513", "abs": "https://arxiv.org/abs/2505.12513", "authors": ["Yang Mu", "Zhitong Xiong", "Yi Wang", "Muhammad Shahzad", "Franz Essl", "Mark van Kleunen", "Xiao Xiang Zhu"], "title": "GlobalGeoTree: A Multi-Granular Vision-Language Dataset for Global Tree Species Classification", "categories": ["cs.CV"], "comment": null, "summary": "Global tree species mapping using remote sensing data is vital for\nbiodiversity monitoring, forest management, and ecological research. However,\nprogress in this field has been constrained by the scarcity of large-scale,\nlabeled datasets. To address this, we introduce GlobalGeoTree, a comprehensive\nglobal dataset for tree species classification. GlobalGeoTree comprises 6.3\nmillion geolocated tree occurrences, spanning 275 families, 2,734 genera, and\n21,001 species across the hierarchical taxonomic levels. Each sample is paired\nwith Sentinel-2 image time series and 27 auxiliary environmental variables,\nencompassing bioclimatic, geographic, and soil data. The dataset is partitioned\ninto GlobalGeoTree-6M for model pretraining and curated evaluation subsets,\nprimarily GlobalGeoTree-10kEval for zero-shot and few-shot benchmarking. To\ndemonstrate the utility of the dataset, we introduce a baseline model,\nGeoTreeCLIP, which leverages paired remote sensing data and taxonomic text\nlabels within a vision-language framework pretrained on GlobalGeoTree-6M.\nExperimental results show that GeoTreeCLIP achieves substantial improvements in\nzero- and few-shot classification on GlobalGeoTree-10kEval over existing\nadvanced models. By making the dataset, models, and code publicly available, we\naim to establish a benchmark to advance tree species classification and foster\ninnovation in biodiversity research and ecological applications."}
{"id": "2505.12545", "pdf": "https://arxiv.org/pdf/2505.12545", "abs": "https://arxiv.org/abs/2505.12545", "authors": ["Yang Zhao", "Pu Wang", "Yibo Zhao", "Hongru Du", "Hao", "Yang"], "title": "Towards Reliable and Interpretable Traffic Crash Pattern Prediction and Safety Interventions Using Customized Large Language Models", "categories": ["cs.CL"], "comment": "Last revised 13 Feb 2025. Under review in Nature portfolio", "summary": "Predicting crash events is crucial for understanding crash distributions and\ntheir contributing factors, thereby enabling the design of proactive traffic\nsafety policy interventions. However, existing methods struggle to interpret\nthe complex interplay among various sources of traffic crash data, including\nnumeric characteristics, textual reports, crash imagery, environmental\nconditions, and driver behavior records. As a result, they often fail to\ncapture the rich semantic information and intricate interrelationships embedded\nin these diverse data sources, limiting their ability to identify critical\ncrash risk factors. In this research, we propose TrafficSafe, a framework that\nadapts LLMs to reframe crash prediction and feature attribution as text-based\nreasoning. A multi-modal crash dataset including 58,903 real-world reports\ntogether with belonged infrastructure, environmental, driver, and vehicle\ninformation is collected and textualized into TrafficSafe Event Dataset. By\ncustomizing and fine-tuning LLMs on this dataset, the TrafficSafe LLM achieves\na 42% average improvement in F1-score over baselines. To interpret these\npredictions and uncover contributing factors, we introduce TrafficSafe\nAttribution, a sentence-level feature attribution framework enabling\nconditional risk analysis. Findings show that alcohol-impaired driving is the\nleading factor in severe crashes, with aggressive and impairment-related\nbehaviors having nearly twice the contribution for severe crashes compared to\nother driver behaviors. Furthermore, TrafficSafe Attribution highlights pivotal\nfeatures during model training, guiding strategic crash data collection for\niterative performance improvements. The proposed TrafficSafe offers a\ntransformative leap in traffic safety research, providing a blueprint for\ntranslating advanced AI technologies into responsible, actionable, and\nlife-saving outcomes."}
{"id": "2505.13400", "pdf": "https://arxiv.org/pdf/2505.13400", "abs": "https://arxiv.org/abs/2505.13400", "authors": ["Ali Essam Ghareeb", "Benjamin Chang", "Ludovico Mitchener", "Angela Yiu", "Caralyn J. Szostkiewicz", "Jon M. Laurent", "Muhammed T. Razzak", "Andrew D. White", "Michaela M. Hinks", "Samuel G. Rodriques"], "title": "Robin: A multi-agent system for automating scientific discovery", "categories": ["cs.AI", "cs.MA", "q-bio.QM"], "comment": null, "summary": "Scientific discovery is driven by the iterative process of background\nresearch, hypothesis generation, experimentation, and data analysis. Despite\nrecent advancements in applying artificial intelligence to scientific\ndiscovery, no system has yet automated all of these stages in a single\nworkflow. Here, we introduce Robin, the first multi-agent system capable of\nfully automating the key intellectual steps of the scientific process. By\nintegrating literature search agents with data analysis agents, Robin can\ngenerate hypotheses, propose experiments, interpret experimental results, and\ngenerate updated hypotheses, achieving a semi-autonomous approach to scientific\ndiscovery. By applying this system, we were able to identify a novel treatment\nfor dry age-related macular degeneration (dAMD), the major cause of blindness\nin the developed world. Robin proposed enhancing retinal pigment epithelium\nphagocytosis as a therapeutic strategy, and identified and validated a\npromising therapeutic candidate, ripasudil. Ripasudil is a clinically-used rho\nkinase (ROCK) inhibitor that has never previously been proposed for treating\ndAMD. To elucidate the mechanism of ripasudil-induced upregulation of\nphagocytosis, Robin then proposed and analyzed a follow-up RNA-seq experiment,\nwhich revealed upregulation of ABCA1, a critical lipid efflux pump and possible\nnovel target. All hypotheses, experimental plans, data analyses, and data\nfigures in the main text of this report were produced by Robin. As the first AI\nsystem to autonomously discover and validate a novel therapeutic candidate\nwithin an iterative lab-in-the-loop framework, Robin establishes a new paradigm\nfor AI-driven scientific discovery."}
{"id": "2505.12532", "pdf": "https://arxiv.org/pdf/2505.12532", "abs": "https://arxiv.org/abs/2505.12532", "authors": ["Ahmet Bilican", "M. Akın Yılmaz", "A. Murat Tekalp", "R. Gökberk Cinbiş"], "title": "Exploring Sparsity for Parameter Efficient Fine Tuning Using Wavelets", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV", "eess.SP"], "comment": null, "summary": "Efficiently adapting large foundation models is critical, especially with\ntight compute and memory budgets. Parameter-Efficient Fine-Tuning (PEFT)\nmethods such as LoRA offer limited granularity and effectiveness in\nfew-parameter regimes. We propose Wavelet Fine-Tuning (WaveFT), a novel PEFT\nmethod that learns highly sparse updates in the wavelet domain of residual\nmatrices. WaveFT allows precise control of trainable parameters, offering\nfine-grained capacity adjustment and excelling with remarkably low parameter\ncount, potentially far fewer than LoRA's minimum -- ideal for extreme\nparameter-efficient scenarios. In order to demonstrate the effect of the\nwavelet transform, we compare WaveFT with a special case, called SHiRA, that\nentails applying sparse updates directly in the weight domain. Evaluated on\npersonalized text-to-image generation using Stable Diffusion XL as baseline,\nWaveFT significantly outperforms LoRA and other PEFT methods, especially at low\nparameter counts; achieving superior subject fidelity, prompt alignment, and\nimage diversity."}
{"id": "2505.12546", "pdf": "https://arxiv.org/pdf/2505.12546", "abs": "https://arxiv.org/abs/2505.12546", "authors": ["A. Feder Cooper", "Aaron Gokaslan", "Amy B. Cyphert", "Christopher De Sa", "Mark A. Lemley", "Daniel E. Ho", "Percy Liang"], "title": "Extracting memorized pieces of (copyrighted) books from open-weight language models", "categories": ["cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "Plaintiffs and defendants in copyright lawsuits over generative AI often make\nsweeping, opposing claims about the extent to which large language models\n(LLMs) have memorized plaintiffs' protected expression. Drawing on adversarial\nML and copyright law, we show that these polarized positions dramatically\noversimplify the relationship between memorization and copyright. To do so, we\nleverage a recent probabilistic extraction technique to extract pieces of the\nBooks3 dataset from 13 open-weight LLMs. Through numerous experiments, we show\nthat it's possible to extract substantial parts of at least some books from\ndifferent LLMs. This is evidence that the LLMs have memorized the extracted\ntext; this memorized content is copied inside the model parameters. But the\nresults are complicated: the extent of memorization varies both by model and by\nbook. With our specific experiments, we find that the largest LLMs don't\nmemorize most books -- either in whole or in part. However, we also find that\nLlama 3.1 70B memorizes some books, like Harry Potter and 1984, almost\nentirely. We discuss why our results have significant implications for\ncopyright cases, though not ones that unambiguously favor either side."}
{"id": "2505.13406", "pdf": "https://arxiv.org/pdf/2505.13406", "abs": "https://arxiv.org/abs/2505.13406", "authors": ["Rong Bian", "Yu Geng", "Zijian Yang", "Bing Cheng"], "title": "AutoMathKG: The automated mathematical knowledge graph based on LLM and vector database", "categories": ["cs.AI"], "comment": null, "summary": "A mathematical knowledge graph (KG) presents knowledge within the field of\nmathematics in a structured manner. Constructing a math KG using natural\nlanguage is an essential but challenging task. There are two major limitations\nof existing works: first, they are constrained by corpus completeness, often\ndiscarding or manually supplementing incomplete knowledge; second, they\ntypically fail to fully automate the integration of diverse knowledge sources.\nThis paper proposes AutoMathKG, a high-quality, wide-coverage, and\nmulti-dimensional math KG capable of automatic updates. AutoMathKG regards\nmathematics as a vast directed graph composed of Definition, Theorem, and\nProblem entities, with their reference relationships as edges. It integrates\nknowledge from ProofWiki, textbooks, arXiv papers, and TheoremQA, enhancing\nentities and relationships with large language models (LLMs) via in-context\nlearning for data augmentation. To search for similar entities, MathVD, a\nvector database, is built through two designed embedding strategies using\nSBERT. To automatically update, two mechanisms are proposed. For knowledge\ncompletion mechanism, Math LLM is developed to interact with AutoMathKG,\nproviding missing proofs or solutions. For knowledge fusion mechanism, MathVD\nis used to retrieve similar entities, and LLM is used to determine whether to\nmerge with a candidate or add as a new entity. A wide range of experiments\ndemonstrate the advanced performance and broad applicability of the AutoMathKG\nsystem, including superior reachability query results in MathVD compared to\nfive baselines and robust mathematical reasoning capability in Math LLM."}
{"id": "2505.12547", "pdf": "https://arxiv.org/pdf/2505.12547", "abs": "https://arxiv.org/abs/2505.12547", "authors": ["Florent Chiaroni", "Ali Ayub", "Ola Ahmad"], "title": "ProMi: An Efficient Prototype-Mixture Baseline for Few-Shot Segmentation with Bounding-Box Annotations", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "In robotics applications, few-shot segmentation is crucial because it allows\nrobots to perform complex tasks with minimal training data, facilitating their\nadaptation to diverse, real-world environments. However, pixel-level\nannotations of even small amount of images is highly time-consuming and costly.\nIn this paper, we present a novel few-shot binary segmentation method based on\nbounding-box annotations instead of pixel-level labels. We introduce, ProMi, an\nefficient prototype-mixture-based method that treats the background class as a\nmixture of distributions. Our approach is simple, training-free, and effective,\naccommodating coarse annotations with ease. Compared to existing baselines,\nProMi achieves the best results across different datasets with significant\ngains, demonstrating its effectiveness. Furthermore, we present qualitative\nexperiments tailored to real-world mobile robot tasks, demonstrating the\napplicability of our approach in such scenarios. Our code:\nhttps://github.com/ThalesGroup/promi."}
{"id": "2505.12560", "pdf": "https://arxiv.org/pdf/2505.12560", "abs": "https://arxiv.org/abs/2505.12560", "authors": ["Hiram Ring"], "title": "The taggedPBC: Annotating a massive parallel corpus for crosslinguistic investigations", "categories": ["cs.CL"], "comment": null, "summary": "Existing datasets available for crosslinguistic investigations have tended to\nfocus on large amounts of data for a small group of languages or a small amount\nof data for a large number of languages. This means that claims based on these\ndatasets are limited in what they reveal about universal properties of the\nhuman language faculty. While this has begun to change through the efforts of\nprojects seeking to develop tagged corpora for a large number of languages,\nsuch efforts are still constrained by limits on resources. The current paper\nreports on a large automatically tagged parallel dataset which has been\ndeveloped to partially address this issue. The taggedPBC contains more than\n1,800 sentences of pos-tagged parallel text data from over 1,500 languages,\nrepresenting 133 language families and 111 isolates, dwarfing previously\navailable resources. The accuracy of tags in this dataset is shown to correlate\nwell with both existing SOTA taggers for high-resource languages (SpaCy,\nTrankit) as well as hand-tagged corpora (Universal Dependencies Treebanks).\nAdditionally, a novel measure derived from this dataset, the N1 ratio,\ncorrelates with expert determinations of word order in three typological\ndatabases (WALS, Grambank, Autotyp) such that a Gaussian Naive Bayes classifier\ntrained on this feature can accurately identify basic word order for languages\nnot in those databases. While much work is still needed to expand and develop\nthis dataset, the taggedPBC is an important step to enable corpus-based\ncrosslinguistic investigations, and is made available for research and\ncollaboration via GitHub."}
{"id": "2505.13408", "pdf": "https://arxiv.org/pdf/2505.13408", "abs": "https://arxiv.org/abs/2505.13408", "authors": ["Jinhe Bi", "Danqi Yan", "Yifan Wang", "Wenke Huang", "Haokun Chen", "Guancheng Wan", "Mang Ye", "Xun Xiao", "Hinrich Schuetze", "Volker Tresp", "Yunpu Ma"], "title": "CoT-Kinetics: A Theoretical Modeling Assessing LRM Reasoning Process", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recent Large Reasoning Models significantly improve the reasoning ability of\nLarge Language Models by learning to reason, exhibiting the promising\nperformance in solving complex tasks. LRMs solve tasks that require complex\nreasoning by explicitly generating reasoning trajectories together with\nanswers. Nevertheless, judging the quality of such an output answer is not easy\nbecause only considering the correctness of the answer is not enough and the\nsoundness of the reasoning trajectory part matters as well. Logically, if the\nsoundness of the reasoning part is poor, even if the answer is correct, the\nconfidence of the derived answer should be low. Existing methods did consider\njointly assessing the overall output answer by taking into account the\nreasoning part, however, their capability is still not satisfactory as the\ncausal relationship of the reasoning to the concluded answer cannot properly\nreflected. In this paper, inspired by classical mechanics, we present a novel\napproach towards establishing a CoT-Kinetics energy equation. Specifically, our\nCoT-Kinetics energy equation formulates the token state transformation process,\nwhich is regulated by LRM internal transformer layers, as like a particle\nkinetics dynamics governed in a mechanical field. Our CoT-Kinetics energy\nassigns a scalar score to evaluate specifically the soundness of the reasoning\nphase, telling how confident the derived answer could be given the evaluated\nreasoning. As such, the LRM's overall output quality can be accurately\nmeasured, rather than a coarse judgment (e.g., correct or incorrect) anymore."}
{"id": "2505.12549", "pdf": "https://arxiv.org/pdf/2505.12549", "abs": "https://arxiv.org/abs/2505.12549", "authors": ["Dominic Maggio", "Hyungtae Lim", "Luca Carlone"], "title": "VGGT-SLAM: Dense RGB SLAM Optimized on the SL(4) Manifold", "categories": ["cs.CV"], "comment": null, "summary": "We present VGGT-SLAM, a dense RGB SLAM system constructed by incrementally\nand globally aligning submaps created from the feed-forward scene\nreconstruction approach VGGT using only uncalibrated monocular cameras. While\nrelated works align submaps using similarity transforms (i.e., translation,\nrotation, and scale), we show that such approaches are inadequate in the case\nof uncalibrated cameras. In particular, we revisit the idea of reconstruction\nambiguity, where given a set of uncalibrated cameras with no assumption on the\ncamera motion or scene structure, the scene can only be reconstructed up to a\n15-degrees-of-freedom projective transformation of the true geometry. This\ninspires us to recover a consistent scene reconstruction across submaps by\noptimizing over the SL(4) manifold, thus estimating 15-degrees-of-freedom\nhomography transforms between sequential submaps while accounting for potential\nloop closure constraints. As verified by extensive experiments, we demonstrate\nthat VGGT-SLAM achieves improved map quality using long video sequences that\nare infeasible for VGGT due to its high GPU requirements."}
{"id": "2505.12568", "pdf": "https://arxiv.org/pdf/2505.12568", "abs": "https://arxiv.org/abs/2505.12568", "authors": ["Lekang Jiang", "Chengzu Li", "Stephan Goetz"], "title": "Enriching Patent Claim Generation with European Patent Dataset", "categories": ["cs.CL"], "comment": "18 pages, 13 tables, 4 figures", "summary": "Drafting patent claims is time-intensive, costly, and requires professional\nskill. Therefore, researchers have investigated large language models (LLMs) to\nassist inventors in writing claims. However, existing work has largely relied\non datasets from the United States Patent and Trademark Office (USPTO). To\nenlarge research scope regarding various jurisdictions, drafting conventions,\nand legal standards, we introduce EPD, a European patent dataset. EPD presents\nrich textual data and structured metadata to support multiple patent-related\ntasks, including claim generation. This dataset enriches the field in three\ncritical aspects: (1) Jurisdictional diversity: Patents from different offices\nvary in legal and drafting conventions. EPD fills a critical gap by providing a\nbenchmark for European patents to enable more comprehensive evaluation. (2)\nQuality improvement: EPD offers high-quality granted patents with finalized and\nlegally approved texts, whereas others consist of patent applications that are\nunexamined or provisional. Experiments show that LLMs fine-tuned on EPD\nsignificantly outperform those trained on previous datasets and even GPT-4o in\nclaim quality and cross-domain generalization. (3) Real-world simulation: We\npropose a difficult subset of EPD to better reflect real-world challenges of\nclaim generation. Results reveal that all tested LLMs perform substantially\nworse on these challenging samples, which highlights the need for future\nresearch."}
{"id": "2505.13427", "pdf": "https://arxiv.org/pdf/2505.13427", "abs": "https://arxiv.org/abs/2505.13427", "authors": ["Lingxiao Du", "Fanqing Meng", "Zongkai Liu", "Zhixiang Zhou", "Ping Luo", "Qiaosheng Zhang", "Wenqi Shao"], "title": "MM-PRM: Enhancing Multimodal Mathematical Reasoning with Scalable Step-Level Supervision", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "While Multimodal Large Language Models (MLLMs) have achieved impressive\nprogress in vision-language understanding, they still struggle with complex\nmulti-step reasoning, often producing logically inconsistent or partially\ncorrect solutions. A key limitation lies in the lack of fine-grained\nsupervision over intermediate reasoning steps. To address this, we propose\nMM-PRM, a process reward model trained within a fully automated, scalable\nframework. We first build MM-Policy, a strong multimodal model trained on\ndiverse mathematical reasoning data. Then, we construct MM-K12, a curated\ndataset of 10,000 multimodal math problems with verifiable answers, which\nserves as seed data. Leveraging a Monte Carlo Tree Search (MCTS)-based\npipeline, we generate over 700k step-level annotations without human labeling.\nThe resulting PRM is used to score candidate reasoning paths in the Best-of-N\ninference setup and achieves significant improvements across both in-domain\n(MM-K12 test set) and out-of-domain (OlympiadBench, MathVista, etc.)\nbenchmarks. Further analysis confirms the effectiveness of soft labels, smaller\nlearning rates, and path diversity in optimizing PRM performance. MM-PRM\ndemonstrates that process supervision is a powerful tool for enhancing the\nlogical robustness of multimodal reasoning systems. We release all our codes\nand data at https://github.com/ModalMinds/MM-PRM."}
{"id": "2505.12580", "pdf": "https://arxiv.org/pdf/2505.12580", "abs": "https://arxiv.org/abs/2505.12580", "authors": ["Priyank Pathak", "Yogesh S Rawat"], "title": "Coarse Attribute Prediction with Task Agnostic Distillation for Real World Clothes Changing ReID", "categories": ["cs.CV"], "comment": null, "summary": "This work focuses on Clothes Changing Re-IDentification (CC-ReID) for the\nreal world. Existing works perform well with high-quality (HQ) images, but\nstruggle with low-quality (LQ) where we can have artifacts like pixelation,\nout-of-focus blur, and motion blur. These artifacts introduce noise to not only\nexternal biometric attributes (e.g. pose, body shape, etc.) but also corrupt\nthe model's internal feature representation. Models usually cluster LQ image\nfeatures together, making it difficult to distinguish between them, leading to\nincorrect matches. We propose a novel framework Robustness against Low-Quality\n(RLQ) to improve CC-ReID model on real-world data. RLQ relies on Coarse\nAttributes Prediction (CAP) and Task Agnostic Distillation (TAD) operating in\nalternate steps in a novel training mechanism. CAP enriches the model with\nexternal fine-grained attributes via coarse predictions, thereby reducing the\neffect of noisy inputs. On the other hand, TAD enhances the model's internal\nfeature representation by bridging the gap between HQ and LQ features, via an\nexternal dataset through task-agnostic self-supervision and distillation. RLQ\noutperforms the existing approaches by 1.6%-2.9% Top-1 on real-world datasets\nlike LaST, and DeepChange, while showing consistent improvement of 5.3%-6%\nTop-1 on PRCC with competitive performance on LTCC. *The code will be made\npublic soon.*"}
{"id": "2505.12572", "pdf": "https://arxiv.org/pdf/2505.12572", "abs": "https://arxiv.org/abs/2505.12572", "authors": ["Hanwen Shen", "Ting Ying"], "title": "Measuring Information Distortion in Hierarchical Ultra long Novel Generation:The Optimal Expansion Ratio", "categories": ["cs.CL", "cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "Writing novels with Large Language Models (LLMs) raises a critical question:\nhow much human-authored outline is necessary to generate high-quality\nmillion-word novels? While frameworks such as DOME, Plan&Write, and Long Writer\nhave improved stylistic coherence and logical consistency, they primarily\ntarget shorter novels (10k--100k words), leaving ultra-long generation largely\nunexplored. Drawing on insights from recent text compression methods like\nLLMZip and LLM2Vec, we conduct an information-theoretic analysis that\nquantifies distortion occurring when LLMs compress and reconstruct ultra-long\nnovels under varying compression-expansion ratios. We introduce a hierarchical\ntwo-stage generation pipeline (outline -> detailed outline -> manuscript) and\nfind an optimal outline length that balances information preservation with\nhuman effort. Through extensive experimentation with Chinese novels, we\nestablish that a two-stage hierarchical outline approach significantly reduces\nsemantic distortion compared to single-stage methods. Our findings provide\nempirically-grounded guidance for authors and researchers collaborating with\nLLMs to create million-word novels."}
{"id": "2505.13445", "pdf": "https://arxiv.org/pdf/2505.13445", "abs": "https://arxiv.org/abs/2505.13445", "authors": ["Xiaoyuan Liu", "Tian Liang", "Zhiwei He", "Jiahao Xu", "Wenxuan Wang", "Pinjia He", "Zhaopeng Tu", "Haitao Mi", "Dong Yu"], "title": "Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards", "categories": ["cs.AI", "cs.CL"], "comment": "code available at https://github.com/xyliu-cs/RISE", "summary": "Large Language Models (LLMs) show great promise in complex reasoning, with\nReinforcement Learning with Verifiable Rewards (RLVR) being a key enhancement\nstrategy. However, a prevalent issue is ``superficial self-reflection'', where\nmodels fail to robustly verify their own outputs. We introduce RISE\n(Reinforcing Reasoning with Self-Verification), a novel online RL framework\ndesigned to tackle this. RISE explicitly and simultaneously trains an LLM to\nimprove both its problem-solving and self-verification abilities within a\nsingle, integrated RL process. The core mechanism involves leveraging\nverifiable rewards from an outcome verifier to provide on-the-fly feedback for\nboth solution generation and self-verification tasks. In each iteration, the\nmodel generates solutions, then critiques its own on-policy generated\nsolutions, with both trajectories contributing to the policy update. Extensive\nexperiments on diverse mathematical reasoning benchmarks show that RISE\nconsistently improves model's problem-solving accuracy while concurrently\nfostering strong self-verification skills. Our analyses highlight the\nadvantages of online verification and the benefits of increased verification\ncompute. Additionally, RISE models exhibit more frequent and accurate\nself-verification behaviors during reasoning. These advantages reinforce RISE\nas a flexible and effective path towards developing more robust and self-aware\nreasoners."}
{"id": "2505.12588", "pdf": "https://arxiv.org/pdf/2505.12588", "abs": "https://arxiv.org/abs/2505.12588", "authors": ["Samya Bagchi", "Peter Anastasiou", "Matthew Tetlow", "Tat-Jun Chin", "Yasir Latif"], "title": "Event-based Star Tracking under Spacecraft Jitter: the e-STURT Dataset", "categories": ["cs.CV", "eess.SP"], "comment": null, "summary": "Jitter degrades a spacecraft's fine-pointing ability required for optical\ncommunication, earth observation, and space domain awareness. Development of\njitter estimation and compensation algorithms requires high-fidelity sensor\nobservations representative of on-board jitter. In this work, we present the\nEvent-based Star Tracking Under Jitter (e-STURT) dataset -- the first event\ncamera based dataset of star observations under controlled jitter conditions.\nSpecialized hardware employed for the dataset emulates an event-camera\nundergoing on-board jitter. While the event camera provides asynchronous, high\ntemporal resolution star observations, systematic and repeatable jitter is\nintroduced using a micrometer accurate piezoelectric actuator. Various jitter\nsources are simulated using distinct frequency bands and utilizing both axes of\nmotion. Ground-truth jitter is captured in hardware from the piezoelectric\nactuator. The resulting dataset consists of 200 sequences and is made publicly\navailable. This work highlights the dataset generation process, technical\nchallenges and the resulting limitations. To serve as a baseline, we propose a\nhigh-frequency jitter estimation algorithm that operates directly on the event\nstream. The e-STURT dataset will enable the development of jitter aware\nalgorithms for mission critical event-based space sensing applications."}
{"id": "2505.12584", "pdf": "https://arxiv.org/pdf/2505.12584", "abs": "https://arxiv.org/abs/2505.12584", "authors": ["Omar Mahmoud", "Buddhika Laknath Semage", "Thommen George Karimpanal", "Santu Rana"], "title": "Improving Multilingual Language Models by Aligning Representations through Steering", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we investigate how large language models (LLMS) process\nnon-English tokens within their layer representations, an open question despite\nsignificant advancements in the field. Using representation steering,\nspecifically by adding a learned vector to a single model layer's activations,\nwe demonstrate that steering a single model layer can notably enhance\nperformance. Our analysis shows that this approach achieves results comparable\nto translation baselines and surpasses state of the art prompt optimization\nmethods. Additionally, we highlight how advanced techniques like supervised\nfine tuning (\\textsc{sft}) and reinforcement learning from human feedback\n(\\textsc{rlhf}) improve multilingual capabilities by altering representation\nspaces. We further illustrate how these methods align with our approach to\nreshaping LLMS layer representations."}
{"id": "2309.11091", "pdf": "https://arxiv.org/pdf/2309.11091", "abs": "https://arxiv.org/abs/2309.11091", "authors": ["Chen Jiang", "Kaiming Huang", "Sifeng He", "Xudong Yang", "Wei Zhang", "Xiaobo Zhang", "Yuan Cheng", "Lei Yang", "Qing Wang", "Furong Xu", "Tan Pan", "Wei Chu"], "title": "Learning Segment Similarity and Alignment in Large-Scale Content Based Video Retrieval", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "Accepted by ACM MM 2021", "summary": "With the explosive growth of web videos in recent years, large-scale\nContent-Based Video Retrieval (CBVR) becomes increasingly essential in video\nfiltering, recommendation, and copyright protection. Segment-level CBVR\n(S-CBVR) locates the start and end time of similar segments in finer\ngranularity, which is beneficial for user browsing efficiency and infringement\ndetection especially in long video scenarios. The challenge of S-CBVR task is\nhow to achieve high temporal alignment accuracy with efficient computation and\nlow storage consumption. In this paper, we propose a Segment Similarity and\nAlignment Network (SSAN) in dealing with the challenge which is firstly trained\nend-to-end in S-CBVR. SSAN is based on two newly proposed modules in video\nretrieval: (1) An efficient Self-supervised Keyframe Extraction (SKE) module to\nreduce redundant frame features, (2) A robust Similarity Pattern Detection\n(SPD) module for temporal alignment. In comparison with uniform frame\nextraction, SKE not only saves feature storage and search time, but also\nintroduces comparable accuracy and limited extra computation time. In terms of\ntemporal alignment, SPD localizes similar segments with higher accuracy and\nefficiency than existing deep learning methods. Furthermore, we jointly train\nSSAN with SKE and SPD and achieve an end-to-end improvement. Meanwhile, the two\nkey modules SKE and SPD can also be effectively inserted into other video\nretrieval pipelines and gain considerable performance improvements.\nExperimental results on public datasets show that SSAN can obtain higher\nalignment accuracy while saving storage and online query computational cost\ncompared to existing methods."}
{"id": "2505.12589", "pdf": "https://arxiv.org/pdf/2505.12589", "abs": "https://arxiv.org/abs/2505.12589", "authors": ["Bo Liu", "Pengfei Qiao", "Minhan Ma", "Xuange Zhang", "Yinan Tang", "Peng Xu", "Kun Liu", "Tongtong Yuan"], "title": "SurveillanceVQA-589K: A Benchmark for Comprehensive Surveillance Video-Language Understanding with Large Models", "categories": ["cs.CV"], "comment": "The dataset and code are publicly available at:\n  https://huggingface.co/datasets/fei213/SurveillanceVQA-589K", "summary": "Understanding surveillance video content remains a critical yet underexplored\nchallenge in vision-language research, particularly due to its real-world\ncomplexity, irregular event dynamics, and safety-critical implications. In this\nwork, we introduce SurveillanceVQA-589K, the largest open-ended video question\nanswering benchmark tailored to the surveillance domain. The dataset comprises\n589,380 QA pairs spanning 12 cognitively diverse question types, including\ntemporal reasoning, causal inference, spatial understanding, and anomaly\ninterpretation, across both normal and abnormal video scenarios. To construct\nthe benchmark at scale, we design a hybrid annotation pipeline that combines\ntemporally aligned human-written captions with Large Vision-Language\nModel-assisted QA generation using prompt-based techniques. We also propose a\nmulti-dimensional evaluation protocol to assess contextual, temporal, and\ncausal comprehension. We evaluate eight LVLMs under this framework, revealing\nsignificant performance gaps, especially in causal and anomaly-related tasks,\nunderscoring the limitations of current models in real-world surveillance\ncontexts. Our benchmark provides a practical and comprehensive resource for\nadvancing video-language understanding in safety-critical applications such as\nintelligent monitoring, incident analysis, and autonomous decision-making."}
{"id": "2505.12587", "pdf": "https://arxiv.org/pdf/2505.12587", "abs": "https://arxiv.org/abs/2505.12587", "authors": ["Aditeya Baral", "Allen George Ajith", "Roshan Nayak", "Mrityunjay Abhijeet Bhanja"], "title": "CMLFormer: A Dual Decoder Transformer with Switching Point Learning for Code-Mixed Language Modeling", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Code-mixed languages, characterized by frequent within-sentence language\ntransitions, present structural challenges that standard language models fail\nto address. In this work, we propose CMLFormer, an enhanced multi-layer\ndual-decoder Transformer with a shared encoder and synchronized decoder\ncross-attention, designed to model the linguistic and semantic dynamics of\ncode-mixed text. CMLFormer is pre-trained on an augmented Hinglish corpus with\nswitching point and translation annotations with multiple new objectives\nspecifically aimed at capturing switching behavior, cross-lingual structure,\nand code-mixing complexity. Our experiments show that CMLFormer improves F1\nscore, precision, and accuracy over other approaches on the HASOC-2021\nbenchmark under select pre-training setups. Attention analyses further show\nthat it can identify and attend to switching points, validating its sensitivity\nto code-mixed structure. These results demonstrate the effectiveness of\nCMLFormer's architecture and multi-task pre-training strategy for modeling\ncode-mixed languages."}
{"id": "2401.04354", "pdf": "https://arxiv.org/pdf/2401.04354", "abs": "https://arxiv.org/abs/2401.04354", "authors": ["Xuzheng Yu", "Chen Jiang", "Wei Zhang", "Tian Gan", "Linlin Chao", "Jianan Zhao", "Yuan Cheng", "Qingpei Guo", "Wei Chu"], "title": "Knowledge-enhanced Multi-perspective Video Representation Learning for Scene Recognition", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": null, "summary": "With the explosive growth of video data in real-world applications, a\ncomprehensive representation of videos becomes increasingly important. In this\npaper, we address the problem of video scene recognition, whose goal is to\nlearn a high-level video representation to classify scenes in videos. Due to\nthe diversity and complexity of video contents in realistic scenarios, this\ntask remains a challenge. Most existing works identify scenes for videos only\nfrom visual or textual information in a temporal perspective, ignoring the\nvaluable information hidden in single frames, while several earlier studies\nonly recognize scenes for separate images in a non-temporal perspective. We\nargue that these two perspectives are both meaningful for this task and\ncomplementary to each other, meanwhile, externally introduced knowledge can\nalso promote the comprehension of videos. We propose a novel two-stream\nframework to model video representations from multiple perspectives, i.e.\ntemporal and non-temporal perspectives, and integrate the two perspectives in\nan end-to-end manner by self-distillation. Besides, we design a\nknowledge-enhanced feature fusion and label prediction method that contributes\nto naturally introducing knowledge into the task of video scene recognition.\nExperiments conducted on a real-world dataset demonstrate the effectiveness of\nour proposed method."}
{"id": "2505.12593", "pdf": "https://arxiv.org/pdf/2505.12593", "abs": "https://arxiv.org/abs/2505.12593", "authors": ["Mia Thomas", "Trevor Ablett", "Jonathan Kelly"], "title": "Learning Cross-Spectral Point Features with Task-Oriented Training", "categories": ["cs.CV"], "comment": "Proceedings of the {IEEE} International Conference on Robotics and\n  Automation {(ICRA'25)} Thermal Infrared in Robotics (TIRO) Workshop, Atlanta,\n  Georgia, USA, May 19, 2025", "summary": "Unmanned aerial vehicles (UAVs) enable operations in remote and hazardous\nenvironments, yet the visible-spectrum, camera-based navigation systems often\nrelied upon by UAVs struggle in low-visibility conditions. Thermal cameras,\nwhich capture long-wave infrared radiation, are able to function effectively in\ndarkness and smoke, where visible-light cameras fail. This work explores\nlearned cross-spectral (thermal-visible) point features as a means to integrate\nthermal imagery into established camera-based navigation systems. Existing\nmethods typically train a feature network's detection and description outputs\ndirectly, which often focuses training on image regions where thermal and\nvisible-spectrum images exhibit similar appearance. Aiming to more fully\nutilize the available data, we propose a method to train the feature network on\nthe tasks of matching and registration. We run our feature network on\nthermal-visible image pairs, then feed the network response into a\ndifferentiable registration pipeline. Losses are applied to the matching and\nregistration estimates of this pipeline. Our selected model, trained on the\ntask of matching, achieves a registration error (corner error) below 10 pixels\nfor more than 75% of estimates on the MultiPoint dataset. We further\ndemonstrate that our model can also be used with a classical pipeline for\nmatching and registration."}
{"id": "2505.12592", "pdf": "https://arxiv.org/pdf/2505.12592", "abs": "https://arxiv.org/abs/2505.12592", "authors": ["Sullam Jeoung", "Yueyan Chen", "Yi Zhang", "Shuai Wang", "Haibo Ding", "Lin Lee Cheong"], "title": "PromptPrism: A Linguistically-Inspired Taxonomy for Prompts", "categories": ["cs.CL"], "comment": null, "summary": "Prompts are the interface for eliciting the capabilities of large language\nmodels (LLMs). Understanding their structure and components is critical for\nanalyzing LLM behavior and optimizing performance. However, the field lacks a\ncomprehensive framework for systematic prompt analysis and understanding. We\nintroduce PromptPrism, a linguistically-inspired taxonomy that enables prompt\nanalysis across three hierarchical levels: functional structure, semantic\ncomponent, and syntactic pattern. We show the practical utility of PromptPrism\nby applying it to three applications: (1) a taxonomy-guided prompt refinement\napproach that automatically improves prompt quality and enhances model\nperformance across a range of tasks; (2) a multi-dimensional dataset profiling\nmethod that extracts and aggregates structural, semantic, and syntactic\ncharacteristics from prompt datasets, enabling comprehensive analysis of prompt\ndistributions and patterns; (3) a controlled experimental framework for prompt\nsensitivity analysis by quantifying the impact of semantic reordering and\ndelimiter modifications on LLM performance. Our experimental results validate\nthe effectiveness of our taxonomy across these applications, demonstrating that\nPromptPrism provides a foundation for refining, profiling, and analyzing\nprompts."}
{"id": "2505.11520", "pdf": "https://arxiv.org/pdf/2505.11520", "abs": "https://arxiv.org/abs/2505.11520", "authors": ["Himaja Papala", "Daniel Polani", "Stas Tiomkin"], "title": "Decentralized Traffic Flow Optimization Through Intrinsic Motivation", "categories": ["physics.soc-ph", "cs.AI"], "comment": "9 pages, 6 figures, Published in the Proceedings of the 2024 IEEE\n  27th International Conference on Intelligent Transportation Systems (ITSC)", "summary": "Traffic congestion has long been an ubiquitous problem that is exacerbating\nwith the rapid growth of megacities. In this proof-of-concept work we study\nintrinsic motivation, implemented via the empowerment principle, to control\nautonomous car behavior to improve traffic flow. In standard models of traffic\ndynamics, self-organized traffic jams emerge spontaneously from the individual\nbehavior of cars, affecting traffic over long distances. Our novel car behavior\nstrategy improves traffic flow while still being decentralized and using only\nlocally available information without explicit coordination. Decentralization\nis essential for various reasons, not least to be able to absorb robustly\nsubstantial levels of uncertainty. Our scenario is based on the\nwell-established traffic dynamics model, the Nagel-Schreckenberg cellular\nautomaton. In a fraction of the cars in this model, we substitute the default\nbehavior by empowerment, our intrinsic motivation-based method. This proposed\nmodel significantly improves overall traffic flow, mitigates congestion, and\nreduces the average traffic jam time."}
{"id": "2505.12605", "pdf": "https://arxiv.org/pdf/2505.12605", "abs": "https://arxiv.org/abs/2505.12605", "authors": ["Thong Nguyen", "Zhiyuan Hu", "Xu Lin", "Cong-Duy Nguyen", "See-Kiong Ng", "Luu Anh Tuan"], "title": "Temporal-Oriented Recipe for Transferring Large Vision-Language Model to Video Understanding", "categories": ["cs.CV"], "comment": "In Progress", "summary": "Recent years have witnessed outstanding advances of large vision-language\nmodels (LVLMs). In order to tackle video understanding, most of them depend\nupon their implicit temporal understanding capacity. As such, they have not\ndeciphered important components that contribute to temporal understanding\nability, which might limit the potential of these LVLMs for video\nunderstanding. In this work, we conduct a thorough empirical study to demystify\ncrucial components that influence the temporal understanding of LVLMs. Our\nempirical study reveals that significant impacts are centered around the\nintermediate interface between the visual encoder and the large language model.\nBuilding on these insights, we propose a temporal-oriented recipe that\nencompasses temporal-oriented training schemes and an upscaled interface. Our\nfinal model developed using our recipe significantly enhances previous LVLMs on\nstandard video understanding tasks."}
{"id": "2505.12594", "pdf": "https://arxiv.org/pdf/2505.12594", "abs": "https://arxiv.org/abs/2505.12594", "authors": ["Tiankai Yang", "Junjun Liu", "Wingchun Siu", "Jiahang Wang", "Zhuangzhuang Qian", "Chanjuan Song", "Cheng Cheng", "Xiyang Hu", "Yue Zhao"], "title": "AD-AGENT: A Multi-agent Framework for End-to-end Anomaly Detection", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Anomaly detection (AD) is essential in areas such as fraud detection, network\nmonitoring, and scientific research. However, the diversity of data modalities\nand the increasing number of specialized AD libraries pose challenges for\nnon-expert users who lack in-depth library-specific knowledge and advanced\nprogramming skills. To tackle this, we present AD-AGENT, an LLM-driven\nmulti-agent framework that turns natural-language instructions into fully\nexecutable AD pipelines. AD-AGENT coordinates specialized agents for intent\nparsing, data preparation, library and model selection, documentation mining,\nand iterative code generation and debugging. Using a shared short-term\nworkspace and a long-term cache, the agents integrate popular AD libraries like\nPyOD, PyGOD, and TSLib into a unified workflow. Experiments demonstrate that\nAD-AGENT produces reliable scripts and recommends competitive models across\nlibraries. The system is open-sourced to support further research and practical\napplications in AD."}
{"id": "2505.11526", "pdf": "https://arxiv.org/pdf/2505.11526", "abs": "https://arxiv.org/abs/2505.11526", "authors": ["Tianxing Yang", "Huigen Ye", "Hua Xu"], "title": "Code Retrieval for MILP Instance Generation", "categories": ["math.OC", "cs.AI"], "comment": null, "summary": "Mixed-Integer Linear Programming (MILP) is widely used in fields such as\nscheduling, logistics, and planning. Enhancing the performance of MILP solvers,\nparticularly learning-based solvers, requires substantial amounts of\nhigh-quality data. However, existing methods for MILP instance generation\ntypically necessitate training a separate model for each problem class and are\ncomputationally intensive when generating new instances. To address these\nlimitations, we reformulate the MILP Instance Generation task as MILP Code\nGeneration task, enabling efficient, flexible, and interpretable instance\ngeneration through code. Since MILP instances generated from code can vary\nsignificantly in scale, we introduce MILP-EmbedSim, a new similarity metric\nthat accurately measures the similarity between instances of varying sizes\nwithin the same problem class. Leveraging this metric, we propose\nMILP-Retrieval, a pipeline that retrieves generation code from library to\nproduce MILP instances highly similar to target instance. MILP-Retrieval\noutperforms baselines in both MILP Code Generation and Instance Generation\ntasks, provides a novel perspective on MILP instance generation and opens new\npossibilities for learning-based solvers."}
{"id": "2505.12606", "pdf": "https://arxiv.org/pdf/2505.12606", "abs": "https://arxiv.org/abs/2505.12606", "authors": ["Shiyu Xuan", "Zechao Li", "Jinhui Tang"], "title": "Diff-MM: Exploring Pre-trained Text-to-Image Generation Model for Unified Multi-modal Object Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Multi-modal object tracking integrates auxiliary modalities such as depth,\nthermal infrared, event flow, and language to provide additional information\nbeyond RGB images, showing great potential in improving tracking stabilization\nin complex scenarios. Existing methods typically start from an RGB-based\ntracker and learn to understand auxiliary modalities only from training data.\nConstrained by the limited multi-modal training data, the performance of these\nmethods is unsatisfactory. To alleviate this limitation, this work proposes a\nunified multi-modal tracker Diff-MM by exploiting the multi-modal understanding\ncapability of the pre-trained text-to-image generation model. Diff-MM leverages\nthe UNet of pre-trained Stable Diffusion as a tracking feature extractor\nthrough the proposed parallel feature extraction pipeline, which enables\npairwise image inputs for object tracking. We further introduce a multi-modal\nsub-module tuning method that learns to gain complementary information between\ndifferent modalities. By harnessing the extensive prior knowledge in the\ngeneration model, we achieve a unified tracker with uniform parameters for\nRGB-N/D/T/E tracking. Experimental results demonstrate the promising\nperformance of our method compared with recently proposed trackers, e.g., its\nAUC outperforms OneTracker by 8.3% on TNL2K."}
{"id": "2505.12616", "pdf": "https://arxiv.org/pdf/2505.12616", "abs": "https://arxiv.org/abs/2505.12616", "authors": ["Shujauddin Syed", "Ted Pedersen"], "title": "Duluth at SemEval-2025 Task 7: TF-IDF with Optimized Vector Dimensions for Multilingual Fact-Checked Claim Retrieval", "categories": ["cs.CL", "68T50"], "comment": "SemEval-2025", "summary": "This paper presents the Duluth approach to the SemEval-2025 Task 7 on\nMultilingual and Crosslingual Fact-Checked Claim Retrieval. We implemented a\nTF-IDF-based retrieval system with experimentation on vector dimensions and\ntokenization strategies. Our best-performing configuration used word-level\ntokenization with a vocabulary size of 15,000 features, achieving an average\nsuccess@10 score of 0.78 on the development set and 0.69 on the test set across\nten languages. Our system showed stronger performance on higher-resource\nlanguages but still lagged significantly behind the top-ranked system, which\nachieved 0.96 average success@10. Our findings suggest that though advanced\nneural architectures are increasingly dominant in multilingual retrieval tasks,\nproperly optimized traditional methods like TF-IDF remain competitive\nbaselines, especially in limited compute resource scenarios."}
{"id": "2505.11528", "pdf": "https://arxiv.org/pdf/2505.11528", "abs": "https://arxiv.org/abs/2505.11528", "authors": ["Yuhang Huang", "JIazhao Zhang", "Shilong Zou", "XInwang Liu", "Ruizhen Hu", "Kai Xu"], "title": "LaDi-WM: A Latent Diffusion-based World Model for Predictive Manipulation", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Predictive manipulation has recently gained considerable attention in the\nEmbodied AI community due to its potential to improve robot policy performance\nby leveraging predicted states. However, generating accurate future visual\nstates of robot-object interactions from world models remains a well-known\nchallenge, particularly in achieving high-quality pixel-level representations.\nTo this end, we propose LaDi-WM, a world model that predicts the latent space\nof future states using diffusion modeling. Specifically, LaDi-WM leverages the\nwell-established latent space aligned with pre-trained Visual Foundation Models\n(VFMs), which comprises both geometric features (DINO-based) and semantic\nfeatures (CLIP-based). We find that predicting the evolution of the latent\nspace is easier to learn and more generalizable than directly predicting\npixel-level images. Building on LaDi-WM, we design a diffusion policy that\niteratively refines output actions by incorporating forecasted states, thereby\ngenerating more consistent and accurate results. Extensive experiments on both\nsynthetic and real-world benchmarks demonstrate that LaDi-WM significantly\nenhances policy performance by 27.9\\% on the LIBERO-LONG benchmark and 20\\% on\nthe real-world scenario. Furthermore, our world model and policies achieve\nimpressive generalizability in real-world experiments."}
{"id": "2505.12620", "pdf": "https://arxiv.org/pdf/2505.12620", "abs": "https://arxiv.org/abs/2505.12620", "authors": ["Haiquan Wen", "Yiwei He", "Zhenglin Huang", "Tianxiao Li", "Zihan YU", "Xingru Huang", "Lu Qi", "Baoyuan Wu", "Xiangtai Li", "Guangliang Cheng"], "title": "BusterX: MLLM-Powered AI-Generated Video Forgery Detection and Explanation", "categories": ["cs.CV"], "comment": null, "summary": "Advances in AI generative models facilitate super-realistic video synthesis,\namplifying misinformation risks via social media and eroding trust in digital\ncontent. Several research works have explored new deepfake detection methods on\nAI-generated images to alleviate these risks. However, with the fast\ndevelopment of video generation models, such as Sora and WanX, there is\ncurrently a lack of large-scale, high-quality AI-generated video datasets for\nforgery detection. In addition, existing detection approaches predominantly\ntreat the task as binary classification, lacking explainability in model\ndecision-making and failing to provide actionable insights or guidance for the\npublic. To address these challenges, we propose \\textbf{GenBuster-200K}, a\nlarge-scale AI-generated video dataset featuring 200K high-resolution video\nclips, diverse latest generative techniques, and real-world scenes. We further\nintroduce \\textbf{BusterX}, a novel AI-generated video detection and\nexplanation framework leveraging multimodal large language model (MLLM) and\nreinforcement learning for authenticity determination and explainable\nrationale. To our knowledge, GenBuster-200K is the {\\it \\textbf{first}}\nlarge-scale, high-quality AI-generated video dataset that incorporates the\nlatest generative techniques for real-world scenarios. BusterX is the {\\it\n\\textbf{first}} framework to integrate MLLM with reinforcement learning for\nexplainable AI-generated video detection. Extensive comparisons with\nstate-of-the-art methods and ablation studies validate the effectiveness and\ngeneralizability of BusterX. The code, models, and datasets will be released."}
{"id": "2505.12621", "pdf": "https://arxiv.org/pdf/2505.12621", "abs": "https://arxiv.org/abs/2505.12621", "authors": ["João Eduardo Batista", "Emil Vatai", "Mohamed Wahib"], "title": "Think Before You Attribute: Improving the Performance of LLMs Attribution Systems", "categories": ["cs.CL", "cs.IR"], "comment": "22 pages (9 pages of content, 4 pages of references, 9 pages of\n  supplementary material), 7 figures, 10 tables", "summary": "Large Language Models (LLMs) are increasingly applied in various science\ndomains, yet their broader adoption remains constrained by a critical\nchallenge: the lack of trustworthy, verifiable outputs. Current LLMs often\ngenerate answers without reliable source attribution, or worse, with incorrect\nattributions, posing a barrier to their use in scientific and high-stakes\nsettings, where traceability and accountability are non-negotiable. To be\nreliable, attribution systems need high accuracy and retrieve data with short\nlengths, i.e., attribute to a sentence within a document rather than a whole\ndocument. We propose a sentence-level pre-attribution step for\nRetrieve-Augmented Generation (RAG) systems that classify sentences into three\ncategories: not attributable, attributable to a single quote, and attributable\nto multiple quotes. By separating sentences before attribution, a proper\nattribution method can be selected for the type of sentence, or the attribution\ncan be skipped altogether. Our results indicate that classifiers are\nwell-suited for this task. In this work, we propose a pre-attribution step to\nreduce the computational complexity of attribution, provide a clean version of\nthe HAGRID dataset, and provide an end-to-end attribution system that works out\nof the box."}
{"id": "2505.11545", "pdf": "https://arxiv.org/pdf/2505.11545", "abs": "https://arxiv.org/abs/2505.11545", "authors": ["Xingyu Ji", "Parker Glenn", "Aditya G. Parameswaran", "Madelon Hulsebos"], "title": "TARGET: Benchmarking Table Retrieval for Generative Tasks", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.DB"], "comment": null, "summary": "The data landscape is rich with structured data, often of high value to\norganizations, driving important applications in data analysis and machine\nlearning. Recent progress in representation learning and generative models for\nsuch data has led to the development of natural language interfaces to\nstructured data, including those leveraging text-to-SQL. Contextualizing\ninteractions, either through conversational interfaces or agentic components,\nin structured data through retrieval-augmented generation can provide\nsubstantial benefits in the form of freshness, accuracy, and comprehensiveness\nof answers. The key question is: how do we retrieve the right table(s) for the\nanalytical query or task at hand? To this end, we introduce TARGET: a benchmark\nfor evaluating TAble Retrieval for GEnerative Tasks. With TARGET we analyze the\nretrieval performance of different retrievers in isolation, as well as their\nimpact on downstream tasks. We find that dense embedding-based retrievers far\noutperform a BM25 baseline which is less effective than it is for retrieval\nover unstructured text. We also surface the sensitivity of retrievers across\nvarious metadata (e.g., missing table titles), and demonstrate a stark\nvariation of retrieval performance across datasets and tasks. TARGET is\navailable at https://target-benchmark.github.io."}
{"id": "2505.12630", "pdf": "https://arxiv.org/pdf/2505.12630", "abs": "https://arxiv.org/abs/2505.12630", "authors": ["Xiangpeng Tian", "Xiangyu Liao", "Xiao Liu", "Meng Li", "Chao Ren"], "title": "Degradation-Aware Feature Perturbation for All-in-One Image Restoration", "categories": ["cs.CV", "cs.AI", "I.4.5"], "comment": "Accepted to CVPR 2025. 8 pages, 7 figures", "summary": "All-in-one image restoration aims to recover clear images from various\ndegradation types and levels with a unified model. Nonetheless, the significant\nvariations among degradation types present challenges for training a universal\nmodel, often resulting in task interference, where the gradient update\ndirections of different tasks may diverge due to shared parameters. To address\nthis issue, motivated by the routing strategy, we propose DFPIR, a novel\nall-in-one image restorer that introduces Degradation-aware Feature\nPerturbations(DFP) to adjust the feature space to align with the unified\nparameter space. In this paper, the feature perturbations primarily include\nchannel-wise perturbations and attention-wise perturbations. Specifically,\nchannel-wise perturbations are implemented by shuffling the channels in\nhigh-dimensional space guided by degradation types, while attention-wise\nperturbations are achieved through selective masking in the attention space. To\nachieve these goals, we propose a Degradation-Guided Perturbation Block (DGPB)\nto implement these two functions, positioned between the encoding and decoding\nstages of the encoder-decoder architecture. Extensive experimental results\ndemonstrate that DFPIR achieves state-of-the-art performance on several\nall-in-one image restoration tasks including image denoising, image dehazing,\nimage deraining, motion deblurring, and low-light image enhancement. Our codes\nare available at https://github.com/TxpHome/DFPIR."}
{"id": "2505.12625", "pdf": "https://arxiv.org/pdf/2505.12625", "abs": "https://arxiv.org/abs/2505.12625", "authors": ["Ali Naseh", "Harsh Chaudhari", "Jaechul Roh", "Mingshi Wu", "Alina Oprea", "Amir Houmansadr"], "title": "R1dacted: Investigating Local Censorship in DeepSeek's R1 Language Model", "categories": ["cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "DeepSeek recently released R1, a high-performing large language model (LLM)\noptimized for reasoning tasks. Despite its efficient training pipeline, R1\nachieves competitive performance, even surpassing leading reasoning models like\nOpenAI's o1 on several benchmarks. However, emerging reports suggest that R1\nrefuses to answer certain prompts related to politically sensitive topics in\nChina. While existing LLMs often implement safeguards to avoid generating\nharmful or offensive outputs, R1 represents a notable shift - exhibiting\ncensorship-like behavior on politically charged queries. In this paper, we\ninvestigate this phenomenon by first introducing a large-scale set of heavily\ncurated prompts that get censored by R1, covering a range of politically\nsensitive topics, but are not censored by other models. We then conduct a\ncomprehensive analysis of R1's censorship patterns, examining their\nconsistency, triggers, and variations across topics, prompt phrasing, and\ncontext. Beyond English-language queries, we explore censorship behavior in\nother languages. We also investigate the transferability of censorship to\nmodels distilled from the R1 language model. Finally, we propose techniques for\nbypassing or removing this censorship. Our findings reveal possible additional\ncensorship integration likely shaped by design choices during training or\nalignment, raising concerns about transparency, bias, and governance in\nlanguage model deployment."}
{"id": "2505.11546", "pdf": "https://arxiv.org/pdf/2505.11546", "abs": "https://arxiv.org/abs/2505.11546", "authors": ["Xiao Li", "Tianhao Wei", "Changliu Liu", "Anouck Girard", "Ilya Kolmanovsky"], "title": "Control Invariant Sets for Neural Network Dynamical Systems and Recursive Feasibility in Model Predictive Control", "categories": ["eess.SY", "cs.AI", "cs.SY"], "comment": null, "summary": "Neural networks are powerful tools for data-driven modeling of complex\ndynamical systems, enhancing predictive capability for control applications.\nHowever, their inherent nonlinearity and black-box nature challenge control\ndesigns that prioritize rigorous safety and recursive feasibility guarantees.\nThis paper presents algorithmic methods for synthesizing control invariant sets\nspecifically tailored to neural network based dynamical models. These\nalgorithms employ set recursion, ensuring termination after a finite number of\niterations and generating subsets in which closed-loop dynamics are forward\ninvariant, thus guaranteeing perpetual operational safety. Additionally, we\npropose model predictive control designs that integrate these control invariant\nsets into mixed-integer optimization, with guaranteed adherence to safety\nconstraints and recursive feasibility at the computational level. We also\npresent a comprehensive theoretical analysis examining the properties and\nguarantees of the proposed methods. Numerical simulations in an autonomous\ndriving scenario demonstrate the methods' effectiveness in synthesizing\ncontrol-invariant sets offline and implementing model predictive control\nonline, ensuring safety and recursive feasibility."}
{"id": "2505.12631", "pdf": "https://arxiv.org/pdf/2505.12631", "abs": "https://arxiv.org/abs/2505.12631", "authors": ["Li Lin"], "title": "Multi-Resolution Haar Network: Enhancing human motion prediction via Haar transform", "categories": ["cs.CV"], "comment": null, "summary": "The 3D human pose is vital for modern computer vision and computer graphics,\nand its prediction has drawn attention in recent years. 3D human pose\nprediction aims at forecasting a human's future motion from the previous\nsequence. Ignoring that the arbitrariness of human motion sequences has a firm\norigin in transition in both temporal and spatial axes limits the performance\nof state-of-the-art methods, leading them to struggle with making precise\npredictions on complex cases, e.g., arbitrarily posing or greeting. To\nalleviate this problem, a network called HaarMoDic is proposed in this paper,\nwhich utilizes the 2D Haar transform to project joints to higher resolution\ncoordinates where the network can access spatial and temporal information\nsimultaneously. An ablation study proves that the significant contributing\nmodule within the HaarModic Network is the Multi-Resolution Haar (MR-Haar)\nblock. Instead of mining in one of two axes or extracting separately, the\nMR-Haar block projects whole motion sequences to a mixed-up coordinate in\nhigher resolution with 2D Haar Transform, allowing the network to give scope to\ninformation from both axes in different resolutions. With the MR-Haar block,\nthe HaarMoDic network can make predictions referring to a broader range of\ninformation. Experimental results demonstrate that HaarMoDic surpasses\nstate-of-the-art methods in every testing interval on the Human3.6M dataset in\nthe Mean Per Joint Position Error (MPJPE) metric."}
{"id": "2505.12636", "pdf": "https://arxiv.org/pdf/2505.12636", "abs": "https://arxiv.org/abs/2505.12636", "authors": ["Jiakuan Xie", "Pengfei Cao", "Yubo Chen", "Kang Liu", "Jun Zhao"], "title": "Revealing the Deceptiveness of Knowledge Editing: A Mechanistic Analysis of Superficial Editing", "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 main", "summary": "Knowledge editing, which aims to update the knowledge encoded in language\nmodels, can be deceptive. Despite the fact that many existing knowledge editing\nalgorithms achieve near-perfect performance on conventional metrics, the models\nedited by them are still prone to generating original knowledge. This paper\nintroduces the concept of \"superficial editing\" to describe this phenomenon.\nOur comprehensive evaluation reveals that this issue presents a significant\nchallenge to existing algorithms. Through systematic investigation, we identify\nand validate two key factors contributing to this issue: (1) the residual\nstream at the last subject position in earlier layers and (2) specific\nattention modules in later layers. Notably, certain attention heads in later\nlayers, along with specific left singular vectors in their output matrices,\nencapsulate the original knowledge and exhibit a causal relationship with\nsuperficial editing. Furthermore, we extend our analysis to the task of\nsuperficial unlearning, where we observe consistent patterns in the behavior of\nspecific attention heads and their corresponding left singular vectors, thereby\ndemonstrating the robustness and broader applicability of our methodology and\nconclusions. Our code is available here."}
{"id": "2505.11547", "pdf": "https://arxiv.org/pdf/2505.11547", "abs": "https://arxiv.org/abs/2505.11547", "authors": ["Kyla Guru", "Robert J. Moss", "Mykel J. Kochenderfer"], "title": "On Technique Identification and Threat-Actor Attribution using LLMs and Embedding Models", "categories": ["cs.CR", "cs.AI", "cs.CY"], "comment": null, "summary": "Attribution of cyber-attacks remains a complex but critical challenge for\ncyber defenders. Currently, manual extraction of behavioral indicators from\ndense forensic documentation causes significant attribution delays, especially\nfollowing major incidents at the international scale. This research evaluates\nlarge language models (LLMs) for cyber-attack attribution based on behavioral\nindicators extracted from forensic documentation. We test OpenAI's GPT-4 and\ntext-embedding-3-large for identifying threat actors' tactics, techniques, and\nprocedures (TTPs) by comparing LLM-generated TTPs against human-generated data\nfrom MITRE ATT&CK Groups. Our framework then identifies TTPs from text using\nvector embedding search and builds profiles to attribute new attacks for a\nmachine learning model to learn. Key contributions include: (1) assessing\noff-the-shelf LLMs for TTP extraction and attribution, and (2) developing an\nend-to-end pipeline from raw CTI documents to threat-actor prediction. This\nresearch finds that standard LLMs generate TTP datasets with noise, resulting\nin a low similarity to human-generated datasets. However, the TTPs generated\nare similar in frequency to those within the existing MITRE datasets.\nAdditionally, although these TTPs are different than human-generated datasets,\nour work demonstrates that they still prove useful for training a model that\nperforms above baseline on attribution. Project code and files are contained\nhere: https://github.com/kylag/ttp_attribution."}
{"id": "2505.12632", "pdf": "https://arxiv.org/pdf/2505.12632", "abs": "https://arxiv.org/abs/2505.12632", "authors": ["Yunseok Jang", "Yeda Song", "Sungryull Sohn", "Lajanugen Logeswaran", "Tiange Luo", "Dong-Ki Kim", "Kyunghoon Bae", "Honglak Lee"], "title": "Scalable Video-to-Dataset Generation for Cross-Platform Mobile Agents", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "CVPR 2025", "summary": "Recent advancements in Large Language Models (LLMs) and Vision-Language\nModels (VLMs) have sparked significant interest in developing GUI visual\nagents. We introduce MONDAY (Mobile OS Navigation Task Dataset for Agents from\nYouTube), a large-scale dataset of 313K annotated frames from 20K instructional\nvideos capturing diverse real-world mobile OS navigation across multiple\nplatforms. Models that include MONDAY in their pre-training phases demonstrate\nrobust cross-platform generalization capabilities, consistently outperforming\nmodels trained on existing single OS datasets while achieving an average\nperformance gain of 18.11%p on an unseen mobile OS platform. To enable\ncontinuous dataset expansion as mobile platforms evolve, we present an\nautomated framework that leverages publicly available video content to create\ncomprehensive task datasets without manual annotation. Our framework comprises\nrobust OCR-based scene detection (95.04% F1score), near-perfect UI element\ndetection (99.87% hit ratio), and novel multi-step action identification to\nextract reliable action sequences across diverse interface configurations. We\ncontribute both the MONDAY dataset and our automated collection framework to\nfacilitate future research in mobile OS navigation."}
{"id": "2505.12654", "pdf": "https://arxiv.org/pdf/2505.12654", "abs": "https://arxiv.org/abs/2505.12654", "authors": ["Yuxin Lin", "Yinglin Zheng", "Ming Zeng", "Wangzheng Shi"], "title": "Predicting Turn-Taking and Backchannel in Human-Machine Conversations Using Linguistic, Acoustic, and Visual Signals", "categories": ["cs.CL", "cs.AI"], "comment": "Accepected by ACL 2025", "summary": "This paper addresses the gap in predicting turn-taking and backchannel\nactions in human-machine conversations using multi-modal signals (linguistic,\nacoustic, and visual). To overcome the limitation of existing datasets, we\npropose an automatic data collection pipeline that allows us to collect and\nannotate over 210 hours of human conversation videos. From this, we construct a\nMulti-Modal Face-to-Face (MM-F2F) human conversation dataset, including over\n1.5M words and corresponding turn-taking and backchannel annotations from\napproximately 20M frames. Additionally, we present an end-to-end framework that\npredicts the probability of turn-taking and backchannel actions from\nmulti-modal signals. The proposed model emphasizes the interrelation between\nmodalities and supports any combination of text, audio, and video inputs,\nmaking it adaptable to a variety of realistic scenarios. Our experiments show\nthat our approach achieves state-of-the-art performance on turn-taking and\nbackchannel prediction tasks, achieving a 10\\% increase in F1-score on\nturn-taking and a 33\\% increase on backchannel prediction. Our dataset and code\nare publicly available online to ease of subsequent research."}
{"id": "2505.11548", "pdf": "https://arxiv.org/pdf/2505.11548", "abs": "https://arxiv.org/abs/2505.11548", "authors": ["Zhiyuan Chang", "Xiaojun Jia", "Mingyang Li", "Junjie Wang", "Yuekai Huang", "Qing Wang", "Ziyou Jiang", "Yang Liu"], "title": "One Shot Dominance: Knowledge Poisoning Attack on Retrieval-Augmented Generation Systems", "categories": ["cs.CR", "cs.AI"], "comment": "15pages, 4 figures", "summary": "Large Language Models (LLMs) enhanced with Retrieval-Augmented Generation\n(RAG) have shown improved performance in generating accurate responses.\nHowever, the dependence on external knowledge bases introduces potential\nsecurity vulnerabilities, particularly when these knowledge bases are publicly\naccessible and modifiable. Poisoning attacks on knowledge bases for RAG systems\nface two fundamental challenges: the injected malicious content must compete\nwith multiple authentic documents retrieved by the retriever, and LLMs tend to\ntrust retrieved information that aligns with their internal memorized\nknowledge. Previous works attempt to address these challenges by injecting\nmultiple malicious documents, but such saturation attacks are easily detectable\nand impractical in real-world scenarios. To enable the effective single\ndocument poisoning attack, we propose AuthChain, a novel knowledge poisoning\nattack method that leverages Chain-of-Evidence theory and authority effect to\ncraft more convincing poisoned documents. AuthChain generates poisoned content\nthat establishes strong evidence chains and incorporates authoritative\nstatements, effectively overcoming the interference from both authentic\ndocuments and LLMs' internal knowledge. Extensive experiments across six\npopular LLMs demonstrate that AuthChain achieves significantly higher attack\nsuccess rates while maintaining superior stealthiness against RAG defense\nmechanisms compared to state-of-the-art baselines."}
{"id": "2505.12635", "pdf": "https://arxiv.org/pdf/2505.12635", "abs": "https://arxiv.org/abs/2505.12635", "authors": ["Mingqi Shao", "Feng Xiong", "Zhaoxu Sun", "Mu Xu"], "title": "MVPainter: Accurate and Detailed 3D Texture Generation via Multi-View Diffusion with Geometric Control", "categories": ["cs.CV"], "comment": "Project page: https://amap-cvlab.github.io/MV-Painter", "summary": "Recently, significant advances have been made in 3D object generation.\nBuilding upon the generated geometry, current pipelines typically employ image\ndiffusion models to generate multi-view RGB images, followed by UV texture\nreconstruction through texture baking. While 3D geometry generation has\nimproved significantly, supported by multiple open-source frameworks, 3D\ntexture generation remains underexplored. In this work, we systematically\ninvestigate 3D texture generation through the lens of three core dimensions:\nreference-texture alignment, geometry-texture consistency, and local texture\nquality. To tackle these issues, we propose MVPainter, which employs data\nfiltering and augmentation strategies to enhance texture fidelity and detail,\nand introduces ControlNet-based geometric conditioning to improve\ntexture-geometry alignment. Furthermore, we extract physically-based rendering\n(PBR) attributes from the generated views to produce PBR meshes suitable for\nreal-world rendering applications. MVPainter achieves state-of-the-art results\nacross all three dimensions, as demonstrated by human-aligned evaluations. To\nfacilitate further research and reproducibility, we also release our full\npipeline as an open-source system, including data construction, model\narchitecture, and evaluation tools."}
{"id": "2505.12662", "pdf": "https://arxiv.org/pdf/2505.12662", "abs": "https://arxiv.org/abs/2505.12662", "authors": ["Xukai Liu", "Ye Liu", "Shiwen Wu", "Yanghai Zhang", "Yihao Yuan", "Kai Zhang", "Qi Liu"], "title": "Know3-RAG: A Knowledge-aware RAG Framework with Adaptive Retrieval, Generation, and Filtering", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have led to impressive\nprogress in natural language generation, yet their tendency to produce\nhallucinated or unsubstantiated content remains a critical concern. To improve\nfactual reliability, Retrieval-Augmented Generation (RAG) integrates external\nknowledge during inference. However, existing RAG systems face two major\nlimitations: (1) unreliable adaptive control due to limited external knowledge\nsupervision, and (2) hallucinations caused by inaccurate or irrelevant\nreferences. To address these issues, we propose Know3-RAG, a knowledge-aware\nRAG framework that leverages structured knowledge from knowledge graphs (KGs)\nto guide three core stages of the RAG process, including retrieval, generation,\nand filtering. Specifically, we introduce a knowledge-aware adaptive retrieval\nmodule that employs KG embedding to assess the confidence of the generated\nanswer and determine retrieval necessity, a knowledge-enhanced reference\ngeneration strategy that enriches queries with KG-derived entities to improve\ngenerated reference relevance, and a knowledge-driven reference filtering\nmechanism that ensures semantic alignment and factual accuracy of references.\nExperiments on multiple open-domain QA benchmarks demonstrate that Know3-RAG\nconsistently outperforms strong baselines, significantly reducing\nhallucinations and enhancing answer reliability."}
{"id": "2505.11550", "pdf": "https://arxiv.org/pdf/2505.11550", "abs": "https://arxiv.org/abs/2505.11550", "authors": ["Harika Abburi", "Sanmitra Bhattacharya", "Edward Bowen", "Nirmala Pudota"], "title": "AI-generated Text Detection: A Multifaceted Approach to Binary and Multiclass Classification", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ngenerating text that closely resembles human writing across a wide range of\nstyles and genres. However, such capabilities are prone to potential misuse,\nsuch as fake news generation, spam email creation, and misuse in academic\nassignments. As a result, accurate detection of AI-generated text and\nidentification of the model that generated it are crucial for maintaining the\nresponsible use of LLMs. In this work, we addressed two sub-tasks put forward\nby the Defactify workshop under AI-Generated Text Detection shared task at the\nAssociation for the Advancement of Artificial Intelligence (AAAI 2025): Task A\ninvolved distinguishing between human-authored or AI-generated text, while Task\nB focused on attributing text to its originating language model. For each task,\nwe proposed two neural architectures: an optimized model and a simpler variant.\nFor Task A, the optimized neural architecture achieved fifth place with $F1$\nscore of 0.994, and for Task B, the simpler neural architecture also ranked\nfifth place with $F1$ score of 0.627."}
{"id": "2505.12641", "pdf": "https://arxiv.org/pdf/2505.12641", "abs": "https://arxiv.org/abs/2505.12641", "authors": ["Yue Huang", "Zi'ang Li", "Tianle Hu", "Jie Wen", "Guanbin Li", "Jinglin Zhang", "Guoxu Zhou", "Xiaozhao Fang"], "title": "Single Image Reflection Removal via inter-layer Complementarity", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Although dual-stream architectures have achieved remarkable success in single\nimage reflection removal, they fail to fully exploit inter-layer\ncomplementarity in their physical modeling and network design, which limits the\nquality of image separation. To address this fundamental limitation, we propose\ntwo targeted improvements to enhance dual-stream architectures: First, we\nintroduce a novel inter-layer complementarity model where low-frequency\ncomponents extracted from the residual layer interact with the transmission\nlayer through dual-stream architecture to enhance inter-layer complementarity.\nMeanwhile, high-frequency components from the residual layer provide inverse\nmodulation to both streams, improving the detail quality of the transmission\nlayer. Second, we propose an efficient inter-layer complementarity attention\nmechanism which first cross-reorganizes dual streams at the channel level to\nobtain reorganized streams with inter-layer complementary structures, then\nperforms attention computation on the reorganized streams to achieve better\ninter-layer separation, and finally restores the original stream structure for\noutput. Experimental results demonstrate that our method achieves\nstate-of-the-art separation quality on multiple public datasets while\nsignificantly reducing both computational cost and model complexity."}
{"id": "2505.12716", "pdf": "https://arxiv.org/pdf/2505.12716", "abs": "https://arxiv.org/abs/2505.12716", "authors": ["Taiqiang Wu", "Runming Yang", "Jiayi Li", "Pengfei Hu", "Ngai Wong", "Yujiu Yang"], "title": "Shadow-FT: Tuning Instruct via Base", "categories": ["cs.CL", "cs.AI"], "comment": "Under review", "summary": "Large language models (LLMs) consistently benefit from further fine-tuning on\nvarious tasks. However, we observe that directly tuning the INSTRUCT (i.e.,\ninstruction tuned) models often leads to marginal improvements and even\nperformance degeneration. Notably, paired BASE models, the foundation for these\nINSTRUCT variants, contain highly similar weight values (i.e., less than 2% on\naverage for Llama 3.1 8B). Therefore, we propose a novel Shadow-FT framework to\ntune the INSTRUCT models by leveraging the corresponding BASE models. The key\ninsight is to fine-tune the BASE model, and then directly graft the learned\nweight updates to the INSTRUCT model. Our proposed Shadow-FT introduces no\nadditional parameters, is easy to implement, and significantly improves\nperformance. We conduct extensive experiments on tuning mainstream LLMs, such\nas Qwen 3 and Llama 3 series, and evaluate them across 19 benchmarks covering\ncoding, reasoning, and mathematical tasks. Experimental results demonstrate\nthat Shadow-FT consistently outperforms conventional full-parameter and\nparameter-efficient tuning approaches. Further analyses indicate that Shadow-FT\ncan be applied to multimodal large language models (MLLMs) and combined with\ndirect preference optimization (DPO). Codes and weights are available at\n\\href{https://github.com/wutaiqiang/Shadow-FT}{Github}."}
{"id": "2505.11552", "pdf": "https://arxiv.org/pdf/2505.11552", "abs": "https://arxiv.org/abs/2505.11552", "authors": ["Ahmad Bin Rabiah", "Julian McAuley"], "title": "GSPRec: Temporal-Aware Graph Spectral Filtering for Recommendation", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Graph-based recommendation systems are effective at modeling collaborative\npatterns but often suffer from two limitations: overreliance on low-pass\nfiltering, which suppresses user-specific signals, and omission of sequential\ndynamics in graph construction. We introduce GSPRec, a graph spectral model\nthat integrates temporal transitions through sequentially-informed graph\nconstruction and applies frequency-aware filtering in the spectral domain.\nGSPRec encodes item transitions via multi-hop diffusion to enable the use of\nsymmetric Laplacians for spectral processing. To capture user preferences, we\ndesign a dual-filtering mechanism: a Gaussian bandpass filter to extract\nmid-frequency, user-level patterns, and a low-pass filter to retain global\ntrends. Extensive experiments on four public datasets show that GSPRec\nconsistently outperforms baselines, with an average improvement of 6.77% in\nNDCG@10. Ablation studies show the complementary benefits of both sequential\ngraph augmentation and bandpass filtering."}
{"id": "2505.12644", "pdf": "https://arxiv.org/pdf/2505.12644", "abs": "https://arxiv.org/abs/2505.12644", "authors": ["Bo Yang", "Hengwei Zhang", "Jindong Wang", "Yuchen Ren", "Chenhao Lin", "Chao Shen", "Zhengyu Zhao"], "title": "Use as Many Surrogates as You Want: Selective Ensemble Attack to Unleash Transferability without Sacrificing Resource Efficiency", "categories": ["cs.CV"], "comment": null, "summary": "In surrogate ensemble attacks, using more surrogate models yields higher\ntransferability but lower resource efficiency. This practical trade-off between\ntransferability and efficiency has largely limited existing attacks despite\nmany pre-trained models are easily accessible online. In this paper, we argue\nthat such a trade-off is caused by an unnecessary common assumption, i.e., all\nmodels should be identical across iterations. By lifting this assumption, we\ncan use as many surrogates as we want to unleash transferability without\nsacrificing efficiency. Concretely, we propose Selective Ensemble Attack (SEA),\nwhich dynamically selects diverse models (from easily accessible pre-trained\nmodels) across iterations based on our new interpretation of decoupling\nwithin-iteration and cross-iteration model diversity.In this way, the number of\nwithin-iteration models is fixed for maintaining efficiency, while only\ncross-iteration model diversity is increased for higher transferability.\nExperiments on ImageNet demonstrate the superiority of SEA in various\nscenarios. For example, when dynamically selecting 4 from 20 accessible models,\nSEA yields 8.5% higher transferability than existing attacks under the same\nefficiency. The superiority of SEA also generalizes to real-world systems, such\nas commercial vision APIs and large vision-language models. Overall, SEA opens\nup the possibility of adaptively balancing transferability and efficiency\naccording to specific resource requirements."}
{"id": "2505.12717", "pdf": "https://arxiv.org/pdf/2505.12717", "abs": "https://arxiv.org/abs/2505.12717", "authors": ["Haoyuan Wu", "Xueyi Chen", "Rui Ming", "Jilong Gao", "Shoubo Hu", "Zhuolun He", "Bei Yu"], "title": "ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) demonstrate significant reasoning capabilities,\nparticularly through long chain-of-thought (CoT) processes, which can be\nelicited by reinforcement learning (RL). However, prolonged CoT reasoning\npresents limitations, primarily verbose outputs due to excessive introspection.\nThe reasoning process in these LLMs often appears to follow a trial-and-error\nmethodology rather than a systematic, logical deduction. In contrast,\ntree-of-thoughts (ToT) offers a conceptually more advanced approach by modeling\nreasoning as an exploration within a tree structure. This reasoning structure\nfacilitates the parallel generation and evaluation of multiple reasoning\nbranches, allowing for the active identification, assessment, and pruning of\nunproductive paths. This process can potentially lead to improved performance\nand reduced token costs. Building upon the long CoT capability of LLMs, we\nintroduce tree-of-thoughts RL (ToTRL), a novel on-policy RL framework with a\nrule-based reward. ToTRL is designed to guide LLMs in developing the parallel\nToT strategy based on the sequential CoT strategy. Furthermore, we employ LLMs\nas players in a puzzle game during the ToTRL training process. Solving puzzle\ngames inherently necessitates exploring interdependent choices and managing\nmultiple constraints, which requires the construction and exploration of a\nthought tree, providing challenging tasks for cultivating the ToT reasoning\ncapability. Our empirical evaluations demonstrate that our ToTQwen3-8B model,\ntrained with our ToTRL, achieves significant improvement in performance and\nreasoning efficiency on complex reasoning tasks."}
{"id": "2505.11556", "pdf": "https://arxiv.org/pdf/2505.11556", "abs": "https://arxiv.org/abs/2505.11556", "authors": ["Yuxuan Li", "Aoi Naito", "Hirokazu Shirado"], "title": "Assessing Collective Reasoning in Multi-Agent LLMs via Hidden Profile Tasks", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": null, "summary": "Multi-agent systems built on large language models (LLMs) promise enhanced\nproblem-solving through distributed information integration, but also risk\nreplicating collective reasoning failures observed in human groups. Yet, no\ntheory-grounded benchmark exists to systematically evaluate such failures. In\nthis paper, we introduce the Hidden Profile paradigm from social psychology as\na diagnostic testbed for multi-agent LLM systems. By distributing critical\ninformation asymmetrically across agents, the paradigm reveals how inter-agent\ndynamics support or hinder collective reasoning. We first formalize the\nparadigm for multi-agent decision-making under distributed knowledge and\ninstantiate it as a benchmark with nine tasks spanning diverse scenarios,\nincluding adaptations from prior human studies. We then conduct experiments\nwith GPT-4.1 and five other leading LLMs, including reasoning-enhanced\nvariants, showing that multi-agent systems across all models fail to match the\naccuracy of single agents given complete information. While agents' collective\nperformance is broadly comparable to that of human groups, nuanced behavioral\ndifferences emerge, such as increased sensitivity to social desirability.\nFinally, we demonstrate the paradigm's diagnostic utility by exploring a\ncooperation-contradiction trade-off in multi-agent LLM systems. We find that\nwhile cooperative agents are prone to over-coordination in collective settings,\nincreased contradiction impairs group convergence. This work contributes a\nreproducible framework for evaluating multi-agent LLM systems and motivates\nfuture research on artificial collective intelligence and human-AI interaction."}
{"id": "2505.12650", "pdf": "https://arxiv.org/pdf/2505.12650", "abs": "https://arxiv.org/abs/2505.12650", "authors": ["Yaotian Yang", "Yiwen Tang", "Yizhe Chen", "Xiao Chen", "Jiangjie Qiu", "Hao Xiong", "Haoyu Yin", "Zhiyao Luo", "Yifei Zhang", "Sijia Tao", "Wentao Li", "Qinghua Zhang", "Yuqiang Li", "Wanli Ouyang", "Bin Zhao", "Xiaonan Wang", "Fei Wei"], "title": "AutoMat: Enabling Automated Crystal Structure Reconstruction from Microscopy via Agentic Tool Use", "categories": ["cs.CV", "cs.AI"], "comment": "The code and dataset are publicly available at\n  https://github.com/yyt-2378/AutoMat and\n  https://huggingface.co/datasets/yaotianvector/STEM2Mat", "summary": "Machine learning-based interatomic potentials and force fields depend\ncritically on accurate atomic structures, yet such data are scarce due to the\nlimited availability of experimentally resolved crystals. Although\natomic-resolution electron microscopy offers a potential source of structural\ndata, converting these images into simulation-ready formats remains\nlabor-intensive and error-prone, creating a bottleneck for model training and\nvalidation. We introduce AutoMat, an end-to-end, agent-assisted pipeline that\nautomatically transforms scanning transmission electron microscopy (STEM)\nimages into atomic crystal structures and predicts their physical properties.\nAutoMat combines pattern-adaptive denoising, physics-guided template retrieval,\nsymmetry-aware atomic reconstruction, fast relaxation and property prediction\nvia MatterSim, and coordinated orchestration across all stages. We propose the\nfirst dedicated STEM2Mat-Bench for this task and evaluate performance using\nlattice RMSD, formation energy MAE, and structure-matching success rate. By\norchestrating external tool calls, AutoMat enables a text-only LLM to\noutperform vision-language models in this domain, achieving closed-loop\nreasoning throughout the pipeline. In large-scale experiments over 450\nstructure samples, AutoMat substantially outperforms existing multimodal large\nlanguage models and tools. These results validate both AutoMat and\nSTEM2Mat-Bench, marking a key step toward bridging microscopy and atomistic\nsimulation in materials science.The code and dataset are publicly available at\nhttps://github.com/yyt-2378/AutoMat and\nhttps://huggingface.co/datasets/yaotianvector/STEM2Mat."}
{"id": "2505.12718", "pdf": "https://arxiv.org/pdf/2505.12718", "abs": "https://arxiv.org/abs/2505.12718", "authors": ["Jingyang Peng", "Wenyuan Shen", "Jiarui Rao", "Jionghao Lin"], "title": "Automated Bias Assessment in AI-Generated Educational Content Using CEAT Framework", "categories": ["cs.CL", "cs.HC"], "comment": "Accepted by AIED 2025: Late-Breaking Results (LBR) Track", "summary": "Recent advances in Generative Artificial Intelligence (GenAI) have\ntransformed educational content creation, particularly in developing tutor\ntraining materials. However, biases embedded in AI-generated content--such as\ngender, racial, or national stereotypes--raise significant ethical and\neducational concerns. Despite the growing use of GenAI, systematic methods for\ndetecting and evaluating such biases in educational materials remain limited.\nThis study proposes an automated bias assessment approach that integrates the\nContextualized Embedding Association Test with a prompt-engineered word\nextraction method within a Retrieval-Augmented Generation framework. We applied\nthis method to AI-generated texts used in tutor training lessons. Results show\na high alignment between the automated and manually curated word sets, with a\nPearson correlation coefficient of r = 0.993, indicating reliable and\nconsistent bias assessment. Our method reduces human subjectivity and enhances\nfairness, scalability, and reproducibility in auditing GenAI-produced\neducational content."}
{"id": "2505.11557", "pdf": "https://arxiv.org/pdf/2505.11557", "abs": "https://arxiv.org/abs/2505.11557", "authors": ["Lara Magdalena Lazier", "Aritra Dhar", "Vasilije Stambolic", "Lukas Cavigelli"], "title": "AC-LoRA: (Almost) Training-Free Access Control-Aware Multi-Modal LLMs", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Corporate LLMs are gaining traction for efficient knowledge dissemination and\nmanagement within organizations. However, as current LLMs are vulnerable to\nleaking sensitive information, it has proven difficult to apply them in\nsettings where strict access control is necessary. To this end, we design\nAC-LoRA, an end-to-end system for access control-aware corporate LLM chatbots\nthat maintains a strong information isolation guarantee. AC-LoRA maintains\nseparate LoRA adapters for permissioned datasets, along with the document\nembedding they are finetuned on. AC-LoRA retrieves a precise set of LoRA\nadapters based on the similarity score with the user query and their\npermission. This similarity score is later used to merge the responses if more\nthan one LoRA is retrieved, without requiring any additional training for LoRA\nrouting. We provide an end-to-end prototype of AC-LoRA, evaluate it on two\ndatasets, and show that AC-LoRA matches or even exceeds the performance of\nstate-of-the-art LoRA mixing techniques while providing strong isolation\nguarantees. Furthermore, we show that AC-LoRA design can be directly applied to\ndifferent modalities."}
{"id": "2505.12656", "pdf": "https://arxiv.org/pdf/2505.12656", "abs": "https://arxiv.org/abs/2505.12656", "authors": ["Yongchang Gao", "Meiling Jin", "Zhaofei Yu", "Tiejun Huang", "Guozhang Chen"], "title": "SPKLIP: Aligning Spike Video Streams with Natural Language", "categories": ["cs.CV"], "comment": null, "summary": "Spike cameras offer unique sensing capabilities but their sparse,\nasynchronous output challenges semantic understanding, especially for Spike\nVideo-Language Alignment (Spike-VLA) where models like CLIP underperform due to\nmodality mismatch. We introduce SPKLIP, the first architecture specifically for\nSpike-VLA. SPKLIP employs a hierarchical spike feature extractor that\nadaptively models multi-scale temporal dynamics in event streams, and uses\nspike-text contrastive learning to directly align spike video with language,\nenabling effective few-shot learning. A full-spiking visual encoder variant,\nintegrating SNN components into our pipeline, demonstrates enhanced energy\nefficiency. Experiments show state-of-the-art performance on benchmark spike\ndatasets and strong few-shot generalization on a newly contributed real-world\ndataset. SPKLIP's energy efficiency highlights its potential for neuromorphic\ndeployment, advancing event-based multimodal research. The source code and\ndataset are available at [link removed for anonymity]."}
{"id": "2505.12723", "pdf": "https://arxiv.org/pdf/2505.12723", "abs": "https://arxiv.org/abs/2505.12723", "authors": ["Haoyuan Wu", "Rui Ming", "Jilong Gao", "Hangyu Zhao", "Xueyi Chen", "Yikai Yang", "Haisheng Zheng", "Zhuolun He", "Bei Yu"], "title": "On-Policy Optimization with Group Equivalent Preference for Multi-Programming Language Understanding", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) achieve remarkable performance in code\ngeneration tasks. However, a significant performance disparity persists between\npopular programming languages (e.g., Python, C++) and others. To address this\ncapability gap, we leverage the code translation task to train LLMs, thereby\nfacilitating the transfer of coding proficiency across diverse programming\nlanguages. Moreover, we introduce OORL for training, a novel reinforcement\nlearning (RL) framework that integrates on-policy and off-policy strategies.\nWithin OORL, on-policy RL is applied during code translation, guided by a\nrule-based reward signal derived from unit tests. Complementing this\ncoarse-grained rule-based reward, we propose Group Equivalent Preference\nOptimization (GEPO), a novel preference optimization method. Specifically, GEPO\ntrains the LLM using intermediate representations (IRs) groups. LLMs can be\nguided to discern IRs equivalent to the source code from inequivalent ones,\nwhile also utilizing signals about the mutual equivalence between IRs within\nthe group. This process allows LLMs to capture nuanced aspects of code\nfunctionality. By employing OORL for training with code translation tasks, LLMs\nimprove their recognition of code functionality and their understanding of the\nrelationships between code implemented in different languages. Extensive\nexperiments demonstrate that our OORL for LLMs training with code translation\ntasks achieves significant performance improvements on code benchmarks across\nmultiple programming languages."}
{"id": "2505.11559", "pdf": "https://arxiv.org/pdf/2505.11559", "abs": "https://arxiv.org/abs/2505.11559", "authors": ["Sushrit Kafle", "Shreejan Pandey"], "title": "Analysis and Resilience of the U.S. Flight Network", "categories": ["physics.soc-ph", "cs.AI", "cs.SI"], "comment": "Investigates resilience of the U.S. flight network under node\n  failures. Includes percolation threshold detection, cascade simulations, and\n  community structure analysis. 9 pages, 14 figures", "summary": "Air travel is one of the most widely used transportation services in the\nUnited States. This paper analyzes the U.S. Flight Network (USFN) using complex\nnetwork theory by exploring how the network's topology contributes to its\nefficiency and vulnerability. This is done by examining the structural\nproperties, degree distributions, and community structures in the network. USFN\nwas observed to follow power-law distribution and falls under the anomalous\nregime, suggesting that the network is hub dominant. Compared to null networks,\nUSFN has a higher clustering coefficient and modularity. Various percolation\ntest revealed that USFN is vulnerable to targeted attacks and is susceptible to\ncomplete cascading failure if one of the major hubs fails. The overall results\nsuggest that while the USFN is designed for efficiency, it is highly vulnerable\nto disruptions. Protecting key hub airports is important to make the network\nmore robust and prevent large-scale failures."}
{"id": "2505.12660", "pdf": "https://arxiv.org/pdf/2505.12660", "abs": "https://arxiv.org/abs/2505.12660", "authors": ["Ziqi Wen", "Jonathan Skaza", "Shravan Murlidaran", "William Y. Wang", "Miguel P. Eckstein"], "title": "Predicting Reaction Time to Comprehend Scenes with Foveated Scene Understanding Maps", "categories": ["cs.CV"], "comment": null, "summary": "Although models exist that predict human response times (RTs) in tasks such\nas target search and visual discrimination, the development of image-computable\npredictors for scene understanding time remains an open challenge. Recent\nadvances in vision-language models (VLMs), which can generate scene\ndescriptions for arbitrary images, combined with the availability of\nquantitative metrics for comparing linguistic descriptions, offer a new\nopportunity to model human scene understanding. We hypothesize that the primary\nbottleneck in human scene understanding and the driving source of variability\nin response times across scenes is the interaction between the foveated nature\nof the human visual system and the spatial distribution of task-relevant visual\ninformation within an image. Based on this assumption, we propose a novel\nimage-computable model that integrates foveated vision with VLMs to produce a\nspatially resolved map of scene understanding as a function of fixation\nlocation (Foveated Scene Understanding Map, or F-SUM), along with an aggregate\nF-SUM score. This metric correlates with average (N=17) human RTs (r=0.47) and\nnumber of saccades (r=0.51) required to comprehend a scene (across 277 scenes).\nThe F-SUM score also correlates with average (N=16) human description accuracy\n(r=-0.56) in time-limited presentations. These correlations significantly\nexceed those of standard image-based metrics such as clutter, visual\ncomplexity, and scene ambiguity based on language entropy. Together, our work\nintroduces a new image-computable metric for predicting human response times in\nscene understanding and demonstrates the importance of foveated visual\nprocessing in shaping comprehension difficulty."}
{"id": "2505.12727", "pdf": "https://arxiv.org/pdf/2505.12727", "abs": "https://arxiv.org/abs/2505.12727", "authors": ["Han Meng", "Yancan Chen", "Yunan Li", "Yitian Yang", "Jungup Lee", "Renwen Zhang", "Yi-Chieh Lee"], "title": "What is Stigma Attributed to? A Theory-Grounded, Expert-Annotated Interview Corpus for Demystifying Mental-Health Stigma", "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": "Accepted to ACL 2025 Main Conference, 35 Pages", "summary": "Mental-health stigma remains a pervasive social problem that hampers\ntreatment-seeking and recovery. Existing resources for training neural models\nto finely classify such stigma are limited, relying primarily on social-media\nor synthetic data without theoretical underpinnings. To remedy this gap, we\npresent an expert-annotated, theory-informed corpus of human-chatbot\ninterviews, comprising 4,141 snippets from 684 participants with documented\nsocio-cultural backgrounds. Our experiments benchmark state-of-the-art neural\nmodels and empirically unpack the challenges of stigma detection. This dataset\ncan facilitate research on computationally detecting, neutralizing, and\ncounteracting mental-health stigma."}
{"id": "2505.11563", "pdf": "https://arxiv.org/pdf/2505.11563", "abs": "https://arxiv.org/abs/2505.11563", "authors": ["Alexandre Chapin", "Bruno Machado", "Emmanuel Dellandrea", "Liming Chen"], "title": "Object-Centric Representations Improve Policy Generalization in Robot Manipulation", "categories": ["cs.RO", "cs.AI", "eess.IV"], "comment": null, "summary": "Visual representations are central to the learning and generalization\ncapabilities of robotic manipulation policies. While existing methods rely on\nglobal or dense features, such representations often entangle task-relevant and\nirrelevant scene information, limiting robustness under distribution shifts. In\nthis work, we investigate object-centric representations (OCR) as a structured\nalternative that segments visual input into a finished set of entities,\nintroducing inductive biases that align more naturally with manipulation tasks.\nWe benchmark a range of visual encoders-object-centric, global and dense\nmethods-across a suite of simulated and real-world manipulation tasks ranging\nfrom simple to complex, and evaluate their generalization under diverse visual\nconditions including changes in lighting, texture, and the presence of\ndistractors. Our findings reveal that OCR-based policies outperform dense and\nglobal representations in generalization settings, even without task-specific\npretraining. These insights suggest that OCR is a promising direction for\ndesigning visual systems that generalize effectively in dynamic, real-world\nrobotic environments."}
{"id": "2505.12667", "pdf": "https://arxiv.org/pdf/2505.12667", "abs": "https://arxiv.org/abs/2505.12667", "authors": ["Zihan Su", "Xuerui Qiu", "Hongbin Xu", "Tangyu Jiang", "Junhao Zhuang", "Chun Yuan", "Ming Li", "Shengfeng He", "Fei Richard Yu"], "title": "Safe-Sora: Safe Text-to-Video Generation via Graphical Watermarking", "categories": ["cs.CV"], "comment": null, "summary": "The explosive growth of generative video models has amplified the demand for\nreliable copyright preservation of AI-generated content. Despite its popularity\nin image synthesis, invisible generative watermarking remains largely\nunderexplored in video generation. To address this gap, we propose Safe-Sora,\nthe first framework to embed graphical watermarks directly into the video\ngeneration process. Motivated by the observation that watermarking performance\nis closely tied to the visual similarity between the watermark and cover\ncontent, we introduce a hierarchical coarse-to-fine adaptive matching\nmechanism. Specifically, the watermark image is divided into patches, each\nassigned to the most visually similar video frame, and further localized to the\noptimal spatial region for seamless embedding. To enable spatiotemporal fusion\nof watermark patches across video frames, we develop a 3D wavelet\ntransform-enhanced Mamba architecture with a novel spatiotemporal local\nscanning strategy, effectively modeling long-range dependencies during\nwatermark embedding and retrieval. To the best of our knowledge, this is the\nfirst attempt to apply state space models to watermarking, opening new avenues\nfor efficient and robust watermark protection. Extensive experiments\ndemonstrate that Safe-Sora achieves state-of-the-art performance in terms of\nvideo quality, watermark fidelity, and robustness, which is largely attributed\nto our proposals. We will release our code upon publication."}
{"id": "2505.12768", "pdf": "https://arxiv.org/pdf/2505.12768", "abs": "https://arxiv.org/abs/2505.12768", "authors": ["Yaxun Dai", "Wenxuan Xie", "Xialie Zhuang", "Tianyu Yang", "Yiying Yang", "Haiqin Yang", "Yuhang Zhao", "Pingfu Chao", "Wenhao Jiang"], "title": "ReEx-SQL: Reasoning with Execution-Aware Reinforcement Learning for Text-to-SQL", "categories": ["cs.CL"], "comment": null, "summary": "In Text-to-SQL, execution feedback is essential for guiding large language\nmodels (LLMs) to reason accurately and generate reliable SQL queries. However,\nexisting methods treat execution feedback solely as a post-hoc signal for\ncorrection or selection, failing to integrate it into the generation process.\nThis limitation hinders their ability to address reasoning errors as they\noccur, ultimately reducing query accuracy and robustness. To address this\nissue, we propose ReEx-SQL (Reasoning with Execution-Aware Reinforcement\nLearning), a framework for Text-to-SQL that enables models to interact with the\ndatabase during decoding and dynamically adjust their reasoning based on\nexecution feedback. ReEx-SQL introduces an execution-aware reasoning paradigm\nthat interleaves intermediate SQL execution into reasoning paths, facilitating\ncontext-sensitive revisions. It achieves this through structured prompts with\nmarkup tags and a stepwise rollout strategy that integrates execution feedback\ninto each stage of generation. To supervise policy learning, we develop a\ncomposite reward function that includes an exploration reward, explicitly\nencouraging effective database interaction. Additionally, ReEx-SQL adopts a\ntree-based decoding strategy to support exploratory reasoning, enabling dynamic\nexpansion of alternative reasoning paths. Notably, ReEx-SQL achieves 88.8% on\nSpider and 64.9% on BIRD at the 7B scale, surpassing the standard reasoning\nbaseline by 2.7% and 2.6%, respectively. It also shows robustness, achieving\n85.2% on Spider-Realistic with leading performance. In addition, its\ntree-structured decoding improves efficiency and performance over linear\ndecoding, reducing inference time by 51.9% on the BIRD development set."}
{"id": "2505.11565", "pdf": "https://arxiv.org/pdf/2505.11565", "abs": "https://arxiv.org/abs/2505.11565", "authors": ["Sarthak Munshi", "Swapnil Pathak", "Sonam Ghatode", "Thenuga Priyadarshini", "Dhivya Chandramouleeswaran", "Ashutosh Rana"], "title": "ACSE-Eval: Can LLMs threat model real-world cloud infrastructure?", "categories": ["cs.CR", "cs.AI"], "comment": "Submitted to the 39th Annual Conference on Neural Information\n  Processing Systems", "summary": "While Large Language Models have shown promise in cybersecurity applications,\ntheir effectiveness in identifying security threats within cloud deployments\nremains unexplored. This paper introduces AWS Cloud Security Engineering Eval,\na novel dataset for evaluating LLMs cloud security threat modeling\ncapabilities. ACSE-Eval contains 100 production grade AWS deployment scenarios,\neach featuring detailed architectural specifications, Infrastructure as Code\nimplementations, documented security vulnerabilities, and associated threat\nmodeling parameters. Our dataset enables systemic assessment of LLMs abilities\nto identify security risks, analyze attack vectors, and propose mitigation\nstrategies in cloud environments. Our evaluations on ACSE-Eval demonstrate that\nGPT 4.1 and Gemini 2.5 Pro excel at threat identification, with Gemini 2.5 Pro\nperforming optimally in 0-shot scenarios and GPT 4.1 showing superior results\nin few-shot settings. While GPT 4.1 maintains a slight overall performance\nadvantage, Claude 3.7 Sonnet generates the most semantically sophisticated\nthreat models but struggles with threat categorization and generalization. To\npromote reproducibility and advance research in automated cybersecurity threat\nanalysis, we open-source our dataset, evaluation metrics, and methodologies."}
{"id": "2505.12670", "pdf": "https://arxiv.org/pdf/2505.12670", "abs": "https://arxiv.org/abs/2505.12670", "authors": ["Lihong Chen", "Hossein Hassani", "Soodeh Nikan"], "title": "TS-VLM: Text-Guided SoftSort Pooling for Vision-Language Models in Multi-View Driving Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Vision-Language Models (VLMs) have shown remarkable potential in advancing\nautonomous driving by leveraging multi-modal fusion in order to enhance scene\nperception, reasoning, and decision-making. Despite their potential, existing\nmodels suffer from computational overhead and inefficient integration of\nmulti-view sensor data that make them impractical for real-time deployment in\nsafety-critical autonomous driving applications. To address these shortcomings,\nthis paper is devoted to designing a lightweight VLM called TS-VLM, which\nincorporates a novel Text-Guided SoftSort Pooling (TGSSP) module. By resorting\nto semantics of the input queries, TGSSP ranks and fuses visual features from\nmultiple views, enabling dynamic and query-aware multi-view aggregation without\nreliance on costly attention mechanisms. This design ensures the query-adaptive\nprioritization of semantically related views, which leads to improved\ncontextual accuracy in multi-view reasoning for autonomous driving. Extensive\nevaluations on the DriveLM benchmark demonstrate that, on the one hand, TS-VLM\noutperforms state-of-the-art models with a BLEU-4 score of 56.82, METEOR of\n41.91, ROUGE-L of 74.64, and CIDEr of 3.39. On the other hand, TS-VLM reduces\ncomputational cost by up to 90%, where the smallest version contains only 20.1\nmillion parameters, making it more practical for real-time deployment in\nautonomous vehicles."}
{"id": "2505.12781", "pdf": "https://arxiv.org/pdf/2505.12781", "abs": "https://arxiv.org/abs/2505.12781", "authors": ["Jitai Hao", "Qiang Huang", "Hao Liu", "Xinyan Xiao", "Zhaochun Ren", "Jun Yu"], "title": "A Token is Worth over 1,000 Tokens: Efficient Knowledge Distillation through Low-Rank Clone", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Training high-performing Small Language Models (SLMs) remains costly, even\nwith knowledge distillation and pruning from larger teacher models. Existing\nwork often faces three key challenges: (1) information loss from hard pruning,\n(2) inefficient alignment of representations, and (3) underutilization of\ninformative activations, particularly from Feed-Forward Networks (FFNs). To\naddress these challenges, we introduce Low-Rank Clone (LRC), an efficient\npre-training method that constructs SLMs aspiring to behavioral equivalence\nwith strong teacher models. LRC trains a set of low-rank projection matrices\nthat jointly enable soft pruning by compressing teacher weights, and activation\nclone by aligning student activations, including FFN signals, with those of the\nteacher. This unified design maximizes knowledge transfer while removing the\nneed for explicit alignment modules. Extensive experiments with open-source\nteachers (e.g., Llama-3.2-3B-Instruct, Qwen2.5-3B/7B-Instruct) show that LRC\nmatches or surpasses state-of-the-art models trained on trillions of\ntokens--while using only 20B tokens, achieving over 1,000x training efficiency.\nOur codes and model checkpoints are available at\nhttps://github.com/CURRENTF/LowRankClone and\nhttps://huggingface.co/collections/JitaiHao/low-rank-clone-lrc-6828389e96a93f1d4219dfaf."}
{"id": "2505.11567", "pdf": "https://arxiv.org/pdf/2505.11567", "abs": "https://arxiv.org/abs/2505.11567", "authors": ["Tianyi Shi", "Zhu Meng", "Yue Chen", "Siyang Zheng", "Fei Su", "Jin Huang", "Changrui Ren", "Zhicheng Zhao"], "title": "Beyond Time: Cross-Dimensional Frequency Supervision for Time Series Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Time series forecasting plays a crucial role in various fields, and the\nmethods based on frequency domain analysis have become an important branch.\nHowever, most existing studies focus on the design of elaborate model\narchitectures and are often tailored for limited datasets, still lacking\nuniversality. Besides, the assumption of independent and identically\ndistributed (IID) data also contradicts the strong correlation of the time\ndomain labels. To address these issues, abandoning time domain supervision, we\npropose a purely frequency domain supervision approach named cross-dimensional\nfrequency (X-Freq) loss. Specifically, based on a statistical phenomenon, we\nfirst prove that the information entropy of the time series is higher than its\nspectral entropy, which implies higher certainty in frequency domain and thus\ncan provide better supervision. Secondly, the Fourier Transform and the Wavelet\nTransform are applied to the time dimension and the channel dimension of the\ntime series respectively, to capture the long-term and short-term frequency\nvariations as well as the spatial configuration features. Thirdly, the loss\nbetween predictions and targets is uniformly computed in the frequency domain.\nMoreover, we plug-and-play incorporate X-Freq into multiple advanced\nforecasting models and compare on 14 real-world datasets. The experimental\nresults demonstrate that, without making any modification to the original\narchitectures or hyperparameters, X-Freq can improve the forecasting\nperformance by an average of 3.3% on long-term forecasting datasets and 27.7%\non short-term ones, showcasing superior generality and practicality. The code\nwill be released publicly."}
{"id": "2505.12674", "pdf": "https://arxiv.org/pdf/2505.12674", "abs": "https://arxiv.org/abs/2505.12674", "authors": ["Mingyuan Zhou", "Yi Gu", "Zhendong Wang"], "title": "Few-Step Diffusion via Score identity Distillation", "categories": ["cs.CV", "cs.LG", "stat.ML"], "comment": null, "summary": "Diffusion distillation has emerged as a promising strategy for accelerating\ntext-to-image (T2I) diffusion models by distilling a pretrained score network\ninto a one- or few-step generator. While existing methods have made notable\nprogress, they often rely on real or teacher-synthesized images to perform well\nwhen distilling high-resolution T2I diffusion models such as Stable Diffusion\nXL (SDXL), and their use of classifier-free guidance (CFG) introduces a\npersistent trade-off between text-image alignment and generation diversity. We\naddress these challenges by optimizing Score identity Distillation (SiD) -- a\ndata-free, one-step distillation framework -- for few-step generation. Backed\nby theoretical analysis that justifies matching a uniform mixture of outputs\nfrom all generation steps to the data distribution, our few-step distillation\nalgorithm avoids step-specific networks and integrates seamlessly into existing\npipelines, achieving state-of-the-art performance on SDXL at 1024x1024\nresolution. To mitigate the alignment-diversity trade-off when real text-image\npairs are available, we introduce a Diffusion GAN-based adversarial loss\napplied to the uniform mixture and propose two new guidance strategies:\nZero-CFG, which disables CFG in the teacher and removes text conditioning in\nthe fake score network, and Anti-CFG, which applies negative CFG in the fake\nscore network. This flexible setup improves diversity without sacrificing\nalignment. Comprehensive experiments on SD1.5 and SDXL demonstrate\nstate-of-the-art performance in both one-step and few-step generation settings,\nalong with robustness to the absence of real images. Our efficient PyTorch\nimplementation, along with the resulting one- and few-step distilled\ngenerators, will be released publicly as a separate branch at\nhttps://github.com/mingyuanzhou/SiD-LSG."}
{"id": "2505.12792", "pdf": "https://arxiv.org/pdf/2505.12792", "abs": "https://arxiv.org/abs/2505.12792", "authors": ["Wenhao Zhu", "Yuhang Xie", "Guojie Song", "Xin Zhang"], "title": "EAVIT: Efficient and Accurate Human Value Identification from Text data via LLMs", "categories": ["cs.CL"], "comment": null, "summary": "The rapid evolution of large language models (LLMs) has revolutionized\nvarious fields, including the identification and discovery of human values\nwithin text data. While traditional NLP models, such as BERT, have been\nemployed for this task, their ability to represent textual data is\nsignificantly outperformed by emerging LLMs like GPTs. However, the performance\nof online LLMs often degrades when handling long contexts required for value\nidentification, which also incurs substantial computational costs. To address\nthese challenges, we propose EAVIT, an efficient and accurate framework for\nhuman value identification that combines the strengths of both locally\nfine-tunable and online black-box LLMs. Our framework employs a value detector\n- a small, local language model - to generate initial value estimations. These\nestimations are then used to construct concise input prompts for online LLMs,\nenabling accurate final value identification. To train the value detector, we\nintroduce explanation-based training and data generation techniques\nspecifically tailored for value identification, alongside sampling strategies\nto optimize the brevity of LLM input prompts. Our approach effectively reduces\nthe number of input tokens by up to 1/6 compared to directly querying online\nLLMs, while consistently outperforming traditional NLP methods and other\nLLM-based strategies."}
{"id": "2505.11568", "pdf": "https://arxiv.org/pdf/2505.11568", "abs": "https://arxiv.org/abs/2505.11568", "authors": ["Stylianos Stasinos", "Martino Mensio", "Elena Lazovik", "Athanasios Trantas"], "title": "BioCube: A Multimodal Dataset for Biodiversity Research", "categories": ["q-bio.QM", "cs.AI", "cs.LG"], "comment": "submitted to BiDS'25, 5 pages, 1 figure", "summary": "Biodiversity research requires complete and detailed information to study\necosystem dynamics at different scales. Employing data-driven methods like\nMachine Learning is getting traction in ecology and more specific biodiversity,\noffering alternative modelling pathways. For these methods to deliver accurate\nresults there is the need for large, curated and multimodal datasets that offer\ngranular spatial and temporal resolutions. In this work, we introduce BioCube,\na multimodal, fine-grained global dataset for ecology and biodiversity\nresearch. BioCube incorporates species observations through images, audio\nrecordings and descriptions, environmental DNA, vegetation indices,\nagricultural, forest, land indicators, and high-resolution climate variables.\nAll observations are geospatially aligned under the WGS84 geodetic system,\nspanning from 2000 to 2020. The dataset will become available at\nhttps://huggingface.co/datasets/BioDT/BioCube while the acquisition and\nprocessing code base at https://github.com/BioDT/bfm-data."}
{"id": "2505.12677", "pdf": "https://arxiv.org/pdf/2505.12677", "abs": "https://arxiv.org/abs/2505.12677", "authors": ["Shristi Das Biswas", "Arani Roy", "Kaushik Roy"], "title": "CURE: Concept Unlearning via Orthogonal Representation Editing in Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "As Text-to-Image models continue to evolve, so does the risk of generating\nunsafe, copyrighted, or privacy-violating content. Existing safety\ninterventions - ranging from training data curation and model fine-tuning to\ninference-time filtering and guidance - often suffer from incomplete concept\nremoval, susceptibility to jail-breaking, computational inefficiency, or\ncollateral damage to unrelated capabilities. In this paper, we introduce CURE,\na training-free concept unlearning framework that operates directly in the\nweight space of pre-trained diffusion models, enabling fast, interpretable, and\nhighly specific suppression of undesired concepts. At the core of our method is\nthe Spectral Eraser, a closed-form, orthogonal projection module that\nidentifies discriminative subspaces using Singular Value Decomposition over\ntoken embeddings associated with the concepts to forget and retain.\nIntuitively, the Spectral Eraser identifies and isolates features unique to the\nundesired concept while preserving safe attributes. This operator is then\napplied in a single step update to yield an edited model in which the target\nconcept is effectively unlearned - without retraining, supervision, or\niterative optimization. To balance the trade-off between filtering toxicity and\npreserving unrelated concepts, we further introduce an Expansion Mechanism for\nspectral regularization which selectively modulates singular vectors based on\ntheir relative significance to control the strength of forgetting. All the\nprocesses above are in closed-form, guaranteeing extremely efficient erasure in\nonly $2$ seconds. Benchmarking against prior approaches, CURE achieves a more\nefficient and thorough removal for targeted artistic styles, objects,\nidentities, or explicit content, with minor damage to original generation\nability and demonstrates enhanced robustness against red-teaming."}
{"id": "2505.12808", "pdf": "https://arxiv.org/pdf/2505.12808", "abs": "https://arxiv.org/abs/2505.12808", "authors": ["Yanbin Yin", "Kun Zhou", "Zhen Wang", "Xiangdong Zhang", "Yifei Shao", "Shibo Hao", "Yi Gu", "Jieyuan Liu", "Somanshu Singla", "Tianyang Liu", "Eric P. Xing", "Zhengzhong Liu", "Haojian Jin", "Zhiting Hu"], "title": "Decentralized Arena: Towards Democratic and Scalable Automatic Evaluation of Language Models", "categories": ["cs.CL", "cs.LG"], "comment": "20 pages, ongoing work", "summary": "The recent explosion of large language models (LLMs), each with its own\ngeneral or specialized strengths, makes scalable, reliable benchmarking more\nurgent than ever. Standard practices nowadays face fundamental trade-offs:\nclosed-ended question-based benchmarks (eg MMLU) struggle with saturation as\nnewer models emerge, while crowd-sourced leaderboards (eg Chatbot Arena) rely\non costly and slow human judges. Recently, automated methods (eg\nLLM-as-a-judge) shed light on the scalability, but risk bias by relying on one\nor a few \"authority\" models. To tackle these issues, we propose Decentralized\nArena (dearena), a fully automated framework leveraging collective intelligence\nfrom all LLMs to evaluate each other. It mitigates single-model judge bias by\ndemocratic, pairwise evaluation, and remains efficient at scale through two key\ncomponents: (1) a coarse-to-fine ranking algorithm for fast incremental\ninsertion of new models with sub-quadratic complexity, and (2) an automatic\nquestion selection strategy for the construction of new evaluation dimensions.\nAcross extensive experiments across 66 LLMs, dearena attains up to 97%\ncorrelation with human judgements, while significantly reducing the cost. Our\ncode and data will be publicly released on\nhttps://github.com/maitrix-org/de-arena."}
{"id": "2505.11569", "pdf": "https://arxiv.org/pdf/2505.11569", "abs": "https://arxiv.org/abs/2505.11569", "authors": ["Pooja Mangal", "Sudaksh Kalra", "Dolly Sapra"], "title": "Towards Adaptive Deep Learning: Model Elasticity via Prune-and-Grow CNN Architectures", "categories": ["cs.LG", "cs.AI"], "comment": "50 Pages, 11 figures, Preprint", "summary": "Deploying deep convolutional neural networks (CNNs) on resource-constrained\ndevices presents significant challenges due to their high computational demands\nand rigid, static architectures. To overcome these limitations, this thesis\nexplores methods for enabling CNNs to dynamically adjust their computational\ncomplexity based on available hardware resources. We introduce adaptive CNN\narchitectures capable of scaling their capacity at runtime, thus efficiently\nbalancing performance and resource utilization. To achieve this adaptability,\nwe propose a structured pruning and dynamic re-construction approach that\ncreates nested subnetworks within a single CNN model. This approach allows the\nnetwork to dynamically switch between compact and full-sized configurations\nwithout retraining, making it suitable for deployment across varying hardware\nplatforms. Experiments conducted across multiple CNN architectures including\nVGG-16, AlexNet, ResNet-20, and ResNet-56 on CIFAR-10 and Imagenette datasets\ndemonstrate that adaptive models effectively maintain or even enhance\nperformance under varying computational constraints. Our results highlight that\nembedding adaptability directly into CNN architectures significantly improves\ntheir robustness and flexibility, paving the way for efficient real-world\ndeployment in diverse computational environments."}
{"id": "2505.12685", "pdf": "https://arxiv.org/pdf/2505.12685", "abs": "https://arxiv.org/abs/2505.12685", "authors": ["Fei Xie", "Jiahao Nie", "Yujin Tang", "Wenkang Zhang", "Hongshen Zhao"], "title": "Mamba-Adaptor: State Space Model Adaptor for Visual Recognition", "categories": ["cs.CV"], "comment": "CVPR paper", "summary": "Recent State Space Models (SSM), especially Mamba, have demonstrated\nimpressive performance in visual modeling and possess superior model\nefficiency. However, the application of Mamba to visual tasks suffers inferior\nperformance due to three main constraints existing in the sequential model: 1)\nCasual computing is incapable of accessing global context; 2) Long-range\nforgetting when computing the current hidden states; 3) Weak spatial structural\nmodeling due to the transformed sequential input. To address these issues, we\ninvestigate a simple yet powerful vision task Adaptor for Mamba models, which\nconsists of two functional modules: Adaptor-T and Adaptor-S. When solving the\nhidden states for SSM, we apply a lightweight prediction module Adaptor-T to\nselect a set of learnable locations as memory augmentations to ease long-range\nforgetting issues. Moreover, we leverage Adapator-S, composed of multi-scale\ndilated convolutional kernels, to enhance the spatial modeling and introduce\nthe image inductive bias into the feature output. Both modules can enlarge the\ncontext modeling in casual computing, as the output is enhanced by the\ninaccessible features. We explore three usages of Mamba-Adaptor: A general\nvisual backbone for various vision tasks; A booster module to raise the\nperformance of pretrained backbones; A highly efficient fine-tuning module that\nadapts the base model for transfer learning tasks. Extensive experiments verify\nthe effectiveness of Mamba-Adaptor in three settings. Notably, our\nMamba-Adaptor achieves state-of the-art performance on the ImageNet and COCO\nbenchmarks."}
{"id": "2505.12814", "pdf": "https://arxiv.org/pdf/2505.12814", "abs": "https://arxiv.org/abs/2505.12814", "authors": ["Xilong Cheng", "Yunxiao Qin", "Yuting Tan", "Zhengnan Li", "Ye Wang", "Hongjiang Xiao", "Yuan Zhang"], "title": "PsyMem: Fine-grained psychological alignment and Explicit Memory Control for Advanced Role-Playing LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Existing LLM-based role-playing methods often rely on superficial textual\ndescriptions or simplistic metrics, inadequately modeling both intrinsic and\nextrinsic character dimensions. Additionally, they typically simulate character\nmemory with implicit model knowledge or basic retrieval augment generation\nwithout explicit memory alignment, compromising memory consistency. The two\nissues weaken reliability of role-playing LLMs in several applications, such as\ntrustworthy social simulation. To address these limitations, we propose PsyMem,\na novel framework integrating fine-grained psychological attributes and\nexplicit memory control for role-playing. PsyMem supplements textual\ndescriptions with 26 psychological indicators to detailed model character.\nAdditionally, PsyMem implements memory alignment training, explicitly trains\nthe model to align character's response with memory, thereby enabling dynamic\nmemory-controlled responding during inference. By training Qwen2.5-7B-Instruct\non our specially designed dataset (including 5,414 characters and 38,962\ndialogues extracted from novels), the resulting model, termed as PsyMem-Qwen,\noutperforms baseline models in role-playing, achieving the best performance in\nhuman-likeness and character fidelity."}
{"id": "2505.11570", "pdf": "https://arxiv.org/pdf/2505.11570", "abs": "https://arxiv.org/abs/2505.11570", "authors": ["Chongyang Tan", "Ruoqi Wen", "Rongpeng Li", "Zhifeng Zhao", "Ekram Hossain", "Honggang Zhang"], "title": "Tool-Aided Evolutionary LLM for Generative Policy Toward Efficient Resource Management in Wireless Federated Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Federated Learning (FL) enables distributed model training across edge\ndevices in a privacy-friendly manner. However, its efficiency heavily depends\non effective device selection and high-dimensional resource allocation in\ndynamic and heterogeneous wireless environments. Conventional methods demand a\nconfluence of domain-specific expertise, extensive hyperparameter tuning,\nand/or heavy interaction cost. This paper proposes a Tool-aided Evolutionary\nLarge Language Model (T-ELLM) framework to generate a qualified policy for\ndevice selection in a wireless FL environment. Unlike conventional optimization\nmethods, T-ELLM leverages natural language-based scenario prompts to enhance\ngeneralization across varying network conditions. The framework decouples the\njoint optimization problem mathematically, enabling tractable learning of\ndevice selection policies while delegating resource allocation to convex\noptimization tools. To improve adaptability, T-ELLM integrates a\nsample-efficient, model-based virtual learning environment that captures the\nrelationship between device selection and learning performance, facilitating\nsubsequent group relative policy optimization. This concerted approach reduces\nreliance on real-world interactions, minimizing communication overhead while\nmaintaining high-fidelity decision-making. Theoretical analysis proves that the\ndiscrepancy between virtual and real environments is bounded, ensuring the\nadvantage function learned in the virtual environment maintains a provably\nsmall deviation from real-world conditions. Experimental results demonstrate\nthat T-ELLM outperforms benchmark methods in energy efficiency and exhibits\nrobust adaptability to environmental changes."}
{"id": "2505.12693", "pdf": "https://arxiv.org/pdf/2505.12693", "abs": "https://arxiv.org/abs/2505.12693", "authors": ["Luyao Lei", "Shuo Xu", "Yifan Bai", "Xing Wei"], "title": "TACOcc:Target-Adaptive Cross-Modal Fusion with Volume Rendering for 3D Semantic Occupancy", "categories": ["cs.CV"], "comment": null, "summary": "The performance of multi-modal 3D occupancy prediction is limited by\nineffective fusion, mainly due to geometry-semantics mismatch from fixed fusion\nstrategies and surface detail loss caused by sparse, noisy annotations. The\nmismatch stems from the heterogeneous scale and distribution of point cloud and\nimage features, leading to biased matching under fixed neighborhood fusion. To\naddress this, we propose a target-scale adaptive, bidirectional symmetric\nretrieval mechanism. It expands the neighborhood for large targets to enhance\ncontext awareness and shrinks it for small ones to improve efficiency and\nsuppress noise, enabling accurate cross-modal feature alignment. This mechanism\nexplicitly establishes spatial correspondences and improves fusion accuracy.\nFor surface detail loss, sparse labels provide limited supervision, resulting\nin poor predictions for small objects. We introduce an improved volume\nrendering pipeline based on 3D Gaussian Splatting, which takes fused features\nas input to render images, applies photometric consistency supervision, and\njointly optimizes 2D-3D consistency. This enhances surface detail\nreconstruction while suppressing noise propagation. In summary, we propose\nTACOcc, an adaptive multi-modal fusion framework for 3D semantic occupancy\nprediction, enhanced by volume rendering supervision. Experiments on the\nnuScenes and SemanticKITTI benchmarks validate its effectiveness."}
{"id": "2505.12821", "pdf": "https://arxiv.org/pdf/2505.12821", "abs": "https://arxiv.org/abs/2505.12821", "authors": ["Han Sun", "Zhen Sun", "Zongmin Zhang", "Linzhao Jia", "Wei Shao", "Min Zhang"], "title": "SynDec: A Synthesize-then-Decode Approach for Arbitrary Textual Style Transfer via Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are emerging as dominant forces for textual\nstyle transfer. However, for arbitrary style transfer, LLMs face two key\nchallenges: (1) considerable reliance on manually-constructed prompts and (2)\nrigid stylistic biases inherent in LLMs. In this paper, we propose a novel\nSynthesize-then-Decode (SynDec) approach, which automatically synthesizes\nhigh-quality prompts and amplifies their roles during decoding process.\nSpecifically, our approach synthesizes prompts by selecting representative\nfew-shot samples, conducting a four-dimensional style analysis, and reranking\nthe candidates. At LLM decoding stage, the TST effect is amplified by\nmaximizing the contrast in output probabilities between scenarios with and\nwithout the synthesized prompt, as well as between prompts and negative\nsamples. We conduct extensive experiments and the results show that SynDec\noutperforms existing state-of-the-art LLM-based methods on five out of six\nbenchmarks (e.g., achieving up to a 9\\% increase in accuracy for\nmodern-to-Elizabethan English transfer). Detailed ablation studies further\nvalidate the effectiveness of SynDec."}
{"id": "2505.11574", "pdf": "https://arxiv.org/pdf/2505.11574", "abs": "https://arxiv.org/abs/2505.11574", "authors": ["Zhen Li", "Yupeng Su", "Songmiao Wang", "Runming Yang", "Congkai Xie", "Aofan Liu", "Ming Li", "Jiannong Cao", "Yuan Xie", "Ngai Wong", "Hongxia Yang"], "title": "InfiJanice: Joint Analysis and In-situ Correction Engine for Quantization-Induced Math Degradation in Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": "23pages", "summary": "Large Language Models (LLMs) have demonstrated impressive performance on\ncomplex reasoning benchmarks such as GSM8K, MATH, and AIME. However, the\nsubstantial computational demands of these tasks pose significant challenges\nfor real-world deployment. Model quantization has emerged as a promising\napproach to reduce memory footprint and inference latency by representing\nweights and activations with lower bit-widths. In this work, we conduct a\ncomprehensive study of mainstream quantization methods(e.g., AWQ, GPTQ,\nSmoothQuant) on the most popular open-sourced models (e.g., Qwen2.5, LLaMA3\nseries), and reveal that quantization can degrade mathematical reasoning\naccuracy by up to 69.81%. To better understand this degradation, we develop an\nautomated assignment and judgment pipeline that qualitatively categorizes\nfailures into four error types and quantitatively identifies the most impacted\nreasoning capabilities. Building on these findings, we employ an automated\ndata-curation pipeline to construct a compact \"Silver Bullet\" datasets.\nTraining a quantized model on as few as 332 carefully selected examples for\njust 3-5 minutes on a single GPU is enough to restore its reasoning accuracy to\nmatch that of the full-precision baseline."}
{"id": "2505.12702", "pdf": "https://arxiv.org/pdf/2505.12702", "abs": "https://arxiv.org/abs/2505.12702", "authors": ["Tianming Liang", "Haichao Jiang", "Yuting Yang", "Chaolei Tan", "Shuai Li", "Wei-Shi Zheng", "Jian-Fang Hu"], "title": "Long-RVOS: A Comprehensive Benchmark for Long-term Referring Video Object Segmentation", "categories": ["cs.CV"], "comment": "Project Page: \\url{https://isee-laboratory.github.io/Long-RVOS}", "summary": "Referring video object segmentation (RVOS) aims to identify, track and\nsegment the objects in a video based on language descriptions, which has\nreceived great attention in recent years. However, existing datasets remain\nfocus on short video clips within several seconds, with salient objects visible\nin most frames. To advance the task towards more practical scenarios, we\nintroduce \\textbf{Long-RVOS}, a large-scale benchmark for long-term referring\nvideo object segmentation. Long-RVOS contains 2,000+ videos of an average\nduration exceeding 60 seconds, covering a variety of objects that undergo\nocclusion, disappearance-reappearance and shot changing. The objects are\nmanually annotated with three different types of descriptions to individually\nevaluate the understanding of static attributes, motion patterns and\nspatiotemporal relationships. Moreover, unlike previous benchmarks that rely\nsolely on the per-frame spatial evaluation, we introduce two new metrics to\nassess the temporal and spatiotemporal consistency. We benchmark 6\nstate-of-the-art methods on Long-RVOS. The results show that current approaches\nstruggle severely with the long-video challenges. To address this, we further\npropose ReferMo, a promising baseline method that integrates motion information\nto expand the temporal receptive field, and employs a local-to-global\narchitecture to capture both short-term dynamics and long-term dependencies.\nDespite simplicity, ReferMo achieves significant improvements over current\nmethods in long-term scenarios. We hope that Long-RVOS and our baseline can\ndrive future RVOS research towards tackling more realistic and long-form\nvideos."}
{"id": "2505.12831", "pdf": "https://arxiv.org/pdf/2505.12831", "abs": "https://arxiv.org/abs/2505.12831", "authors": ["Zifeng Cheng", "Zhonghui Wang", "Yuchen Fu", "Zhiwei Jiang", "Yafeng Yin", "Cong Wang", "Qing Gu"], "title": "Contrastive Prompting Enhances Sentence Embeddings in LLMs through Inference-Time Steering", "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Extracting sentence embeddings from large language models (LLMs) is a\npractical direction, as it requires neither additional data nor fine-tuning.\nPrevious studies usually focus on prompt engineering to guide LLMs to encode\nthe core semantic information of the sentence into the embedding of the last\ntoken. However, the last token in these methods still encodes an excess of\nnon-essential information, such as stop words, limiting its encoding capacity.\nTo this end, we propose a Contrastive Prompting (CP) method that introduces an\nextra auxiliary prompt to elicit better sentence embedding. By contrasting with\nthe auxiliary prompt, CP can steer existing prompts to encode the core\nsemantics of the sentence, rather than non-essential information. CP is a\nplug-and-play inference-time intervention method that can be combined with\nvarious prompt-based methods. Extensive experiments on Semantic Textual\nSimilarity (STS) tasks and downstream classification tasks demonstrate that our\nmethod can improve the performance of existing prompt-based methods across\ndifferent LLMs. Our code will be released at https://github.com/zifengcheng/CP."}
{"id": "2505.11576", "pdf": "https://arxiv.org/pdf/2505.11576", "abs": "https://arxiv.org/abs/2505.11576", "authors": ["Shuchen Wu", "Stephan Alaniz", "Shyamgopal Karthik", "Peter Dayan", "Eric Schulz", "Zeynep Akata"], "title": "Concept-Guided Interpretability via Neural Chunking", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "35 pages, 32 figures. arXiv admin note: text overlap with\n  arXiv:2502.01803", "summary": "Neural networks are often black boxes, reflecting the significant challenge\nof understanding their internal workings. We propose a different perspective\nthat challenges the prevailing view: rather than being inscrutable, neural\nnetworks exhibit patterns in their raw population activity that mirror\nregularities in the training data. We refer to this as the Reflection\nHypothesis and provide evidence for this phenomenon in both simple recurrent\nneural networks (RNNs) and complex large language models (LLMs). Building on\nthis insight, we propose to leverage cognitively-inspired methods of chunking\nto segment high-dimensional neural population dynamics into interpretable units\nthat reflect underlying concepts. We propose three methods to extract these\nemerging entities, complementing each other based on label availability and\ndimensionality. Discrete sequence chunking (DSC) creates a dictionary of\nentities; population averaging (PA) extracts recurring entities that correspond\nto known labels; and unsupervised chunk discovery (UCD) can be used when labels\nare absent. We demonstrate the effectiveness of these methods in extracting\nentities across varying model sizes, ranging from inducing compositionality in\nRNNs to uncovering recurring neural population states in large models with\ndiverse architectures, and illustrate their advantage over other methods.\nThroughout, we observe a robust correspondence between the extracted entities\nand concrete or abstract concepts. Artificially inducing the extracted entities\nin neural populations effectively alters the network's generation of associated\nconcepts. Our work points to a new direction for interpretability, one that\nharnesses both cognitive principles and the structure of naturalistic data to\nreveal the hidden computations of complex learning systems, gradually\ntransforming them from black boxes into systems we can begin to understand."}
{"id": "2505.12703", "pdf": "https://arxiv.org/pdf/2505.12703", "abs": "https://arxiv.org/abs/2505.12703", "authors": ["Jiabin Chen", "Haiping Wang", "Jinpeng Li", "Yuan Liu", "Zhen Dong", "Bisheng Yang"], "title": "SpatialLLM: From Multi-modality Data to Urban Spatial Intelligence", "categories": ["cs.CV"], "comment": null, "summary": "We propose SpatialLLM, a novel approach advancing spatial intelligence tasks\nin complex urban scenes. Unlike previous methods requiring geographic analysis\ntools or domain expertise, SpatialLLM is a unified language model directly\naddressing various spatial intelligence tasks without any training,\nfine-tuning, or expert intervention. The core of SpatialLLM lies in\nconstructing detailed and structured scene descriptions from raw spatial data\nto prompt pre-trained LLMs for scene-based analysis. Extensive experiments show\nthat, with our designs, pretrained LLMs can accurately perceive spatial\ndistribution information and enable zero-shot execution of advanced spatial\nintelligence tasks, including urban planning, ecological analysis, traffic\nmanagement, etc. We argue that multi-field knowledge, context length, and\nreasoning ability are key factors influencing LLM performances in urban\nanalysis. We hope that SpatialLLM will provide a novel viable perspective for\nurban intelligent analysis and management. The code and dataset are available\nat https://github.com/WHU-USI3DV/SpatialLLM."}
{"id": "2505.12835", "pdf": "https://arxiv.org/pdf/2505.12835", "abs": "https://arxiv.org/abs/2505.12835", "authors": ["Hengxing Cai", "Jinhan Dong", "Jingjun Tan", "Jingcheng Deng", "Sihang Li", "Zhifeng Gao", "Haidong Wang", "Zicheng Su", "Agachai Sumalee", "Renxin Zhong"], "title": "FlightGPT: Towards Generalizable and Interpretable UAV Vision-and-Language Navigation with Vision-Language Models", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Unmanned Aerial Vehicle (UAV) Vision-and-Language Navigation (VLN) is vital\nfor applications such as disaster response, logistics delivery, and urban\ninspection. However, existing methods often struggle with insufficient\nmultimodal fusion, weak generalization, and poor interpretability. To address\nthese challenges, we propose FlightGPT, a novel UAV VLN framework built upon\nVision-Language Models (VLMs) with powerful multimodal perception capabilities.\nWe design a two-stage training pipeline: first, Supervised Fine-Tuning (SFT)\nusing high-quality demonstrations to improve initialization and structured\nreasoning; then, Group Relative Policy Optimization (GRPO) algorithm, guided by\na composite reward that considers goal accuracy, reasoning quality, and format\ncompliance, to enhance generalization and adaptability. Furthermore, FlightGPT\nintroduces a Chain-of-Thought (CoT)-based reasoning mechanism to improve\ndecision interpretability. Extensive experiments on the city-scale dataset\nCityNav demonstrate that FlightGPT achieves state-of-the-art performance across\nall scenarios, with a 9.22\\% higher success rate than the strongest baseline in\nunseen environments. Our implementation is publicly available."}
{"id": "2505.11577", "pdf": "https://arxiv.org/pdf/2505.11577", "abs": "https://arxiv.org/abs/2505.11577", "authors": ["FLorian A. D. Burnat", "Brittany I. Davidson"], "title": "The Accountability Paradox: How Platform API Restrictions Undermine AI Transparency Mandates", "categories": ["cs.CY", "cs.AI", "I.2.0; E.0; K.4.1; K.4.2; K.4.3; K.5.0; K.5.2"], "comment": null, "summary": "Recent application programming interface (API) restrictions on major social\nmedia platforms challenge compliance with the EU Digital Services Act [20],\nwhich mandates data access for algorithmic transparency. We develop a\nstructured audit framework to assess the growing misalignment between\nregulatory requirements and platform implementations. Our comparative analysis\nof X/Twitter, Reddit, TikTok, and Meta identifies critical ``audit\nblind-spots'' where platform content moderation and algorithmic amplification\nremain inaccessible to independent verification. Our findings reveal an\n``accountability paradox'': as platforms increasingly rely on AI systems, they\nsimultaneously restrict the capacity for independent oversight. We propose\ntargeted policy interventions aligned with the AI Risk Management Framework of\nthe National Institute of Standards and Technology [80], emphasizing federated\naccess models and enhanced regulatory enforcement."}
{"id": "2505.12711", "pdf": "https://arxiv.org/pdf/2505.12711", "abs": "https://arxiv.org/abs/2505.12711", "authors": ["Qichen Sun", "Zhengrui Guo", "Rui Peng", "Hao Chen", "Jinzhuo Wang"], "title": "Any-to-Any Learning in Computational Pathology via Triplet Multimodal Pretraining", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in computational pathology and artificial intelligence have\nsignificantly enhanced the utilization of gigapixel whole-slide images and and\nadditional modalities (e.g., genomics) for pathological diagnosis. Although\ndeep learning has demonstrated strong potential in pathology, several key\nchallenges persist: (1) fusing heterogeneous data types requires sophisticated\nstrategies beyond simple concatenation due to high computational costs; (2)\ncommon scenarios of missing modalities necessitate flexible strategies that\nallow the model to learn robustly in the absence of certain modalities; (3) the\ndownstream tasks in CPath are diverse, ranging from unimodal to multimodal,\ncnecessitating a unified model capable of handling all modalities. To address\nthese challenges, we propose ALTER, an any-to-any tri-modal pretraining\nframework that integrates WSIs, genomics, and pathology reports. The term \"any\"\nemphasizes ALTER's modality-adaptive design, enabling flexible pretraining with\nany subset of modalities, and its capacity to learn robust, cross-modal\nrepresentations beyond WSI-centric approaches. We evaluate ALTER across\nextensive clinical tasks including survival prediction, cancer subtyping, gene\nmutation prediction, and report generation, achieving superior or comparable\nperformance to state-of-the-art baselines."}
{"id": "2505.12837", "pdf": "https://arxiv.org/pdf/2505.12837", "abs": "https://arxiv.org/abs/2505.12837", "authors": ["Christian Braun", "Alexander Lilienbeck", "Daniel Mentjukov"], "title": "The Hidden Structure -- Improving Legal Document Understanding Through Explicit Text Formatting", "categories": ["cs.CL", "cs.AI"], "comment": "20 pages, 3 figures", "summary": "Legal contracts possess an inherent, semantically vital structure (e.g.,\nsections, clauses) that is crucial for human comprehension but whose impact on\nLLM processing remains under-explored. This paper investigates the effects of\nexplicit input text structure and prompt engineering on the performance of\nGPT-4o and GPT-4.1 on a legal question-answering task using an excerpt of the\nCUAD. We compare model exact-match accuracy across various input formats:\nwell-structured plain-text (human-generated from CUAD), plain-text cleaned of\nline breaks, extracted plain-text from Azure OCR, plain-text extracted by\nGPT-4o Vision, and extracted (and interpreted) Markdown (MD) from GPT-4o\nVision. To give an indication of the impact of possible prompt engineering, we\nassess the impact of shifting task instructions to the system prompt and\nexplicitly informing the model about the structured nature of the input. Our\nfindings reveal that GPT-4o demonstrates considerable robustness to variations\nin input structure, but lacks in overall performance. Conversely, GPT-4.1's\nperformance is markedly sensitive; poorly structured inputs yield suboptimal\nresults (but identical with GPT-4o), while well-structured formats (original\nCUAD text, GPT-4o Vision text and GPT-4o MD) improve exact-match accuracy by\n~20 percentage points. Optimizing the system prompt to include task details and\nan advisory about structured input further elevates GPT-4.1's accuracy by an\nadditional ~10-13 percentage points, with Markdown ultimately achieving the\nhighest performance under these conditions (79 percentage points overall\nexact-match accuracy). This research empirically demonstrates that while newer\nmodels exhibit greater resilience, careful input structuring and strategic\nprompt design remain critical for optimizing the performance of LLMs, and can\nsignificantly affect outcomes in high-stakes legal applications."}
{"id": "2505.11578", "pdf": "https://arxiv.org/pdf/2505.11578", "abs": "https://arxiv.org/abs/2505.11578", "authors": ["Peimian Du", "Jiabin Liu", "Xiaowei Jin", "Mengwang Zuo", "Hui Li"], "title": "Spatiotemporal Field Generation Based on Hybrid Mamba-Transformer with Physics-informed Fine-tuning", "categories": ["cs.LG", "cs.AI", "physics.comp-ph"], "comment": null, "summary": "This research confronts the challenge of substantial physical equation\ndiscrepancies encountered in the generation of spatiotemporal physical fields\nthrough data-driven trained models. A spatiotemporal physical field generation\nmodel, named HMT-PF, is developed based on the hybrid Mamba-Transformer\narchitecture, incorporating unstructured grid information as input. A\nfine-tuning block, enhanced with physical information, is introduced to\neffectively reduce the physical equation discrepancies. The physical equation\nresiduals are computed through a point query mechanism for efficient gradient\nevaluation, then encoded into latent space for refinement. The fine-tuning\nprocess employs a self-supervised learning approach to achieve physical\nconsistency while maintaining essential field characteristics. Results show\nthat the hybrid Mamba-Transformer model achieves good performance in generating\nspatiotemporal fields, while the physics-informed fine-tuning mechanism further\nreduces significant physical errors effectively. A MSE-R evaluation method is\ndeveloped to assess the accuracy and realism of physical field generation."}
{"id": "2505.12714", "pdf": "https://arxiv.org/pdf/2505.12714", "abs": "https://arxiv.org/abs/2505.12714", "authors": ["Yinzhe Wang", "Yiwen Xiao", "Hu Wang", "Yiping Xu", "Yan Tian"], "title": "IA-MVS: Instance-Focused Adaptive Depth Sampling for Multi-View Stereo", "categories": ["cs.CV"], "comment": null, "summary": "Multi-view stereo (MVS) models based on progressive depth hypothesis\nnarrowing have made remarkable advancements. However, existing methods haven't\nfully utilized the potential that the depth coverage of individual instances is\nsmaller than that of the entire scene, which restricts further improvements in\ndepth estimation precision. Moreover, inevitable deviations in the initial\nstage accumulate as the process advances. In this paper, we propose\nInstance-Adaptive MVS (IA-MVS). It enhances the precision of depth estimation\nby narrowing the depth hypothesis range and conducting refinement on each\ninstance. Additionally, a filtering mechanism based on intra-instance depth\ncontinuity priors is incorporated to boost robustness. Furthermore, recognizing\nthat existing confidence estimation can degrade IA-MVS performance on point\nclouds. We have developed a detailed mathematical model for confidence\nestimation based on conditional probability. The proposed method can be widely\napplied in models based on MVSNet without imposing extra training burdens. Our\nmethod achieves state-of-the-art performance on the DTU benchmark. The source\ncode is available at https://github.com/KevinWang73106/IA-MVS."}
{"id": "2505.12859", "pdf": "https://arxiv.org/pdf/2505.12859", "abs": "https://arxiv.org/abs/2505.12859", "authors": ["Lucas Georges Gabriel Charpentier", "Pierre Lison"], "title": "Re-identification of De-identified Documents with Autoregressive Infilling", "categories": ["cs.CL"], "comment": "To be presented a ACL 2025, Main, Long paper", "summary": "Documents revealing sensitive information about individuals must typically be\nde-identified. This de-identification is often done by masking all mentions of\npersonally identifiable information (PII), thereby making it more difficult to\nuncover the identity of the person(s) in question. To investigate the\nrobustness of de-identification methods, we present a novel, RAG-inspired\napproach that attempts the reverse process of re-identification based on a\ndatabase of documents representing background knowledge. Given a text in which\npersonal identifiers have been masked, the re-identification proceeds in two\nsteps. A retriever first selects from the background knowledge passages deemed\nrelevant for the re-identification. Those passages are then provided to an\ninfilling model which seeks to infer the original content of each text span.\nThis process is repeated until all masked spans are replaced. We evaluate the\nre-identification on three datasets (Wikipedia biographies, court rulings and\nclinical notes). Results show that (1) as many as 80% of de-identified text\nspans can be successfully recovered and (2) the re-identification accuracy\nincreases along with the level of background knowledge."}
{"id": "2505.11579", "pdf": "https://arxiv.org/pdf/2505.11579", "abs": "https://arxiv.org/abs/2505.11579", "authors": ["Zeynep Engin", "David Hand"], "title": "Toward Adaptive Categories: Dimensional Governance for Agentic AI", "categories": ["cs.CY", "cs.AI", "cs.HC", "cs.LG", "cs.MA"], "comment": "12 pages core text, 14 pages including references, 2 figures", "summary": "As AI systems evolve from static tools to dynamic agents, traditional\ncategorical governance frameworks -- based on fixed risk tiers, levels of\nautonomy, or human oversight models -- are increasingly insufficient on their\nown. Systems built on foundation models, self-supervised learning, and\nmulti-agent architectures increasingly blur the boundaries that categories were\ndesigned to police. In this Perspective, we make the case for dimensional\ngovernance: a framework that tracks how decision authority, process autonomy,\nand accountability (the 3As) distribute dynamically across human-AI\nrelationships. A critical advantage of this approach is its ability to\nexplicitly monitor system movement toward and across key governance thresholds,\nenabling preemptive adjustments before risks materialize. This dimensional\napproach provides the necessary foundation for more adaptive categorization,\nenabling thresholds and classifications that can evolve with emerging\ncapabilities. While categories remain essential for decision-making, building\nthem upon dimensional foundations allows for context-specific adaptability and\nstakeholder-responsive governance that static approaches cannot achieve. We\noutline key dimensions, critical trust thresholds, and practical examples\nillustrating where rigid categorical frameworks fail -- and where a dimensional\nmindset could offer a more resilient and future-proof path forward for both\ngovernance and innovation at the frontier of artificial intelligence."}
{"id": "2505.12715", "pdf": "https://arxiv.org/pdf/2505.12715", "abs": "https://arxiv.org/abs/2505.12715", "authors": ["Aditya Taparia", "Noel Ngu", "Mario Leiva", "Joshua Shay Kricheli", "John Corcoran", "Nathaniel D. Bastian", "Gerardo Simari", "Paulo Shakarian", "Ransalu Senanayake"], "title": "VLC Fusion: Vision-Language Conditioned Sensor Fusion for Robust Object Detection", "categories": ["cs.CV"], "comment": "12 pages, 19 figures", "summary": "Although fusing multiple sensor modalities can enhance object detection\nperformance, existing fusion approaches often overlook subtle variations in\nenvironmental conditions and sensor inputs. As a result, they struggle to\nadaptively weight each modality under such variations. To address this\nchallenge, we introduce Vision-Language Conditioned Fusion (VLC Fusion), a\nnovel fusion framework that leverages a Vision-Language Model (VLM) to\ncondition the fusion process on nuanced environmental cues. By capturing\nhigh-level environmental context such as as darkness, rain, and camera\nblurring, the VLM guides the model to dynamically adjust modality weights based\non the current scene. We evaluate VLC Fusion on real-world autonomous driving\nand military target detection datasets that include image, LIDAR, and mid-wave\ninfrared modalities. Our experiments show that VLC Fusion consistently\noutperforms conventional fusion baselines, achieving improved detection\naccuracy in both seen and unseen scenarios."}
{"id": "2505.12864", "pdf": "https://arxiv.org/pdf/2505.12864", "abs": "https://arxiv.org/abs/2505.12864", "authors": ["Yu Fan", "Jingwei Ni", "Jakob Merane", "Etienne Salimbeni", "Yang Tian", "Yoan Hermstrüwer", "Yinya Huang", "Mubashara Akhtar", "Florian Geering", "Oliver Dreyer", "Daniel Brunner", "Markus Leippold", "Mrinmaya Sachan", "Alexander Stremitzer", "Christoph Engel", "Elliott Ash", "Joel Niklaus"], "title": "LEXam: Benchmarking Legal Reasoning on 340 Law Exams", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2"], "comment": null, "summary": "Long-form legal reasoning remains a key challenge for large language models\n(LLMs) in spite of recent advances in test-time scaling. We introduce LEXam, a\nnovel benchmark derived from 340 law exams spanning 116 law school courses\nacross a range of subjects and degree levels. The dataset comprises 4,886 law\nexam questions in English and German, including 2,841 long-form, open-ended\nquestions and 2,045 multiple-choice questions. Besides reference answers, the\nopen questions are also accompanied by explicit guidance outlining the expected\nlegal reasoning approach such as issue spotting, rule recall, or rule\napplication. Our evaluation on both open-ended and multiple-choice questions\npresent significant challenges for current LLMs; in particular, they notably\nstruggle with open questions that require structured, multi-step legal\nreasoning. Moreover, our results underscore the effectiveness of the dataset in\ndifferentiating between models with varying capabilities. Adopting an\nLLM-as-a-Judge paradigm with rigorous human expert validation, we demonstrate\nhow model-generated reasoning steps can be evaluated consistently and\naccurately. Our evaluation setup provides a scalable method to assess legal\nreasoning quality beyond simple accuracy metrics. Project page:\nhttps://lexam-benchmark.github.io/"}
{"id": "2505.11580", "pdf": "https://arxiv.org/pdf/2505.11580", "abs": "https://arxiv.org/abs/2505.11580", "authors": ["Andrew Liu", "Axel Elaldi", "Nicholas T Franklin", "Nathan Russell", "Gurinder S Atwal", "Yih-En A Ban", "Olivia Viessmann"], "title": "Flash Invariant Point Attention", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "comment": null, "summary": "Invariant Point Attention (IPA) is a key algorithm for geometry-aware\nmodeling in structural biology, central to many protein and RNA models.\nHowever, its quadratic complexity limits the input sequence length. We\nintroduce FlashIPA, a factorized reformulation of IPA that leverages\nhardware-efficient FlashAttention to achieve linear scaling in GPU memory and\nwall-clock time with sequence length. FlashIPA matches or exceeds standard IPA\nperformance while substantially reducing computational costs. FlashIPA extends\ntraining to previously unattainable lengths, and we demonstrate this by\nre-training generative models without length restrictions and generating\nstructures of thousands of residues. FlashIPA is available at\nhttps://github.com/flagshippioneering/flash_ipa."}
{"id": "2505.12728", "pdf": "https://arxiv.org/pdf/2505.12728", "abs": "https://arxiv.org/abs/2505.12728", "authors": ["Zihua Wang", "Ruibo Li", "Haozhe Du", "Joey Tianyi Zhou", "Yu Zhang", "Xu Yang"], "title": "FLASH: Latent-Aware Semi-Autoregressive Speculative Decoding for Multimodal Tasks", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Large language and multimodal models (LLMs and LMMs) exhibit strong inference\ncapabilities but are often limited by slow decoding speeds. This challenge is\nespecially acute in LMMs, where visual inputs typically comprise more tokens\nwith lower information density than text -- an issue exacerbated by recent\ntrends toward finer-grained visual tokenizations to boost performance.\nSpeculative decoding has been effective in accelerating LLM inference by using\na smaller draft model to generate candidate tokens, which are then selectively\nverified by the target model, improving speed without sacrificing output\nquality. While this strategy has been extended to LMMs, existing methods\nlargely overlook the unique properties of visual inputs and depend solely on\ntext-based draft models. In this work, we propose \\textbf{FLASH} (Fast\nLatent-Aware Semi-Autoregressive Heuristics), a speculative decoding framework\ndesigned specifically for LMMs, which leverages two key properties of\nmultimodal data to design the draft model. First, to address redundancy in\nvisual tokens, we propose a lightweight latent-aware token compression\nmechanism. Second, recognizing that visual objects often co-occur within a\nscene, we employ a semi-autoregressive decoding strategy to generate multiple\ntokens per forward pass. These innovations accelerate draft decoding while\nmaintaining high acceptance rates, resulting in faster overall inference.\nExperiments show that FLASH significantly outperforms prior speculative\ndecoding approaches in both unimodal and multimodal settings, achieving up to\n\\textbf{2.68$\\times$} speed-up on video captioning and \\textbf{2.55$\\times$} on\nvisual instruction tuning tasks compared to the original LMM."}
{"id": "2505.12888", "pdf": "https://arxiv.org/pdf/2505.12888", "abs": "https://arxiv.org/abs/2505.12888", "authors": ["Jialun Zhong", "Yanzeng Li", "Sen Hu", "Yang Zhang", "Teng Xu", "Lei Zou"], "title": "GAP: Graph-Assisted Prompts for Dialogue-based Medication Recommendation", "categories": ["cs.CL"], "comment": null, "summary": "Medication recommendations have become an important task in the healthcare\ndomain, especially in measuring the accuracy and safety of medical dialogue\nsystems (MDS). Different from the recommendation task based on electronic\nhealth records (EHRs), dialogue-based medication recommendations require\nresearch on the interaction details between patients and doctors, which is\ncrucial but may not exist in EHRs. Recent advancements in large language models\n(LLM) have extended the medical dialogue domain. These LLMs can interpret\npatients' intent and provide medical suggestions including medication\nrecommendations, but some challenges are still worth attention. During a\nmulti-turn dialogue, LLMs may ignore the fine-grained medical information or\nconnections across the dialogue turns, which is vital for providing accurate\nsuggestions. Besides, LLMs may generate non-factual responses when there is a\nlack of domain-specific knowledge, which is more risky in the medical domain.\nTo address these challenges, we propose a \\textbf{G}raph-\\textbf{A}ssisted\n\\textbf{P}rompts (\\textbf{GAP}) framework for dialogue-based medication\nrecommendation. It extracts medical concepts and corresponding states from\ndialogue to construct an explicitly patient-centric graph, which can describe\nthe neglected but important information. Further, combined with external\nmedical knowledge graphs, GAP can generate abundant queries and prompts, thus\nretrieving information from multiple sources to reduce the non-factual\nresponses. We evaluate GAP on a dialogue-based medication recommendation\ndataset and further explore its potential in a more difficult scenario,\ndynamically diagnostic interviewing. Extensive experiments demonstrate its\ncompetitive performance when compared with strong baselines."}
{"id": "2505.11582", "pdf": "https://arxiv.org/pdf/2505.11582", "abs": "https://arxiv.org/abs/2505.11582", "authors": ["Lee Harris", "Philippe De Wilde", "James Bentham"], "title": "Comparing Lexical and Semantic Vector Search Methods When Classifying Medical Documents", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Classification is a common AI problem, and vector search is a typical\nsolution. This transforms a given body of text into a numerical representation,\nknown as an embedding, and modern improvements to vector search focus on\noptimising speed and predictive accuracy. This is often achieved through neural\nmethods that aim to learn language semantics. However, our results suggest that\nthese are not always the best solution. Our task was to classify\nrigidly-structured medical documents according to their content, and we found\nthat using off-the-shelf semantic vector search produced slightly worse\npredictive accuracy than creating a bespoke lexical vector search model, and\nthat it required significantly more time to execute. These findings suggest\nthat traditional methods deserve to be contenders in the information retrieval\ntoolkit, despite the prevalence and success of neural models."}
{"id": "2505.12742", "pdf": "https://arxiv.org/pdf/2505.12742", "abs": "https://arxiv.org/abs/2505.12742", "authors": ["Jinhua Zhang", "Wei Long", "Minghao Han", "Weiyi You", "Shuhang Gu"], "title": "MVAR: Visual Autoregressive Modeling with Scale and Spatial Markovian Conditioning", "categories": ["cs.CV"], "comment": null, "summary": "Essential to visual generation is efficient modeling of visual data priors.\nConventional next-token prediction methods define the process as learning the\nconditional probability distribution of successive tokens. Recently, next-scale\nprediction methods redefine the process to learn the distribution over\nmulti-scale representations, significantly reducing generation latency.\nHowever, these methods condition each scale on all previous scales and require\neach token to consider all preceding tokens, exhibiting scale and spatial\nredundancy. To better model the distribution by mitigating redundancy, we\npropose Markovian Visual AutoRegressive modeling (MVAR), a novel autoregressive\nframework that introduces scale and spatial Markov assumptions to reduce the\ncomplexity of conditional probability modeling. Specifically, we introduce a\nscale-Markov trajectory that only takes as input the features of adjacent\npreceding scale for next-scale prediction, enabling the adoption of a parallel\ntraining strategy that significantly reduces GPU memory consumption.\nFurthermore, we propose spatial-Markov attention, which restricts the attention\nof each token to a localized neighborhood of size k at corresponding positions\non adjacent scales, rather than attending to every token across these scales,\nfor the pursuit of reduced modeling complexity. Building on these improvements,\nwe reduce the computational complexity of attention calculation from O(N^2) to\nO(Nk), enabling training with just eight NVIDIA RTX 4090 GPUs and eliminating\nthe need for KV cache during inference. Extensive experiments on ImageNet\ndemonstrate that MVAR achieves comparable or superior performance with both\nsmall model trained from scratch and large fine-tuned models, while reducing\nthe average GPU memory footprint by 3.0x."}
{"id": "2505.12896", "pdf": "https://arxiv.org/pdf/2505.12896", "abs": "https://arxiv.org/abs/2505.12896", "authors": ["Chenxi Liu", "Yongqiang Chen", "Tongliang Liu", "James Cheng", "Bo Han", "Kun Zhang"], "title": "On the Thinking-Language Modeling Gap in Large Language Models", "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": "Chenxi and Yongqiang contributed equally; project page:\n  https://causalcoat.github.io/lot.html", "summary": "System 2 reasoning is one of the defining characteristics of intelligence,\nwhich requires slow and logical thinking. Human conducts System 2 reasoning via\nthe language of thoughts that organizes the reasoning process as a causal\nsequence of mental language, or thoughts. Recently, it has been observed that\nSystem 2 reasoning can be elicited from Large Language Models (LLMs)\npre-trained on large-scale natural languages. However, in this work, we show\nthat there is a significant gap between the modeling of languages and thoughts.\nAs language is primarily a tool for humans to share knowledge and thinking,\nmodeling human language can easily absorb language biases into LLMs deviated\nfrom the chain of thoughts in minds. Furthermore, we show that the biases will\nmislead the eliciting of \"thoughts\" in LLMs to focus only on a biased part of\nthe premise. To this end, we propose a new prompt technique termed\nLanguage-of-Thoughts (LoT) to demonstrate and alleviate this gap. Instead of\ndirectly eliciting the chain of thoughts from partial information, LoT\ninstructs LLMs to adjust the order and token used for the expressions of all\nthe relevant information. We show that the simple strategy significantly\nreduces the language modeling biases in LLMs and improves the performance of\nLLMs across a variety of reasoning tasks."}
{"id": "2505.11586", "pdf": "https://arxiv.org/pdf/2505.11586", "abs": "https://arxiv.org/abs/2505.11586", "authors": ["Rui Zhang", "Yun Shen", "Hongwei Li", "Wenbo Jiang", "Hanxiao Chen", "Yuan Zhang", "Guowen Xu", "Yang Zhang"], "title": "The Ripple Effect: On Unforeseen Complications of Backdoor Attacks", "categories": ["cs.CR", "cs.AI"], "comment": "Accepted by ICML 2025", "summary": "Recent research highlights concerns about the trustworthiness of third-party\nPre-Trained Language Models (PTLMs) due to potential backdoor attacks. These\nbackdoored PTLMs, however, are effective only for specific pre-defined\ndownstream tasks. In reality, these PTLMs can be adapted to many other\nunrelated downstream tasks. Such adaptation may lead to unforeseen consequences\nin downstream model outputs, consequently raising user suspicion and\ncompromising attack stealthiness. We refer to this phenomenon as backdoor\ncomplications. In this paper, we undertake the first comprehensive\nquantification of backdoor complications. Through extensive experiments using 4\nprominent PTLMs and 16 text classification benchmark datasets, we demonstrate\nthe widespread presence of backdoor complications in downstream models\nfine-tuned from backdoored PTLMs. The output distribution of triggered samples\nsignificantly deviates from that of clean samples. Consequently, we propose a\nbackdoor complication reduction method leveraging multi-task learning to\nmitigate complications without prior knowledge of downstream tasks. The\nexperimental results demonstrate that our proposed method can effectively\nreduce complications while maintaining the efficacy and consistency of backdoor\nattacks. Our code is available at\nhttps://github.com/zhangrui4041/Backdoor_Complications."}
{"id": "2505.12753", "pdf": "https://arxiv.org/pdf/2505.12753", "abs": "https://arxiv.org/abs/2505.12753", "authors": ["Martha Teiko Teye", "Ori Maoz", "Matthias Rottmann"], "title": "LiDAR MOT-DETR: A LiDAR-based Two-Stage Transformer for 3D Multiple Object Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Multi-object tracking from LiDAR point clouds presents unique challenges due\nto the sparse and irregular nature of the data, compounded by the need for\ntemporal coherence across frames. Traditional tracking systems often rely on\nhand-crafted features and motion models, which can struggle to maintain\nconsistent object identities in crowded or fast-moving scenes. We present a\nlidar-based two-staged DETR inspired transformer; a smoother and tracker. The\nsmoother stage refines lidar object detections, from any off-the-shelf\ndetector, across a moving temporal window. The tracker stage uses a DETR-based\nattention block to maintain tracks across time by associating tracked objects\nwith the refined detections using the point cloud as context. The model is\ntrained on the datasets nuScenes and KITTI in both online and offline (forward\npeeking) modes demonstrating strong performance across metrics such as\nID-switch and multiple object tracking accuracy (MOTA). The numerical results\nindicate that the online mode outperforms the lidar-only baseline and SOTA\nmodels on the nuScenes dataset, with an aMOTA of 0.722 and an aMOTP of 0.475,\nwhile the offline mode provides an additional 3 pp aMOTP"}
{"id": "2505.12920", "pdf": "https://arxiv.org/pdf/2505.12920", "abs": "https://arxiv.org/abs/2505.12920", "authors": ["Paul Van Eecke", "Katrien Beuls"], "title": "PyFCG: Fluid Construction Grammar in Python", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": null, "summary": "We present PyFCG, an open source software library that ports Fluid\nConstruction Grammar (FCG) to the Python programming language. PyFCG enables\nits users to seamlessly integrate FCG functionality into Python programs, and\nto use FCG in combination with other libraries within Python's rich ecosystem.\nApart from a general description of the library, this paper provides three\nwalkthrough tutorials that demonstrate example usage of PyFCG in typical use\ncases of FCG: (i) formalising and testing construction grammar analyses, (ii)\nlearning usage-based construction grammars from corpora, and (iii) implementing\nagent-based experiments on emergent communication."}
{"id": "2505.11594", "pdf": "https://arxiv.org/pdf/2505.11594", "abs": "https://arxiv.org/abs/2505.11594", "authors": ["Jintao Zhang", "Jia Wei", "Pengle Zhang", "Xiaoming Xu", "Haofeng Huang", "Haoxu Wang", "Kai Jiang", "Jun Zhu", "Jianfei Chen"], "title": "SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training", "categories": ["cs.LG", "cs.AI", "cs.AR", "cs.CV", "cs.PF"], "comment": null, "summary": "The efficiency of attention is important due to its quadratic time\ncomplexity. We enhance the efficiency of attention through two key\ncontributions: First, we leverage the new FP4 Tensor Cores in Blackwell GPUs to\naccelerate attention computation. Our implementation achieves 1038 TOPS on\nRTX5090, which is a 5x speedup over the fastest FlashAttention on RTX5090.\nExperiments show that our FP4 attention can accelerate inference of various\nmodels in a plug-and-play way. Second, we pioneer low-bit attention to training\ntasks. Existing low-bit attention works like FlashAttention3 and SageAttention\nfocus only on inference. However, the efficiency of training large models is\nalso important. To explore whether low-bit attention can be effectively applied\nto training tasks, we design an accurate and efficient 8-bit attention for both\nforward and backward propagation. Experiments indicate that 8-bit attention\nachieves lossless performance in fine-tuning tasks but exhibits slower\nconvergence in pretraining tasks. The code will be available at\nhttps://github.com/thu-ml/SageAttention."}
{"id": "2505.12758", "pdf": "https://arxiv.org/pdf/2505.12758", "abs": "https://arxiv.org/abs/2505.12758", "authors": ["Matias Quintana", "Youlong Gu", "Xiucheng Liang", "Yujun Hou", "Koichi Ito", "Yihan Zhu", "Mahmoud Abdelrahman", "Filip Biljecki"], "title": "It's not you, it's me -- Global urban visual perception varies across demographics and personalities", "categories": ["cs.CV", "cs.LG"], "comment": "Under review", "summary": "Understanding people's preferences and needs is crucial for urban planning\ndecisions, yet current approaches often combine them from multi-cultural and\nmulti-city populations, obscuring important demographic differences and risking\namplifying biases. We conducted a large-scale urban visual perception survey of\nstreetscapes worldwide using street view imagery, examining how demographics --\nincluding gender, age, income, education, race and ethnicity, and, for the\nfirst time, personality traits -- shape perceptions among 1,000 participants,\nwith balanced demographics, from five countries and 45 nationalities. This\ndataset, introduced as Street Perception Evaluation Considering Socioeconomics\n(SPECS), exhibits statistically significant differences in perception scores in\nsix traditionally used indicators (safe, lively, wealthy, beautiful, boring,\nand depressing) and four new ones we propose (live nearby, walk, cycle, green)\namong demographics and personalities. We revealed that location-based\nsentiments are carried over in people's preferences when comparing urban\nstreetscapes with other cities. Further, we compared the perception scores\nbased on where participants and streetscapes are from. We found that an\noff-the-shelf machine learning model trained on an existing global perception\ndataset tends to overestimate positive indicators and underestimate negative\nones compared to human responses, suggesting that targeted intervention should\nconsider locals' perception. Our study aspires to rectify the myopic treatment\nof street perception, which rarely considers demographics or personality\ntraits."}
{"id": "2505.12929", "pdf": "https://arxiv.org/pdf/2505.12929", "abs": "https://arxiv.org/abs/2505.12929", "authors": ["Zhihe Yang", "Xufang Luo", "Zilong Wang", "Dongqi Han", "Zhiyuan He", "Dongsheng Li", "Yunjian Xu"], "title": "Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "24 pages, 12 figures", "summary": "Reinforcement learning (RL) has become a cornerstone for enhancing the\nreasoning capabilities of large language models (LLMs), with recent innovations\nsuch as Group Relative Policy Optimization (GRPO) demonstrating exceptional\neffectiveness. In this study, we identify a critical yet underexplored issue in\nRL training: low-probability tokens disproportionately influence model updates\ndue to their large gradient magnitudes. This dominance hinders the effective\nlearning of high-probability tokens, whose gradients are essential for LLMs'\nperformance but are substantially suppressed. To mitigate this interference, we\npropose two novel methods: Advantage Reweighting and Low-Probability Token\nIsolation (Lopti), both of which effectively attenuate gradients from\nlow-probability tokens while emphasizing parameter updates driven by\nhigh-probability tokens. Our approaches promote balanced updates across tokens\nwith varying probabilities, thereby enhancing the efficiency of RL training.\nExperimental results demonstrate that they substantially improve the\nperformance of GRPO-trained LLMs, achieving up to a 46.2% improvement in K&K\nLogic Puzzle reasoning tasks. Our implementation is available at\nhttps://github.com/zhyang2226/AR-Lopti."}
{"id": "2505.11595", "pdf": "https://arxiv.org/pdf/2505.11595", "abs": "https://arxiv.org/abs/2505.11595", "authors": ["Peter Chen", "Xiaopeng Li", "Ziniu Li", "Xi Chen", "Tianyi Lin"], "title": "Spectral Policy Optimization: Coloring your Incorrect Reasoning in GRPO", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "28 pages", "summary": "Reinforcement learning (RL) has demonstrated significant success in enhancing\nreasoning capabilities in large language models (LLMs). One of the most widely\nused RL methods is Group Relative Policy Optimization\n(GRPO)~\\cite{Shao-2024-Deepseekmath}, known for its memory efficiency and\nsuccess in training DeepSeek-R1~\\cite{Guo-2025-Deepseek}. However, GRPO stalls\nwhen all sampled responses in a group are incorrect -- referred to as an\n\\emph{all-negative-sample} group -- as it fails to update the policy, hindering\nlearning progress. The contributions of this paper are two-fold. First, we\npropose a simple yet effective framework that introduces response diversity\nwithin all-negative-sample groups in GRPO using AI feedback. We also provide a\ntheoretical analysis, via a stylized model, showing how this diversification\nimproves learning dynamics. Second, we empirically validate our approach,\nshowing the improved performance across various model sizes (7B, 14B, 32B) in\nboth offline and online learning settings with 10 benchmarks, including base\nand distilled variants. Our findings highlight that learning from\nall-negative-sample groups is not only feasible but beneficial, advancing\nrecent insights from \\citet{Xiong-2025-Minimalist}."}
{"id": "2505.12766", "pdf": "https://arxiv.org/pdf/2505.12766", "abs": "https://arxiv.org/abs/2505.12766", "authors": ["Haibin He", "Maoyuan Ye", "Jing Zhang", "Xiantao Cai", "Juhua Liu", "Bo Du", "Dacheng Tao"], "title": "Reasoning-OCR: Can Large Multimodal Models Solve Complex Logical Reasoning Problems from OCR Cues?", "categories": ["cs.CV"], "comment": null, "summary": "Large Multimodal Models (LMMs) have become increasingly versatile,\naccompanied by impressive Optical Character Recognition (OCR) related\ncapabilities. Existing OCR-related benchmarks emphasize evaluating LMMs'\nabilities of relatively simple visual question answering, visual-text parsing,\netc. However, the extent to which LMMs can deal with complex logical reasoning\nproblems based on OCR cues is relatively unexplored. To this end, we introduce\nthe Reasoning-OCR benchmark, which challenges LMMs to solve complex reasoning\nproblems based on the cues that can be extracted from rich visual-text.\nReasoning-OCR covers six visual scenarios and encompasses 150 meticulously\ndesigned questions categorized into six reasoning challenges. Additionally,\nReasoning-OCR minimizes the impact of field-specialized knowledge. Our\nevaluation offers some insights for proprietary and open-source LMMs in\ndifferent reasoning challenges, underscoring the urgent to improve the\nreasoning performance. We hope Reasoning-OCR can inspire and facilitate future\nresearch on enhancing complex reasoning ability based on OCR cues.\nReasoning-OCR is publicly available at\nhttps://github.com/Hxyz-123/ReasoningOCR."}
{"id": "2505.12942", "pdf": "https://arxiv.org/pdf/2505.12942", "abs": "https://arxiv.org/abs/2505.12942", "authors": ["Jeffrey T. H. Wong", "Cheng Zhang", "Xinye Cao", "Pedro Gimenes", "George A. Constantinides", "Wayne Luk", "Yiren Zhao"], "title": "A3 : an Analytical Low-Rank Approximation Framework for Attention", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance."}
{"id": "2505.11601", "pdf": "https://arxiv.org/pdf/2505.11601", "abs": "https://arxiv.org/abs/2505.11601", "authors": ["Rui Liu", "Rui Xie", "Zijun Yao", "Yanjie Fu", "Dongjie Wang"], "title": "Continuous Optimization for Feature Selection with Permutation-Invariant Embedding and Policy-Guided Search", "categories": ["cs.LG", "cs.AI"], "comment": "KDD 2025", "summary": "Feature selection removes redundant features to enhanc performance and\ncomputational efficiency in downstream tasks. Existing works often struggle to\ncapture complex feature interactions and adapt to diverse scenarios. Recent\nadvances in this domain have incorporated generative intelligence to address\nthese drawbacks by uncovering intricate relationships between features.\nHowever, two key limitations remain: 1) embedding feature subsets in a\ncontinuous space is challenging due to permutation sensitivity, as changes in\nfeature order can introduce biases and weaken the embedding learning process;\n2) gradient-based search in the embedding space assumes convexity, which is\nrarely guaranteed, leading to reduced search effectiveness and suboptimal\nsubsets. To address these limitations, we propose a new framework that can: 1)\npreserve feature subset knowledge in a continuous embedding space while\nensuring permutation invariance; 2) effectively explore the embedding space\nwithout relying on strong convex assumptions. For the first objective, we\ndevelop an encoder-decoder paradigm to preserve feature selection knowledge\ninto a continuous embedding space. This paradigm captures feature interactions\nthrough pairwise relationships within the subset, removing the influence of\nfeature order on the embedding. Moreover, an inducing point mechanism is\nintroduced to accelerate pairwise relationship computations. For the second\nobjective, we employ a policy-based reinforcement learning (RL) approach to\nguide the exploration of the embedding space. The RL agent effectively\nnavigates the space by balancing multiple objectives. By prioritizing\nhigh-potential regions adaptively and eliminating the reliance on convexity\nassumptions, the RL agent effectively reduces the risk of converging to local\noptima. Extensive experiments demonstrate the effectiveness, efficiency,\nrobustness and explicitness of our model."}
{"id": "2505.12772", "pdf": "https://arxiv.org/pdf/2505.12772", "abs": "https://arxiv.org/abs/2505.12772", "authors": ["Junyi Hu", "Tian Bai", "Fengyi Wu", "Zhengming Peng", "Yi Zhang"], "title": "Pyramid Sparse Transformer: Enhancing Multi-Scale Feature Fusion with Dynamic Token Selection", "categories": ["cs.CV"], "comment": "13 pages, 5 figures", "summary": "Feature fusion is critical for high-performance vision models but often\nincurs prohibitive complexity. However, prevailing attention-based fusion\nmethods often involve significant computational complexity and implementation\nchallenges, limiting their efficiency in resource-constrained environments. To\naddress these issues, we introduce the Pyramid Sparse Transformer (PST), a\nlightweight, plug-and-play module that integrates coarse-to-fine token\nselection and shared attention parameters to reduce computation while\npreserving spatial detail. PST can be trained using only coarse attention and\nseamlessly activated at inference for further accuracy gains without\nretraining. When added to state-of-the-art real-time detection models, such as\nYOLOv11-N/S/M, PST yields mAP improvements of 0.9%, 0.5%, and 0.4% on MS COCO\nwith minimal latency impact. Likewise, embedding PST into ResNet-18/50/101 as\nbackbones, boosts ImageNet top-1 accuracy by 6.5%, 1.7%, and 1.0%,\nrespectively. These results demonstrate PST's effectiveness as a simple,\nhardware-friendly enhancement for both detection and classification tasks."}
{"id": "2505.12949", "pdf": "https://arxiv.org/pdf/2505.12949", "abs": "https://arxiv.org/abs/2505.12949", "authors": ["Cael Marquard", "Simbarashe Mawere", "Francois Meyer"], "title": "Neural Morphological Tagging for Nguni Languages", "categories": ["cs.CL"], "comment": null, "summary": "Morphological parsing is the task of decomposing words into morphemes, the\nsmallest units of meaning in a language, and labelling their grammatical roles.\nIt is a particularly challenging task for agglutinative languages, such as the\nNguni languages of South Africa, which construct words by concatenating\nmultiple morphemes. A morphological parsing system can be framed as a pipeline\nwith two separate components, a segmenter followed by a tagger. This paper\ninvestigates the use of neural methods to build morphological taggers for the\nfour Nguni languages. We compare two classes of approaches: training neural\nsequence labellers (LSTMs and neural CRFs) from scratch and finetuning\npretrained language models. We compare performance across these two categories,\nas well as to a traditional rule-based morphological parser. Neural taggers\ncomfortably outperform the rule-based baseline and models trained from scratch\ntend to outperform pretrained models. We also compare parsing results across\ndifferent upstream segmenters and with varying linguistic input features. Our\nfindings confirm the viability of employing neural taggers based on\npre-existing morphological segmenters for the Nguni languages."}
{"id": "2505.11615", "pdf": "https://arxiv.org/pdf/2505.11615", "abs": "https://arxiv.org/abs/2505.11615", "authors": ["Jian-Qiao Zhu", "Haijiang Yan", "Thomas L. Griffiths"], "title": "Steering Risk Preferences in Large Language Models by Aligning Behavioral and Neural Representations", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Changing the behavior of large language models (LLMs) can be as\nstraightforward as editing the Transformer's residual streams using\nappropriately constructed \"steering vectors.\" These modifications to internal\nneural activations, a form of representation engineering, offer an effective\nand targeted means of influencing model behavior without retraining or\nfine-tuning the model. But how can such steering vectors be systematically\nidentified? We propose a principled approach for uncovering steering vectors by\naligning latent representations elicited through behavioral methods\n(specifically, Markov chain Monte Carlo with LLMs) with their neural\ncounterparts. To evaluate this approach, we focus on extracting latent risk\npreferences from LLMs and steering their risk-related outputs using the aligned\nrepresentations as steering vectors. We show that the resulting steering\nvectors successfully and reliably modulate LLM outputs in line with the\ntargeted behavior."}
{"id": "2505.12789", "pdf": "https://arxiv.org/pdf/2505.12789", "abs": "https://arxiv.org/abs/2505.12789", "authors": ["Hemanth Saratchandran", "Simon Lucey"], "title": "Enhancing Transformers Through Conditioned Embedded Tokens", "categories": ["cs.CV"], "comment": null, "summary": "Transformers have transformed modern machine learning, driving breakthroughs\nin computer vision, natural language processing, and robotics. At the core of\ntheir success lies the attention mechanism, which enables the modeling of\nglobal dependencies among input tokens. However, we reveal that the attention\nblock in transformers suffers from inherent ill-conditioning, which hampers\ngradient-based optimization and leads to inefficient training. To address this,\nwe develop a theoretical framework that establishes a direct relationship\nbetween the conditioning of the attention block and that of the embedded\ntokenized data. Building on this insight, we introduce conditioned embedded\ntokens, a method that systematically modifies the embedded tokens to improve\nthe conditioning of the attention mechanism. Our analysis demonstrates that\nthis approach significantly mitigates ill-conditioning, leading to more stable\nand efficient training. We validate our methodology across various transformer\narchitectures, achieving consistent improvements in image classification,\nobject detection, instance segmentation, and natural language processing,\nhighlighting its broad applicability and effectiveness."}
{"id": "2505.12950", "pdf": "https://arxiv.org/pdf/2505.12950", "abs": "https://arxiv.org/abs/2505.12950", "authors": ["Daehee Kim", "Deokhyung Kang", "Jonghwi Kim", "Sangwon Ryu", "Gary Geunbae Lee"], "title": "GuRE:Generative Query REwriter for Legal Passage Retrieval", "categories": ["cs.CL"], "comment": "14 pages, 9 figures", "summary": "Legal Passage Retrieval (LPR) systems are crucial as they help practitioners\nsave time when drafting legal arguments. However, it remains an underexplored\navenue. One primary reason is the significant vocabulary mismatch between the\nquery and the target passage. To address this, we propose a simple yet\neffective method, the Generative query REwriter (GuRE). We leverage the\ngenerative capabilities of Large Language Models (LLMs) by training the LLM for\nquery rewriting. \"Rewritten queries\" help retrievers to retrieve target\npassages by mitigating vocabulary mismatch. Experimental results show that GuRE\nsignificantly improves performance in a retriever-agnostic manner,\noutperforming all baseline methods. Further analysis reveals that different\ntraining objectives lead to distinct retrieval behaviors, making GuRE more\nsuitable than direct retriever fine-tuning for real-world applications. Codes\nare avaiable at github.com/daehuikim/GuRE."}
{"id": "2505.11621", "pdf": "https://arxiv.org/pdf/2505.11621", "abs": "https://arxiv.org/abs/2505.11621", "authors": ["Junhyung Park", "Patrick Bloebaum", "Shiva Prasad Kasiviswanathan"], "title": "A Classical View on Benign Overfitting: The Role of Sample Size", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "The results here subsume: arXiv:2410.06191", "summary": "Benign overfitting is a phenomenon in machine learning where a model\nperfectly fits (interpolates) the training data, including noisy examples, yet\nstill generalizes well to unseen data. Understanding this phenomenon has\nattracted considerable attention in recent years. In this work, we introduce a\nconceptual shift, by focusing on almost benign overfitting, where models\nsimultaneously achieve both arbitrarily small training and test errors. This\nbehavior is characteristic of neural networks, which often achieve low (but\nnon-zero) training error while still generalizing well. We hypothesize that\nthis almost benign overfitting can emerge even in classical regimes, by\nanalyzing how the interaction between sample size and model complexity enables\nlarger models to achieve both good training fit but still approach\nBayes-optimal generalization. We substantiate this hypothesis with theoretical\nevidence from two case studies: (i) kernel ridge regression, and (ii)\nleast-squares regression using a two-layer fully connected ReLU neural network\ntrained via gradient flow. In both cases, we overcome the strong assumptions\noften required in prior work on benign overfitting.\n  Our results on neural networks also provide the first generalization result\nin this setting that does not rely on any assumptions about the underlying\nregression function or noise, beyond boundedness. Our analysis introduces a\nnovel proof technique based on decomposing the excess risk into estimation and\napproximation errors, interpreting gradient flow as an implicit regularizer,\nthat helps avoid uniform convergence traps. This analysis idea could be of\nindependent interest."}
{"id": "2505.12803", "pdf": "https://arxiv.org/pdf/2505.12803", "abs": "https://arxiv.org/abs/2505.12803", "authors": ["Jiawen Xu", "Odej Kao", "Margret Keuper"], "title": "Informed Mixing -- Improving Open Set Recognition via Attribution-based Augmentation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Open set recognition (OSR) is devised to address the problem of detecting\nnovel classes during model inference. Even in recent vision models, this\nremains an open issue which is receiving increasing attention. Thereby, a\ncrucial challenge is to learn features that are relevant for unseen categories\nfrom given data, for which these features might not be discriminative. To\nfacilitate this process and \"optimize to learn\" more diverse features, we\npropose GradMix, a data augmentation method that dynamically leverages\ngradient-based attribution maps of the model during training to mask out\nalready learned concepts. Thus GradMix encourages the model to learn a more\ncomplete set of representative features from the same data source. Extensive\nexperiments on open set recognition, close set classification, and\nout-of-distribution detection reveal that our method can often outperform the\nstate-of-the-art. GradMix can further increase model robustness to corruptions\nas well as downstream classification performance for self-supervised learning,\nindicating its benefit for model generalization."}
{"id": "2505.12964", "pdf": "https://arxiv.org/pdf/2505.12964", "abs": "https://arxiv.org/abs/2505.12964", "authors": ["Shanshan Liu", "Noriki Nishida", "Rumana Ferdous Munne", "Narumi Tokunaga", "Yuki Yamagata", "Kouji Kozaki", "Yuji Matsumoto"], "title": "MA-COIR: Leveraging Semantic Search Index and Generative Models for Ontology-Driven Biomedical Concept Recognition", "categories": ["cs.CL"], "comment": "preprint", "summary": "Recognizing biomedical concepts in the text is vital for ontology refinement,\nknowledge graph construction, and concept relationship discovery. However,\ntraditional concept recognition methods, relying on explicit mention\nidentification, often fail to capture complex concepts not explicitly stated in\nthe text. To overcome this limitation, we introduce MA-COIR, a framework that\nreformulates concept recognition as an indexing-recognition task. By assigning\nsemantic search indexes (ssIDs) to concepts, MA-COIR resolves ambiguities in\nontology entries and enhances recognition efficiency. Using a pretrained\nBART-based model fine-tuned on small datasets, our approach reduces\ncomputational requirements to facilitate adoption by domain experts.\nFurthermore, we incorporate large language models (LLMs)-generated queries and\nsynthetic data to improve recognition in low-resource settings. Experimental\nresults on three scenarios (CDR, HPO, and HOIP) highlight the effectiveness of\nMA-COIR in recognizing both explicit and implicit concepts without the need for\nmention-level annotations during inference, advancing ontology-driven concept\nrecognition in biomedical domain applications. Our code and constructed data\nare available at https://github.com/sl-633/macoir-master."}
{"id": "2505.11625", "pdf": "https://arxiv.org/pdf/2505.11625", "abs": "https://arxiv.org/abs/2505.11625", "authors": ["Huiliang Zhang", "Ping Nie", "Lijun Sun", "Benoit Boulet"], "title": "Nearest Neighbor Multivariate Time Series Forecasting", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Multivariate time series (MTS) forecasting has a wide range of applications\nin both industry and academia. Recently, spatial-temporal graph neural networks\n(STGNNs) have gained popularity as MTS forecasting methods. However, current\nSTGNNs can only use the finite length of MTS input data due to the\ncomputational complexity. Moreover, they lack the ability to identify similar\npatterns throughout the entire dataset and struggle with data that exhibit\nsparsely and discontinuously distributed correlations among variables over an\nextensive historical period, resulting in only marginal improvements. In this\narticle, we introduce a simple yet effective k-nearest neighbor MTS forecasting\n( kNN-MTS) framework, which forecasts with a nearest neighbor retrieval\nmechanism over a large datastore of cached series, using representations from\nthe MTS model for similarity search. This approach requires no additional\ntraining and scales to give the MTS model direct access to the whole dataset at\ntest time, resulting in a highly expressive model that consistently improves\nperformance, and has the ability to extract sparse distributed but similar\npatterns spanning over multivariables from the entire dataset. Furthermore, a\nhybrid spatial-temporal encoder (HSTEncoder) is designed for kNN-MTS which can\ncapture both long-term temporal and short-term spatial-temporal dependencies\nand is shown to provide accurate representation for kNN-MTSfor better\nforecasting. Experimental results on several real-world datasets show a\nsignificant improvement in the forecasting performance of kNN-MTS. The\nquantitative analysis also illustrates the interpretability and efficiency of\nkNN-MTS, showing better application prospects and opening up a new path for\nefficiently using the large dataset in MTS models."}
{"id": "2505.12820", "pdf": "https://arxiv.org/pdf/2505.12820", "abs": "https://arxiv.org/abs/2505.12820", "authors": ["Hulin Li"], "title": "Rethinking Features-Fused-Pyramid-Neck for Object Detection", "categories": ["cs.CV"], "comment": "ECCV 2024", "summary": "Multi-head detectors typically employ a features-fused-pyramid-neck for\nmulti-scale detection and are widely adopted in the industry. However, this\napproach faces feature misalignment when representations from different\nhierarchical levels of the feature pyramid are forcibly fused point-to-point.\nTo address this issue, we designed an independent hierarchy pyramid (IHP)\narchitecture to evaluate the effectiveness of the features-unfused-pyramid-neck\nfor multi-head detectors. Subsequently, we introduced soft nearest neighbor\ninterpolation (SNI) with a weight downscaling factor to mitigate the impact of\nfeature fusion at different hierarchies while preserving key textures.\nFurthermore, we present a features adaptive selection method for down sampling\nin extended spatial windows (ESD) to retain spatial features and enhance\nlightweight convolutional techniques (GSConvE). These advancements culminate in\nour secondary features alignment solution (SA) for real-time detection,\nachieving state-of-the-art results on Pascal VOC and MS COCO. Code will be\nreleased at https://github.com/AlanLi1997/rethinking-fpn. This paper has been\naccepted by ECCV2024 and published on Springer Nature."}
{"id": "2505.12969", "pdf": "https://arxiv.org/pdf/2505.12969", "abs": "https://arxiv.org/abs/2505.12969", "authors": ["Yingzhi Wang", "Anas Alhmoud", "Saad Alsahly", "Muhammad Alqurishi", "Mirco Ravanelli"], "title": "Calm-Whisper: Reduce Whisper Hallucination On Non-Speech By Calming Crazy Heads Down", "categories": ["cs.CL"], "comment": "Accepted to Interspeech 2025", "summary": "OpenAI's Whisper has achieved significant success in Automatic Speech\nRecognition. However, it has consistently been found to exhibit hallucination\nissues, particularly in non-speech segments, which limits its broader\napplication in complex industrial settings.\n  In this paper, we introduce a novel method to reduce Whisper's hallucination\non non-speech segments without using any pre- or post-possessing techniques.\nSpecifically, we benchmark the contribution of each self-attentional head in\nthe Whisper-large-v3 decoder to the hallucination problem by performing a\nhead-wise mask. Our findings reveal that only 3 of the 20 heads account for\nover 75% of the hallucinations on the UrbanSound dataset. We then fine-tune\nthese three crazy heads using a collection of non-speech data. The results show\nthat our best fine-tuned model, namely Calm-Whisper, achieves over 80%\nreduction in non-speech hallucination with only less than 0.1% WER degradation\non LibriSpeech test-clean and test-other."}
{"id": "2505.11633", "pdf": "https://arxiv.org/pdf/2505.11633", "abs": "https://arxiv.org/abs/2505.11633", "authors": ["Vyacheslav Tykhonov", "Han Yang", "Philipp Mayr", "Jetze Touber", "Andrea Scharnhorst"], "title": "Chatting with Papers: A Hybrid Approach Using LLMs and Knowledge Graphs", "categories": ["cs.DL", "cs.AI"], "comment": "9 pages, 3 figures, Submitted to Joint Workshop of the 5th AI +\n  Informetrics (AII) and the 6th Extraction and Evaluation of Knowledge\n  Entities from Scientific Documents (EEKE)", "summary": "This demo paper reports on a new workflow \\textit{GhostWriter} that combines\nthe use of Large Language Models and Knowledge Graphs (semantic artifacts) to\nsupport navigation through collections. Situated in the research area of\nRetrieval Augmented Generation, this specific workflow details the creation of\nlocal and adaptable chatbots. Based on the tool-suite \\textit{EverythingData}\nat the backend, \\textit{GhostWriter} provides an interface that enables\nquerying and ``chatting'' with a collection. Applied iteratively, the workflow\nsupports the information needs of researchers when interacting with a\ncollection of papers, whether it be to gain an overview, to learn more about a\nspecific concept and its context, and helps the researcher ultimately to refine\ntheir research question in a controlled way. We demonstrate the workflow for a\ncollection of articles from the \\textit{method data analysis} journal published\nby GESIS -- Leibniz-Institute for the Social Sciences. We also point to further\napplication areas."}
{"id": "2505.12826", "pdf": "https://arxiv.org/pdf/2505.12826", "abs": "https://arxiv.org/abs/2505.12826", "authors": ["Jianfeng Cai", "Wengang Zhou", "Zongmeng Zhang", "Jiale Hong", "Nianji Zhan", "Houqiang Li"], "title": "Mitigating Hallucination in VideoLLMs via Temporal-Aware Activation Engineering", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal large language models (MLLMs) have achieved remarkable progress in\nvideo understanding.However, hallucination, where the model generates plausible\nyet incorrect outputs, persists as a significant and under-addressed challenge\nin the video domain. Among existing solutions, activation engineering has\nproven successful in mitigating hallucinations in LLMs and ImageLLMs, yet its\napplicability to VideoLLMs remains largely unexplored. In this work, we are the\nfirst to systematically investigate the effectiveness and underlying mechanisms\nof activation engineering for mitigating hallucinations in VideoLLMs. We\ninitially conduct an investigation of the key factors affecting the performance\nof activation engineering and find that a model's sensitivity to hallucination\ndepends on $\\textbf{temporal variation}$ rather than task type. Moreover,\nselecting appropriate internal modules and dataset for activation engineering\nis critical for reducing hallucination. Guided by these findings, we propose a\ntemporal-aware activation engineering framework for VideoLLMs, which adaptively\nidentifies and manipulates hallucination-sensitive modules based on the\ntemporal variation characteristic, substantially mitigating hallucinations\nwithout additional LLM fine-tuning. Experiments across multiple models and\nbenchmarks demonstrate that our method markedly reduces hallucination in\nVideoLLMs, thereby validating the robustness of our findings."}
{"id": "2505.12970", "pdf": "https://arxiv.org/pdf/2505.12970", "abs": "https://arxiv.org/abs/2505.12970", "authors": ["Robin Jegan", "Andreas Henrich"], "title": "A Structured Literature Review on Traditional Approaches in Current Natural Language Processing", "categories": ["cs.CL"], "comment": "14 pages, 1 figure", "summary": "The continued rise of neural networks and large language models in the more\nrecent past has altered the natural language processing landscape, enabling new\napproaches towards typical language tasks and achieving mainstream success.\nDespite the huge success of large language models, many disadvantages still\nremain and through this work we assess the state of the art in five application\nscenarios with a particular focus on the future perspectives and sensible\napplication scenarios of traditional and older approaches and techniques.\n  In this paper we survey recent publications in the application scenarios\nclassification, information and relation extraction, text simplification as\nwell as text summarization. After defining our terminology, i.e., which\nfeatures are characteristic for traditional techniques in our interpretation\nfor the five scenarios, we survey if such traditional approaches are still\nbeing used, and if so, in what way they are used. It turns out that all five\napplication scenarios still exhibit traditional models in one way or another,\nas part of a processing pipeline, as a comparison/baseline to the core model of\nthe respective paper, or as the main model(s) of the paper. For the complete\nstatistics, see https://zenodo.org/records/13683801"}
{"id": "2505.11642", "pdf": "https://arxiv.org/pdf/2505.11642", "abs": "https://arxiv.org/abs/2505.11642", "authors": ["Falong Fan", "Xi Li"], "title": "PeerGuard: Defending Multi-Agent Systems Against Backdoor Attacks Through Mutual Reasoning", "categories": ["cs.MA", "cs.AI", "cs.LG"], "comment": null, "summary": "Multi-agent systems leverage advanced AI models as autonomous agents that\ninteract, cooperate, or compete to complete complex tasks across applications\nsuch as robotics and traffic management. Despite their growing importance,\nsafety in multi-agent systems remains largely underexplored, with most research\nfocusing on single AI models rather than interacting agents. This work\ninvestigates backdoor vulnerabilities in multi-agent systems and proposes a\ndefense mechanism based on agent interactions. By leveraging reasoning\nabilities, each agent evaluates responses from others to detect illogical\nreasoning processes, which indicate poisoned agents. Experiments on LLM-based\nmulti-agent systems, including ChatGPT series and Llama 3, demonstrate the\neffectiveness of the proposed method, achieving high accuracy in identifying\npoisoned agents while minimizing false positives on clean agents. We believe\nthis work provides insights into multi-agent system safety and contributes to\nthe development of robust, trustworthy AI interactions."}
{"id": "2505.12834", "pdf": "https://arxiv.org/pdf/2505.12834", "abs": "https://arxiv.org/abs/2505.12834", "authors": ["Avinash Kumar", "Kyeolhee Kang", "Ammar ul Hassan", "Jaeyoung Choi"], "title": "A Study on the Refining Handwritten Font by Mixing Font Styles", "categories": ["cs.CV"], "comment": "4 pages, 3 figures, MITA 2023 (The 19th International Conference on\n  Multimedia Information Technology and Applications July. 11 ~ July 14, 2023,\n  Technical University of Ostrava, Ostrava, Czech)", "summary": "Handwritten fonts have a distinct expressive character, but they are often\ndifficult to read due to unclear or inconsistent handwriting. FontFusionGAN\n(FFGAN) is a novel method for improving handwritten fonts by combining them\nwith printed fonts. Our method implements generative adversarial network (GAN)\nto generate font that mix the desirable features of handwritten and printed\nfonts. By training the GAN on a dataset of handwritten and printed fonts, it\ncan generate legible and visually appealing font images. We apply our method to\na dataset of handwritten fonts and demonstrate that it significantly enhances\nthe readability of the original fonts while preserving their unique aesthetic.\nOur method has the potential to improve the readability of handwritten fonts,\nwhich would be helpful for a variety of applications including document\ncreation, letter writing, and assisting individuals with reading and writing\ndifficulties. In addition to addressing the difficulties of font creation for\nlanguages with complex character sets, our method is applicable to other\ntext-image-related tasks, such as font attribute control and multilingual font\nstyle transfer."}
{"id": "2505.12973", "pdf": "https://arxiv.org/pdf/2505.12973", "abs": "https://arxiv.org/abs/2505.12973", "authors": ["Mahta Fetrat Qharabagh", "Zahra Dehghanian", "Hamid R. Rabiee"], "title": "Fast, Not Fancy: Rethinking G2P with Rich Data and Rule-Based Models", "categories": ["cs.CL"], "comment": "8 main body pages, total 25 pages, 15 figures", "summary": "Homograph disambiguation remains a significant challenge in\ngrapheme-to-phoneme (G2P) conversion, especially for low-resource languages.\nThis challenge is twofold: (1) creating balanced and comprehensive homograph\ndatasets is labor-intensive and costly, and (2) specific disambiguation\nstrategies introduce additional latency, making them unsuitable for real-time\napplications such as screen readers and other accessibility tools. In this\npaper, we address both issues. First, we propose a semi-automated pipeline for\nconstructing homograph-focused datasets, introduce the HomoRich dataset\ngenerated through this pipeline, and demonstrate its effectiveness by applying\nit to enhance a state-of-the-art deep learning-based G2P system for Persian.\nSecond, we advocate for a paradigm shift - utilizing rich offline datasets to\ninform the development of fast, rule-based methods suitable for\nlatency-sensitive accessibility applications like screen readers. To this end,\nwe improve one of the most well-known rule-based G2P systems, eSpeak, into a\nfast homograph-aware version, HomoFast eSpeak. Our results show an approximate\n30% improvement in homograph disambiguation accuracy for the deep\nlearning-based and eSpeak systems."}
{"id": "2505.11659", "pdf": "https://arxiv.org/pdf/2505.11659", "abs": "https://arxiv.org/abs/2505.11659", "authors": ["Loubnan Abou-Hamdan", "Emil Marinov", "Peter Wiecha", "Philipp del Hougne", "Tianyu Wang", "Patrice Genevet"], "title": "Programmable metasurfaces for future photonic artificial intelligence", "categories": ["physics.optics", "cs.AI", "physics.app-ph"], "comment": "Nat. Rev. Phys. (2025)", "summary": "Photonic neural networks (PNNs), which share the inherent benefits of\nphotonic systems, such as high parallelism and low power consumption, could\nchallenge traditional digital neural networks in terms of energy efficiency,\nlatency, and throughput. However, producing scalable photonic artificial\nintelligence (AI) solutions remains challenging. To make photonic AI models\nviable, the scalability problem needs to be solved. Large optical AI models\nimplemented on PNNs are only commercially feasible if the advantages of optical\ncomputation outweigh the cost of their input-output overhead. In this\nPerspective, we discuss how field-programmable metasurface technology may\nbecome a key hardware ingredient in achieving scalable photonic AI accelerators\nand how it can compete with current digital electronic technologies.\nProgrammability or reconfigurability is a pivotal component for PNN hardware,\nenabling in situ training and accommodating non-stationary use cases that\nrequire fine-tuning or transfer learning. Co-integration with electronics, 3D\nstacking, and large-scale manufacturing of metasurfaces would significantly\nimprove PNN scalability and functionalities. Programmable metasurfaces could\naddress some of the current challenges that PNNs face and enable\nnext-generation photonic AI technology."}
{"id": "2505.12849", "pdf": "https://arxiv.org/pdf/2505.12849", "abs": "https://arxiv.org/abs/2505.12849", "authors": ["Ben Liu", "Zhen Qin"], "title": "Accelerate TarFlow Sampling with GS-Jacobi Iteration", "categories": ["cs.CV"], "comment": "17 pages, 7 figures, 5 tables", "summary": "Image generation models have achieved widespread applications. As an\ninstance, the TarFlow model combines the transformer architecture with\nNormalizing Flow models, achieving state-of-the-art results on multiple\nbenchmarks. However, due to the causal form of attention requiring sequential\ncomputation, TarFlow's sampling process is extremely slow. In this paper, we\ndemonstrate that through a series of optimization strategies, TarFlow sampling\ncan be greatly accelerated by using the Gauss-Seidel-Jacobi (abbreviated as\nGS-Jacobi) iteration method. Specifically, we find that blocks in the TarFlow\nmodel have varying importance: a small number of blocks play a major role in\nimage generation tasks, while other blocks contribute relatively little; some\nblocks are sensitive to initial values and prone to numerical overflow, while\nothers are relatively robust. Based on these two characteristics, we propose\nthe Convergence Ranking Metric (CRM) and the Initial Guessing Metric (IGM): CRM\nis used to identify whether a TarFlow block is \"simple\" (converges in few\niterations) or \"tough\" (requires more iterations); IGM is used to evaluate\nwhether the initial value of the iteration is good. Experiments on four TarFlow\nmodels demonstrate that GS-Jacobi sampling can significantly enhance sampling\nefficiency while maintaining the quality of generated images (measured by FID),\nachieving speed-ups of 4.53x in Img128cond, 5.32x in AFHQ, 2.96x in\nImg64uncond, and 2.51x in Img64cond without degrading FID scores or sample\nquality. Code and checkpoints are accessible on\nhttps://github.com/encoreus/GS-Jacobi_for_TarFlow"}
{"id": "2505.12983", "pdf": "https://arxiv.org/pdf/2505.12983", "abs": "https://arxiv.org/abs/2505.12983", "authors": ["Jiaan Wang", "Fandong Meng", "Zengkui Sun", "Yunlong Liang", "Yuxuan Cao", "Jiarong Xu", "Haoxiang Shi", "Jie Zhou"], "title": "An Empirical Study of Many-to-Many Summarization with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 main conference", "summary": "Many-to-many summarization (M2MS) aims to process documents in any language\nand generate the corresponding summaries also in any language. Recently, large\nlanguage models (LLMs) have shown strong multi-lingual abilities, giving them\nthe potential to perform M2MS in real applications. This work presents a\nsystematic empirical study on LLMs' M2MS ability. Specifically, we first\nreorganize M2MS data based on eight previous domain-specific datasets. The\nreorganized data contains 47.8K samples spanning five domains and six\nlanguages, which could be used to train and evaluate LLMs. Then, we benchmark\n18 LLMs in a zero-shot manner and an instruction-tuning manner. Fine-tuned\ntraditional models (e.g., mBART) are also conducted for comparisons. Our\nexperiments reveal that, zero-shot LLMs achieve competitive results with\nfine-tuned traditional models. After instruct-tuning, open-source LLMs can\nsignificantly improve their M2MS ability, and outperform zero-shot LLMs\n(including GPT-4) in terms of automatic evaluations. In addition, we\ndemonstrate that this task-specific improvement does not sacrifice the LLMs'\ngeneral task-solving abilities. However, as revealed by our human evaluation,\nLLMs still face the factuality issue, and the instruction tuning might\nintensify the issue. Thus, how to control factual errors becomes the key when\nbuilding LLM summarizers in real applications, and is worth noting in future\nresearch."}
{"id": "2505.11665", "pdf": "https://arxiv.org/pdf/2505.11665", "abs": "https://arxiv.org/abs/2505.11665", "authors": ["Shubham Vatsal", "Harsh Dubey", "Aditi Singh"], "title": "Multilingual Prompt Engineering in Large Language Models: A Survey Across NLP Tasks", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive performance across\na wide range of Natural Language Processing (NLP) tasks. However, ensuring\ntheir effectiveness across multiple languages presents unique challenges.\nMultilingual prompt engineering has emerged as a key approach to enhance LLMs'\ncapabilities in diverse linguistic settings without requiring extensive\nparameter re-training or fine-tuning. With growing interest in multilingual\nprompt engineering over the past two to three years, researchers have explored\nvarious strategies to improve LLMs' performance across languages and NLP tasks.\nBy crafting structured natural language prompts, researchers have successfully\nextracted knowledge from LLMs across different languages, making these\ntechniques an accessible pathway for a broader audience, including those\nwithout deep expertise in machine learning, to harness the capabilities of\nLLMs. In this paper, we survey and categorize different multilingual prompting\ntechniques based on the NLP tasks they address across a diverse set of datasets\nthat collectively span around 250 languages. We further highlight the LLMs\nemployed, present a taxonomy of approaches and discuss potential\nstate-of-the-art (SoTA) methods for specific multilingual datasets.\nAdditionally, we derive a range of insights across language families and\nresource levels (high-resource vs. low-resource), including analyses such as\nthe distribution of NLP tasks by language resource type and the frequency of\nprompting methods across different language families. Our survey reviews 36\nresearch papers covering 39 prompting techniques applied to 30 multilingual NLP\ntasks, with the majority of these studies published in the last two years."}
{"id": "2505.12854", "pdf": "https://arxiv.org/pdf/2505.12854", "abs": "https://arxiv.org/abs/2505.12854", "authors": ["Anna Maschek", "David C. Schedl"], "title": "The Way Up: A Dataset for Hold Usage Detection in Sport Climbing", "categories": ["cs.CV"], "comment": "accepted at the International Workshop on Computer Vision in Sports\n  (CVsports) at CVPR 2025", "summary": "Detecting an athlete's position on a route and identifying hold usage are\ncrucial in various climbing-related applications. However, no climbing dataset\nwith detailed hold usage annotations exists to our knowledge. To address this\nissue, we introduce a dataset of 22 annotated climbing videos, providing\nground-truth labels for hold locations, usage order, and time of use.\nFurthermore, we explore the application of keypoint-based 2D pose-estimation\nmodels for detecting hold usage in sport climbing. We determine usage by\nanalyzing the key points of certain joints and the corresponding overlap with\nclimbing holds. We evaluate multiple state-of-the-art models and analyze their\naccuracy on our dataset, identifying and highlighting climbing-specific\nchallenges. Our dataset and results highlight key challenges in\nclimbing-specific pose estimation and establish a foundation for future\nresearch toward AI-assisted systems for sports climbing."}
{"id": "2505.12996", "pdf": "https://arxiv.org/pdf/2505.12996", "abs": "https://arxiv.org/abs/2505.12996", "authors": ["Jiaan Wang", "Fandong Meng", "Jie Zhou"], "title": "ExTrans: Multilingual Deep Reasoning Translation via Exemplar-Enhanced Reinforcement Learning", "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 2 figures", "summary": "In recent years, the emergence of large reasoning models (LRMs), such as\nOpenAI-o1 and DeepSeek-R1, has shown impressive capabilities in complex\nproblems, e.g., mathematics and coding. Some pioneering studies attempt to\nbring the success of LRMs in neural machine translation (MT). They try to build\nLRMs with deep reasoning MT ability via reinforcement learning (RL). Despite\nsome progress that has been made, these attempts generally focus on several\nhigh-resource languages, e.g., English and Chinese, leaving the performance on\nother languages unclear. Besides, the reward modeling methods in previous work\ndo not fully unleash the potential of reinforcement learning in MT. In this\nwork, we first design a new reward modeling method that compares the\ntranslation results of the policy MT model with a strong LRM (i.e.,\nDeepSeek-R1-671B), and quantifies the comparisons to provide rewards.\nExperimental results demonstrate the superiority of the reward modeling method.\nUsing Qwen2.5-7B-Instruct as the backbone, the trained model achieves the new\nstate-of-the-art performance in literary translation, and outperforms strong\nLRMs including OpenAI-o1 and DeepSeeK-R1. Furthermore, we extend our method to\nthe multilingual settings with 11 languages. With a carefully designed\nlightweight reward modeling in RL, we can simply transfer the strong MT ability\nfrom a single direction into multiple (i.e., 90) translation directions and\nachieve impressive multilingual MT performance."}
{"id": "2505.11669", "pdf": "https://arxiv.org/pdf/2505.11669", "abs": "https://arxiv.org/abs/2505.11669", "authors": ["Yiming Zhang", "Sitong Liu", "Alex Cloninger"], "title": "OT Score: An OT based Confidence Score for Unsupervised Domain Adaptation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We address the computational and theoretical limitations of existing\ndistributional alignment methods for unsupervised domain adaptation (UDA),\nparticularly regarding the estimation of classification performance and\nconfidence without target labels. Current theoretical frameworks for these\nmethods often yield computationally intractable quantities and fail to\nadequately reflect the properties of the alignment algorithms employed. To\novercome these challenges, we introduce the Optimal Transport (OT) score, a\nconfidence metric derived from a novel theoretical analysis that exploits the\nflexibility of decision boundaries induced by Semi-Discrete Optimal Transport\nalignment. The proposed OT score is intuitively interpretable, theoretically\nrigorous, and computationally efficient. It provides principled uncertainty\nestimates for any given set of target pseudo-labels without requiring model\nretraining, and can flexibly adapt to varying degrees of available source\ninformation. Experimental results on standard UDA benchmarks demonstrate that\nclassification accuracy consistently improves by identifying and removing\nlow-confidence predictions, and that OT score significantly outperforms\nexisting confidence metrics across diverse adaptation scenarios."}
{"id": "2505.12860", "pdf": "https://arxiv.org/pdf/2505.12860", "abs": "https://arxiv.org/abs/2505.12860", "authors": ["Wenbo Yang", "Zhongling Wang", "Zhou Wang"], "title": "Towards a Universal Image Degradation Model via Content-Degradation Disentanglement", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Image degradation synthesis is highly desirable in a wide variety of\napplications ranging from image restoration to simulating artistic effects.\nExisting models are designed to generate one specific or a narrow set of\ndegradations, which often require user-provided degradation parameters. As a\nresult, they lack the generalizability to synthesize degradations beyond their\ninitial design or adapt to other applications. Here we propose the first\nuniversal degradation model that can synthesize a broad spectrum of complex and\nrealistic degradations containing both homogeneous (global) and inhomogeneous\n(spatially varying) components. Our model automatically extracts and\ndisentangles homogeneous and inhomogeneous degradation features, which are\nlater used for degradation synthesis without user intervention. A\ndisentangle-by-compression method is proposed to separate degradation\ninformation from images. Two novel modules for extracting and incorporating\ninhomogeneous degradations are created to model inhomogeneous components in\ncomplex degradations. We demonstrate the model's accuracy and adaptability in\nfilm-grain simulation and blind image restoration tasks. The demo video, code,\nand dataset of this project will be released upon publication at\ngithub.com/yangwenbo99/content-degradation-disentanglement."}
{"id": "2505.13004", "pdf": "https://arxiv.org/pdf/2505.13004", "abs": "https://arxiv.org/abs/2505.13004", "authors": ["Yuhao Qing", "Boyu Zhu", "Mingzhe Du", "Zhijiang Guo", "Terry Yue Zhuo", "Qianru Zhang", "Jie M. Zhang", "Heming Cui", "Siu-Ming Yiu", "Dong Huang", "See-Kiong Ng", "Luu Anh Tuan"], "title": "EffiBench-X: A Multi-Language Benchmark for Measuring Efficiency of LLM-Generated Code", "categories": ["cs.CL"], "comment": "Under Review", "summary": "Existing code generation benchmarks primarily evaluate functional\ncorrectness, with limited focus on code efficiency and often restricted to a\nsingle language like Python. To address this gap, we introduce EffiBench-X, the\nfirst multi-language benchmark designed to measure the efficiency of\nLLM-generated code. EffiBench-X supports Python, C++, Java, JavaScript, Ruby,\nand Golang. It comprises competitive programming tasks with human-expert\nsolutions as efficiency baselines. Evaluating state-of-the-art LLMs on\nEffiBench-X reveals that while models generate functionally correct code, they\nconsistently underperform human experts in efficiency. Even the most efficient\nLLM-generated solutions (Qwen3-32B) achieve only around \\textbf{62\\%} of human\nefficiency on average, with significant language-specific variations. LLMs show\nbetter efficiency in Python, Ruby, and JavaScript than in Java, C++, and\nGolang. For instance, DeepSeek-R1's Python code is significantly more efficient\nthan its Java code. These results highlight the critical need for research into\nLLM optimization techniques to improve code efficiency across diverse\nlanguages. The dataset and evaluation infrastructure are submitted and\navailable at https://github.com/EffiBench/EffiBench-X.git and\nhttps://huggingface.co/datasets/EffiBench/effibench-x."}
{"id": "2505.11687", "pdf": "https://arxiv.org/pdf/2505.11687", "abs": "https://arxiv.org/abs/2505.11687", "authors": ["Philipp Schaer", "Christin Katharina Kreutz", "Krisztian Balog", "Timo Breuer", "Andreas Konstantin Kruff"], "title": "Second SIGIR Workshop on Simulations for Information Access (Sim4IA 2025)", "categories": ["cs.IR", "cs.AI", "cs.HC"], "comment": "Proceedings of the 48th International ACM SIGIR Conference on\n  Research and Development in Information Retrieval (SIGIR '25), July 13--18,\n  2025, Padua, Italy", "summary": "Simulations in information access (IA) have recently gained interest, as\nshown by various tutorials and workshops around that topic. Simulations can be\nkey contributors to central IA research and evaluation questions, especially\naround interactive settings when real users are unavailable, or their\nparticipation is impossible due to ethical reasons. In addition, simulations in\nIA can help contribute to a better understanding of users, reduce complexity of\nevaluation experiments, and improve reproducibility. Building on recent\ndevelopments in methods and toolkits, the second iteration of our Sim4IA\nworkshop aims to again bring together researchers and practitioners to form an\ninteractive and engaging forum for discussions on the future perspectives of\nthe field. An additional aim is to plan an upcoming TREC/CLEF campaign."}
{"id": "2505.12861", "pdf": "https://arxiv.org/pdf/2505.12861", "abs": "https://arxiv.org/abs/2505.12861", "authors": ["Jiaqi Tan", "Xu Zheng", "Yang Liu"], "title": "Robust Multimodal Segmentation with Representation Regularization and Hybrid Prototype Distillation", "categories": ["cs.CV"], "comment": null, "summary": "Multi-modal semantic segmentation (MMSS) faces significant challenges in\nreal-world scenarios due to dynamic environments, sensor failures, and noise\ninterference, creating a gap between theoretical models and practical\nperformance. To address this, we propose a two-stage framework called\nRobustSeg, which enhances multi-modal robustness through two key components:\nthe Hybrid Prototype Distillation Module (HPDM) and the Representation\nRegularization Module (RRM). In the first stage, RobustSeg pre-trains a\nmulti-modal teacher model using complete modalities. In the second stage, a\nstudent model is trained with random modality dropout while learning from the\nteacher via HPDM and RRM. HPDM transforms features into compact prototypes,\nenabling cross-modal hybrid knowledge distillation and mitigating bias from\nmissing modalities. RRM reduces representation discrepancies between the\nteacher and student by optimizing functional entropy through the log-Sobolev\ninequality. Extensive experiments on three public benchmarks demonstrate that\nRobustSeg outperforms previous state-of-the-art methods, achieving improvements\nof +2.76%, +4.56%, and +0.98%, respectively. Code is available at:\nhttps://github.com/RobustSeg/RobustSeg."}
{"id": "2505.13006", "pdf": "https://arxiv.org/pdf/2505.13006", "abs": "https://arxiv.org/abs/2505.13006", "authors": ["Yuyang Li", "Philip J. M. Kerbusch", "Raimon H. R. Pruim", "Tobias Käfer"], "title": "Evaluating the Performance of RAG Methods for Conversational AI in the Airport Domain", "categories": ["cs.CL"], "comment": "Accepted by NAACL 2025 industry track", "summary": "Airports from the top 20 in terms of annual passengers are highly dynamic\nenvironments with thousands of flights daily, and they aim to increase the\ndegree of automation. To contribute to this, we implemented a Conversational AI\nsystem that enables staff in an airport to communicate with flight information\nsystems. This system not only answers standard airport queries but also\nresolves airport terminology, jargon, abbreviations, and dynamic questions\ninvolving reasoning. In this paper, we built three different\nRetrieval-Augmented Generation (RAG) methods, including traditional RAG, SQL\nRAG, and Knowledge Graph-based RAG (Graph RAG). Experiments showed that\ntraditional RAG achieved 84.84% accuracy using BM25 + GPT-4 but occasionally\nproduced hallucinations, which is risky to airport safety. In contrast, SQL RAG\nand Graph RAG achieved 80.85% and 91.49% accuracy respectively, with\nsignificantly fewer hallucinations. Moreover, Graph RAG was especially\neffective for questions that involved reasoning. Based on our observations, we\nthus recommend SQL RAG and Graph RAG are better for airport environments, due\nto fewer hallucinations and the ability to handle dynamic questions."}
{"id": "2505.11692", "pdf": "https://arxiv.org/pdf/2505.11692", "abs": "https://arxiv.org/abs/2505.11692", "authors": ["Sahil Rajesh Dhayalkar"], "title": "The Geometry of ReLU Networks through the ReLU Transition Graph", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": "13 pages, 4 figures", "summary": "We develop a novel theoretical framework for analyzing ReLU neural networks\nthrough the lens of a combinatorial object we term the ReLU Transition Graph\n(RTG). In this graph, each node corresponds to a linear region induced by the\nnetwork's activation patterns, and edges connect regions that differ by a\nsingle neuron flip. Building on this structure, we derive a suite of new\ntheoretical results connecting RTG geometry to expressivity, generalization,\nand robustness. Our contributions include tight combinatorial bounds on RTG\nsize and diameter, a proof of RTG connectivity, and graph-theoretic\ninterpretations of VC-dimension. We also relate entropy and average degree of\nthe RTG to generalization error. Each theoretical result is rigorously\nvalidated via carefully controlled experiments across varied network depths,\nwidths, and data regimes. This work provides the first unified treatment of\nReLU network structure via graph theory and opens new avenues for compression,\nregularization, and complexity control rooted in RTG analysis."}
{"id": "2505.12890", "pdf": "https://arxiv.org/pdf/2505.12890", "abs": "https://arxiv.org/abs/2505.12890", "authors": ["Ege Özsoy", "Chantal Pellegrini", "David Bani-Harouni", "Kun Yuan", "Matthias Keicher", "Nassir Navab"], "title": "ORQA: A Benchmark and Foundation Model for Holistic Operating Room Modeling", "categories": ["cs.CV"], "comment": null, "summary": "The real-world complexity of surgeries necessitates surgeons to have deep and\nholistic comprehension to ensure precision, safety, and effective\ninterventions. Computational systems are required to have a similar level of\ncomprehension within the operating room. Prior works, limited to single-task\nefforts like phase recognition or scene graph generation, lack scope and\ngeneralizability. In this work, we introduce ORQA, a novel OR question\nanswering benchmark and foundational multimodal model to advance OR\nintelligence. By unifying all four public OR datasets into a comprehensive\nbenchmark, we enable our approach to concurrently address a diverse range of OR\nchallenges. The proposed multimodal large language model fuses diverse OR\nsignals such as visual, auditory, and structured data, for a holistic modeling\nof the OR. Finally, we propose a novel, progressive knowledge distillation\nparadigm, to generate a family of models optimized for different speed and\nmemory requirements. We show the strong performance of ORQA on our proposed\nbenchmark, and its zero-shot generalization, paving the way for scalable,\nunified OR modeling and significantly advancing multimodal surgical\nintelligence. We will release our code and data upon acceptance."}
{"id": "2505.13010", "pdf": "https://arxiv.org/pdf/2505.13010", "abs": "https://arxiv.org/abs/2505.13010", "authors": ["Himel Ghosh", "Ahmed Mosharafa", "Georg Groh"], "title": "To Bias or Not to Bias: Detecting bias in News with bias-detector", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "7 pages, 5 figures, 2 tables", "summary": "Media bias detection is a critical task in ensuring fair and balanced\ninformation dissemination, yet it remains challenging due to the subjectivity\nof bias and the scarcity of high-quality annotated data. In this work, we\nperform sentence-level bias classification by fine-tuning a RoBERTa-based model\non the expert-annotated BABE dataset. Using McNemar's test and the 5x2\ncross-validation paired t-test, we show statistically significant improvements\nin performance when comparing our model to a domain-adaptively pre-trained\nDA-RoBERTa baseline. Furthermore, attention-based analysis shows that our model\navoids common pitfalls like oversensitivity to politically charged terms and\ninstead attends more meaningfully to contextually relevant tokens. For a\ncomprehensive examination of media bias, we present a pipeline that combines\nour model with an already-existing bias-type classifier. Our method exhibits\ngood generalization and interpretability, despite being constrained by\nsentence-level analysis and dataset size because of a lack of larger and more\nadvanced bias corpora. We talk about context-aware modeling, bias\nneutralization, and advanced bias type classification as potential future\ndirections. Our findings contribute to building more robust, explainable, and\nsocially responsible NLP systems for media bias detection."}
{"id": "2505.11694", "pdf": "https://arxiv.org/pdf/2505.11694", "abs": "https://arxiv.org/abs/2505.11694", "authors": ["Sahil Rajesh Dhayalkar"], "title": "Neural Networks as Universal Finite-State Machines: A Constructive Deterministic Finite Automaton Theory", "categories": ["cs.LG", "cs.AI", "cs.FL"], "comment": "15 pages, 1 figure", "summary": "We present a complete theoretical and empirical framework establishing\nfeedforward neural networks as universal finite-state machines (N-FSMs). Our\nresults prove that finite-depth ReLU and threshold networks can exactly\nsimulate deterministic finite automata (DFAs) by unrolling state transitions\ninto depth-wise neural layers, with formal characterizations of required depth,\nwidth, and state compression. We demonstrate that DFA transitions are linearly\nseparable, binary threshold activations allow exponential compression, and\nMyhill-Nerode equivalence classes can be embedded into continuous latent spaces\nwhile preserving separability. We also formalize the expressivity boundary:\nfixed-depth feedforward networks cannot recognize non-regular languages\nrequiring unbounded memory. Unlike prior heuristic or probing-based studies, we\nprovide constructive proofs and design explicit DFA-unrolled neural\narchitectures that empirically validate every claim. Our results bridge deep\nlearning, automata theory, and neural-symbolic computation, offering a rigorous\nblueprint for how discrete symbolic processes can be realized in continuous\nneural systems."}
{"id": "2505.12897", "pdf": "https://arxiv.org/pdf/2505.12897", "abs": "https://arxiv.org/abs/2505.12897", "authors": ["Piotr Borycki", "Magdalena Trędowicz", "Szymon Janusz", "Jacek Tabor", "Przemysław Spurek", "Arkadiusz Lewicki", "Łukasz Struski"], "title": "EPIC: Explanation of Pretrained Image Classification Networks via Prototype", "categories": ["cs.CV"], "comment": null, "summary": "Explainable AI (XAI) methods generally fall into two categories. Post-hoc\napproaches generate explanations for pre-trained models and are compatible with\nvarious neural network architectures. These methods often use feature\nimportance visualizations, such as saliency maps, to indicate which input\nregions influenced the model's prediction. Unfortunately, they typically offer\na coarse understanding of the model's decision-making process. In contrast,\nante-hoc (inherently explainable) methods rely on specially designed model\narchitectures trained from scratch. A notable subclass of these methods\nprovides explanations through prototypes, representative patches extracted from\nthe training data. However, prototype-based approaches have limitations: they\nrequire dedicated architectures, involve specialized training procedures, and\nperform well only on specific datasets. In this work, we propose EPIC\n(Explanation of Pretrained Image Classification), a novel approach that bridges\nthe gap between these two paradigms. Like post-hoc methods, EPIC operates on\npre-trained models without architectural modifications. Simultaneously, it\ndelivers intuitive, prototype-based explanations inspired by ante-hoc\ntechniques. To the best of our knowledge, EPIC is the first post-hoc method\ncapable of fully replicating the core explanatory power of inherently\ninterpretable models. We evaluate EPIC on benchmark datasets commonly used in\nprototype-based explanations, such as CUB-200-2011 and Stanford Cars, alongside\nlarge-scale datasets like ImageNet, typically employed by post-hoc methods.\nEPIC uses prototypes to explain model decisions, providing a flexible and\neasy-to-understand tool for creating clear, high-quality explanations."}
{"id": "2505.13034", "pdf": "https://arxiv.org/pdf/2505.13034", "abs": "https://arxiv.org/abs/2505.13034", "authors": ["Márton Kardos", "Kenneth C. Enevoldsen", "Kristoffer Laigaard Nielbo"], "title": "topicwizard -- a Modern, Model-agnostic Framework for Topic Model Visualization and Interpretation", "categories": ["cs.CL"], "comment": "9 pages, 9 figures", "summary": "Topic models are statistical tools that allow their users to gain qualitative\nand quantitative insights into the contents of textual corpora without the need\nfor close reading. They can be applied in a wide range of settings from\ndiscourse analysis, through pretraining data curation, to text filtering. Topic\nmodels are typically parameter-rich, complex models, and interpreting these\nparameters can be challenging for their users. It is typical practice for users\nto interpret topics based on the top 10 highest ranking terms on a given topic.\nThis list-of-words approach, however, gives users a limited and biased picture\nof the content of topics. Thoughtful user interface design and visualizations\ncan help users gain a more complete and accurate understanding of topic models'\noutput. While some visualization utilities do exist for topic models, these are\ntypically limited to a certain type of topic model. We introduce topicwizard, a\nframework for model-agnostic topic model interpretation, that provides\nintuitive and interactive tools that help users examine the complex semantic\nrelations between documents, words and topics learned by topic models."}
{"id": "2505.11695", "pdf": "https://arxiv.org/pdf/2505.11695", "abs": "https://arxiv.org/abs/2505.11695", "authors": ["Shihao Zhang", "Haoyu Zhang", "Ian Colbert", "Rayan Saab"], "title": "Qronos: Correcting the Past by Shaping the Future... in Post-Training Quantization", "categories": ["cs.LG", "cs.AI", "math.OC"], "comment": null, "summary": "We introduce Qronos -- a new state-of-the-art post-training quantization\nalgorithm that sequentially rounds and updates neural network weights. Qronos\nnot only explicitly corrects errors due to both weight and activation\nquantization, but also errors resulting from quantizing previous layers. Our\niterative algorithm is based on an interpretable and disciplined optimization\nframework that subsumes and surpasses existing data-driven approaches. At each\nstep, Qronos alternates between error correction and diffusion via optimal\nupdate rules. Importantly, we prove that Qronos admits an efficient\nimplementation that uses the Cholesky decomposition for solving least-squares\nproblems. We also demonstrate that Qronos is compatible with existing\ntransformation techniques such as Hadamard-based incoherence processing and\nweight-activation scaling equalization, among others. We evaluate Qronos using\nrecent autoregressive language generation models in the Llama3 family; Qronos\nconsistently outperforms previous state-of-the-art adaptive rounding methods\nwhen quantizing the weights, activations, and/or KV caches."}
{"id": "2505.12903", "pdf": "https://arxiv.org/pdf/2505.12903", "abs": "https://arxiv.org/abs/2505.12903", "authors": ["Shiao Wang", "Xiao Wang", "Liye Jin", "Bo Jiang", "Lin Zhu", "Lan Chen", "Yonghong Tian", "Bin Luo"], "title": "Towards Low-Latency Event Stream-based Visual Object Tracking: A Slow-Fast Approach", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Existing tracking algorithms typically rely on low-frame-rate RGB cameras\ncoupled with computationally intensive deep neural network architectures to\nachieve effective tracking. However, such frame-based methods inherently face\nchallenges in achieving low-latency performance and often fail in\nresource-constrained environments. Visual object tracking using bio-inspired\nevent cameras has emerged as a promising research direction in recent years,\noffering distinct advantages for low-latency applications. In this paper, we\npropose a novel Slow-Fast Tracking paradigm that flexibly adapts to different\noperational requirements, termed SFTrack. The proposed framework supports two\ncomplementary modes, i.e., a high-precision slow tracker for scenarios with\nsufficient computational resources, and an efficient fast tracker tailored for\nlatency-aware, resource-constrained environments. Specifically, our framework\nfirst performs graph-based representation learning from\nhigh-temporal-resolution event streams, and then integrates the learned\ngraph-structured information into two FlashAttention-based vision backbones,\nyielding the slow and fast trackers, respectively. The fast tracker achieves\nlow latency through a lightweight network design and by producing multiple\nbounding box outputs in a single forward pass. Finally, we seamlessly combine\nboth trackers via supervised fine-tuning and further enhance the fast tracker's\nperformance through a knowledge distillation strategy. Extensive experiments on\npublic benchmarks, including FE240, COESOT, and EventVOT, demonstrate the\neffectiveness and efficiency of our proposed method across different real-world\nscenarios. The source code has been released on\nhttps://github.com/Event-AHU/SlowFast_Event_Track."}
{"id": "2505.13036", "pdf": "https://arxiv.org/pdf/2505.13036", "abs": "https://arxiv.org/abs/2505.13036", "authors": ["Sai Koneru", "Maike Züfle", "Thai-Binh Nguyen", "Seymanur Akti", "Jan Niehues", "Alexander Waibel"], "title": "KIT's Offline Speech Translation and Instruction Following Submission for IWSLT 2025", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The scope of the International Workshop on Spoken Language Translation\n(IWSLT) has recently broadened beyond traditional Speech Translation (ST) to\nencompass a wider array of tasks, including Speech Question Answering and\nSummarization. This shift is partly driven by the growing capabilities of\nmodern systems, particularly with the success of Large Language Models (LLMs).\nIn this paper, we present the Karlsruhe Institute of Technology's submissions\nfor the Offline ST and Instruction Following (IF) tracks, where we leverage\nLLMs to enhance performance across all tasks. For the Offline ST track, we\npropose a pipeline that employs multiple automatic speech recognition systems,\nwhose outputs are fused using an LLM with document-level context. This is\nfollowed by a two-step translation process, incorporating additional refinement\nstep to improve translation quality. For the IF track, we develop an end-to-end\nmodel that integrates a speech encoder with an LLM to perform a wide range of\ninstruction-following tasks. We complement it with a final document-level\nrefinement stage to further enhance output quality by using contextual\ninformation."}
{"id": "2505.11714", "pdf": "https://arxiv.org/pdf/2505.11714", "abs": "https://arxiv.org/abs/2505.11714", "authors": ["Arjun Prakash", "Naicheng He", "Denizalp Goktas", "Amy Greenwald"], "title": "Bi-Level Policy Optimization with Nyström Hypergradients", "categories": ["cs.LG", "cs.AI", "cs.GT"], "comment": null, "summary": "The dependency of the actor on the critic in actor-critic (AC) reinforcement\nlearning means that AC can be characterized as a bilevel optimization (BLO)\nproblem, also called a Stackelberg game. This characterization motivates two\nmodifications to vanilla AC algorithms. First, the critic's update should be\nnested to learn a best response to the actor's policy. Second, the actor should\nupdate according to a hypergradient that takes changes in the critic's behavior\ninto account. Computing this hypergradient involves finding an inverse Hessian\nvector product, a process that can be numerically unstable. We thus propose a\nnew algorithm, Bilevel Policy Optimization with Nystr\\\"om Hypergradients\n(BLPO), which uses nesting to account for the nested structure of BLO, and\nleverages the Nystr\\\"om method to compute the hypergradient. Theoretically, we\nprove BLPO converges to (a point that satisfies the necessary conditions for) a\nlocal strong Stackelberg equilibrium in polynomial time with high probability,\nassuming a linear parametrization of the critic's objective. Empirically, we\ndemonstrate that BLPO performs on par with or better than PPO on a variety of\ndiscrete and continuous control tasks."}
{"id": "2505.12908", "pdf": "https://arxiv.org/pdf/2505.12908", "abs": "https://arxiv.org/abs/2505.12908", "authors": ["Xiao Wang", "Yu Jin", "Lan Chen", "Bo Jiang", "Lin Zhu", "Yonghong Tian", "Jin Tang", "Bin Luo"], "title": "Dynamic Graph Induced Contour-aware Heat Conduction Network for Event-based Object Detection", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Event-based Vision Sensors (EVS) have demonstrated significant advantages\nover traditional RGB frame-based cameras in low-light conditions, high-speed\nmotion capture, and low latency. Consequently, object detection based on EVS\nhas attracted increasing attention from researchers. Current event stream\nobject detection algorithms are typically built upon Convolutional Neural\nNetworks (CNNs) or Transformers, which either capture limited local features\nusing convolutional filters or incur high computational costs due to the\nutilization of self-attention. Recently proposed vision heat conduction\nbackbone networks have shown a good balance between efficiency and accuracy;\nhowever, these models are not specifically designed for event stream data. They\nexhibit weak capability in modeling object contour information and fail to\nexploit the benefits of multi-scale features. To address these issues, this\npaper proposes a novel dynamic graph induced contour-aware heat conduction\nnetwork for event stream based object detection, termed CvHeat-DET. The\nproposed model effectively leverages the clear contour information inherent in\nevent streams to predict the thermal diffusivity coefficients within the heat\nconduction model, and integrates hierarchical structural graph features to\nenhance feature learning across multiple scales. Extensive experiments on three\nbenchmark datasets for event stream-based object detection fully validated the\neffectiveness of the proposed model. The source code of this paper will be\nreleased on https://github.com/Event-AHU/OpenEvDET."}
{"id": "2505.13053", "pdf": "https://arxiv.org/pdf/2505.13053", "abs": "https://arxiv.org/abs/2505.13053", "authors": ["Amelie S. Robrecht", "Christoph R. Kowalski", "Stefan Kopp"], "title": "SNAPE-PM: Building and Utilizing Dynamic Partner Models for Adaptive Explanation Generation", "categories": ["cs.CL", "cs.AI"], "comment": "currently under review at Frontiers in Communication", "summary": "Adapting to the addressee is crucial for successful explanations, yet poses\nsignificant challenges for dialogsystems. We adopt the approach of treating\nexplanation generation as a non-stationary decision process, where the optimal\nstrategy varies according to changing beliefs about the explainee and the\ninteraction context. In this paper we address the questions of (1) how to track\nthe interaction context and the relevant listener features in a formally\ndefined computational partner model, and (2) how to utilize this model in the\ndynamically adjusted, rational decision process that determines the currently\nbest explanation strategy. We propose a Bayesian inference-based approach to\ncontinuously update the partner model based on user feedback, and a\nnon-stationary Markov Decision Process to adjust decision-making based on the\npartner model values. We evaluate an implementation of this framework with five\nsimulated interlocutors, demonstrating its effectiveness in adapting to\ndifferent partners with constant and even changing feedback behavior. The\nresults show high adaptivity with distinct explanation strategies emerging for\ndifferent partners, highlighting the potential of our approach to improve\nexplainable AI systems and dialogsystems in general."}
{"id": "2505.11717", "pdf": "https://arxiv.org/pdf/2505.11717", "abs": "https://arxiv.org/abs/2505.11717", "authors": ["Xilong Wang", "John Bloch", "Zedian Shao", "Yuepeng Hu", "Shuyan Zhou", "Neil Zhenqiang Gong"], "title": "EnvInjection: Environmental Prompt Injection Attack to Multi-modal Web Agents", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Multi-modal large language model (MLLM)-based web agents interact with\nwebpage environments by generating actions based on screenshots of the\nwebpages. Environmental prompt injection attacks manipulate the environment to\ninduce the web agent to perform a specific, attacker-chosen action--referred to\nas the target action. However, existing attacks suffer from limited\neffectiveness or stealthiness, or are impractical in real-world settings. In\nthis work, we propose EnvInjection, a new attack that addresses these\nlimitations. Our attack adds a perturbation to the raw pixel values of the\nrendered webpage, which can be implemented by modifying the webpage's source\ncode. After these perturbed pixels are mapped into a screenshot, the\nperturbation induces the web agent to perform the target action. We formulate\nthe task of finding the perturbation as an optimization problem. A key\nchallenge in solving this problem is that the mapping between raw pixel values\nand screenshot is non-differentiable, making it difficult to backpropagate\ngradients to the perturbation. To overcome this, we train a neural network to\napproximate the mapping and apply projected gradient descent to solve the\nreformulated optimization problem. Extensive evaluation on multiple webpage\ndatasets shows that EnvInjection is highly effective and significantly\noutperforms existing baselines."}
{"id": "2505.12911", "pdf": "https://arxiv.org/pdf/2505.12911", "abs": "https://arxiv.org/abs/2505.12911", "authors": ["Simone Alberto Peirone", "Francesca Pistilli", "Giuseppe Averta"], "title": "HiERO: understanding the hierarchy of human behavior enhances reasoning on egocentric videos", "categories": ["cs.CV"], "comment": "Project page https://github.com/sapeirone/hiero", "summary": "Human activities are particularly complex and variable, and this makes\nchallenging for deep learning models to reason about them. However, we note\nthat such variability does have an underlying structure, composed of a\nhierarchy of patterns of related actions. We argue that such structure can\nemerge naturally from unscripted videos of human activities, and can be\nleveraged to better reason about their content. We present HiERO, a\nweakly-supervised method to enrich video segments features with the\ncorresponding hierarchical activity threads. By aligning video clips with their\nnarrated descriptions, HiERO infers contextual, semantic and temporal reasoning\nwith an hierarchical architecture. We prove the potential of our enriched\nfeatures with multiple video-text alignment benchmarks (EgoMCQ, EgoNLQ) with\nminimal additional training, and in zero-shot for procedure learning tasks\n(EgoProceL and Ego4D Goal-Step). Notably, HiERO achieves state-of-the-art\nperformance in all the benchmarks, and for procedure learning tasks it\noutperforms fully-supervised methods by a large margin (+12.5% F1 on EgoProceL)\nin zero shot. Our results prove the relevance of using knowledge of the\nhierarchy of human activities for multiple reasoning tasks in egocentric\nvision."}
{"id": "2505.13069", "pdf": "https://arxiv.org/pdf/2505.13069", "abs": "https://arxiv.org/abs/2505.13069", "authors": ["Ambre Marie", "Ilias Maoudj", "Guillaume Dardenne", "Gwenolé Quellec"], "title": "Suicide Risk Assessment Using Multimodal Speech Features: A Study on the SW1 Challenge Dataset", "categories": ["cs.CL", "cs.SD", "eess.AS", "I.2.7; I.5.1"], "comment": "Submitted to the SpeechWellness Challenge at Interspeech 2025; 5\n  pages, 2 figures, 2 tables", "summary": "The 1st SpeechWellness Challenge conveys the need for speech-based suicide\nrisk assessment in adolescents. This study investigates a multimodal approach\nfor this challenge, integrating automatic transcription with WhisperX,\nlinguistic embeddings from Chinese RoBERTa, and audio embeddings from WavLM.\nAdditionally, handcrafted acoustic features -- including MFCCs, spectral\ncontrast, and pitch-related statistics -- were incorporated. We explored three\nfusion strategies: early concatenation, modality-specific processing, and\nweighted attention with mixup regularization. Results show that weighted\nattention provided the best generalization, achieving 69% accuracy on the\ndevelopment set, though a performance gap between development and test sets\nhighlights generalization challenges. Our findings, strictly tied to the\nMINI-KID framework, emphasize the importance of refining embedding\nrepresentations and fusion mechanisms to enhance classification reliability."}
{"id": "2505.11719", "pdf": "https://arxiv.org/pdf/2505.11719", "abs": "https://arxiv.org/abs/2505.11719", "authors": ["Sumeet Batra", "Gaurav Sukhatme"], "title": "Zero-Shot Visual Generalization in Robot Manipulation", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Training vision-based manipulation policies that are robust across diverse\nvisual environments remains an important and unresolved challenge in robot\nlearning. Current approaches often sidestep the problem by relying on invariant\nrepresentations such as point clouds and depth, or by brute-forcing\ngeneralization through visual domain randomization and/or large, visually\ndiverse datasets. Disentangled representation learning - especially when\ncombined with principles of associative memory - has recently shown promise in\nenabling vision-based reinforcement learning policies to be robust to visual\ndistribution shifts. However, these techniques have largely been constrained to\nsimpler benchmarks and toy environments. In this work, we scale disentangled\nrepresentation learning and associative memory to more visually and dynamically\ncomplex manipulation tasks and demonstrate zero-shot adaptability to visual\nperturbations in both simulation and on real hardware. We further extend this\napproach to imitation learning, specifically Diffusion Policy, and empirically\nshow significant gains in visual generalization compared to state-of-the-art\nimitation learning methods. Finally, we introduce a novel technique adapted\nfrom the model equivariance literature that transforms any trained neural\nnetwork policy into one invariant to 2D planar rotations, making our policy not\nonly visually robust but also resilient to certain camera perturbations. We\nbelieve that this work marks a significant step towards manipulation policies\nthat are not only adaptable out of the box, but also robust to the complexities\nand dynamical nature of real-world deployment. Supplementary videos are\navailable at https://sites.google.com/view/vis-gen-robotics/home."}
{"id": "2505.12912", "pdf": "https://arxiv.org/pdf/2505.12912", "abs": "https://arxiv.org/abs/2505.12912", "authors": ["Kazuki Adachi", "Shin'ya Yamaguchi", "Tomoki Hamagami"], "title": "Uniformity First: Uniformity-aware Test-time Adaptation of Vision-language Models against Image Corruption", "categories": ["cs.CV"], "comment": "Code is available at https://github.com/kzkadc/uninfo", "summary": "Pre-trained vision-language models such as contrastive language-image\npre-training (CLIP) have demonstrated a remarkable generalizability, which has\nenabled a wide range of applications represented by zero-shot classification.\nHowever, vision-language models still suffer when they face datasets with large\ngaps from training ones, i.e., distribution shifts. We found that CLIP is\nespecially vulnerable to sensor degradation, a type of realistic distribution\nshift caused by sensor conditions such as weather, light, or noise. Collecting\na new dataset from a test distribution for fine-tuning highly costs since\nsensor degradation occurs unexpectedly and has a range of variety. Thus, we\ninvestigate test-time adaptation (TTA) of zero-shot classification, which\nenables on-the-fly adaptation to the test distribution with unlabeled test\ndata. Existing TTA methods for CLIP mainly focus on modifying image and text\nembeddings or predictions to address distribution shifts. Although these\nmethods can adapt to domain shifts, such as fine-grained labels spaces or\ndifferent renditions in input images, they fail to adapt to distribution shifts\ncaused by sensor degradation. We found that this is because image embeddings\nare \"corrupted\" in terms of uniformity, a measure related to the amount of\ninformation. To make models robust to sensor degradation, we propose a novel\nmethod called uniformity-aware information-balanced TTA (UnInfo). To address\nthe corruption of image embeddings, we introduce uniformity-aware confidence\nmaximization, information-aware loss balancing, and knowledge distillation from\nthe exponential moving average (EMA) teacher. Through experiments, we\ndemonstrate that our UnInfo improves accuracy under sensor degradation by\nretaining information in terms of uniformity."}
{"id": "2505.13077", "pdf": "https://arxiv.org/pdf/2505.13077", "abs": "https://arxiv.org/abs/2505.13077", "authors": ["Xiang Fei", "Jinghui Lu", "Qi Sun", "Hao Feng", "Yanjie Wang", "Wei Shi", "An-Lan Wang", "Jingqun Tang", "Can Huang"], "title": "Advancing Sequential Numerical Prediction in Autoregressive Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Autoregressive models have become the de facto choice for sequence generation\ntasks, but standard approaches treat digits as independent tokens and apply\ncross-entropy loss, overlooking the coherent structure of numerical sequences.\nThis paper introduces Numerical Token Integrity Loss (NTIL) to address this\ngap. NTIL operates at two levels: (1) token-level, where it extends the Earth\nMover's Distance (EMD) to preserve ordinal relationships between numerical\nvalues, and (2) sequence-level, where it penalizes the overall discrepancy\nbetween the predicted and actual sequences. This dual approach improves\nnumerical prediction and integrates effectively with LLMs/MLLMs. Extensive\nexperiments show significant performance improvements with NTIL."}
{"id": "2505.11725", "pdf": "https://arxiv.org/pdf/2505.11725", "abs": "https://arxiv.org/abs/2505.11725", "authors": ["Imon Banerjee", "Sayak Chakrabarty"], "title": "CLT and Edgeworth Expansion for m-out-of-n Bootstrap Estimators of The Studentized Median", "categories": ["cs.LG", "cs.AI", "cs.CE", "math.ST", "stat.ME", "stat.ML", "stat.TH"], "comment": "48 pages", "summary": "The m-out-of-n bootstrap, originally proposed by Bickel, Gotze, and Zwet\n(1992), approximates the distribution of a statistic by repeatedly drawing m\nsubsamples (with m much smaller than n) without replacement from an original\nsample of size n. It is now routinely used for robust inference with\nheavy-tailed data, bandwidth selection, and other large-sample applications.\nDespite its broad applicability across econometrics, biostatistics, and machine\nlearning, rigorous parameter-free guarantees for the soundness of the\nm-out-of-n bootstrap when estimating sample quantiles have remained elusive.\n  This paper establishes such guarantees by analyzing the estimator of sample\nquantiles obtained from m-out-of-n resampling of a dataset of size n. We first\nprove a central limit theorem for a fully data-driven version of the estimator\nthat holds under a mild moment condition and involves no unknown nuisance\nparameters. We then show that the moment assumption is essentially tight by\nconstructing a counter-example in which the CLT fails. Strengthening the\nassumptions slightly, we derive an Edgeworth expansion that provides exact\nconvergence rates and, as a corollary, a Berry Esseen bound on the bootstrap\napproximation error. Finally, we illustrate the scope of our results by\nderiving parameter-free asymptotic distributions for practical statistics,\nincluding the quantiles for random walk Metropolis-Hastings and the rewards of\nergodic Markov decision processes, thereby demonstrating the usefulness of our\ntheory in modern estimation and learning tasks."}
{"id": "2505.12935", "pdf": "https://arxiv.org/pdf/2505.12935", "abs": "https://arxiv.org/abs/2505.12935", "authors": ["Di You", "Daniel Siromani", "Pier Luigi Dragotti"], "title": "LatentINDIGO: An INN-Guided Latent Diffusion Algorithm for Image Restoration", "categories": ["cs.CV"], "comment": "Submitted to IEEE Transactions on Image Processing (TIP)", "summary": "There is a growing interest in the use of latent diffusion models (LDMs) for\nimage restoration (IR) tasks due to their ability to model effectively the\ndistribution of natural images. While significant progress has been made, there\nare still key challenges that need to be addressed. First, many approaches\ndepend on a predefined degradation operator, making them ill-suited for complex\nor unknown degradations that deviate from standard analytical models. Second,\nmany methods struggle to provide a stable guidance in the latent space and\nfinally most methods convert latent representations back to the pixel domain\nfor guidance at every sampling iteration, which significantly increases\ncomputational and memory overhead. To overcome these limitations, we introduce\na wavelet-inspired invertible neural network (INN) that simulates degradations\nthrough a forward transform and reconstructs lost details via the inverse\ntransform. We further integrate this design into a latent diffusion pipeline\nthrough two proposed approaches: LatentINDIGO-PixelINN, which operates in the\npixel domain, and LatentINDIGO-LatentINN, which stays fully in the latent space\nto reduce complexity. Both approaches alternate between updating intermediate\nlatent variables under the guidance of our INN and refining the INN forward\nmodel to handle unknown degradations. In addition, a regularization step\npreserves the proximity of latent variables to the natural image manifold.\nExperiments demonstrate that our algorithm achieves state-of-the-art\nperformance on synthetic and real-world low-quality images, and can be readily\nadapted to arbitrary output sizes."}
{"id": "2505.13089", "pdf": "https://arxiv.org/pdf/2505.13089", "abs": "https://arxiv.org/abs/2505.13089", "authors": ["Sondre Wold", "Lucas Georges Gabriel Charpentier", "Étienne Simon"], "title": "Systematic Generalization in Language Models Scales with Information Entropy", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025: Findings", "summary": "Systematic generalization remains challenging for current language models,\nwhich are known to be both sensitive to semantically similar permutations of\nthe input and to struggle with known concepts presented in novel contexts.\nAlthough benchmarks exist for assessing compositional behavior, it is unclear\nhow to measure the difficulty of a systematic generalization problem. In this\nwork, we show how one aspect of systematic generalization can be described by\nthe entropy of the distribution of component parts in the training data. We\nformalize a framework for measuring entropy in a sequence-to-sequence task and\nfind that the performance of popular model architectures scales with the\nentropy. Our work connects systematic generalization to information efficiency,\nand our results indicate that success at high entropy can be achieved even\nwithout built-in priors, and that success at low entropy can serve as a target\nfor assessing progress towards robust systematic generalization."}
{"id": "2505.11731", "pdf": "https://arxiv.org/pdf/2505.11731", "abs": "https://arxiv.org/abs/2505.11731", "authors": ["Harshil Vejendla", "Haizhou Shi", "Yibin Wang", "Tunyu Zhang", "Huan Zhang", "Hao Wang"], "title": "Efficient Uncertainty Estimation via Distillation of Bayesian Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Preprint; work in progress", "summary": "Recent advances in uncertainty estimation for Large Language Models (LLMs)\nduring downstream adaptation have addressed key challenges of reliability and\nsimplicity. However, existing Bayesian methods typically require multiple\nsampling iterations during inference, creating significant efficiency issues\nthat limit practical deployment. In this paper, we investigate the possibility\nof eliminating the need for test-time sampling for LLM uncertainty estimation.\nSpecifically, when given an off-the-shelf Bayesian LLM, we distill its aligned\nconfidence into a non-Bayesian student LLM by minimizing the divergence between\ntheir predictive distributions. Unlike typical calibration methods, our\ndistillation is carried out solely on the training dataset without the need of\nan additional validation dataset. This simple yet effective approach achieves\nN-times more efficient uncertainty estimation during testing, where N is the\nnumber of samples traditionally required by Bayesian LLMs. Our extensive\nexperiments demonstrate that uncertainty estimation capabilities on training\ndata can successfully generalize to unseen test data through our distillation\ntechnique, consistently producing results comparable to (or even better than)\nstate-of-the-art Bayesian LLMs."}
{"id": "2505.12966", "pdf": "https://arxiv.org/pdf/2505.12966", "abs": "https://arxiv.org/abs/2505.12966", "authors": ["Zihan Xiong", "Xiaohua Wu", "Lei Chen", "Fangqi Lou"], "title": "Multiscale Adaptive Conflict-Balancing Model For Multimedia Deepfake Detection", "categories": ["cs.CV", "cs.AI"], "comment": "9 pages,ICMR accepted", "summary": "Advances in computer vision and deep learning have blurred the line between\ndeepfakes and authentic media, undermining multimedia credibility through\naudio-visual forgery. Current multimodal detection methods remain limited by\nunbalanced learning between modalities. To tackle this issue, we propose an\nAudio-Visual Joint Learning Method (MACB-DF) to better mitigate modality\nconflicts and neglect by leveraging contrastive learning to assist in\nmulti-level and cross-modal fusion, thereby fully balancing and exploiting\ninformation from each modality. Additionally, we designed an\northogonalization-multimodal pareto module that preserves unimodal information\nwhile addressing gradient conflicts in audio-video encoders caused by differing\noptimization targets of the loss functions. Extensive experiments and ablation\nstudies conducted on mainstream deepfake datasets demonstrate consistent\nperformance gains of our model across key evaluation metrics, achieving an\naverage accuracy of 95.5% across multiple datasets. Notably, our method\nexhibits superior cross-dataset generalization capabilities, with absolute\nimprovements of 8.0% and 7.7% in ACC scores over the previous best-performing\napproach when trained on DFDC and tested on DefakeAVMiT and FakeAVCeleb\ndatasets."}
{"id": "2505.13090", "pdf": "https://arxiv.org/pdf/2505.13090", "abs": "https://arxiv.org/abs/2505.13090", "authors": ["David Stap", "Christof Monz"], "title": "The Effect of Language Diversity When Fine-Tuning Large Language Models for Translation", "categories": ["cs.CL"], "comment": null, "summary": "Prior research diverges on language diversity in LLM fine-tuning: Some\nstudies report benefits while others find no advantages. Through controlled\nfine-tuning experiments across 132 translation directions, we systematically\nresolve these disparities. We find that expanding language diversity during\nfine-tuning improves translation quality for both unsupervised and --\nsurprisingly -- supervised pairs, despite less diverse models being fine-tuned\nexclusively on these supervised pairs. However, benefits plateau or decrease\nbeyond a certain diversity threshold. We show that increased language diversity\ncreates more language-agnostic representations. These representational\nadaptations help explain the improved performance in models fine-tuned with\ngreater diversity."}
{"id": "2505.11737", "pdf": "https://arxiv.org/pdf/2505.11737", "abs": "https://arxiv.org/abs/2505.11737", "authors": ["Tunyu Zhang", "Haizhou Shi", "Yibin Wang", "Hengyi Wang", "Xiaoxiao He", "Zhuowei Li", "Haoxian Chen", "Ligong Han", "Kai Xu", "Huan Zhang", "Dimitris Metaxas", "Hao Wang"], "title": "Token-Level Uncertainty Estimation for Large Language Model Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Preprint; Work in progress", "summary": "While Large Language Models (LLMs) have demonstrated impressive capabilities,\ntheir output quality remains inconsistent across various application scenarios,\nmaking it difficult to identify trustworthy responses, especially in complex\ntasks requiring multi-step reasoning. In this paper, we propose a token-level\nuncertainty estimation framework to enable LLMs to self-assess and self-improve\ntheir generation quality in mathematical reasoning. Specifically, we introduce\nlow-rank random weight perturbation to LLM decoding, generating predictive\ndistributions that we use to estimate token-level uncertainties. We then\naggregate these uncertainties to reflect semantic uncertainty of the generated\nsequences. Experiments on mathematical reasoning datasets of varying difficulty\ndemonstrate that our token-level uncertainty metrics strongly correlate with\nanswer correctness and model robustness. Additionally, we explore using\nuncertainty to directly enhance the model's reasoning performance through\nmultiple generations and the particle filtering algorithm. Our approach\nconsistently outperforms existing uncertainty estimation methods, establishing\neffective uncertainty estimation as a valuable tool for both evaluating and\nimproving reasoning generation in LLMs."}
{"id": "2505.12998", "pdf": "https://arxiv.org/pdf/2505.12998", "abs": "https://arxiv.org/abs/2505.12998", "authors": ["Vinkle Srivastav", "Juliette Puel", "Jonathan Vappou", "Elijah Van Houten", "Paolo Cabras", "Nicolas Padoy"], "title": "A Skull-Adaptive Framework for AI-Based 3D Transcranial Focused Ultrasound Simulation", "categories": ["cs.CV"], "comment": "The project page is available at\n  https://github.com/CAMMA-public/TFUScapes", "summary": "Transcranial focused ultrasound (tFUS) is an emerging modality for\nnon-invasive brain stimulation and therapeutic intervention, offering\nmillimeter-scale spatial precision and the ability to target deep brain\nstructures. However, the heterogeneous and anisotropic nature of the human\nskull introduces significant distortions to the propagating ultrasound\nwavefront, which require time-consuming patient-specific planning and\ncorrections using numerical solvers for accurate targeting. To enable\ndata-driven approaches in this domain, we introduce TFUScapes, the first\nlarge-scale, high-resolution dataset of tFUS simulations through anatomically\nrealistic human skulls derived from T1-weighted MRI images. We have developed a\nscalable simulation engine pipeline using the k-Wave pseudo-spectral solver,\nwhere each simulation returns a steady-state pressure field generated by a\nfocused ultrasound transducer placed at realistic scalp locations. In addition\nto the dataset, we present DeepTFUS, a deep learning model that estimates\nnormalized pressure fields directly from input 3D CT volumes and transducer\nposition. The model extends a U-Net backbone with transducer-aware\nconditioning, incorporating Fourier-encoded position embeddings and MLP layers\nto create global transducer embeddings. These embeddings are fused with U-Net\nencoder features via feature-wise modulation, dynamic convolutions, and\ncross-attention mechanisms. The model is trained using a combination of\nspatially weighted and gradient-sensitive loss functions, enabling it to\napproximate high-fidelity wavefields. The TFUScapes dataset is publicly\nreleased to accelerate research at the intersection of computational acoustics,\nneurotechnology, and deep learning. The project page is available at\nhttps://github.com/CAMMA-public/TFUScapes."}
{"id": "2505.13115", "pdf": "https://arxiv.org/pdf/2505.13115", "abs": "https://arxiv.org/abs/2505.13115", "authors": ["Debarpan Bhattacharya", "Apoorva Kulkarni", "Sriram Ganapathy"], "title": "Benchmarking and Confidence Evaluation of LALMs For Temporal Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": "Accepted in INTERSPEECH, 2025, Rotterdam, The Netherlands", "summary": "The popular success of text-based large language models (LLM) has streamlined\nthe attention of the multimodal community to combine other modalities like\nvision and audio along with text to achieve similar multimodal capabilities. In\nthis quest, large audio language models (LALMs) have to be evaluated on\nreasoning related tasks which are different from traditional classification or\ngeneration tasks. Towards this goal, we propose a novel dataset called temporal\nreasoning evaluation of audio (TREA).\n  We benchmark open-source LALMs and observe that they are consistently behind\nhuman capabilities on the tasks in the TREA dataset. While evaluating LALMs, we\nalso propose an uncertainty metric, which computes the invariance of the model\nto semantically identical perturbations of the input. Our analysis shows that\nthe accuracy and uncertainty metrics are not necessarily correlated and thus,\npoints to a need for wholesome evaluation of LALMs for high-stakes\napplications."}
{"id": "2505.11739", "pdf": "https://arxiv.org/pdf/2505.11739", "abs": "https://arxiv.org/abs/2505.11739", "authors": ["Feijiang Han", "Xiaodong Yu", "Jianheng Tang", "Lyle Ungar"], "title": "ZeroTuning: Unlocking the Initial Token's Power to Enhance Large Language Models Without Training", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recently, training-free methods for improving large language models (LLMs)\nhave attracted growing interest, with token-level attention tuning emerging as\na promising and interpretable direction. However, existing methods typically\nrely on auxiliary mechanisms to identify important or irrelevant task-specific\ntokens, introducing potential bias and limiting applicability. In this paper,\nwe uncover a surprising and elegant alternative: the semantically empty initial\ntoken is a powerful and underexplored control point for optimizing model\nbehavior. Through theoretical analysis, we show that tuning the initial token's\nattention sharpens or flattens the attention distribution over subsequent\ntokens, and its role as an attention sink amplifies this effect. Empirically,\nwe find that: (1) tuning its attention improves LLM performance more\neffectively than tuning other task-specific tokens; (2) the effect follows a\nconsistent trend across layers, with earlier layers having greater impact, but\nvaries across attention heads, with different heads showing distinct\npreferences in how they attend to this token. Based on these findings, we\npropose ZeroTuning, a training-free approach that improves LLM performance by\napplying head-specific attention adjustments to this special token. Despite\ntuning only one token, ZeroTuning achieves higher performance on text\nclassification, multiple-choice, and multi-turn conversation tasks across\nmodels such as Llama, Qwen, and DeepSeek. For example, ZeroTuning improves\nLlama-3.1-8B by 11.71% on classification, 2.64% on QA tasks, and raises its\nmulti-turn score from 7.804 to 7.966. The method is also robust to limited\nresources, few-shot settings, long contexts, quantization, decoding strategies,\nand prompt variations. Our work sheds light on a previously overlooked control\npoint in LLMs, offering new insights into both inference-time tuning and model\ninterpretability."}
{"id": "2505.13023", "pdf": "https://arxiv.org/pdf/2505.13023", "abs": "https://arxiv.org/abs/2505.13023", "authors": ["Yimao Guo", "Zuomin Qu", "Wei Lu", "Xiangyang Luo"], "title": "Anti-Inpainting: A Proactive Defense against Malicious Diffusion-based Inpainters under Unknown Conditions", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": null, "summary": "As diffusion-based malicious image manipulation becomes increasingly\nprevalent, multiple proactive defense methods are developed to safeguard images\nagainst unauthorized tampering. However, most proactive defense methods only\ncan safeguard images against manipulation under known conditions, and fail to\nprotect images from manipulations guided by tampering conditions crafted by\nmalicious users. To tackle this issue, we propose Anti-Inpainting, a proactive\ndefense method that achieves adequate protection under unknown conditions\nthrough a triple mechanism to address this challenge. Specifically, a\nmulti-level deep feature extractor is presented to obtain intricate features\nduring the diffusion denoising process to improve protective effectiveness. We\ndesign multi-scale semantic-preserving data augmentation to enhance the\ntransferability of adversarial perturbations across unknown conditions by\nmulti-scale transformations while preserving semantic integrity. In addition,\nwe propose a selection-based distribution deviation optimization strategy to\nimprove the protection of adversarial perturbation against manipulation under\ndiverse random seeds. Extensive experiments indicate the proactive defensive\nperformance of Anti-Inpainting against diffusion-based inpainters guided by\nunknown conditions in InpaintGuardBench and CelebA-HQ. At the same time, we\nalso demonstrate the proposed approach's robustness under various image\npurification methods and its transferability across different versions of\ndiffusion models."}
{"id": "2505.13136", "pdf": "https://arxiv.org/pdf/2505.13136", "abs": "https://arxiv.org/abs/2505.13136", "authors": ["Anton Ehrmanntraut", "Julia Wunderle", "Jan Pfister", "Fotis Jannidis", "Andreas Hotho"], "title": "ModernGBERT: German-only 1B Encoder Model Trained from Scratch", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "under review @ARR", "summary": "Despite the prominence of decoder-only language models, encoders remain\ncrucial for resource-constrained applications. We introduce ModernGBERT (134M,\n1B), a fully transparent family of German encoder models trained from scratch,\nincorporating architectural innovations from ModernBERT. To evaluate the\npractical trade-offs of training encoders from scratch, we also present\nLL\\\"aMmlein2Vec (120M, 1B, 7B), a family of encoders derived from German\ndecoder-only models via LLM2Vec. We benchmark all models on natural language\nunderstanding, text embedding, and long-context reasoning tasks, enabling a\ncontrolled comparison between dedicated encoders and converted decoders. Our\nresults show that ModernGBERT 1B outperforms prior state-of-the-art German\nencoders as well as encoders adapted via LLM2Vec, with regard to performance\nand parameter-efficiency. All models, training data, checkpoints and code are\npublicly available, advancing the German NLP ecosystem with transparent,\nhigh-performance encoder models."}
{"id": "2505.11740", "pdf": "https://arxiv.org/pdf/2505.11740", "abs": "https://arxiv.org/abs/2505.11740", "authors": ["Alberto Sinigaglia", "Davide Sartor", "Marina Ceccon", "Gian Antonio Susto"], "title": "Simple and Effective Specialized Representations for Fair Classifiers", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Fair classification is a critical challenge that has gained increasing\nimportance due to international regulations and its growing use in high-stakes\ndecision-making settings. Existing methods often rely on adversarial learning\nor distribution matching across sensitive groups; however, adversarial learning\ncan be unstable, and distribution matching can be computationally intensive. To\naddress these limitations, we propose a novel approach based on the\ncharacteristic function distance. Our method ensures that the learned\nrepresentation contains minimal sensitive information while maintaining high\neffectiveness for downstream tasks. By utilizing characteristic functions, we\nachieve a more stable and efficient solution compared to traditional methods.\nAdditionally, we introduce a simple relaxation of the objective function that\nguarantees fairness in common classification models with no performance\ndegradation. Experimental results on benchmark datasets demonstrate that our\napproach consistently matches or achieves better fairness and predictive\naccuracy than existing methods. Moreover, our method maintains robustness and\ncomputational efficiency, making it a practical solution for real-world\napplications."}
{"id": "2505.13039", "pdf": "https://arxiv.org/pdf/2505.13039", "abs": "https://arxiv.org/abs/2505.13039", "authors": ["Xiao Wu", "Xiaoqing Zhang", "Zunjie Xiao", "Lingxi Hu", "Risa Higashita", "Jiang Liu"], "title": "Expert-Like Reparameterization of Heterogeneous Pyramid Receptive Fields in Efficient CNNs for Fair Medical Image Classification", "categories": ["cs.CV"], "comment": null, "summary": "Efficient convolutional neural network (CNN) architecture designs have\nattracted growing research interests. However, they usually apply single\nreceptive field (RF), small asymmetric RFs, or pyramid RFs to learn different\nfeature representations, still encountering two significant challenges in\nmedical image classification tasks: 1) They have limitations in capturing\ndiverse lesion characteristics efficiently, e.g., tiny, coordination, small and\nsalient, which have unique roles on results, especially imbalanced medical\nimage classification. 2) The predictions generated by those CNNs are often\nunfair/biased, bringing a high risk by employing them to real-world medical\ndiagnosis conditions. To tackle these issues, we develop a new concept,\nExpert-Like Reparameterization of Heterogeneous Pyramid Receptive Fields\n(ERoHPRF), to simultaneously boost medical image classification performance and\nfairness. This concept aims to mimic the multi-expert consultation mode by\napplying the well-designed heterogeneous pyramid RF bags to capture different\nlesion characteristics effectively via convolution operations with multiple\nheterogeneous kernel sizes. Additionally, ERoHPRF introduces an expert-like\nstructural reparameterization technique to merge its parameters with the\ntwo-stage strategy, ensuring competitive computation cost and inference speed\nthrough comparisons to a single RF. To manifest the effectiveness and\ngeneralization ability of ERoHPRF, we incorporate it into mainstream efficient\nCNN architectures. The extensive experiments show that our method maintains a\nbetter trade-off than state-of-the-art methods in terms of medical image\nclassification, fairness, and computation overhead. The codes of this paper\nwill be released soon."}
{"id": "2505.13141", "pdf": "https://arxiv.org/pdf/2505.13141", "abs": "https://arxiv.org/abs/2505.13141", "authors": ["Zheng Wei Lim", "Alham Fikri Aji", "Trevor Cohn"], "title": "Understanding Cross-Lingual Inconsistency in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are demonstrably capable of cross-lingual\ntransfer, but can produce inconsistent output when prompted with the same\nqueries written in different languages. To understand how language models are\nable to generalize knowledge from one language to the others, we apply the\nlogit lens to interpret the implicit steps taken by LLMs to solve multilingual\nmulti-choice reasoning questions. We find LLMs predict inconsistently and are\nless accurate because they rely on subspaces of individual languages, rather\nthan working in a shared semantic space. While larger models are more\nmultilingual, we show their hidden states are more likely to dissociate from\nthe shared representation compared to smaller models, but are nevertheless more\ncapable of retrieving knowledge embedded across different languages. Finally,\nwe demonstrate that knowledge sharing can be modulated by steering the models'\nlatent processing towards the shared semantic space. We find reinforcing\nutilization of the shared space improves the models' multilingual reasoning\nperformance, as a result of more knowledge transfer from, and better output\nconsistency with English."}
{"id": "2505.11743", "pdf": "https://arxiv.org/pdf/2505.11743", "abs": "https://arxiv.org/abs/2505.11743", "authors": ["Cheng Ji", "Huaiying Luo"], "title": "Cloud-Based AI Systems: Leveraging Large Language Models for Intelligent Fault Detection and Autonomous Self-Healing", "categories": ["cs.DC", "cs.AI"], "comment": null, "summary": "With the rapid development of cloud computing systems and the increasing\ncomplexity of their infrastructure, intelligent mechanisms to detect and\nmitigate failures in real time are becoming increasingly important. Traditional\nmethods of failure detection are often difficult to cope with the scale and\ndynamics of modern cloud environments. In this study, we propose a novel AI\nframework based on Massive Language Model (LLM) for intelligent fault detection\nand self-healing mechanisms in cloud systems. The model combines existing\nmachine learning fault detection algorithms with LLM's natural language\nunderstanding capabilities to process and parse system logs, error reports, and\nreal-time data streams through semantic context. The method adopts a\nmulti-level architecture, combined with supervised learning for fault\nclassification and unsupervised learning for anomaly detection, so that the\nsystem can predict potential failures before they occur and automatically\ntrigger the self-healing mechanism. Experimental results show that the proposed\nmodel is significantly better than the traditional fault detection system in\nterms of fault detection accuracy, system downtime reduction and recovery\nspeed."}
{"id": "2505.13043", "pdf": "https://arxiv.org/pdf/2505.13043", "abs": "https://arxiv.org/abs/2505.13043", "authors": ["Hao-Ran Yang", "Xiaohui Chen", "Chuan-Xian Ren"], "title": "A Generalized Label Shift Perspective for Cross-Domain Gaze Estimation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Aiming to generalize the well-trained gaze estimation model to new target\ndomains, Cross-domain Gaze Estimation (CDGE) is developed for real-world\napplication scenarios. Existing CDGE methods typically extract the\ndomain-invariant features to mitigate domain shift in feature space, which is\nproved insufficient by Generalized Label Shift (GLS) theory. In this paper, we\nintroduce a novel GLS perspective to CDGE and modelize the cross-domain problem\nby label and conditional shift problem. A GLS correction framework is presented\nand a feasible realization is proposed, in which a importance reweighting\nstrategy based on truncated Gaussian distribution is introduced to overcome the\ncontinuity challenges in label shift correction. To embed the reweighted source\ndistribution to conditional invariant learning, we further derive a\nprobability-aware estimation of conditional operator discrepancy. Extensive\nexperiments on standard CDGE tasks with different backbone models validate the\nsuperior generalization capability across domain and applicability on various\nmodels of proposed method."}
{"id": "2505.13147", "pdf": "https://arxiv.org/pdf/2505.13147", "abs": "https://arxiv.org/abs/2505.13147", "authors": ["Aswathy Velutharambath", "Roman Klinger", "Kai Sassenberg"], "title": "What if Deception Cannot be Detected? A Cross-Linguistic Study on the Limits of Deception Detection from Text", "categories": ["cs.CL"], "comment": null, "summary": "Can deception be detected solely from written text? Cues of deceptive\ncommunication are inherently subtle, even more so in text-only communication.\nYet, prior studies have reported considerable success in automatic deception\ndetection. We hypothesize that such findings are largely driven by artifacts\nintroduced during data collection and do not generalize beyond specific\ndatasets. We revisit this assumption by introducing a belief-based deception\nframework, which defines deception as a misalignment between an author's claims\nand true beliefs, irrespective of factual accuracy, allowing deception cues to\nbe studied in isolation. Based on this framework, we construct three corpora,\ncollectively referred to as DeFaBel, including a German-language corpus of\ndeceptive and non-deceptive arguments and a multilingual version in German and\nEnglish, each collected under varying conditions to account for belief change\nand enable cross-linguistic analysis. Using these corpora, we evaluate commonly\nreported linguistic cues of deception. Across all three DeFaBel variants, these\ncues show negligible, statistically insignificant correlations with deception\nlabels, contrary to prior work that treats such cues as reliable indicators. We\nfurther benchmark against other English deception datasets following similar\ndata collection protocols. While some show statistically significant\ncorrelations, effect sizes remain low and, critically, the set of predictive\ncues is inconsistent across datasets. We also evaluate deception detection\nusing feature-based models, pretrained language models, and instruction-tuned\nlarge language models. While some models perform well on established deception\ndatasets, they consistently perform near chance on DeFaBel. Our findings\nchallenge the assumption that deception can be reliably inferred from\nlinguistic cues and call for rethinking how deception is studied and modeled in\nNLP."}
{"id": "2505.11745", "pdf": "https://arxiv.org/pdf/2505.11745", "abs": "https://arxiv.org/abs/2505.11745", "authors": ["Joshua Inman", "Tanmay Khandait", "Lalitha Sankar", "Giulia Pedrielli"], "title": "POCAII: Parameter Optimization with Conscious Allocation using Iterative Intelligence", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "21 pages, 4 figures", "summary": "In this paper we propose for the first time the hyperparameter optimization\n(HPO) algorithm POCAII. POCAII differs from the Hyperband and Successive\nHalving literature by explicitly separating the search and evaluation phases\nand utilizing principled approaches to exploration and exploitation principles\nduring both phases. Such distinction results in a highly flexible scheme for\nmanaging a hyperparameter optimization budget by focusing on search (i.e.,\ngenerating competing configurations) towards the start of the HPO process while\nincreasing the evaluation effort as the HPO comes to an end.\n  POCAII was compared to state of the art approaches SMAC, BOHB and DEHB. Our\nalgorithm shows superior performance in low-budget hyperparameter optimization\nregimes. Since many practitioners do not have exhaustive resources to assign to\nHPO, it has wide applications to real-world problems. Moreover, the empirical\nevidence showed how POCAII demonstrates higher robustness and lower variance in\nthe results. This is again very important when considering realistic scenarios\nwith extremely expensive models to train."}
{"id": "2505.13050", "pdf": "https://arxiv.org/pdf/2505.13050", "abs": "https://arxiv.org/abs/2505.13050", "authors": ["Beibei Lin", "Zifeng Yuan", "Tingting Chen"], "title": "RGB-to-Polarization Estimation: A New Task and Benchmark Study", "categories": ["cs.CV"], "comment": null, "summary": "Polarization images provide rich physical information that is fundamentally\nabsent from standard RGB images, benefiting a wide range of computer vision\napplications such as reflection separation and material classification.\nHowever, the acquisition of polarization images typically requires additional\noptical components, which increases both the cost and the complexity of the\napplications. To bridge this gap, we introduce a new task: RGB-to-polarization\nimage estimation, which aims to infer polarization information directly from\nRGB images. In this work, we establish the first comprehensive benchmark for\nthis task by leveraging existing polarization datasets and evaluating a diverse\nset of state-of-the-art deep learning models, including both\nrestoration-oriented and generative architectures. Through extensive\nquantitative and qualitative analysis, our benchmark not only establishes the\ncurrent performance ceiling of RGB-to-polarization estimation, but also\nsystematically reveals the respective strengths and limitations of different\nmodel families -- such as direct reconstruction versus generative synthesis,\nand task-specific training versus large-scale pre-training. In addition, we\nprovide some potential directions for future research on polarization\nestimation. This benchmark is intended to serve as a foundational resource to\nfacilitate the design and evaluation of future methods for polarization\nestimation from standard RGB inputs."}
{"id": "2505.13156", "pdf": "https://arxiv.org/pdf/2505.13156", "abs": "https://arxiv.org/abs/2505.13156", "authors": ["Zhi Liu", "Tao Yang", "Jing Wang", "Yexin Chen", "Zhan Gao", "Jiaxi Yang", "Kui Chen", "Bingji Lu", "Xiaochen Li", "Changyong Luo", "Yan Li", "Xiaohong Gu", "Peng Cao"], "title": "Tianyi: A Traditional Chinese Medicine all-rounder language model and its Real-World Clinical Practice", "categories": ["cs.CL", "cs.AI"], "comment": "23 pages, 4 figures, and 1 tables", "summary": "Natural medicines, particularly Traditional Chinese Medicine (TCM), are\ngaining global recognition for their therapeutic potential in addressing human\nsymptoms and diseases. TCM, with its systematic theories and extensive\npractical experience, provides abundant resources for healthcare. However, the\neffective application of TCM requires precise syndrome diagnosis, determination\nof treatment principles, and prescription formulation, which demand decades of\nclinical expertise. Despite advancements in TCM-based decision systems, machine\nlearning, and deep learning research, limitations in data and single-objective\nconstraints hinder their practical application. In recent years, large language\nmodels (LLMs) have demonstrated potential in complex tasks, but lack\nspecialization in TCM and face significant challenges, such as too big model\nscale to deploy and issues with hallucination. To address these challenges, we\nintroduce Tianyi with 7.6-billion-parameter LLM, a model scale proper and\nspecifically designed for TCM, pre-trained and fine-tuned on diverse TCM\ncorpora, including classical texts, expert treatises, clinical records, and\nknowledge graphs. Tianyi is designed to assimilate interconnected and\nsystematic TCM knowledge through a progressive learning manner. Additionally,\nwe establish TCMEval, a comprehensive evaluation benchmark, to assess LLMs in\nTCM examinations, clinical tasks, domain-specific question-answering, and\nreal-world trials. The extensive evaluations demonstrate the significant\npotential of Tianyi as an AI assistant in TCM clinical practice and research,\nbridging the gap between TCM knowledge and practical application."}
{"id": "2505.11746", "pdf": "https://arxiv.org/pdf/2505.11746", "abs": "https://arxiv.org/abs/2505.11746", "authors": ["Xianglong Xu", "John Bowen", "Rojin Taheri"], "title": "Token Masking Improves Transformer-Based Text Classification", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While transformer-based models achieve strong performance on text\nclassification, we explore whether masking input tokens can further enhance\ntheir effectiveness. We propose token masking regularization, a simple yet\ntheoretically motivated method that randomly replaces input tokens with a\nspecial [MASK] token at probability p. This introduces stochastic perturbations\nduring training, leading to implicit gradient averaging that encourages the\nmodel to capture deeper inter-token dependencies. Experiments on language\nidentification and sentiment analysis -- across diverse models (mBERT,\nQwen2.5-0.5B, TinyLlama-1.1B) -- show consistent improvements over standard\nregularization techniques. We identify task-specific optimal masking rates,\nwith p = 0.1 as a strong general default. We attribute the gains to two key\neffects: (1) input perturbation reduces overfitting, and (2) gradient-level\nsmoothing acts as implicit ensembling."}
{"id": "2505.13061", "pdf": "https://arxiv.org/pdf/2505.13061", "abs": "https://arxiv.org/abs/2505.13061", "authors": ["CHengtang Yao", "Zhidan Liu", "Jiaxi Zeng", "Lidong Yu", "Yuwei Wu", "Yunde Jia"], "title": "3D Visual Illusion Depth Estimation", "categories": ["cs.CV"], "comment": "Project:\n  https://github.com/YaoChengTang/3D-Visual-Illusion-Depth-Estimation", "summary": "3D visual illusion is a perceptual phenomenon where a two-dimensional plane\nis manipulated to simulate three-dimensional spatial relationships, making a\nflat artwork or object look three-dimensional in the human visual system. In\nthis paper, we reveal that the machine visual system is also seriously fooled\nby 3D visual illusions, including monocular and binocular depth estimation. In\norder to explore and analyze the impact of 3D visual illusion on depth\nestimation, we collect a large dataset containing almost 3k scenes and 200k\nimages to train and evaluate SOTA monocular and binocular depth estimation\nmethods. We also propose a robust depth estimation framework that uses common\nsense from a vision-language model to adaptively select reliable depth from\nbinocular disparity and monocular depth. Experiments show that SOTA monocular,\nbinocular, and multi-view depth estimation approaches are all fooled by various\n3D visual illusions, while our method achieves SOTA performance."}
{"id": "2505.13157", "pdf": "https://arxiv.org/pdf/2505.13157", "abs": "https://arxiv.org/abs/2505.13157", "authors": ["Yassine El Boudouri", "Walter Nuninger", "Julian Alvarez", "Yvan Peter"], "title": "Role-Playing Evaluation for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) demonstrate a notable capacity for adopting\npersonas and engaging in role-playing. However, evaluating this ability\npresents significant challenges, as human assessments are resource-intensive\nand automated evaluations can be biased. To address this, we introduce\nRole-Playing Eval (RPEval), a novel benchmark designed to assess LLM\nrole-playing capabilities across four key dimensions: emotional understanding,\ndecision-making, moral alignment, and in-character consistency. This article\ndetails the construction of RPEval and presents baseline evaluations. Our code\nand dataset are available at https://github.com/yelboudouri/RPEval"}
{"id": "2505.11750", "pdf": "https://arxiv.org/pdf/2505.11750", "abs": "https://arxiv.org/abs/2505.11750", "authors": ["Zhanxiang Hua", "Ryan Sobash", "David John Gagne II", "Yingkai Sha", "Alexandra Anderson-Frey"], "title": "Improving Medium Range Severe Weather Prediction through Transformer Post-processing of AI Weather Forecasts", "categories": ["physics.ao-ph", "cs.AI", "cs.LG"], "comment": "16 pages, 10 figures", "summary": "Improving the skill of medium-range (1-8 day) severe weather prediction is\ncrucial for mitigating societal impacts. This study introduces a novel approach\nleveraging decoder-only transformer networks to post-process AI-based weather\nforecasts, specifically from the Pangu-Weather model, for improved severe\nweather guidance. Unlike traditional post-processing methods that use a dense\nneural network to predict the probability of severe weather using discrete\nforecast samples, our method treats forecast lead times as sequential\n``tokens'', enabling the transformer to learn complex temporal relationships\nwithin the evolving atmospheric state. We compare this approach against\npost-processing of the Global Forecast System (GFS) using both a traditional\ndense neural network and our transformer, as well as configurations that\nexclude convective parameters to fairly evaluate the impact of using the\nPangu-Weather AI model. Results demonstrate that the transformer-based\npost-processing significantly enhances forecast skill compared to dense neural\nnetworks. Furthermore, AI-driven forecasts, particularly Pangu-Weather\ninitialized from high resolution analysis, exhibit superior performance to GFS\nin the medium-range, even without explicit convective parameters. Our approach\noffers improved accuracy, and reliability, which also provides interpretability\nthrough feature attribution analysis, advancing medium-range severe weather\nprediction capabilities."}
{"id": "2505.13088", "pdf": "https://arxiv.org/pdf/2505.13088", "abs": "https://arxiv.org/abs/2505.13088", "authors": ["Zhaoyi Wang", "Shengyu Huang", "Jemil Avers Butt", "Yuanzhou Cai", "Matej Varga", "Andreas Wieser"], "title": "Cross-modal feature fusion for robust point cloud registration with ambiguous geometry", "categories": ["cs.CV", "cs.LG"], "comment": "To appear in the ISPRS Journal of Photogrammetry and Remote Sensing.\n  19 pages, 14 figures", "summary": "Point cloud registration has seen significant advancements with the\napplication of deep learning techniques. However, existing approaches often\noverlook the potential of integrating radiometric information from RGB images.\nThis limitation reduces their effectiveness in aligning point clouds pairs,\nespecially in regions where geometric data alone is insufficient. When used\neffectively, radiometric information can enhance the registration process by\nproviding context that is missing from purely geometric data. In this paper, we\npropose CoFF, a novel Cross-modal Feature Fusion method that utilizes both\npoint cloud geometry and RGB images for pairwise point cloud registration.\nAssuming that the co-registration between point clouds and RGB images is\navailable, CoFF explicitly addresses the challenges where geometric information\nalone is unclear, such as in regions with symmetric similarity or planar\nstructures, through a two-stage fusion of 3D point cloud features and 2D image\nfeatures. It incorporates a cross-modal feature fusion module that assigns\npixel-wise image features to 3D input point clouds to enhance learned 3D point\nfeatures, and integrates patch-wise image features with superpoint features to\nimprove the quality of coarse matching. This is followed by a coarse-to-fine\nmatching module that accurately establishes correspondences using the fused\nfeatures. We extensively evaluate CoFF on four common datasets: 3DMatch,\n3DLoMatch, IndoorLRS, and the recently released ScanNet++ datasets. In\naddition, we assess CoFF on specific subset datasets containing geometrically\nambiguous cases. Our experimental results demonstrate that CoFF achieves\nstate-of-the-art registration performance across all benchmarks, including\nremarkable registration recalls of 95.9% and 81.6% on the widely-used 3DMatch\nand 3DLoMatch datasets, respectively...(Truncated to fit arXiv abstract length)"}
{"id": "2505.13171", "pdf": "https://arxiv.org/pdf/2505.13171", "abs": "https://arxiv.org/abs/2505.13171", "authors": ["Yixuan Xu", "Antoine Bosselut", "Imanol Schlag"], "title": "Positional Fragility in LLMs: How Offset Effects Reshape Our Understanding of Memorization Risks", "categories": ["cs.CL"], "comment": null, "summary": "Large language models are known to memorize parts of their training data,\nposing risk of copyright violations. To systematically examine this risk, we\npretrain language models (1B/3B/8B) from scratch on 83B tokens, mixing\nweb-scale data with public domain books used to simulate copyrighted content at\ncontrolled frequencies at lengths at least ten times longer than prior work. We\nthereby identified the offset effect, a phenomenon characterized by two key\nfindings: (1) verbatim memorization is most strongly triggered by short\nprefixes drawn from the beginning of the context window, with memorization\ndecreasing counterintuitively as prefix length increases; and (2) a sharp\ndecline in verbatim recall when prefix begins offset from the initial tokens of\nthe context window. We attribute this to positional fragility: models rely\ndisproportionately on the earliest tokens in their context window as retrieval\nanchors, making them sensitive to even slight shifts. We further observe that\nwhen the model fails to retrieve memorized content, it often produces\ndegenerated text. Leveraging these findings, we show that shifting sensitive\ndata deeper into the context window suppresses both extractable memorization\nand degeneration. Our results suggest that positional offset is a critical and\npreviously overlooked axis for evaluating memorization risks, since prior work\nimplicitly assumed uniformity by probing only from the beginning of training\nsequences."}
{"id": "2505.11755", "pdf": "https://arxiv.org/pdf/2505.11755", "abs": "https://arxiv.org/abs/2505.11755", "authors": ["Matthew Kim", "William Sharpless", "Hyun Joe Jeong", "Sander Tonkens", "Somil Bansal", "Sylvia Herbert"], "title": "Reachability Barrier Networks: Learning Hamilton-Jacobi Solutions for Smooth and Flexible Control Barrier Functions", "categories": ["cs.RO", "cs.AI"], "comment": "15 pages, 7 figures", "summary": "Recent developments in autonomous driving and robotics underscore the\nnecessity of safety-critical controllers. Control barrier functions (CBFs) are\na popular method for appending safety guarantees to a general control\nframework, but they are notoriously difficult to generate beyond low\ndimensions. Existing methods often yield non-differentiable or inaccurate\napproximations that lack integrity, and thus fail to ensure safety. In this\nwork, we use physics-informed neural networks (PINNs) to generate smooth\napproximations of CBFs by computing Hamilton-Jacobi (HJ) optimal control\nsolutions. These reachability barrier networks (RBNs) avoid traditional\ndimensionality constraints and support the tuning of their conservativeness\npost-training through a parameterized discount term. To ensure robustness of\nthe discounted solutions, we leverage conformal prediction methods to derive\nprobabilistic safety guarantees for RBNs. We demonstrate that RBNs are highly\naccurate in low dimensions, and safer than the standard neural CBF approach in\nhigh dimensions. Namely, we showcase the RBNs in a 9D multi-vehicle collision\navoidance problem where it empirically proves to be 5.5x safer and 1.9x less\nconservative than the neural CBFs, offering a promising method to synthesize\nCBFs for general nonlinear autonomous systems."}
{"id": "2505.13091", "pdf": "https://arxiv.org/pdf/2505.13091", "abs": "https://arxiv.org/abs/2505.13091", "authors": ["Yuanbo Wang", "Zhaoxuan Zhang", "Jiajin Qiu", "Dilong Sun", "Zhengyu Meng", "Xiaopeng Wei", "Xin Yang"], "title": "Touch2Shape: Touch-Conditioned 3D Diffusion for Shape Exploration and Reconstruction", "categories": ["cs.CV"], "comment": "10 pages, 6 figures", "summary": "Diffusion models have made breakthroughs in 3D generation tasks. Current 3D\ndiffusion models focus on reconstructing target shape from images or a set of\npartial observations. While excelling in global context understanding, they\nstruggle to capture the local details of complex shapes and limited to the\nocclusion and lighting conditions. To overcome these limitations, we utilize\ntactile images to capture the local 3D information and propose a Touch2Shape\nmodel, which leverages a touch-conditioned diffusion model to explore and\nreconstruct the target shape from touch. For shape reconstruction, we have\ndeveloped a touch embedding module to condition the diffusion model in creating\na compact representation and a touch shape fusion module to refine the\nreconstructed shape. For shape exploration, we combine the diffusion model with\nreinforcement learning to train a policy. This involves using the generated\nlatent vector from the diffusion model to guide the touch exploration policy\ntraining through a novel reward design. Experiments validate the reconstruction\nquality thorough both qualitatively and quantitative analysis, and our touch\nexploration policy further boosts reconstruction performance."}
{"id": "2505.13173", "pdf": "https://arxiv.org/pdf/2505.13173", "abs": "https://arxiv.org/abs/2505.13173", "authors": ["V. S. D. S. Mahesh Akavarapu", "Hrishikesh Terdalkar", "Pramit Bhattacharyya", "Shubhangi Agarwal", "Vishakha Deulgaonkar", "Pralay Manna", "Chaitali Dangarikar", "Arnab Bhattacharya"], "title": "A Case Study of Cross-Lingual Zero-Shot Generalization for Classical Languages in LLMs", "categories": ["cs.CL", "I.2.7"], "comment": "Accepted to ACL 2025 Findings", "summary": "Large Language Models (LLMs) have demonstrated remarkable generalization\ncapabilities across diverse tasks and languages. In this study, we focus on\nnatural language understanding in three classical languages -- Sanskrit,\nAncient Greek and Latin -- to investigate the factors affecting cross-lingual\nzero-shot generalization. First, we explore named entity recognition and\nmachine translation into English. While LLMs perform equal to or better than\nfine-tuned baselines on out-of-domain data, smaller models often struggle,\nespecially with niche or abstract entity types. In addition, we concentrate on\nSanskrit by presenting a factoid question-answering (QA) dataset and show that\nincorporating context via retrieval-augmented generation approach significantly\nboosts performance. In contrast, we observe pronounced performance drops for\nsmaller LLMs across these QA tasks. These results suggest model scale as an\nimportant factor influencing cross-lingual generalization. Assuming that models\nused such as GPT-4o and Llama-3.1 are not instruction fine-tuned on classical\nlanguages, our findings provide insights into how LLMs may generalize on these\nlanguages and their consequent utility in classical studies."}
{"id": "2505.11756", "pdf": "https://arxiv.org/pdf/2505.11756", "abs": "https://arxiv.org/abs/2505.11756", "authors": ["David Chanin", "Tomáš Dulka", "Adrià Garriga-Alonso"], "title": "Feature Hedging: Correlated Features Break Narrow Sparse Autoencoders", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "It is assumed that sparse autoencoders (SAEs) decompose polysemantic\nactivations into interpretable linear directions, as long as the activations\nare composed of sparse linear combinations of underlying features. However, we\nfind that if an SAE is more narrow than the number of underlying \"true\nfeatures\" on which it is trained, and there is correlation between features,\nthe SAE will merge components of correlated features together, thus destroying\nmonosemanticity. In LLM SAEs, these two conditions are almost certainly true.\nThis phenomenon, which we call feature hedging, is caused by SAE reconstruction\nloss, and is more severe the narrower the SAE. In this work, we introduce the\nproblem of feature hedging and study it both theoretically in toy models and\nempirically in SAEs trained on LLMs. We suspect that feature hedging may be one\nof the core reasons that SAEs consistently underperform supervised baselines.\nFinally, we use our understanding of feature hedging to propose an improved\nvariant of matryoshka SAEs. Our work shows there remain fundamental issues with\nSAEs, but we are hopeful that that highlighting feature hedging will catalyze\nfuture advances that allow SAEs to achieve their full potential of interpreting\nLLMs at scale."}
{"id": "2505.13099", "pdf": "https://arxiv.org/pdf/2505.13099", "abs": "https://arxiv.org/abs/2505.13099", "authors": ["Shinichi Mae", "Ryosuke Yamada", "Hirokatsu Kataoka"], "title": "Industry-focused Synthetic Segmentation Pre-training", "categories": ["cs.CV"], "comment": null, "summary": "Pre-training on real-image datasets has been widely proven effective for\nimproving instance segmentation. However, industrial applications face two key\nchallenges: (1) legal and ethical restrictions, such as ImageNet's prohibition\nof commercial use, and (2) limited transferability due to the domain gap\nbetween web images and industrial imagery. Even recent vision foundation\nmodels, including the segment anything model (SAM), show notable performance\ndegradation in industrial settings. These challenges raise critical questions:\nCan we build a vision foundation model for industrial applications without\nrelying on real images or manual annotations? And can such models outperform\neven fine-tuned SAM on industrial datasets? To address these questions, we\npropose the Instance Core Segmentation Dataset (InsCore), a synthetic\npre-training dataset based on formula-driven supervised learning (FDSL).\nInsCore generates fully annotated instance segmentation images that reflect key\ncharacteristics of industrial data, including complex occlusions, dense\nhierarchical masks, and diverse non-rigid shapes, distinct from typical web\nimagery. Unlike previous methods, InsCore requires neither real images nor\nhuman annotations. Experiments on five industrial datasets show that models\npre-trained with InsCore outperform those trained on COCO and ImageNet-21k, as\nwell as fine-tuned SAM, achieving an average improvement of 6.2 points in\ninstance segmentation performance. This result is achieved using only 100k\nsynthetic images, more than 100 times fewer than the 11 million images in SAM's\nSA-1B dataset, demonstrating the data efficiency of our approach. These\nfindings position InsCore as a practical and license-free vision foundation\nmodel for industrial applications."}
{"id": "2505.13176", "pdf": "https://arxiv.org/pdf/2505.13176", "abs": "https://arxiv.org/abs/2505.13176", "authors": ["Zihao Cheng", "Hongru Wang", "Zeming Liu", "Yuhang Guo", "Yuanfang Guo", "Yunhong Wang", "Haifeng Wang"], "title": "ToolSpectrum : Towards Personalized Tool Utilization for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025 Findings", "summary": "While integrating external tools into large language models (LLMs) enhances\ntheir ability to access real-time information and domain-specific services,\nexisting approaches focus narrowly on functional tool selection following user\ninstructions, overlooking the context-aware personalization in tool selection.\nThis oversight leads to suboptimal user satisfaction and inefficient tool\nutilization, particularly when overlapping toolsets require nuanced selection\nbased on contextual factors. To bridge this gap, we introduce ToolSpectrum, a\nbenchmark designed to evaluate LLMs' capabilities in personalized tool\nutilization. Specifically, we formalize two key dimensions of personalization,\nuser profile and environmental factors, and analyze their individual and\nsynergistic impacts on tool utilization. Through extensive experiments on\nToolSpectrum, we demonstrate that personalized tool utilization significantly\nimproves user experience across diverse scenarios. However, even\nstate-of-the-art LLMs exhibit the limited ability to reason jointly about user\nprofiles and environmental factors, often prioritizing one dimension at the\nexpense of the other. Our findings underscore the necessity of context-aware\npersonalization in tool-augmented LLMs and reveal critical limitations for\ncurrent models. Our data and code are available at\nhttps://github.com/Chengziha0/ToolSpectrum."}
{"id": "2505.11758", "pdf": "https://arxiv.org/pdf/2505.11758", "abs": "https://arxiv.org/abs/2505.11758", "authors": ["Sriram Mandalika"], "title": "Generalizable Vision-Language Few-Shot Adaptation with Predictive Prompts and Negative Learning", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.RO"], "comment": null, "summary": "Few-shot adaptation remains a core challenge for vision-language models\n(VLMs), especially under limited supervision and noisy support samples. We\npropose PromptFuseNL, a unified framework that enhances few-shot generalization\nby combining predictive prompt tuning with dual-branch positive and negative\nlearning. The method refines class prototypes through task-conditioned\nresiduals, multi-stage cross-modal coordination, and semantic hard negative\nmining. To address label noise, we introduce an unsupervised instance\nreweighting strategy that downweights unreliable support examples without\nrequiring additional labels or structural changes. PromptFuseNL fuses visual\nand textual cues through lightweight modules for efficient and discriminative\nprediction. Evaluated across 15 benchmarks, it consistently surpasses existing\nprompt- and adapter-based methods in all shot settings while remaining highly\nefficient, achieving up to 300x faster training and 1000x lower FLOPs compared\nto full prompt tuning, achieving a new state-of-the-art for robust and scalable\nfew-shot vision-language adaptation."}
{"id": "2505.13101", "pdf": "https://arxiv.org/pdf/2505.13101", "abs": "https://arxiv.org/abs/2505.13101", "authors": ["Shaowu Wu", "Liting Zeng", "Wei Lu", "Xiangyang Luo"], "title": "ARIW-Framework: Adaptive Robust Iterative Watermarking Framework", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 4 figures", "summary": "With the rapid rise of large models, copyright protection for generated image\ncontent has become a critical security challenge. Although deep learning\nwatermarking techniques offer an effective solution for digital image copyright\nprotection, they still face limitations in terms of visual quality, robustness\nand generalization. To address these issues, this paper proposes an adaptive\nrobust iterative watermarking framework (ARIW-Framework) that achieves\nhigh-quality watermarked images while maintaining exceptional robustness and\ngeneralization performance. Specifically, we introduce an iterative approach to\noptimize the encoder for generating robust residuals. The encoder incorporates\nnoise layers and a decoder to compute robustness weights for residuals under\nvarious noise attacks. By employing a parallel optimization strategy, the\nframework enhances robustness against multiple types of noise attacks.\nFurthermore, we leverage image gradients to determine the embedding strength at\neach pixel location, significantly improving the visual quality of the\nwatermarked images. Extensive experiments demonstrate that the proposed method\nachieves superior visual quality while exhibiting remarkable robustness and\ngeneralization against noise attacks."}
{"id": "2505.13181", "pdf": "https://arxiv.org/pdf/2505.13181", "abs": "https://arxiv.org/abs/2505.13181", "authors": ["Zhengrui Ma", "Yang Feng", "Chenze Shao", "Fandong Meng", "Jie Zhou", "Min Zhang"], "title": "Efficient Speech Language Modeling via Energy Distance in Continuous Latent Space", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Demos and code are available at https://github.com/ictnlp/SLED-TTS", "summary": "We introduce SLED, an alternative approach to speech language modeling by\nencoding speech waveforms into sequences of continuous latent representations\nand modeling them autoregressively using an energy distance objective. The\nenergy distance offers an analytical measure of the distributional gap by\ncontrasting simulated and target samples, enabling efficient training to\ncapture the underlying continuous autoregressive distribution. By bypassing\nreliance on residual vector quantization, SLED avoids discretization errors and\neliminates the need for the complicated hierarchical architectures common in\nexisting speech language models. It simplifies the overall modeling pipeline\nwhile preserving the richness of speech information and maintaining inference\nefficiency. Empirical results demonstrate that SLED achieves strong performance\nin both zero-shot and streaming speech synthesis, showing its potential for\nbroader applications in general-purpose speech language models."}
{"id": "2505.11760", "pdf": "https://arxiv.org/pdf/2505.11760", "abs": "https://arxiv.org/abs/2505.11760", "authors": ["Mansi Sakarvadia", "Nathaniel Hudson", "Tian Li", "Ian Foster", "Kyle Chard"], "title": "Topology-Aware Knowledge Propagation in Decentralized Learning", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": null, "summary": "Decentralized learning enables collaborative training of models across\nnaturally distributed data without centralized coordination or maintenance of a\nglobal model. Instead, devices are organized in arbitrary communication\ntopologies, in which they can only communicate with neighboring devices. Each\ndevice maintains its own local model by training on its local data and\nintegrating new knowledge via model aggregation with neighbors. Therefore,\nknowledge is propagated across the topology via successive aggregation rounds.\nWe study, in particular, the propagation of out-of-distribution (OOD)\nknowledge. We find that popular decentralized learning algorithms struggle to\npropagate OOD knowledge effectively to all devices. Further, we find that both\nthe location of OOD data within a topology, and the topology itself,\nsignificantly impact OOD knowledge propagation. We then propose topology-aware\naggregation strategies to accelerate (OOD) knowledge propagation across\ndevices. These strategies improve OOD data accuracy, compared to\ntopology-unaware baselines, by 123% on average across models in a topology."}
{"id": "2505.13123", "pdf": "https://arxiv.org/pdf/2505.13123", "abs": "https://arxiv.org/abs/2505.13123", "authors": ["Snehashis Majhi", "Giacomo D'Amicantonio", "Antitza Dantcheva", "Quan Kong", "Lorenzo Garattoni", "Gianpiero Francesca", "Egor Bondarev", "Francois Bremond"], "title": "Just Dance with $π$! A Poly-modal Inductor for Weakly-supervised Video Anomaly Detection", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Weakly-supervised methods for video anomaly detection (VAD) are\nconventionally based merely on RGB spatio-temporal features, which continues to\nlimit their reliability in real-world scenarios. This is due to the fact that\nRGB-features are not sufficiently distinctive in setting apart categories such\nas shoplifting from visually similar events. Therefore, towards robust complex\nreal-world VAD, it is essential to augment RGB spatio-temporal features by\nadditional modalities. Motivated by this, we introduce the Poly-modal Induced\nframework for VAD: \"PI-VAD\", a novel approach that augments RGB representations\nby five additional modalities. Specifically, the modalities include sensitivity\nto fine-grained motion (Pose), three dimensional scene and entity\nrepresentation (Depth), surrounding objects (Panoptic masks), global motion\n(optical flow), as well as language cues (VLM). Each modality represents an\naxis of a polygon, streamlined to add salient cues to RGB. PI-VAD includes two\nplug-in modules, namely Pseudo-modality Generation module and Cross Modal\nInduction module, which generate modality-specific prototypical representation\nand, thereby, induce multi-modal information into RGB cues. These modules\noperate by performing anomaly-aware auxiliary tasks and necessitate five\nmodality backbones -- only during training. Notably, PI-VAD achieves\nstate-of-the-art accuracy on three prominent VAD datasets encompassing\nreal-world scenarios, without requiring the computational overhead of five\nmodality backbones at inference."}
{"id": "2505.13204", "pdf": "https://arxiv.org/pdf/2505.13204", "abs": "https://arxiv.org/abs/2505.13204", "authors": ["Jikai Wang", "Zhenxu Tian", "Juntao Li", "Qingrong Xia", "Xinyu Duan", "Zhefeng Wang", "Baoxing Huai", "Min Zhang"], "title": "Alignment-Augmented Speculative Decoding with Alignment Sampling and Conditional Verification", "categories": ["cs.CL"], "comment": "Pre-print", "summary": "Recent works have revealed the great potential of speculative decoding in\naccelerating the autoregressive generation process of large language models.\nThe success of these methods relies on the alignment between draft candidates\nand the sampled outputs of the target model. Existing methods mainly achieve\ndraft-target alignment with training-based methods, e.g., EAGLE, Medusa,\ninvolving considerable training costs. In this paper, we present a\ntraining-free alignment-augmented speculative decoding algorithm. We propose\nalignment sampling, which leverages output distribution obtained in the\nprefilling phase to provide more aligned draft candidates. To further benefit\nfrom high-quality but non-aligned draft candidates, we also introduce a simple\nyet effective flexible verification strategy. Through an adaptive probability\nthreshold, our approach can improve generation accuracy while further improving\ninference efficiency. Experiments on 8 datasets (including question answering,\nsummarization and code completion tasks) show that our approach increases the\naverage generation score by 3.3 points for the LLaMA3 model. Our method\nachieves a mean acceptance length up to 2.39 and speed up generation by 2.23."}
{"id": "2505.11764", "pdf": "https://arxiv.org/pdf/2505.11764", "abs": "https://arxiv.org/abs/2505.11764", "authors": ["Raymond Baartmans", "Matthew Raffel", "Rahul Vikram", "Aiden Deringer", "Lizhong Chen"], "title": "Towards Universal Semantics With Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The Natural Semantic Metalanguage (NSM) is a linguistic theory based on a\nuniversal set of semantic primes: simple, primitive word-meanings that have\nbeen shown to exist in most, if not all, languages of the world. According to\nthis framework, any word, regardless of complexity, can be paraphrased using\nthese primes, revealing a clear and universally translatable meaning. These\nparaphrases, known as explications, can offer valuable applications for many\nnatural language processing (NLP) tasks, but producing them has traditionally\nbeen a slow, manual process. In this work, we present the first study of using\nlarge language models (LLMs) to generate NSM explications. We introduce\nautomatic evaluation methods, a tailored dataset for training and evaluation,\nand fine-tuned models for this task. Our 1B and 8B models outperform GPT-4o in\nproducing accurate, cross-translatable explications, marking a significant step\ntoward universal semantic representation with LLMs and opening up new\npossibilities for applications in semantic analysis, translation, and beyond."}
{"id": "2505.13130", "pdf": "https://arxiv.org/pdf/2505.13130", "abs": "https://arxiv.org/abs/2505.13130", "authors": ["Muhammad Awais Amin", "Adama Ilboudo", "Abdul Samad bin Shahid", "Amjad Ali", "Waqas Haider Khan Bangyal"], "title": "Adaptive Image Restoration for Video Surveillance: A Real-Time Approach", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "One of the major challenges in the field of computer vision especially for\ndetection, segmentation, recognition, monitoring, and automated solutions, is\nthe quality of images. Image degradation, often caused by factors such as rain,\nfog, lighting, etc., has a negative impact on automated\ndecision-making.Furthermore, several image restoration solutions exist,\nincluding restoration models for single degradation and restoration models for\nmultiple degradations. However, these solutions are not suitable for real-time\nprocessing. In this study, the aim was to develop a real-time image restoration\nsolution for video surveillance. To achieve this, using transfer learning with\nResNet_50, we developed a model for automatically identifying the types of\ndegradation present in an image to reference the necessary treatment(s) for\nimage restoration. Our solution has the advantage of being flexible and\nscalable."}
{"id": "2505.13210", "pdf": "https://arxiv.org/pdf/2505.13210", "abs": "https://arxiv.org/abs/2505.13210", "authors": ["Xiaocong Du", "Haoyu Pei", "Haipeng Zhang"], "title": "Picturized and Recited with Dialects: A Multimodal Chinese Representation Framework for Sentiment Analysis of Classical Chinese Poetry", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Classical Chinese poetry is a vital and enduring part of Chinese literature,\nconveying profound emotional resonance. Existing studies analyze sentiment\nbased on textual meanings, overlooking the unique rhythmic and visual features\ninherent in poetry,especially since it is often recited and accompanied by\nChinese paintings. In this work, we propose a dialect-enhanced multimodal\nframework for classical Chinese poetry sentiment analysis. We extract\nsentence-level audio features from the poetry and incorporate audio from\nmultiple dialects,which may retain regional ancient Chinese phonetic features,\nenriching the phonetic representation. Additionally, we generate sentence-level\nvisual features, and the multimodal features are fused with textual features\nenhanced by LLM translation through multimodal contrastive representation\nlearning. Our framework outperforms state-of-the-art methods on two public\ndatasets, achieving at least 2.51% improvement in accuracy and 1.63% in macro\nF1. We open-source the code to facilitate research in this area and provide\ninsights for general multimodal Chinese representation."}
{"id": "2505.11765", "pdf": "https://arxiv.org/pdf/2505.11765", "abs": "https://arxiv.org/abs/2505.11765", "authors": ["Shijun Li", "Hilaf Hasson", "Joydeep Ghosh"], "title": "OMAC: A Broad Optimization Framework for LLM-Based Multi-Agent Collaboration", "categories": ["cs.MA", "cs.AI", "cs.LG"], "comment": null, "summary": "Agents powered by advanced large language models (LLMs) have demonstrated\nimpressive capabilities across diverse complex applications. Recently,\nMulti-Agent Systems (MAS), wherein multiple agents collaborate and communicate\nwith each other, have exhibited enhanced capabilities in complex tasks, such as\nhigh-quality code generation and arithmetic reasoning. However, the development\nof such systems often relies on handcrafted methods, and the literature on\nsystematic design and optimization of LLM-based MAS remains limited.\n  In this work, we introduce OMAC, a general framework designed for holistic\noptimization of LLM-based MAS. Specifically, we identify five key optimization\ndimensions for MAS, encompassing both agent functionality and collaboration\nstructure. Building upon these dimensions, we first propose a general\nalgorithm, utilizing two actors termed the Semantic Initializer and the\nContrastive Comparator, to optimize any single dimension. Then, we present an\nalgorithm for joint optimization across multiple dimensions. Extensive\nexperiments demonstrate the superior performance of OMAC on code generation,\narithmetic reasoning, and general reasoning tasks against state-of-the-art\napproaches."}
{"id": "2505.13137", "pdf": "https://arxiv.org/pdf/2505.13137", "abs": "https://arxiv.org/abs/2505.13137", "authors": ["Robert-Jan Bruintjes", "Jan van Gemert"], "title": "Learning to Adapt to Position Bias in Vision Transformer Classifiers", "categories": ["cs.CV"], "comment": null, "summary": "How discriminative position information is for image classification depends\non the data. On the one hand, the camera position is arbitrary and objects can\nappear anywhere in the image, arguing for translation invariance. At the same\ntime, position information is key for exploiting capture/center bias, and scene\nlayout, e.g.: the sky is up. We show that position bias, the level to which a\ndataset is more easily solved when positional information on input features is\nused, plays a crucial role in the performance of Vision Transformers image\nclassifiers. To investigate, we propose Position-SHAP, a direct measure of\nposition bias by extending SHAP to work with position embeddings. We show\nvarious levels of position bias in different datasets, and find that the\noptimal choice of position embedding depends on the position bias apparent in\nthe dataset. We therefore propose Auto-PE, a single-parameter position\nembedding extension, which allows the position embedding to modulate its norm,\nenabling the unlearning of position information. Auto-PE combines with existing\nPEs to match or improve accuracy on classification datasets."}
{"id": "2505.13220", "pdf": "https://arxiv.org/pdf/2505.13220", "abs": "https://arxiv.org/abs/2505.13220", "authors": ["Jie Ying", "Zihong Chen", "Zhefan Wang", "Wanli Jiang", "Chenyang Wang", "Zhonghang Yuan", "Haoyang Su", "Huanjun Kong", "Fan Yang", "Nanqing Dong"], "title": "SeedBench: A Multi-task Benchmark for Evaluating Large Language Models in Seed Science", "categories": ["cs.CL"], "comment": "Accepted by ACL 2025", "summary": "Seed science is essential for modern agriculture, directly influencing crop\nyields and global food security. However, challenges such as interdisciplinary\ncomplexity and high costs with limited returns hinder progress, leading to a\nshortage of experts and insufficient technological support. While large\nlanguage models (LLMs) have shown promise across various fields, their\napplication in seed science remains limited due to the scarcity of digital\nresources, complex gene-trait relationships, and the lack of standardized\nbenchmarks. To address this gap, we introduce SeedBench -- the first multi-task\nbenchmark specifically designed for seed science. Developed in collaboration\nwith domain experts, SeedBench focuses on seed breeding and simulates key\naspects of modern breeding processes. We conduct a comprehensive evaluation of\n26 leading LLMs, encompassing proprietary, open-source, and domain-specific\nfine-tuned models. Our findings not only highlight the substantial gaps between\nthe power of LLMs and the real-world seed science problems, but also make a\nfoundational step for research on LLMs for seed design."}
{"id": "2505.11766", "pdf": "https://arxiv.org/pdf/2505.11766", "abs": "https://arxiv.org/abs/2505.11766", "authors": ["Haoze Song", "Zhihao Li", "Xiaobo Zhang", "Zecheng Gan", "Zhilu Lai", "Wei Wang"], "title": "Redefining Neural Operators in $d+1$ Dimensions", "categories": ["cs.LG", "cs.AI", "quant-ph"], "comment": null, "summary": "Neural Operators have emerged as powerful tools for learning mappings between\nfunction spaces. Among them, the kernel integral operator has been widely\nvalidated on universally approximating various operators. Although recent\nadvancements following this definition have developed effective modules to\nbetter approximate the kernel function defined on the original domain (with $d$\ndimensions, $d=1, 2, 3...$), the unclarified evolving mechanism in the\nembedding spaces blocks our view to design neural operators that can fully\ncapture the target system evolution.\n  Drawing on recent breakthroughs in quantum simulation of partial differential\nequations (PDEs), we elucidate the linear evolution process in neural\noperators. Based on that, we redefine neural operators on a new $d+1$\ndimensional domain. Within this framework, we implement our proposed\nSchr\\\"odingerised Kernel Neural Operator (SKNO) aligning better with the $d+1$\ndimensional evolution. In experiments, our $d+1$ dimensional evolving linear\nblock performs far better than others. Also, we test SKNO's SOTA performance on\nvarious benchmark tests and also the zero-shot super-resolution task. In\naddition, we analyse the impact of different lifting and recovering operators\non the prediction within the redefined NO framework, reflecting the alignment\nbetween our model and the underlying $d+1$ dimensional evolution."}
{"id": "2505.13140", "pdf": "https://arxiv.org/pdf/2505.13140", "abs": "https://arxiv.org/abs/2505.13140", "authors": ["Takahiro Maeda", "Jinkun Cao", "Norimichi Ukita", "Kris Kitani"], "title": "CacheFlow: Fast Human Motion Prediction by Cached Normalizing Flow", "categories": ["cs.CV"], "comment": null, "summary": "Many density estimation techniques for 3D human motion prediction require a\nsignificant amount of inference time, often exceeding the duration of the\npredicted time horizon. To address the need for faster density estimation for\n3D human motion prediction, we introduce a novel flow-based method for human\nmotion prediction called CacheFlow. Unlike previous conditional generative\nmodels that suffer from time efficiency, CacheFlow takes advantage of an\nunconditional flow-based generative model that transforms a Gaussian mixture\ninto the density of future motions. The results of the computation of the\nflow-based generative model can be precomputed and cached. Then, for\nconditional prediction, we seek a mapping from historical trajectories to\nsamples in the Gaussian mixture. This mapping can be done by a much more\nlightweight model, thus saving significant computation overhead compared to a\ntypical conditional flow model. In such a two-stage fashion and by caching\nresults from the slow flow model computation, we build our CacheFlow without\nloss of prediction accuracy and model expressiveness. This inference process is\ncompleted in approximately one millisecond, making it 4 times faster than\nprevious VAE methods and 30 times faster than previous diffusion-based methods\non standard benchmarks such as Human3.6M and AMASS datasets. Furthermore, our\nmethod demonstrates improved density estimation accuracy and comparable\nprediction accuracy to a SOTA method on Human3.6M. Our code and models will be\npublicly available."}
{"id": "2505.13244", "pdf": "https://arxiv.org/pdf/2505.13244", "abs": "https://arxiv.org/abs/2505.13244", "authors": ["Jieying Xue", "Phuong Minh Nguyen", "Minh Le Nguyen", "Xin Liu"], "title": "JNLP at SemEval-2025 Task 11: Cross-Lingual Multi-Label Emotion Detection Using Generative Models", "categories": ["cs.CL", "cs.LG"], "comment": "Published in The 19th International Workshop on Semantic Evaluation\n  (SemEval-2025)", "summary": "With the rapid advancement of global digitalization, users from different\ncountries increasingly rely on social media for information exchange. In this\ncontext, multilingual multi-label emotion detection has emerged as a critical\nresearch area. This study addresses SemEval-2025 Task 11: Bridging the Gap in\nText-Based Emotion Detection. Our paper focuses on two sub-tracks of this task:\n(1) Track A: Multi-label emotion detection, and (2) Track B: Emotion intensity.\nTo tackle multilingual challenges, we leverage pre-trained multilingual models\nand focus on two architectures: (1) a fine-tuned BERT-based classification\nmodel and (2) an instruction-tuned generative LLM. Additionally, we propose two\nmethods for handling multi-label classification: the base method, which maps an\ninput directly to all its corresponding emotion labels, and the pairwise\nmethod, which models the relationship between the input text and each emotion\ncategory individually. Experimental results demonstrate the strong\ngeneralization ability of our approach in multilingual emotion recognition. In\nTrack A, our method achieved Top 4 performance across 10 languages, ranking 1st\nin Hindi. In Track B, our approach also secured Top 5 performance in 7\nlanguages, highlighting its simplicity and effectiveness\\footnote{Our code is\navailable at https://github.com/yingjie7/mlingual_multilabel_emo_detection."}
{"id": "2505.11770", "pdf": "https://arxiv.org/pdf/2505.11770", "abs": "https://arxiv.org/abs/2505.11770", "authors": ["Jing Huang", "Junyi Tao", "Thomas Icard", "Diyi Yang", "Christopher Potts"], "title": "Internal Causal Mechanisms Robustly Predict Language Model Out-of-Distribution Behaviors", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": "ICML 2025", "summary": "Interpretability research now offers a variety of techniques for identifying\nabstract internal mechanisms in neural networks. Can such techniques be used to\npredict how models will behave on out-of-distribution examples? In this work,\nwe provide a positive answer to this question. Through a diverse set of\nlanguage modeling tasks--including symbol manipulation, knowledge retrieval,\nand instruction following--we show that the most robust features for\ncorrectness prediction are those that play a distinctive causal role in the\nmodel's behavior. Specifically, we propose two methods that leverage causal\nmechanisms to predict the correctness of model outputs: counterfactual\nsimulation (checking whether key causal variables are realized) and value\nprobing (using the values of those variables to make predictions). Both achieve\nhigh AUC-ROC in distribution and outperform methods that rely on\ncausal-agnostic features in out-of-distribution settings, where predicting\nmodel behaviors is more crucial. Our work thus highlights a novel and\nsignificant application for internal causal analysis of language models."}
{"id": "2505.13174", "pdf": "https://arxiv.org/pdf/2505.13174", "abs": "https://arxiv.org/abs/2505.13174", "authors": ["Alp Eren Sari", "Paolo Favaro"], "title": "FlowCut: Unsupervised Video Instance Segmentation via Temporal Mask Matching", "categories": ["cs.CV"], "comment": null, "summary": "We propose FlowCut, a simple and capable method for unsupervised video\ninstance segmentation consisting of a three-stage framework to construct a\nhigh-quality video dataset with pseudo labels. To our knowledge, our work is\nthe first attempt to curate a video dataset with pseudo-labels for unsupervised\nvideo instance segmentation. In the first stage, we generate pseudo-instance\nmasks by exploiting the affinities of features from both images and optical\nflows. In the second stage, we construct short video segments containing\nhigh-quality, consistent pseudo-instance masks by temporally matching them\nacross the frames. In the third stage, we use the YouTubeVIS-2021 video dataset\nto extract our training instance segmentation set, and then train a video\nsegmentation model. FlowCut achieves state-of-the-art performance on the\nYouTubeVIS-2019, YouTubeVIS-2021, DAVIS-2017, and DAVIS-2017 Motion benchmarks."}
{"id": "2505.13251", "pdf": "https://arxiv.org/pdf/2505.13251", "abs": "https://arxiv.org/abs/2505.13251", "authors": ["Sidney Wong"], "title": "Stronger Together: Unleashing the Social Impact of Hate Speech Research", "categories": ["cs.CL"], "comment": "Accepted Proceedings of the Linguistic Society of America 2025 Annual\n  Meeting", "summary": "The advent of the internet has been both a blessing and a curse for once\nmarginalised communities. When used well, the internet can be used to connect\nand establish communities crossing different intersections; however, it can\nalso be used as a tool to alienate people and communities as well as perpetuate\nhate, misinformation, and disinformation especially on social media platforms.\nWe propose steering hate speech research and researchers away from pre-existing\ncomputational solutions and consider social methods to inform social solutions\nto address this social problem. In a similar way linguistics research can\ninform language planning policy, linguists should apply what we know about\nlanguage and society to mitigate some of the emergent risks and dangers of\nanti-social behaviour in digital spaces. We argue linguists and NLP researchers\ncan play a principle role in unleashing the social impact potential of\nlinguistics research working alongside communities, advocates, activists, and\npolicymakers to enable equitable digital inclusion and to close the digital\ndivide."}
{"id": "2505.11771", "pdf": "https://arxiv.org/pdf/2505.11771", "abs": "https://arxiv.org/abs/2505.11771", "authors": ["Yichen Xu", "Ryumei Nakada", "Linjun Zhang", "Lexin Li"], "title": "Residual Feature Integration is Sufficient to Prevent Negative Transfer", "categories": ["cs.LG", "cs.AI", "math.ST", "stat.ML", "stat.TH"], "comment": null, "summary": "Transfer learning typically leverages representations learned from a source\ndomain to improve performance on a target task. A common approach is to extract\nfeatures from a pre-trained model and directly apply them for target\nprediction. However, this strategy is prone to negative transfer where the\nsource representation fails to align with the target distribution. In this\narticle, we propose Residual Feature Integration (REFINE), a simple yet\neffective method designed to mitigate negative transfer. Our approach combines\na fixed source-side representation with a trainable target-side encoder and\nfits a shallow neural network on the resulting joint representation, which\nadapts to the target domain while preserving transferable knowledge from the\nsource domain. Theoretically, we prove that REFINE is sufficient to prevent\nnegative transfer under mild conditions, and derive the generalization bound\ndemonstrating its theoretical benefit. Empirically, we show that REFINE\nconsistently enhances performance across diverse application and data\nmodalities including vision, text, and tabular data, and outperforms numerous\nalternative solutions. Our method is lightweight, architecture-agnostic, and\nrobust, making it a valuable addition to the existing transfer learning\ntoolbox."}
{"id": "2505.13191", "pdf": "https://arxiv.org/pdf/2505.13191", "abs": "https://arxiv.org/abs/2505.13191", "authors": ["Pengcheng Pan", "Yonekura Shogo", "Yasuo Kuniyoshi"], "title": "Emergence of Fixational and Saccadic Movements in a Multi-Level Recurrent Attention Model for Vision", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Inspired by foveal vision, hard attention models promise interpretability and\nparameter economy. However, existing models like the Recurrent Model of Visual\nAttention (RAM) and Deep Recurrent Attention Model (DRAM) failed to model the\nhierarchy of human vision system, that compromise on the visual exploration\ndynamics. As a result, they tend to produce attention that are either overly\nfixational or excessively saccadic, diverging from human eye movement behavior.\nIn this paper, we propose a Multi-Level Recurrent Attention Model (MRAM), a\nnovel hard attention framework that explicitly models the neural hierarchy of\nhuman visual processing. By decoupling the function of glimpse location\ngeneration and task execution in two recurrent layers, MRAM emergent a balanced\nbehavior between fixation and saccadic movement. Our results show that MRAM not\nonly achieves more human-like attention dynamics, but also consistently\noutperforms CNN, RAM and DRAM baselines on standard image classification\nbenchmarks."}
{"id": "2505.13252", "pdf": "https://arxiv.org/pdf/2505.13252", "abs": "https://arxiv.org/abs/2505.13252", "authors": ["Rikhil Amonkar", "Ronan Le Bras", "Li Zhang"], "title": "Natural Language Planning via Coding and Inference Scaling", "categories": ["cs.CL"], "comment": null, "summary": "Real-life textual planning tasks such as meeting scheduling have posed much\nchallenge to LLMs especially when the complexity is high. While previous work\nprimarily studied auto-regressive generation of plans with closed-source\nmodels, we systematically evaluate both closed- and open-source models,\nincluding those that scales output length with complexity during inference, in\ngenerating programs, which are executed to output the plan. We consider not\nonly standard Python code, but also the code to a constraint satisfaction\nproblem solver. Despite the algorithmic nature of the task, we show that\nprogramming often but not always outperforms planning. Our detailed error\nanalysis also indicates a lack of robustness and efficiency in the generated\ncode that hinders generalization."}
{"id": "2505.11774", "pdf": "https://arxiv.org/pdf/2505.11774", "abs": "https://arxiv.org/abs/2505.11774", "authors": ["James V. Roggeveen", "Erik Y. Wang", "Will Flintoft", "Peter Donets", "Lucy S. Nathwani", "Nickholas Gutierrez", "David Ettel", "Anton Marius Graf", "Siddharth Dandavate", "Arjun Nageswaran", "Raglan Ward", "Ava Williamson", "Anne Mykland", "Kacper K. Migacz", "Yijun Wang", "Egemen Bostan", "Duy Thuc Nguyen", "Zhe He", "Marc L. Descoteaux", "Felix Yeung", "Shida Liu", "Jorge García Ponce", "Luke Zhu", "Yuyang Chen", "Ekaterina S. Ivshina", "Miguel Fernandez", "Minjae Kim", "Kennan Gumbs", "Matthew Scott Tan", "Russell Yang", "Mai Hoang", "David Brown", "Isabella A. Silveira", "Lavon Sykes", "Ahmed Roman", "William Fredenberg", "Yiming Chen", "Lucas Martin", "Yixing Tang", "Kelly Werker Smith", "Hongyu Liao", "Logan G. Wilson", "Alexander Dazhen Cai", "Andrea Elizabeth Biju", "Michael P. Brenner"], "title": "HARDMath2: A Benchmark for Applied Mathematics Built by Students as Part of a Graduate Class", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have shown remarkable progress in mathematical\nproblem-solving, but evaluation has largely focused on problems that have exact\nanalytical solutions or involve formal proofs, often overlooking\napproximation-based problems ubiquitous in applied science and engineering. To\nfill this gap, we build on prior work and present HARDMath2, a dataset of 211\noriginal problems covering the core topics in an introductory graduate applied\nmath class, including boundary-layer analysis, WKB methods, asymptotic\nsolutions of nonlinear partial differential equations, and the asymptotics of\noscillatory integrals. This dataset was designed and verified by the students\nand instructors of a core graduate applied mathematics course at Harvard. We\nbuild the dataset through a novel collaborative environment that challenges\nstudents to write and refine difficult problems consistent with the class\nsyllabus, peer-validate solutions, test different models, and automatically\ncheck LLM-generated solutions against their own answers and numerical ground\ntruths. Evaluation results show that leading frontier models still struggle\nwith many of the problems in the dataset, highlighting a gap in the\nmathematical reasoning skills of current LLMs. Importantly, students identified\nstrategies to create increasingly difficult problems by interacting with the\nmodels and exploiting common failure modes. This back-and-forth with the models\nnot only resulted in a richer and more challenging benchmark but also led to\nqualitative improvements in the students' understanding of the course material,\nwhich is increasingly important as we enter an age where state-of-the-art\nlanguage models can solve many challenging problems across a wide domain of\nfields."}
{"id": "2505.13201", "pdf": "https://arxiv.org/pdf/2505.13201", "abs": "https://arxiv.org/abs/2505.13201", "authors": ["Yuzhen Chen", "Hojun Son", "Arpan Kusari"], "title": "MatPredict: a dataset and benchmark for learning material properties of diverse indoor objects", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Determining material properties from camera images can expand the ability to\nidentify complex objects in indoor environments, which is valuable for consumer\nrobotics applications. To support this, we introduce MatPredict, a dataset that\ncombines the high-quality synthetic objects from Replica dataset with MatSynth\ndataset's material properties classes - to create objects with diverse material\nproperties. We select 3D meshes of specific foreground objects and render them\nwith different material properties. In total, we generate \\textbf{18} commonly\noccurring objects with \\textbf{14} different materials. We showcase how we\nprovide variability in terms of lighting and camera placement for these\nobjects. Next, we provide a benchmark for inferring material properties from\nvisual images using these perturbed models in the scene, discussing the\nspecific neural network models involved and their performance based on\ndifferent image comparison metrics. By accurately simulating light interactions\nwith different materials, we can enhance realism, which is crucial for training\nmodels effectively through large-scale simulations. This research aims to\nrevolutionize perception in consumer robotics. The dataset is provided\n\\href{https://huggingface.co/datasets/UMTRI/MatPredict}{here} and the code is\nprovided \\href{https://github.com/arpan-kusari/MatPredict}{here}."}
{"id": "2505.13254", "pdf": "https://arxiv.org/pdf/2505.13254", "abs": "https://arxiv.org/abs/2505.13254", "authors": ["Siran Liu", "Yang Ye", "Qianchao Zhu", "Zheng Cao", "Yongchao He"], "title": "HeteroSpec: Leveraging Contextual Heterogeneity for Efficient Speculative Decoding", "categories": ["cs.CL"], "comment": null, "summary": "Autoregressive decoding, the standard approach for Large Language Model (LLM)\ninference, remains a significant bottleneck due to its sequential nature. While\nspeculative decoding algorithms mitigate this inefficiency through parallel\nverification, they fail to exploit the inherent heterogeneity in linguistic\ncomplexity, a key factor leading to suboptimal resource allocation. We address\nthis by proposing HeteroSpec, a heterogeneity-adaptive speculative decoding\nframework that dynamically optimizes computational resource allocation based on\nlinguistic context complexity. HeteroSpec introduces two key mechanisms: (1) A\nnovel cumulative meta-path Top-$K$ entropy metric for efficiently identifying\npredictable contexts. (2) A dynamic resource allocation strategy based on\ndata-driven entropy partitioning, enabling adaptive speculative expansion and\npruning tailored to local context difficulty. Evaluated on five public\nbenchmarks and four models, HeteroSpec achieves an average speedup of\n4.26$\\times$. It consistently outperforms state-of-the-art EAGLE-3 across\nspeedup rates, average acceptance length, and verification cost. Notably,\nHeteroSpec requires no draft model retraining, incurs minimal overhead, and is\northogonal to other acceleration techniques. It demonstrates enhanced\nacceleration with stronger draft models, establishing a new paradigm for\ncontext-aware LLM inference acceleration."}
{"id": "2505.11776", "pdf": "https://arxiv.org/pdf/2505.11776", "abs": "https://arxiv.org/abs/2505.11776", "authors": ["Jiali Chen", "Avijit Mukherjee"], "title": "Generative and Contrastive Graph Representation Learning", "categories": ["cs.LG", "cs.AI", "I.2.4, I2.6"], "comment": "8 pages, 3 figures", "summary": "Self-supervised learning (SSL) on graphs generates node and graph\nrepresentations (i.e., embeddings) that can be used for downstream tasks such\nas node classification, node clustering, and link prediction. Graph SSL is\nparticularly useful in scenarios with limited or no labeled data. Existing SSL\nmethods predominantly follow contrastive or generative paradigms, each\nexcelling in different tasks: contrastive methods typically perform well on\nclassification tasks, while generative methods often excel in link prediction.\nIn this paper, we present a novel architecture for graph SSL that integrates\nthe strengths of both approaches. Our framework introduces community-aware\nnode-level contrastive learning, providing more robust and effective positive\nand negative node pairs generation, alongside graph-level contrastive learning\nto capture global semantic information. Additionally, we employ a comprehensive\naugmentation strategy that combines feature masking, node perturbation, and\nedge perturbation, enabling robust and diverse representation learning. By\nincorporating these enhancements, our model achieves superior performance\nacross multiple tasks, including node classification, clustering, and link\nprediction. Evaluations on open benchmark datasets demonstrate that our model\noutperforms state-of-the-art methods, achieving a performance lift of\n0.23%-2.01% depending on the task and dataset."}
{"id": "2505.13211", "pdf": "https://arxiv.org/pdf/2505.13211", "abs": "https://arxiv.org/abs/2505.13211", "authors": ["Sand. ai", "Hansi Teng", "Hongyu Jia", "Lei Sun", "Lingzhi Li", "Maolin Li", "Mingqiu Tang", "Shuai Han", "Tianning Zhang", "W. Q. Zhang", "Weifeng Luo", "Xiaoyang Kang", "Yuchen Sun", "Yue Cao", "Yunpeng Huang", "Yutong Lin", "Yuxin Fang", "Zewei Tao", "Zheng Zhang", "Zhongshu Wang", "Zixun Liu", "Dai Shi", "Guoli Su", "Hanwen Sun", "Hong Pan", "Jie Wang", "Jiexin Sheng", "Min Cui", "Min Hu", "Ming Yan", "Shucheng Yin", "Siran Zhang", "Tingting Liu", "Xianping Yin", "Xiaoyu Yang", "Xin Song", "Xuan Hu", "Yankai Zhang", "Yuqiao Li"], "title": "MAGI-1: Autoregressive Video Generation at Scale", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We present MAGI-1, a world model that generates videos by autoregressively\npredicting a sequence of video chunks, defined as fixed-length segments of\nconsecutive frames. Trained to denoise per-chunk noise that increases\nmonotonically over time, MAGI-1 enables causal temporal modeling and naturally\nsupports streaming generation. It achieves strong performance on image-to-video\n(I2V) tasks conditioned on text instructions, providing high temporal\nconsistency and scalability, which are made possible by several algorithmic\ninnovations and a dedicated infrastructure stack. MAGI-1 facilitates\ncontrollable generation via chunk-wise prompting and supports real-time,\nmemory-efficient deployment by maintaining constant peak inference cost,\nregardless of video length. The largest variant of MAGI-1 comprises 24 billion\nparameters and supports context lengths of up to 4 million tokens,\ndemonstrating the scalability and robustness of our approach. The code and\nmodels are available at https://github.com/SandAI-org/MAGI-1 and\nhttps://github.com/SandAI-org/MagiAttention. The product can be accessed at\nhttps://sand.ai."}
{"id": "2505.13257", "pdf": "https://arxiv.org/pdf/2505.13257", "abs": "https://arxiv.org/abs/2505.13257", "authors": ["Zilu Tang", "Afra Feyza Akyürek", "Ekin Akyürek", "Derry Wijaya"], "title": "WikiPersonas: What Can We Learn From Personalized Alignment to Famous People?", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "9 pages, preprint", "summary": "Preference alignment has become a standard pipeline in finetuning models to\nfollow \\emph{generic} human preferences. Majority of work seeks to optimize\nmodel to produce responses that would be preferable \\emph{on average},\nsimplifying the diverse and often \\emph{contradicting} space of human\npreferences. While research has increasingly focused on personalized alignment:\nadapting models to individual user preferences, there is a lack of personalized\npreference dataset which focus on nuanced individual-level preferences. To\naddress this, we introduce WikiPersona: the first fine-grained personalization\nusing well-documented, famous individuals. Our dataset challenges models to\nalign with these personas through an interpretable process: generating\nverifiable textual descriptions of a persona's background and preferences in\naddition to alignment. We systematically evaluate different personalization\napproaches and find that as few-shot prompting with preferences and fine-tuning\nfail to simultaneously ensure effectiveness and efficiency, using\n\\textit{inferred personal preferences} as prefixes enables effective\npersonalization, especially in topics where preferences clash while leading to\nmore equitable generalization across unseen personas."}
{"id": "2505.11785", "pdf": "https://arxiv.org/pdf/2505.11785", "abs": "https://arxiv.org/abs/2505.11785", "authors": ["Gina Wong", "Drew Prinster", "Suchi Saria", "Rama Chellappa", "Anqi Liu"], "title": "Improving Coverage in Combined Prediction Sets with Weighted p-values", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Conformal prediction quantifies the uncertainty of machine learning models by\naugmenting point predictions with valid prediction sets, assuming\nexchangeability. For complex scenarios involving multiple trials, models, or\ndata sources, conformal prediction sets can be aggregated to create a\nprediction set that captures the overall uncertainty, often improving\nprecision. However, aggregating multiple prediction sets with individual\n$1-\\alpha$ coverage inevitably weakens the overall guarantee, typically\nresulting in $1-2\\alpha$ worst-case coverage. In this work, we propose a\nframework for the weighted aggregation of prediction sets, where weights are\nassigned to each prediction set based on their contribution. Our framework\noffers flexible control over how the sets are aggregated, achieving tighter\ncoverage bounds that interpolate between the $1-2\\alpha$ guarantee of the\ncombined models and the $1-\\alpha$ guarantee of an individual model depending\non the distribution of weights. We extend our framework to data-dependent\nweights, and we derive a general procedure for data-dependent weight\naggregation that maintains finite-sample validity. We demonstrate the\neffectiveness of our methods through experiments on synthetic and real data in\nthe mixture-of-experts setting, and we show that aggregation with\ndata-dependent weights provides a form of adaptive coverage."}
{"id": "2505.13212", "pdf": "https://arxiv.org/pdf/2505.13212", "abs": "https://arxiv.org/abs/2505.13212", "authors": ["Qingling Shu", "Sibao Chen", "Zhihui You", "Wei Lu", "Jin Tang", "Bin Luo"], "title": "RB-SCD: A New Benchmark for Semantic Change Detection of Roads and Bridges in Traffic Scenes", "categories": ["cs.CV"], "comment": null, "summary": "Accurate detection of changes in roads and bridges, such as construction,\nrenovation, and demolition, is essential for urban planning and traffic\nmanagement. However, existing methods often struggle to extract fine-grained\nsemantic change information due to the lack of high-quality annotated datasets\nin traffic scenarios. To address this, we introduce the Road and Bridge\nSemantic Change Detection (RB-SCD) dataset, a comprehensive benchmark\ncomprising 260 pairs of high-resolution remote sensing images from diverse\ncities and countries. RB-SCD captures 11 types of semantic changes across\nvaried road and bridge structures, enabling detailed structural and functional\nanalysis. Building on this dataset, we propose a novel framework, Multimodal\nFrequency-Driven Change Detector (MFDCD), which integrates multimodal features\nin the frequency domain. MFDCD includes a Dynamic Frequency Coupler (DFC) that\nfuses hierarchical visual features with wavelet-based frequency components, and\na Textual Frequency Filter (TFF) that transforms CLIP-derived textual features\ninto the frequency domain and applies graph-based filtering. Experimental\nresults on RB-SCD and three public benchmarks demonstrate the effectiveness of\nour approach."}
{"id": "2505.13258", "pdf": "https://arxiv.org/pdf/2505.13258", "abs": "https://arxiv.org/abs/2505.13258", "authors": ["Jingyi Ren", "Yekun Xu", "Xiaolong Wang", "Weitao Li", "Weizhi Ma", "Yang Liu"], "title": "Effective and Transparent RAG: Adaptive-Reward Reinforcement Learning for Decision Traceability", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has significantly improved the\nperformance of large language models (LLMs) on knowledge-intensive domains.\nHowever, although RAG achieved successes across distinct domains, there are\nstill some unsolved challenges: 1) Effectiveness. Existing research mainly\nfocuses on developing more powerful RAG retrievers, but how to enhance the\ngenerator's (LLM's) ability to utilize the retrieved information for reasoning\nand generation? 2) Transparency. Most RAG methods ignore which retrieved\ncontent actually contributes to the reasoning process, resulting in a lack of\ninterpretability and visibility. To address this, we propose ARENA\n(Adaptive-Rewarded Evidence Navigation Agent), a transparent RAG generator\nframework trained via reinforcement learning (RL) with our proposed rewards.\nBased on the structured generation and adaptive reward calculation, our\nRL-based training enables the model to identify key evidence, perform\nstructured reasoning, and generate answers with interpretable decision traces.\nApplied to Qwen2.5-7B-Instruct and Llama3.1-8B-Instruct, abundant experiments\nwith various RAG baselines demonstrate that our model achieves 10-30%\nimprovements on all multi-hop QA datasets, which is comparable with the SOTA\nCommercially-developed LLMs (e.g., OpenAI-o1, DeepSeek-R1). Further analyses\nshow that ARENA has strong flexibility to be adopted on new datasets without\nextra training. Our models and codes are publicly released."}
{"id": "2505.11793", "pdf": "https://arxiv.org/pdf/2505.11793", "abs": "https://arxiv.org/abs/2505.11793", "authors": ["Jianing Wang", "Siying Guo", "Zheng Hua", "Runhu Huang", "Jinyu Hu", "Maoguo Gong"], "title": "CL-CaGAN: Capsule differential adversarial continuous learning for cross-domain hyperspectral anomaly detection", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": null, "summary": "Anomaly detection (AD) has attracted remarkable attention in hyperspectral\nimage (HSI) processing fields, and most existing deep learning (DL)-based\nalgorithms indicate dramatic potential for detecting anomaly samples through\nspecific training process under current scenario. However, the limited prior\ninformation and the catastrophic forgetting problem indicate crucial challenges\nfor existing DL structure in open scenarios cross-domain detection. In order to\nimprove the detection performance, a novel continual learning-based capsule\ndifferential generative adversarial network (CL-CaGAN) is proposed to elevate\nthe cross-scenario learning performance for facilitating the real application\nof DL-based structure in hyperspectral AD (HAD) task. First, a modified capsule\nstructure with adversarial learning network is constructed to estimate the\nbackground distribution for surmounting the deficiency of prior information. To\nmitigate the catastrophic forgetting phenomenon, clustering-based sample replay\nstrategy and a designed extra self-distillation regularization are integrated\nfor merging the history and future knowledge in continual AD task, while the\ndiscriminative learning ability from previous detection scenario to current\nscenario is retained by the elaborately designed structure with continual\nlearning (CL) strategy. In addition, the differentiable enhancement is enforced\nto augment the generation performance of the training data. This further\nstabilizes the training process with better convergence and efficiently\nconsolidates the reconstruction ability of background samples. To verify the\neffectiveness of our proposed CL-CaGAN, we conduct experiments on several real\nHSIs, and the results indicate that the proposed CL-CaGAN demonstrates higher\ndetection performance and continuous learning capacity for mitigating the\ncatastrophic forgetting under cross-domain scenarios."}
{"id": "2505.13215", "pdf": "https://arxiv.org/pdf/2505.13215", "abs": "https://arxiv.org/abs/2505.13215", "authors": ["Seungjun Oh", "Younggeun Lee", "Hyejin Jeon", "Eunbyung Park"], "title": "Hybrid 3D-4D Gaussian Splatting for Fast Dynamic Scene Representation", "categories": ["cs.CV"], "comment": "https://ohsngjun.github.io/3D-4DGS/", "summary": "Recent advancements in dynamic 3D scene reconstruction have shown promising\nresults, enabling high-fidelity 3D novel view synthesis with improved temporal\nconsistency. Among these, 4D Gaussian Splatting (4DGS) has emerged as an\nappealing approach due to its ability to model high-fidelity spatial and\ntemporal variations. However, existing methods suffer from substantial\ncomputational and memory overhead due to the redundant allocation of 4D\nGaussians to static regions, which can also degrade image quality. In this\nwork, we introduce hybrid 3D-4D Gaussian Splatting (3D-4DGS), a novel framework\nthat adaptively represents static regions with 3D Gaussians while reserving 4D\nGaussians for dynamic elements. Our method begins with a fully 4D Gaussian\nrepresentation and iteratively converts temporally invariant Gaussians into 3D,\nsignificantly reducing the number of parameters and improving computational\nefficiency. Meanwhile, dynamic Gaussians retain their full 4D representation,\ncapturing complex motions with high fidelity. Our approach achieves\nsignificantly faster training times compared to baseline 4D Gaussian Splatting\nmethods while maintaining or improving the visual quality."}
{"id": "2505.13259", "pdf": "https://arxiv.org/pdf/2505.13259", "abs": "https://arxiv.org/abs/2505.13259", "authors": ["Tianshi Zheng", "Zheye Deng", "Hong Ting Tsang", "Weiqi Wang", "Jiaxin Bai", "Zihao Wang", "Yangqiu Song"], "title": "From Automation to Autonomy: A Survey on Large Language Models in Scientific Discovery", "categories": ["cs.CL"], "comment": "16 pages", "summary": "Large Language Models (LLMs) are catalyzing a paradigm shift in scientific\ndiscovery, evolving from task-specific automation tools into increasingly\nautonomous agents and fundamentally redefining research processes and human-AI\ncollaboration. This survey systematically charts this burgeoning field, placing\na central focus on the changing roles and escalating capabilities of LLMs in\nscience. Through the lens of the scientific method, we introduce a foundational\nthree-level taxonomy-Tool, Analyst, and Scientist-to delineate their escalating\nautonomy and evolving responsibilities within the research lifecycle. We\nfurther identify pivotal challenges and future research trajectories such as\nrobotic automation, self-improvement, and ethical governance. Overall, this\nsurvey provides a conceptual architecture and strategic foresight to navigate\nand shape the future of AI-driven scientific discovery, fostering both rapid\ninnovation and responsible advancement. Github Repository:\nhttps://github.com/HKUST-KnowComp/Awesome-LLM-Scientific-Discovery."}
{"id": "2505.11802", "pdf": "https://arxiv.org/pdf/2505.11802", "abs": "https://arxiv.org/abs/2505.11802", "authors": ["Chuang Zhao", "Hui Tang", "Hongke Zhao", "Xiaomeng Li"], "title": "Diffmv: A Unified Diffusion Framework for Healthcare Predictions with Random Missing Views and View Laziness", "categories": ["cs.LG", "cs.AI"], "comment": "SIGKDD2025, accepted", "summary": "Advanced healthcare predictions offer significant improvements in patient\noutcomes by leveraging predictive analytics. Existing works primarily utilize\nvarious views of Electronic Health Record (EHR) data, such as diagnoses, lab\ntests, or clinical notes, for model training. These methods typically assume\nthe availability of complete EHR views and that the designed model could fully\nleverage the potential of each view. However, in practice, random missing views\nand view laziness present two significant challenges that hinder further\nimprovements in multi-view utilization. To address these challenges, we\nintroduce Diffmv, an innovative diffusion-based generative framework designed\nto advance the exploitation of multiple views of EHR data. Specifically, to\naddress random missing views, we integrate various views of EHR data into a\nunified diffusion-denoising framework, enriched with diverse contextual\nconditions to facilitate progressive alignment and view transformation. To\nmitigate view laziness, we propose a novel reweighting strategy that assesses\nthe relative advantages of each view, promoting a balanced utilization of\nvarious data views within the model. Our proposed strategy achieves superior\nperformance across multiple health prediction tasks derived from three popular\ndatasets, including multi-view and multi-modality scenarios."}
{"id": "2505.13219", "pdf": "https://arxiv.org/pdf/2505.13219", "abs": "https://arxiv.org/abs/2505.13219", "authors": ["Jiafu Wu", "Yabiao Wang", "Jian Li", "Jinlong Peng", "Yun Cao", "Chengjie Wang", "Jiangning Zhang"], "title": "Swin DiT: Diffusion Transformer using Pseudo Shifted Windows", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion Transformers (DiTs) achieve remarkable performance within the\ndomain of image generation through the incorporation of the transformer\narchitecture. Conventionally, DiTs are constructed by stacking serial isotropic\nglobal information modeling transformers, which face significant computational\ncost when processing high-resolution images. We empirically analyze that latent\nspace image generation does not exhibit a strong dependence on global\ninformation as traditionally assumed. Most of the layers in the model\ndemonstrate redundancy in global computation. In addition, conventional\nattention mechanisms exhibit low-frequency inertia issues. To address these\nissues, we propose \\textbf{P}seudo \\textbf{S}hifted \\textbf{W}indow\n\\textbf{A}ttention (PSWA), which fundamentally mitigates global model\nredundancy. PSWA achieves intermediate global-local information interaction\nthrough window attention, while employing a high-frequency bridging branch to\nsimulate shifted window operations, supplementing appropriate global and\nhigh-frequency information. Furthermore, we propose the Progressive Coverage\nChannel Allocation(PCCA) strategy that captures high-order attention similarity\nwithout additional computational cost. Building upon all of them, we propose a\nseries of Pseudo \\textbf{S}hifted \\textbf{Win}dow DiTs (\\textbf{Swin DiT}),\naccompanied by extensive experiments demonstrating their superior performance.\nFor example, our proposed Swin-DiT-L achieves a 54%$\\uparrow$ FID improvement\nover DiT-XL/2 while requiring less computational.\nhttps://github.com/wujiafu007/Swin-DiT"}
{"id": "2505.13268", "pdf": "https://arxiv.org/pdf/2505.13268", "abs": "https://arxiv.org/abs/2505.13268", "authors": ["Livia Qian", "Carol Figueroa", "Gabriel Skantze"], "title": "Representation of perceived prosodic similarity of conversational feedback", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Interspeech 2025", "summary": "Vocal feedback (e.g., `mhm', `yeah', `okay') is an important component of\nspoken dialogue and is crucial to ensuring common ground in conversational\nsystems. The exact meaning of such feedback is conveyed through both lexical\nand prosodic form. In this work, we investigate the perceived prosodic\nsimilarity of vocal feedback with the same lexical form, and to what extent\nexisting speech representations reflect such similarities. A triadic comparison\ntask with recruited participants is used to measure perceived similarity of\nfeedback responses taken from two different datasets. We find that spectral and\nself-supervised speech representations encode prosody better than extracted\npitch features, especially in the case of feedback from the same speaker. We\nalso find that it is possible to further condense and align the representations\nto human perception through contrastive learning."}
{"id": "2505.11804", "pdf": "https://arxiv.org/pdf/2505.11804", "abs": "https://arxiv.org/abs/2505.11804", "authors": ["Xi Wang", "Eric Nalisnick"], "title": "Are vision language models robust to uncertain inputs?", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Robustness against uncertain and ambiguous inputs is a critical challenge for\ndeep learning models. While recent advancements in large scale vision language\nmodels (VLMs, e.g. GPT4o) might suggest that increasing model and training\ndataset size would mitigate this issue, our empirical evaluation shows a more\ncomplicated picture. Testing models using two classic uncertainty\nquantification tasks, anomaly detection and classification under inherently\nambiguous conditions, we find that newer and larger VLMs indeed exhibit\nimproved robustness compared to earlier models, but still suffer from a\ntendency to strictly follow instructions, often causing them to hallucinate\nconfident responses even when faced with unclear or anomalous inputs.\nRemarkably, for natural images such as ImageNet, this limitation can be\novercome without pipeline modifications: simply prompting models to abstain\nfrom uncertain predictions enables significant reliability gains, achieving\nnear-perfect robustness in several settings. However, for domain-specific tasks\nsuch as galaxy morphology classification, a lack of specialized knowledge\nprevents reliable uncertainty estimation. Finally, we propose a novel mechanism\nbased on caption diversity to reveal a model's internal uncertainty, enabling\npractitioners to predict when models will successfully abstain without relying\non labeled data."}
{"id": "2505.13225", "pdf": "https://arxiv.org/pdf/2505.13225", "abs": "https://arxiv.org/abs/2505.13225", "authors": ["David Levin", "Gonen Singer"], "title": "Automatic Complementary Separation Pruning Toward Lightweight CNNs", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we present Automatic Complementary Separation Pruning (ACSP),\na novel and fully automated pruning method for convolutional neural networks.\nACSP integrates the strengths of both structured pruning and activation-based\npruning, enabling the efficient removal of entire components such as neurons\nand channels while leveraging activations to identify and retain the most\nrelevant components. Our approach is designed specifically for supervised\nlearning tasks, where we construct a graph space that encodes the separation\ncapabilities of each component with respect to all class pairs. By employing\ncomplementary selection principles and utilizing a clustering algorithm, ACSP\nensures that the selected components maintain diverse and complementary\nseparation capabilities, reducing redundancy and maintaining high network\nperformance. The method automatically determines the optimal subset of\ncomponents in each layer, utilizing a knee-finding algorithm to select the\nminimal subset that preserves performance without requiring user-defined\npruning volumes. Extensive experiments on multiple architectures, including\nVGG-16, ResNet-50, and MobileNet-V2, across datasets like CIFAR-10, CIFAR-100,\nand ImageNet-1K, demonstrate that ACSP achieves competitive accuracy compared\nto other methods while significantly reducing computational costs. This fully\nautomated approach not only enhances scalability but also makes ACSP especially\npractical for real-world deployment by eliminating the need for manually\ndefining the pruning volume."}
{"id": "2505.13271", "pdf": "https://arxiv.org/pdf/2505.13271", "abs": "https://arxiv.org/abs/2505.13271", "authors": ["Lei Sheng", "Shuai-Shuai Xu"], "title": "CSC-SQL: Corrective Self-Consistency in Text-to-SQL via Reinforcement Learning", "categories": ["cs.CL"], "comment": "11 pages, 5 figures", "summary": "Large language models (LLMs) have demonstrated strong capabilities in\ntranslating natural language questions about relational databases into SQL\nqueries. In particular, test-time scaling techniques such as Self-Consistency\nand Self-Correction can enhance SQL generation accuracy by increasing\ncomputational effort during inference. However, these methods have notable\nlimitations: Self-Consistency may select suboptimal outputs despite majority\nvotes, while Self-Correction typically addresses only syntactic errors. To\nleverage the strengths of both approaches, we propose CSC-SQL, a novel method\nthat integrates Self-Consistency and Self-Correction. CSC-SQL selects the two\nmost frequently occurring outputs from parallel sampling and feeds them into a\nmerge revision model for correction. Additionally, we employ the Group Relative\nPolicy Optimization (GRPO) algorithm to fine-tune both the SQL generation and\nrevision models via reinforcement learning, significantly enhancing output\nquality. Experimental results confirm the effectiveness and generalizability of\nCSC-SQL. On the BIRD development set, our 3B model achieves 65.28% execution\naccuracy, while the 7B model achieves 69.19%. The code will be open sourced at\nhttps://github.com/CycloneBoy/csc_sql."}
{"id": "2505.11807", "pdf": "https://arxiv.org/pdf/2505.11807", "abs": "https://arxiv.org/abs/2505.11807", "authors": ["Yufei Xiang", "Yiqun Shen", "Yeqin Zhang", "Cam-Tu Nguyen"], "title": "Retrospex: Language Agent Meets Offline Reinforcement Learning Critic", "categories": ["cs.CL", "cs.AI"], "comment": "17 pages", "summary": "Large Language Models (LLMs) possess extensive knowledge and commonsense\nreasoning capabilities, making them valuable for creating powerful agents.\nHowever, existing LLM agent frameworks have not fully utilized past experiences\nfor improvement. This work introduces a new LLM-based agent framework called\nRetrospex, which addresses this challenge by analyzing past experiences in\ndepth. Unlike previous approaches, Retrospex does not directly integrate\nexperiences into the LLM's context. Instead, it combines the LLM's action\nlikelihood with action values estimated by a Reinforcement Learning (RL)\nCritic, which is trained on past experiences through an offline\n''retrospection'' process. Additionally, Retrospex employs a dynamic action\nrescoring mechanism that increases the importance of experience-based values\nfor tasks that require more interaction with the environment. We evaluate\nRetrospex in ScienceWorld, ALFWorld and Webshop environments, demonstrating its\nadvantages over strong, contemporary baselines."}
{"id": "2505.13233", "pdf": "https://arxiv.org/pdf/2505.13233", "abs": "https://arxiv.org/abs/2505.13233", "authors": ["Lincan Cai", "Jingxuan Kang", "Shuang Li", "Wenxuan Ma", "Binhui Xie", "Zhida Qin", "Jian Liang"], "title": "From Local Details to Global Context: Advancing Vision-Language Models with Attention-Based Selection", "categories": ["cs.CV"], "comment": null, "summary": "Pretrained vision-language models (VLMs), e.g., CLIP, demonstrate impressive\nzero-shot capabilities on downstream tasks. Prior research highlights the\ncrucial role of visual augmentation techniques, like random cropping, in\nalignment with fine-grained class descriptions generated by large language\nmodels (LLMs), significantly enhancing zero-shot performance by incorporating\nmulti-view information. However, the inherent randomness of these augmentations\ncan inevitably introduce background artifacts and cause models to overly focus\non local details, compromising global semantic understanding. To address these\nissues, we propose an \\textbf{A}ttention-\\textbf{B}ased \\textbf{S}election\n(\\textbf{ABS}) method from local details to global context, which applies\nattention-guided cropping in both raw images and feature space, supplement\nglobal semantic information through strategic feature selection. Additionally,\nwe introduce a soft matching technique to effectively filter LLM descriptions\nfor better alignment. \\textbf{ABS} achieves state-of-the-art performance on\nout-of-distribution generalization and zero-shot classification tasks. Notably,\n\\textbf{ABS} is training-free and even rivals few-shot and test-time adaptation\nmethods. Our code is available at\n\\href{https://github.com/BIT-DA/ABS}{\\textcolor{darkgreen}{https://github.com/BIT-DA/ABS}}."}
{"id": "2505.13282", "pdf": "https://arxiv.org/pdf/2505.13282", "abs": "https://arxiv.org/abs/2505.13282", "authors": ["Sahil Mishra", "Kumar Arjun", "Tanmoy Chakraborty"], "title": "$\\textit{Rank, Chunk and Expand}$: Lineage-Oriented Reasoning for Taxonomy Expansion", "categories": ["cs.CL"], "comment": null, "summary": "Taxonomies are hierarchical knowledge graphs crucial for recommendation\nsystems, and web applications. As data grows, expanding taxonomies is\nessential, but existing methods face key challenges: (1) discriminative models\nstruggle with representation limits and generalization, while (2) generative\nmethods either process all candidates at once, introducing noise and exceeding\ncontext limits, or discard relevant entities by selecting noisy candidates. We\npropose LORex ($\\textbf{L}$ineage-$\\textbf{O}$riented $\\textbf{Re}$asoning for\nTaxonomy E$\\textbf{x}$pansion), a plug-and-play framework that combines\ndiscriminative ranking and generative reasoning for efficient taxonomy\nexpansion. Unlike prior methods, LORex ranks and chunks candidate terms into\nbatches, filtering noise and iteratively refining selections by reasoning\ncandidates' hierarchy to ensure contextual efficiency. Extensive experiments\nacross four benchmarks and twelve baselines show that LORex improves accuracy\nby 12% and Wu & Palmer similarity by 5% over state-of-the-art methods."}
{"id": "2505.11813", "pdf": "https://arxiv.org/pdf/2505.11813", "abs": "https://arxiv.org/abs/2505.11813", "authors": ["Yixuan Dong", "Fang-Yi Su", "Jung-Hsien Chiang"], "title": "SGD-Mix: Enhancing Domain-Specific Image Classification with Label-Preserving Data Augmentation", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 6 figures, 6 tables", "summary": "Data augmentation for domain-specific image classification tasks often\nstruggles to simultaneously address diversity, faithfulness, and label clarity\nof generated data, leading to suboptimal performance in downstream tasks. While\nexisting generative diffusion model-based methods aim to enhance augmentation,\nthey fail to cohesively tackle these three critical aspects and often overlook\nintrinsic challenges of diffusion models, such as sensitivity to model\ncharacteristics and stochasticity under strong transformations. In this paper,\nwe propose a novel framework that explicitly integrates diversity,\nfaithfulness, and label clarity into the augmentation process. Our approach\nemploys saliency-guided mixing and a fine-tuned diffusion model to preserve\nforeground semantics, enrich background diversity, and ensure label\nconsistency, while mitigating diffusion model limitations. Extensive\nexperiments across fine-grained, long-tail, few-shot, and background robustness\ntasks demonstrate our method's superior performance over state-of-the-art\napproaches."}
{"id": "2505.13235", "pdf": "https://arxiv.org/pdf/2505.13235", "abs": "https://arxiv.org/abs/2505.13235", "authors": ["Dang Hoai Nam", "Huynh Tong Dang Khoa", "Vo Nguyen Le Duy"], "title": "WriteViT: Handwritten Text Generation with Vision Transformer", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Humans can quickly generalize handwriting styles from a single example by\nintuitively separating content from style. Machines, however, struggle with\nthis task, especially in low-data settings, often missing subtle spatial and\nstylistic cues. Motivated by this gap, we introduce WriteViT, a one-shot\nhandwritten text synthesis framework that incorporates Vision Transformers\n(ViT), a family of models that have shown strong performance across various\ncomputer vision tasks. WriteViT integrates a ViT-based Writer Identifier for\nextracting style embeddings, a multi-scale generator built with Transformer\nencoder-decoder blocks enhanced by conditional positional encoding (CPE), and a\nlightweight ViT-based recognizer. While previous methods typically rely on CNNs\nor CRNNs, our design leverages transformers in key components to better capture\nboth fine-grained stroke details and higher-level style information. Although\nhandwritten text synthesis has been widely explored, its application to\nVietnamese -- a language rich in diacritics and complex typography -- remains\nlimited. Experiments on Vietnamese and English datasets demonstrate that\nWriteViT produces high-quality, style-consistent handwriting while maintaining\nstrong recognition performance in low-resource scenarios. These results\nhighlight the promise of transformer-based designs for multilingual handwriting\ngeneration and efficient style adaptation."}
{"id": "2505.13302", "pdf": "https://arxiv.org/pdf/2505.13302", "abs": "https://arxiv.org/abs/2505.13302", "authors": ["Alice Plebe", "Timothy Douglas", "Diana Riazi", "R. Maria del Rio-Chanona"], "title": "I'll believe it when I see it: Images increase misinformation sharing in Vision-Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models are increasingly integrated into news recommendation\nsystems, raising concerns about their role in spreading misinformation. In\nhumans, visual content is known to boost credibility and shareability of\ninformation, yet its effect on vision-language models (VLMs) remains unclear.\nWe present the first study examining how images influence VLMs' propensity to\nreshare news content, whether this effect varies across model families, and how\npersona conditioning and content attributes modulate this behavior. To support\nthis analysis, we introduce two methodological contributions: a\njailbreaking-inspired prompting strategy that elicits resharing decisions from\nVLMs while simulating users with antisocial traits and political alignments;\nand a multimodal dataset of fact-checked political news from PolitiFact, paired\nwith corresponding images and ground-truth veracity labels. Experiments across\nmodel families reveal that image presence increases resharing rates by 4.8% for\ntrue news and 15.0% for false news. Persona conditioning further modulates this\neffect: Dark Triad traits amplify resharing of false news, whereas\nRepublican-aligned profiles exhibit reduced veracity sensitivity. Of all the\ntested models, only Claude-3-Haiku demonstrates robustness to visual\nmisinformation. These findings highlight emerging risks in multimodal model\nbehavior and motivate the development of tailored evaluation frameworks and\nmitigation strategies for personalized AI systems. Code and dataset are\navailable at: https://github.com/3lis/misinfo_vlm"}
{"id": "2505.11824", "pdf": "https://arxiv.org/pdf/2505.11824", "abs": "https://arxiv.org/abs/2505.11824", "authors": ["Minsu Kim", "Jean-Pierre Falet", "Oliver E. Richardson", "Xiaoyin Chen", "Moksh Jain", "Sungjin Ahn", "Sungsoo Ahn", "Yoshua Bengio"], "title": "Search-Based Correction of Reasoning Chains for Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Chain-of-Thought (CoT) reasoning has advanced the capabilities and\ntransparency of language models (LMs); however, reasoning chains can contain\ninaccurate statements that reduce performance and trustworthiness. To address\nthis, we introduce a new self-correction framework that augments each reasoning\nstep in a CoT with a latent variable indicating its veracity, enabling modeling\nof all possible truth assignments rather than assuming correctness throughout.\nTo efficiently explore this expanded space, we introduce Search Corrector, a\ndiscrete search algorithm over boolean-valued veracity assignments. It\nefficiently performs otherwise intractable inference in the posterior\ndistribution over veracity assignments by leveraging the LM's joint likelihood\nover veracity and the final answer as a proxy reward. This efficient\ninference-time correction method facilitates supervised fine-tuning of an\nAmortized Corrector by providing pseudo-labels for veracity. The Amortized\nCorrector generalizes self-correction, enabling accurate zero-shot veracity\ninference in novel contexts. Empirical results demonstrate that Search\nCorrector reliably identifies errors in logical (ProntoQA) and mathematical\nreasoning (GSM8K) benchmarks. The Amortized Corrector achieves comparable\nzero-shot accuracy and improves final answer accuracy by up to 25%."}
{"id": "2505.13250", "pdf": "https://arxiv.org/pdf/2505.13250", "abs": "https://arxiv.org/abs/2505.13250", "authors": ["Hashan K. Weerasooriya", "Prateek Chennuri", "Weijian Zhang", "Istvan Gyongy", "Stanley H. Chan"], "title": "Joint Depth and Reflectivity Estimation using Single-Photon LiDAR", "categories": ["cs.CV"], "comment": null, "summary": "Single-Photon Light Detection and Ranging (SP-LiDAR is emerging as a leading\ntechnology for long-range, high-precision 3D vision tasks. In SP-LiDAR,\ntimestamps encode two complementary pieces of information: pulse travel time\n(depth) and the number of photons reflected by the object (reflectivity).\nExisting SP-LiDAR reconstruction methods typically recover depth and\nreflectivity separately or sequentially use one modality to estimate the other.\nMoreover, the conventional 3D histogram construction is effective mainly for\nslow-moving or stationary scenes. In dynamic scenes, however, it is more\nefficient and effective to directly process the timestamps. In this paper, we\nintroduce an estimation method to simultaneously recover both depth and\nreflectivity in fast-moving scenes. We offer two contributions: (1) A\ntheoretical analysis demonstrating the mutual correlation between depth and\nreflectivity and the conditions under which joint estimation becomes\nbeneficial. (2) A novel reconstruction method, \"SPLiDER\", which exploits the\nshared information to enhance signal recovery. On both synthetic and real\nSP-LiDAR data, our method outperforms existing approaches, achieving superior\njoint reconstruction quality."}
{"id": "2505.13307", "pdf": "https://arxiv.org/pdf/2505.13307", "abs": "https://arxiv.org/abs/2505.13307", "authors": ["Qiguang Chen", "Libo Qin", "Jinhao Liu", "Yue Liao", "Jiaqi Wang", "Jingxuan Zhou", "Wanxiang Che"], "title": "RBF++: Quantifying and Optimizing Reasoning Boundaries across Measurable and Unmeasurable Capabilities for Chain-of-Thought Reasoning", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Manuscript", "summary": "Chain-of-Thought (CoT) reasoning has proven effective in enhancing large\nlanguage models (LLMs) on complex tasks, spurring research into its underlying\nmechanisms. However, two primary challenges remain for real-world applications:\n(1) the lack of quantitative metrics and actionable guidelines for evaluating\nand optimizing measurable boundaries of CoT capability, and (2) the absence of\nmethods to assess boundaries of unmeasurable CoT capability, such as multimodal\nperception. To address these gaps, we introduce the Reasoning Boundary\nFramework++ (RBF++). To tackle the first challenge, we define the reasoning\nboundary (RB) as the maximum limit of CoT performance. We also propose a\ncombination law for RBs, enabling quantitative analysis and offering actionable\nguidance across various CoT tasks. For the second challenge, particularly in\nmultimodal scenarios, we introduce a constant assumption, which replaces\nunmeasurable RBs with scenario-specific constants. Additionally, we propose the\nreasoning boundary division mechanism, which divides unmeasurable RBs into two\nsub-boundaries, facilitating the quantification and optimization of both\nunmeasurable domain knowledge and multimodal perception capabilities. Extensive\nexperiments involving 38 models across 13 tasks validate the feasibility of our\nframework in cross-modal settings. Additionally, we evaluate 10 CoT strategies,\noffer insights into optimization and decay from two complementary perspectives,\nand expand evaluation benchmarks for measuring RBs in LLM reasoning. We hope\nthis work advances the understanding of RBs and optimization strategies in\nLLMs. Code and data are available at\nhttps://github.com/LightChen233/reasoning-boundary."}
{"id": "2505.11825", "pdf": "https://arxiv.org/pdf/2505.11825", "abs": "https://arxiv.org/abs/2505.11825", "authors": ["Xudong Ma"], "title": "Bootstrapping Diffusion: Diffusion Model Training Leveraging Partial and Corrupted Data", "categories": ["cs.CV", "cs.AI"], "comment": "21 pages, 1 figure", "summary": "Training diffusion models requires large datasets. However, acquiring large\nvolumes of high-quality data can be challenging, for example, collecting large\nnumbers of high-resolution images and long videos. On the other hand, there are\nmany complementary data that are usually considered corrupted or partial, such\nas low-resolution images and short videos. Other examples of corrupted data\ninclude videos that contain subtitles, watermarks, and logos. In this study, we\ninvestigate the theoretical problem of whether the above partial data can be\nutilized to train conventional diffusion models. Motivated by our theoretical\nanalysis in this study, we propose a straightforward approach of training\ndiffusion models utilizing partial data views, where we consider each form of\ncomplementary data as a view of conventional data. Our proposed approach first\ntrains one separate diffusion model for each individual view, and then trains a\nmodel for predicting the residual score function. We prove generalization error\nbounds, which show that the proposed diffusion model training approach can\nachieve lower generalization errors if proper regularizations are adopted in\nthe residual score function training. In particular, we prove that the\ndifficulty in training the residual score function scales proportionally with\nthe signal correlations not captured by partial data views. Consequently, the\nproposed approach achieves near first-order optimal data efficiency."}
{"id": "2505.13261", "pdf": "https://arxiv.org/pdf/2505.13261", "abs": "https://arxiv.org/abs/2505.13261", "authors": ["Mingrui Chen", "Haogeng Liu", "Hao Liang", "Huaibo Huang", "Wentao Zhang", "Ran He"], "title": "Unlocking the Potential of Difficulty Prior in RL-based Multimodal Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "In this work, we investigate how explicitly modeling problem's difficulty\nprior information shapes the effectiveness of reinforcement learning based\nfine-tuning for multimodal reasoning. Our exploration mainly comprises of\nfollowing three perspective: First, through offline data curation, we analyze\nthe U-shaped difficulty distribution of two given datasets using the base model\nby multi-round sampling, and then filter out prompts that are either too simple\nor extremely difficult to provide meaningful gradients and perform subsequent\ntwo-stage training. Second, we implement an online advantage differentiation,\ncomputing group-wise empirical accuracy as a difficulty proxy to adaptively\nreweight advantages estimation, providing stronger learning signals for more\nchallenging problems. Finally, we introduce difficulty hints as explicit\nprompts for more complex samples in the second training stage, encouraging the\nmodel to calibrate its reasoning depth and perform reflective validation\nchecks. Our comprehensive approach demonstrates significant performances across\nvarious multi-modal mathematical reasoning benchmarks with only 2K+0.6K\ntwo-stage training data."}
{"id": "2505.13312", "pdf": "https://arxiv.org/pdf/2505.13312", "abs": "https://arxiv.org/abs/2505.13312", "authors": ["Zhijie Deng", "Chris Yuhao Liu", "Zirui Pang", "Xinlei He", "Lei Feng", "Qi Xuan", "Zhaowei Zhu", "Jiaheng Wei"], "title": "GUARD: Generation-time LLM Unlearning via Adaptive Restriction and Detection", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in\nmemorizing vast amounts of knowledge across diverse domains. However, the\nability to selectively forget specific knowledge is critical for ensuring the\nsafety and compliance of deployed models. Existing unlearning efforts typically\nfine-tune the model with resources such as forget data, retain data, and a\ncalibration model. These additional gradient steps blur the decision boundary\nbetween forget and retain knowledge, making unlearning often at the expense of\noverall performance. To avoid the negative impact of fine-tuning, it would be\nbetter to unlearn solely at inference time by safely guarding the model against\ngenerating responses related to the forget target, without destroying the\nfluency of text generation. In this work, we propose Generation-time Unlearning\nvia Adaptive Restriction and Detection (GUARD), a framework that enables\ndynamic unlearning during LLM generation. Specifically, we first employ a\nprompt classifier to detect unlearning targets and extract the corresponding\nforbidden token. We then dynamically penalize and filter candidate tokens\nduring generation using a combination of token matching and semantic matching,\neffectively preventing the model from leaking the forgotten content.\nExperimental results on copyright content unlearning tasks over the Harry\nPotter dataset and the MUSE benchmark, as well as entity unlearning tasks on\nthe TOFU dataset, demonstrate that GUARD achieves strong forget quality across\nvarious tasks while causing almost no degradation to the LLM's general\ncapabilities, striking an excellent trade-off between forgetting and utility."}
{"id": "2505.11827", "pdf": "https://arxiv.org/pdf/2505.11827", "abs": "https://arxiv.org/abs/2505.11827", "authors": ["Yansong Ning", "Wei Li", "Jun Fang", "Naiqiang Tan", "Hao Liu"], "title": "Not All Thoughts are Generated Equal: Efficient LLM Reasoning via Multi-Turn Reinforcement Learning", "categories": ["cs.CL", "cs.AI"], "comment": "In progress", "summary": "Compressing long chain-of-thought (CoT) from large language models (LLMs) is\nan emerging strategy to improve the reasoning efficiency of LLMs. Despite its\npromising benefits, existing studies equally compress all thoughts within a\nlong CoT, hindering more concise and effective reasoning. To this end, we first\ninvestigate the importance of different thoughts by examining their\neffectiveness and efficiency in contributing to reasoning through automatic\nlong CoT chunking and Monte Carlo rollouts. Building upon the insights, we\npropose a theoretically bounded metric to jointly measure the effectiveness and\nefficiency of different thoughts. We then propose Long$\\otimes$Short, an\nefficient reasoning framework that enables two LLMs to collaboratively solve\nthe problem: a long-thought LLM for more effectively generating important\nthoughts, while a short-thought LLM for efficiently generating remaining\nthoughts. Specifically, we begin by synthesizing a small amount of cold-start\ndata to fine-tune LLMs for long-thought and short-thought reasoning styles,\nrespectively. Furthermore, we propose a synergizing-oriented multi-turn\nreinforcement learning, focusing on the model self-evolution and collaboration\nbetween long-thought and short-thought LLMs. Experimental results show that our\nmethod enables Qwen2.5-7B and Llama3.1-8B to achieve comparable performance\ncompared to DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B, while\nreducing token length by over 80% across the MATH500, AIME24/25, AMC23, and\nGPQA Diamond benchmarks. Our data and code are available at\nhttps://github.com/yasNing/Long-otimes-Short/."}
{"id": "2505.13266", "pdf": "https://arxiv.org/pdf/2505.13266", "abs": "https://arxiv.org/abs/2505.13266", "authors": ["Yehao Liu", "Xiaosu Xu", "Zijian Wang", "Yiqing Yao"], "title": "DB3D-L: Depth-aware BEV Feature Transformation for Accurate 3D Lane Detection", "categories": ["cs.CV"], "comment": null, "summary": "3D Lane detection plays an important role in autonomous driving. Recent\nadvances primarily build Birds-Eye-View (BEV) feature from front-view (FV)\nimages to perceive 3D information of Lane more effectively. However,\nconstructing accurate BEV information from FV image is limited due to the\nlacking of depth information, causing previous works often rely heavily on the\nassumption of a flat ground plane. Leveraging monocular depth estimation to\nassist in constructing BEV features is less constrained, but existing methods\nstruggle to effectively integrate the two tasks. To address the above issue, in\nthis paper, an accurate 3D lane detection method based on depth-aware BEV\nfeature transtormation is proposed. In detail, an effective feature extraction\nmodule is designed, in which a Depth Net is integrated to obtain the vital\ndepth information for 3D perception, thereby simplifying the complexity of view\ntransformation. Subquently a feature reduce module is proposed to reduce height\ndimension of FV features and depth features, thereby enables effective fusion\nof crucial FV features and depth features. Then a fusion module is designed to\nbuild BEV feature from prime FV feature and depth information. The proposed\nmethod performs comparably with state-of-the-art methods on both synthetic\nApollo, realistic OpenLane datasets."}
{"id": "2505.13328", "pdf": "https://arxiv.org/pdf/2505.13328", "abs": "https://arxiv.org/abs/2505.13328", "authors": ["Hongru Wang", "Wenyu Huang", "Yufei Wang", "Yuanhao Xi", "Jianqiao Lu", "Huan Zhang", "Nan Hu", "Zeming Liu", "Jeff Z. Pan", "Kam-Fai Wong"], "title": "Rethinking Stateful Tool Use in Multi-Turn Dialogues: Benchmarks and Challenges", "categories": ["cs.CL"], "comment": null, "summary": "Existing benchmarks that assess Language Models (LMs) as Language Agents\n(LAs) for tool use primarily focus on stateless, single-turn interactions or\npartial evaluations, such as tool selection in a single turn, overlooking the\ninherent stateful nature of interactions in multi-turn applications. To fulfill\nthis gap, we propose \\texttt{DialogTool}, a multi-turn dialogue dataset with\nstateful tool interactions considering the whole life cycle of tool use, across\nsix key tasks in three stages: 1) \\textit{tool creation}; 2) \\textit{tool\nutilization}: tool awareness, tool selection, tool execution; and 3)\n\\textit{role-consistent response}: response generation and role play.\nFurthermore, we build \\texttt{VirtualMobile} -- an embodied virtual mobile\nevaluation environment to simulate API calls and assess the robustness of the\ncreated APIs\\footnote{We will use tools and APIs alternatively, there are no\nsignificant differences between them in this paper.}. Taking advantage of these\nartifacts, we conduct comprehensive evaluation on 13 distinct open- and\nclosed-source LLMs and provide detailed analysis at each stage, revealing that\nthe existing state-of-the-art LLMs still cannot perform well to use tools over\nlong horizons."}
{"id": "2505.11830", "pdf": "https://arxiv.org/pdf/2505.11830", "abs": "https://arxiv.org/abs/2505.11830", "authors": ["Hongbo Jin", "Ruyang Liu", "Wenhao Zhang", "Guibo Luo", "Ge Li"], "title": "CoT-Vid: Dynamic Chain-of-Thought Routing with Self Verification for Training-Free Video Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": "19 pages, 7 figures", "summary": "System2 reasoning is developing rapidly these days with the emergence of\nDeep- Thinking Models and chain-of-thought technology, which has become a\ncentralized discussion point in the AI community. However, there is a relative\ngap in the research on complex video reasoning at present. In this work, we\npropose CoT-Vid, a novel training-free paradigm for the video domain with a\nmultistage complex reasoning design. Distinguishing from existing video LLMs,\nwhich rely heavily on perceptual abilities, it achieved surprising performance\ngain with explicit reasoning mechanism. The paradigm consists of three main\ncomponents: dynamic inference path routing, problem decoupling strategy, and\nvideo self-consistency verification. In addition, we propose a new standard for\ncategorization of video questions. CoT- Vid showed outstanding results on a\nwide range of benchmarks, and outperforms its base model by 9.3% on Egochema\nand 5.6% on VideoEspresso, rivalling or even surpassing larger and proprietary\nmodels, such as GPT-4V, GPT-4o and Gemini-1.5-flash. Our codebase will be\npublicly available soon."}
{"id": "2505.13279", "pdf": "https://arxiv.org/pdf/2505.13279", "abs": "https://arxiv.org/abs/2505.13279", "authors": ["Zhiqiang Yan", "Jianhao Jiao", "Zhengxue Wang", "Gim Hee Lee"], "title": "Event-Driven Dynamic Scene Depth Completion", "categories": ["cs.CV"], "comment": "9 pages", "summary": "Depth completion in dynamic scenes poses significant challenges due to rapid\nego-motion and object motion, which can severely degrade the quality of input\nmodalities such as RGB images and LiDAR measurements. Conventional RGB-D\nsensors often struggle to align precisely and capture reliable depth under such\nconditions. In contrast, event cameras with their high temporal resolution and\nsensitivity to motion at the pixel level provide complementary cues that are\n%particularly beneficial in dynamic environments.To this end, we propose\nEventDC, the first event-driven depth completion framework. It consists of two\nkey components: Event-Modulated Alignment (EMA) and Local Depth Filtering\n(LDF). Both modules adaptively learn the two fundamental components of\nconvolution operations: offsets and weights conditioned on motion-sensitive\nevent streams. In the encoder, EMA leverages events to modulate the sampling\npositions of RGB-D features to achieve pixel redistribution for improved\nalignment and fusion. In the decoder, LDF refines depth estimations around\nmoving objects by learning motion-aware masks from events. Additionally,\nEventDC incorporates two loss terms to further benefit global alignment and\nenhance local depth recovery. Moreover, we establish the first benchmark for\nevent-based depth completion comprising one real-world and two synthetic\ndatasets to facilitate future research. Extensive experiments on this benchmark\ndemonstrate the superiority of our EventDC."}
{"id": "2505.13338", "pdf": "https://arxiv.org/pdf/2505.13338", "abs": "https://arxiv.org/abs/2505.13338", "authors": ["Qiongqiong Wang", "Hardik B. Sailor", "Tianchi Liu", "Ai Ti Aw"], "title": "Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data Condensation and Spoken QA Generation", "categories": ["cs.CL", "cs.AI", "eess.AS"], "comment": "Accepted at Interspeech 2025", "summary": "Current speech-LLMs exhibit limited capability in contextual reasoning\nalongside paralinguistic understanding, primarily due to the lack of\nQuestion-Answer (QA) datasets that cover both aspects. We propose a novel\nframework for dataset generation from in-the-wild speech data, that integrates\ncontextual reasoning with paralinguistic information. It consists of a pseudo\nparalinguistic label-based data condensation of in-the-wild speech and\nLLM-based Contextual Paralinguistic QA (CPQA) generation. The effectiveness is\nvalidated by a strong correlation in evaluations of the Qwen2-Audio-7B-Instruct\nmodel on a dataset created by our framework and human-generated CPQA dataset.\nThe results also reveal the speech-LLM's limitations in handling empathetic\nreasoning tasks, highlighting the need for such datasets and more robust\nmodels. The proposed framework is first of its kind and has potential in\ntraining more robust speech-LLMs with paralinguistic reasoning capabilities."}
{"id": "2505.11835", "pdf": "https://arxiv.org/pdf/2505.11835", "abs": "https://arxiv.org/abs/2505.11835", "authors": ["Hongliang Li", "Jinan Xu", "Gengping Cui", "Changhao Guan", "Fengran Mo", "Kaiyu Huang"], "title": "Multilingual Collaborative Defense for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "19 pages, 4figures", "summary": "The robustness and security of large language models (LLMs) has become a\nprominent research area. One notable vulnerability is the ability to bypass LLM\nsafeguards by translating harmful queries into rare or underrepresented\nlanguages, a simple yet effective method of \"jailbreaking\" these models.\nDespite the growing concern, there has been limited research addressing the\nsafeguarding of LLMs in multilingual scenarios, highlighting an urgent need to\nenhance multilingual safety. In this work, we investigate the correlation\nbetween various attack features across different languages and propose\nMultilingual Collaborative Defense (MCD), a novel learning method that\noptimizes a continuous, soft safety prompt automatically to facilitate\nmultilingual safeguarding of LLMs. The MCD approach offers three advantages:\nFirst, it effectively improves safeguarding performance across multiple\nlanguages. Second, MCD maintains strong generalization capabilities while\nminimizing false refusal rates. Third, MCD mitigates the language safety\nmisalignment caused by imbalances in LLM training corpora. To evaluate the\neffectiveness of MCD, we manually construct multilingual versions of commonly\nused jailbreak benchmarks, such as MaliciousInstruct and AdvBench, to assess\nvarious safeguarding methods. Additionally, we introduce these datasets in\nunderrepresented (zero-shot) languages to verify the language transferability\nof MCD. The results demonstrate that MCD outperforms existing approaches in\nsafeguarding against multilingual jailbreak attempts while also exhibiting\nstrong language transfer capabilities. Our code is available at\nhttps://github.com/HLiang-Lee/MCD."}
{"id": "2505.13281", "pdf": "https://arxiv.org/pdf/2505.13281", "abs": "https://arxiv.org/abs/2505.13281", "authors": ["Zekun Wang", "Sashank Varma"], "title": "Computer Vision Models Show Human-Like Sensitivity to Geometric and Topological Concepts", "categories": ["cs.CV"], "comment": "10 pages, 4 figures, CosSci 2025", "summary": "With the rapid improvement of machine learning (ML) models, cognitive\nscientists are increasingly asking about their alignment with how humans think.\nHere, we ask this question for computer vision models and human sensitivity to\ngeometric and topological (GT) concepts. Under the core knowledge account,\nthese concepts are innate and supported by dedicated neural circuitry. In this\nwork, we investigate an alternative explanation, that GT concepts are learned\n``for free'' through everyday interaction with the environment. We do so using\ncomputer visions models, which are trained on large image datasets. We build on\nprior studies to investigate the overall performance and human alignment of\nthree classes of models -- convolutional neural networks (CNNs),\ntransformer-based models, and vision-language models -- on an odd-one-out task\ntesting 43 GT concepts spanning seven classes. Transformer-based models achieve\nthe highest overall accuracy, surpassing that of young children. They also show\nstrong alignment with children's performance, finding the same classes of\nconcepts easy vs. difficult. By contrast, vision-language models underperform\ntheir vision-only counterparts and deviate further from human profiles,\nindicating that na\\\"ive multimodality might compromise abstract geometric\nsensitivity. These findings support the use of computer vision models to\nevaluate the sufficiency of the learning account for explaining human\nsensitivity to GT concepts, while also suggesting that integrating linguistic\nand visual representations might have unpredicted deleterious consequences."}
{"id": "2505.13346", "pdf": "https://arxiv.org/pdf/2505.13346", "abs": "https://arxiv.org/abs/2505.13346", "authors": ["Austin Xu", "Yilun Zhou", "Xuan-Phi Nguyen", "Caiming Xiong", "Shafiq Joty"], "title": "J4R: Learning to Judge with Equivalent Initial State Group Relative Preference Optimization", "categories": ["cs.CL", "cs.AI"], "comment": "25 pages, 4 figures, 6 tables. To be updated with links for\n  code/benchmark", "summary": "To keep pace with the increasing pace of large language models (LLM)\ndevelopment, model output evaluation has transitioned away from time-consuming\nhuman evaluation to automatic evaluation, where LLMs themselves are tasked with\nassessing and critiquing other model outputs. LLM-as-judge models are a class\nof generative evaluators that excel in evaluating relatively simple domains,\nlike chat quality, but struggle in reasoning intensive domains where model\nresponses contain more substantive and challenging content. To remedy existing\njudge shortcomings, we explore training judges with reinforcement learning\n(RL). We make three key contributions: (1) We propose the Equivalent Initial\nState Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us\nto train our judge to be robust to positional biases that arise in more complex\nevaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that\nevaluates judges in diverse reasoning settings not covered by prior work. (3)\nWe train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that\noutperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or\nexceeding the performance of larger GRPO-trained judges on both JudgeBench and\nReasoningJudgeBench."}
{"id": "2505.11836", "pdf": "https://arxiv.org/pdf/2505.11836", "abs": "https://arxiv.org/abs/2505.11836", "authors": ["Jeremy Budd", "Javier Ideami", "Benjamin Macdowall Rynne", "Keith Duggar", "Randall Balestriero"], "title": "SplInterp: Improving our Understanding and Training of Sparse Autoencoders", "categories": ["cs.LG", "cs.AI", "68T07, 65D07"], "comment": "44 pages, 38 figures, under review", "summary": "Sparse autoencoders (SAEs) have received considerable recent attention as\ntools for mechanistic interpretability, showing success at extracting\ninterpretable features even from very large LLMs. However, this research has\nbeen largely empirical, and there have been recent doubts about the true\nutility of SAEs. In this work, we seek to enhance the theoretical understanding\nof SAEs, using the spline theory of deep learning. By situating SAEs in this\nframework: we discover that SAEs generalise ``$k$-means autoencoders'' to be\npiecewise affine, but sacrifice accuracy for interpretability vs. the optimal\n``$k$-means-esque plus local principal component analysis (PCA)'' piecewise\naffine autoencoder. We characterise the underlying geometry of (TopK) SAEs\nusing power diagrams. And we develop a novel proximal alternating method SGD\n(PAM-SGD) algorithm for training SAEs, with both solid theoretical foundations\nand promising empirical results in MNIST and LLM experiments, particularly in\nsample efficiency and (in the LLM setting) improved sparsity of codes. All code\nis available at: https://github.com/splInterp2025/splInterp"}
{"id": "2505.13300", "pdf": "https://arxiv.org/pdf/2505.13300", "abs": "https://arxiv.org/abs/2505.13300", "authors": ["Zekai Li", "Xinhao Zhong", "Samir Khaki", "Zhiyuan Liang", "Yuhao Zhou", "Mingjia Shi", "Ziqiao Wang", "Xuanlei Zhao", "Wangbo Zhao", "Ziheng Qin", "Mengxuan Wu", "Pengfei Zhou", "Haonan Wang", "David Junhao Zhang", "Jia-Wei Liu", "Shaobo Wang", "Dai Liu", "Linfeng Zhang", "Guang Li", "Kun Wang", "Zheng Zhu", "Zhiheng Ma", "Joey Tianyi Zhou", "Jiancheng Lv", "Yaochu Jin", "Peihao Wang", "Kaipeng Zhang", "Lingjuan Lyu", "Yiran Huang", "Zeynep Akata", "Zhiwei Deng", "Xindi Wu", "George Cazenavette", "Yuzhang Shang", "Justin Cui", "Jindong Gu", "Qian Zheng", "Hao Ye", "Shuo Wang", "Xiaobo Wang", "Yan Yan", "Angela Yao", "Mike Zheng Shou", "Tianlong Chen", "Hakan Bilen", "Baharan Mirzasoleiman", "Manolis Kellis", "Konstantinos N. Plataniotis", "Zhangyang Wang", "Bo Zhao", "Yang You", "Kai Wang"], "title": "DD-Ranking: Rethinking the Evaluation of Dataset Distillation", "categories": ["cs.CV"], "comment": "20 pages, 4 figures", "summary": "In recent years, dataset distillation has provided a reliable solution for\ndata compression, where models trained on the resulting smaller synthetic\ndatasets achieve performance comparable to those trained on the original\ndatasets. To further improve the performance of synthetic datasets, various\ntraining pipelines and optimization objectives have been proposed, greatly\nadvancing the field of dataset distillation. Recent decoupled dataset\ndistillation methods introduce soft labels and stronger data augmentation\nduring the post-evaluation phase and scale dataset distillation up to larger\ndatasets (e.g., ImageNet-1K). However, this raises a question: Is accuracy\nstill a reliable metric to fairly evaluate dataset distillation methods? Our\nempirical findings suggest that the performance improvements of these methods\noften stem from additional techniques rather than the inherent quality of the\nimages themselves, with even randomly sampled images achieving superior\nresults. Such misaligned evaluation settings severely hinder the development of\nDD. Therefore, we propose DD-Ranking, a unified evaluation framework, along\nwith new general evaluation metrics to uncover the true performance\nimprovements achieved by different methods. By refocusing on the actual\ninformation enhancement of distilled datasets, DD-Ranking provides a more\ncomprehensive and fair evaluation standard for future research advancements."}
{"id": "2505.13348", "pdf": "https://arxiv.org/pdf/2505.13348", "abs": "https://arxiv.org/abs/2505.13348", "authors": ["Narek Maloyan", "Bislan Ashinov", "Dmitry Namiot"], "title": "Investigating the Vulnerability of LLM-as-a-Judge Architectures to Prompt-Injection Attacks", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly employed as evaluators\n(LLM-as-a-Judge) for assessing the quality of machine-generated text. This\nparadigm offers scalability and cost-effectiveness compared to human\nannotation. However, the reliability and security of such systems, particularly\ntheir robustness against adversarial manipulations, remain critical concerns.\nThis paper investigates the vulnerability of LLM-as-a-Judge architectures to\nprompt-injection attacks, where malicious inputs are designed to compromise the\njudge's decision-making process. We formalize two primary attack strategies:\nComparative Undermining Attack (CUA), which directly targets the final decision\noutput, and Justification Manipulation Attack (JMA), which aims to alter the\nmodel's generated reasoning. Using the Greedy Coordinate Gradient (GCG)\noptimization method, we craft adversarial suffixes appended to one of the\nresponses being compared. Experiments conducted on the MT-Bench Human Judgments\ndataset with open-source instruction-tuned LLMs (Qwen2.5-3B-Instruct and\nFalcon3-3B-Instruct) demonstrate significant susceptibility. The CUA achieves\nan Attack Success Rate (ASR) exceeding 30\\%, while JMA also shows notable\neffectiveness. These findings highlight substantial vulnerabilities in current\nLLM-as-a-Judge systems, underscoring the need for robust defense mechanisms and\nfurther research into adversarial evaluation and trustworthiness in LLM-based\nassessment frameworks."}
{"id": "2505.11837", "pdf": "https://arxiv.org/pdf/2505.11837", "abs": "https://arxiv.org/abs/2505.11837", "authors": ["Ziyao Cui", "Minxing Zhang", "Jian Pei"], "title": "On Membership Inference Attacks in Knowledge Distillation", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Nowadays, Large Language Models (LLMs) are trained on huge datasets, some\nincluding sensitive information. This poses a serious privacy concern because\nprivacy attacks such as Membership Inference Attacks (MIAs) may detect this\nsensitive information. While knowledge distillation compresses LLMs into\nefficient, smaller student models, its impact on privacy remains underexplored.\nIn this paper, we investigate how knowledge distillation affects model\nrobustness against MIA. We focus on two questions. First, how is private data\nprotected in teacher and student models? Second, how can we strengthen privacy\npreservation against MIAs in knowledge distillation? Through comprehensive\nexperiments, we show that while teacher and student models achieve similar\noverall MIA accuracy, teacher models better protect member data, the primary\ntarget of MIA, whereas student models better protect non-member data. To\naddress this vulnerability in student models, we propose 5 privacy-preserving\ndistillation methods and demonstrate that they successfully reduce student\nmodels' vulnerability to MIA, with ensembling further stabilizing the\nrobustness, offering a reliable approach for distilling more secure and\nefficient student models. Our implementation source code is available at\nhttps://github.com/richardcui18/MIA_in_KD."}
{"id": "2505.13306", "pdf": "https://arxiv.org/pdf/2505.13306", "abs": "https://arxiv.org/abs/2505.13306", "authors": ["Chengsong Sun", "Weiping Li", "Xiang Li", "Yuankun Liu", "Lianlei Shan"], "title": "GMM-Based Comprehensive Feature Extraction and Relative Distance Preservation For Few-Shot Cross-Modal Retrieval", "categories": ["cs.CV", "cs.IR"], "comment": null, "summary": "Few-shot cross-modal retrieval focuses on learning cross-modal\nrepresentations with limited training samples, enabling the model to handle\nunseen classes during inference. Unlike traditional cross-modal retrieval\ntasks, which assume that both training and testing data share the same class\ndistribution, few-shot retrieval involves data with sparse representations\nacross modalities. Existing methods often fail to adequately model the\nmulti-peak distribution of few-shot cross-modal data, resulting in two main\nbiases in the latent semantic space: intra-modal bias, where sparse samples\nfail to capture intra-class diversity, and inter-modal bias, where\nmisalignments between image and text distributions exacerbate the semantic gap.\nThese biases hinder retrieval accuracy. To address these issues, we propose a\nnovel method, GCRDP, for few-shot cross-modal retrieval. This approach\neffectively captures the complex multi-peak distribution of data using a\nGaussian Mixture Model (GMM) and incorporates a multi-positive sample\ncontrastive learning mechanism for comprehensive feature modeling.\nAdditionally, we introduce a new strategy for cross-modal semantic alignment,\nwhich constrains the relative distances between image and text feature\ndistributions, thereby improving the accuracy of cross-modal representations.\nWe validate our approach through extensive experiments on four benchmark\ndatasets, demonstrating superior performance over six state-of-the-art methods."}
{"id": "2505.13353", "pdf": "https://arxiv.org/pdf/2505.13353", "abs": "https://arxiv.org/abs/2505.13353", "authors": ["Adam Štorek", "Mukur Gupta", "Samira Hajizadeh", "Prashast Srivastava", "Suman Jana"], "title": "Sense and Sensitivity: Examining the Influence of Semantic Recall on Long Context Code Reasoning", "categories": ["cs.CL", "cs.LG", "cs.SE"], "comment": null, "summary": "Although modern Large Language Models (LLMs) support extremely large\ncontexts, their effectiveness in utilizing long context for code reasoning\nremains unclear. This paper investigates LLM reasoning ability over code\nsnippets within large repositories and how it relates to their recall ability.\nSpecifically, we differentiate between lexical code recall (verbatim retrieval)\nand semantic code recall (remembering what the code does). To measure semantic\nrecall, we propose SemTrace, a code reasoning technique where the impact of\nspecific statements on output is attributable and unpredictable. We also\npresent a method to quantify semantic recall sensitivity in existing\nbenchmarks. Our evaluation of state-of-the-art LLMs reveals a significant drop\nin code reasoning accuracy as a code snippet approaches the middle of the input\ncontext, particularly with techniques requiring high semantic recall like\nSemTrace. Moreover, we find that lexical recall varies by granularity, with\nmodels excelling at function retrieval but struggling with line-by-line recall.\nNotably, a disconnect exists between lexical and semantic recall, suggesting\ndifferent underlying mechanisms. Finally, our findings indicate that current\ncode reasoning benchmarks may exhibit low semantic recall sensitivity,\npotentially underestimating LLM challenges in leveraging in-context\ninformation."}
{"id": "2505.11862", "pdf": "https://arxiv.org/pdf/2505.11862", "abs": "https://arxiv.org/abs/2505.11862", "authors": ["Kalyan Cherukuri", "Aarav Lala", "Yash Yardi"], "title": "Q-Policy: Quantum-Enhanced Policy Evaluation for Scalable Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "quant-ph"], "comment": null, "summary": "We propose Q-Policy, a hybrid quantum-classical reinforcement learning (RL)\nframework that mathematically accelerates policy evaluation and optimization by\nexploiting quantum computing primitives. Q-Policy encodes value functions in\nquantum superposition, enabling simultaneous evaluation of multiple\nstate-action pairs via amplitude encoding and quantum parallelism. We introduce\na quantum-enhanced policy iteration algorithm with provable polynomial\nreductions in sample complexity for the evaluation step, under standard\nassumptions. To demonstrate the technical feasibility and theoretical soundness\nof our approach, we validate Q-Policy on classical emulations of small discrete\ncontrol tasks. Due to current hardware and simulation limitations, our\nexperiments focus on showcasing proof-of-concept behavior rather than\nlarge-scale empirical evaluation. Our results support the potential of Q-Policy\nas a theoretical foundation for scalable RL on future quantum devices,\naddressing RL scalability challenges beyond classical approaches."}
{"id": "2505.13309", "pdf": "https://arxiv.org/pdf/2505.13309", "abs": "https://arxiv.org/abs/2505.13309", "authors": ["Jad Mansour", "Sebastian Realpe", "Hayat Rajani", "Michele Grimaldi", "Rafael Garcia", "Nuno Gracias"], "title": "eStonefish-scenes: A synthetically generated dataset for underwater event-based optical flow prediction tasks", "categories": ["cs.CV", "I.2.5; I.2.6; I.2.9; I.2.10"], "comment": "Submitted to IJRR", "summary": "The combined use of event-based vision and Spiking Neural Networks (SNNs) is\nexpected to significantly impact robotics, particularly in tasks like visual\nodometry and obstacle avoidance. While existing real-world event-based datasets\nfor optical flow prediction, typically captured with Unmanned Aerial Vehicles\n(UAVs), offer valuable insights, they are limited in diversity, scalability,\nand are challenging to collect. Moreover, there is a notable lack of labelled\ndatasets for underwater applications, which hinders the integration of\nevent-based vision with Autonomous Underwater Vehicles (AUVs). To address this,\nsynthetic datasets could provide a scalable solution while bridging the gap\nbetween simulation and reality. In this work, we introduce eStonefish-scenes, a\nsynthetic event-based optical flow dataset based on the Stonefish simulator.\nAlong with the dataset, we present a data generation pipeline that enables the\ncreation of customizable underwater environments. This pipeline allows for\nsimulating dynamic scenarios, such as biologically inspired schools of fish\nexhibiting realistic motion patterns, including obstacle avoidance and reactive\nnavigation around corals. Additionally, we introduce a scene generator that can\nbuild realistic reef seabeds by randomly distributing coral across the terrain.\nTo streamline data accessibility, we present eWiz, a comprehensive library\ndesigned for processing event-based data, offering tools for data loading,\naugmentation, visualization, encoding, and training data generation, along with\nloss functions and performance metrics."}
{"id": "2505.13360", "pdf": "https://arxiv.org/pdf/2505.13360", "abs": "https://arxiv.org/abs/2505.13360", "authors": ["Chenyang Yang", "Yike Shi", "Qianou Ma", "Michael Xieyang Liu", "Christian Kästner", "Tongshuang Wu"], "title": "What Prompts Don't Say: Understanding and Managing Underspecification in LLM Prompts", "categories": ["cs.CL", "cs.SE"], "comment": null, "summary": "Building LLM-powered software requires developers to communicate their\nrequirements through natural language, but developer prompts are frequently\nunderspecified, failing to fully capture many user-important requirements. In\nthis paper, we present an in-depth analysis of prompt underspecification,\nshowing that while LLMs can often (41.1%) guess unspecified requirements by\ndefault, such behavior is less robust: Underspecified prompts are 2x more\nlikely to regress over model or prompt changes, sometimes with accuracy drops\nby more than 20%. We then demonstrate that simply adding more requirements to a\nprompt does not reliably improve performance, due to LLMs' limited\ninstruction-following capabilities and competing constraints, and standard\nprompt optimizers do not offer much help. To address this, we introduce novel\nrequirements-aware prompt optimization mechanisms that can improve performance\nby 4.8% on average over baselines that naively specify everything in the\nprompt. Beyond prompt optimization, we envision that effectively managing\nprompt underspecification requires a broader process, including proactive\nrequirements discovery, evaluation, and monitoring."}
{"id": "2505.11864", "pdf": "https://arxiv.org/pdf/2505.11864", "abs": "https://arxiv.org/abs/2505.11864", "authors": ["Kalyan Cherukuri", "Aarav Lala"], "title": "Learning Pareto-Optimal Rewards from Noisy Preferences: A Framework for Multi-Objective Inverse Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.CG"], "comment": null, "summary": "As generative agents become increasingly capable, alignment of their behavior\nwith complex human values remains a fundamental challenge. Existing approaches\noften simplify human intent through reduction to a scalar reward, overlooking\nthe multi-faceted nature of human feedback. In this work, we introduce a\ntheoretical framework for preference-based Multi-Objective Inverse\nReinforcement Learning (MO-IRL), where human preferences are modeled as latent\nvector-valued reward functions. We formalize the problem of recovering a\nPareto-optimal reward representation from noisy preference queries and\nestablish conditions for identifying the underlying multi-objective structure.\nWe derive tight sample complexity bounds for recovering\n$\\epsilon$-approximations of the Pareto front and introduce a regret\nformulation to quantify suboptimality in this multi-objective setting.\nFurthermore, we propose a provably convergent algorithm for policy optimization\nusing preference-inferred reward cones. Our results bridge the gap between\npractical alignment techniques and theoretical guarantees, providing a\nprincipled foundation for learning aligned behaviors in a high-dimension and\nvalue-pluralistic environment."}
{"id": "2505.13316", "pdf": "https://arxiv.org/pdf/2505.13316", "abs": "https://arxiv.org/abs/2505.13316", "authors": ["Gabriele Spadaro", "Alberto Presta", "Jhony H. Giraldo", "Marco Grangetto", "Wei Hu", "Giuseppe Valenzise", "Attilio Fiandrotti", "Enzo Tartaglione"], "title": "Denoising Diffusion Probabilistic Model for Point Cloud Compression at Low Bit-Rates", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "6 pages, 5 figures, accepted at ICME 2025", "summary": "Efficient compression of low-bit-rate point clouds is critical for\nbandwidth-constrained applications. However, existing techniques mainly focus\non high-fidelity reconstruction, requiring many bits for compression. This\npaper proposes a \"Denoising Diffusion Probabilistic Model\" (DDPM) architecture\nfor point cloud compression (DDPM-PCC) at low bit-rates. A PointNet encoder\nproduces the condition vector for the generation, which is then quantized via a\nlearnable vector quantizer. This configuration allows to achieve a low bitrates\nwhile preserving quality. Experiments on ShapeNet and ModelNet40 show improved\nrate-distortion at low rates compared to standardized and state-of-the-art\napproaches. We publicly released the code at\nhttps://github.com/EIDOSLAB/DDPM-PCC."}
{"id": "2505.13379", "pdf": "https://arxiv.org/pdf/2505.13379", "abs": "https://arxiv.org/abs/2505.13379", "authors": ["Gongfan Fang", "Xinyin Ma", "Xinchao Wang"], "title": "Thinkless: LLM Learns When to Think", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reasoning Language Models, capable of extended chain-of-thought reasoning,\nhave demonstrated remarkable performance on tasks requiring complex logical\ninference. However, applying elaborate reasoning for all queries often results\nin substantial computational inefficiencies, particularly when many problems\nadmit straightforward solutions. This motivates an open question: Can LLMs\nlearn when to think? To answer this, we propose Thinkless, a learnable\nframework that empowers an LLM to adaptively select between short-form and\nlong-form reasoning, based on both task complexity and the model's ability.\nThinkless is trained under a reinforcement learning paradigm and employs two\ncontrol tokens, <short> for concise responses and <think> for detailed\nreasoning. At the core of our method is a Decoupled Group Relative Policy\nOptimization (DeGRPO) algorithm, which decomposes the learning objective of\nhybrid reasoning into two components: (1) a control token loss that governs the\nselection of the reasoning mode, and (2) a response loss that improves the\naccuracy of the generated answers. This decoupled formulation enables\nfine-grained control over the contributions of each objective, stabilizing\ntraining and effectively preventing collapse observed in vanilla GRPO.\nEmpirically, on several benchmarks such as Minerva Algebra, MATH-500, and\nGSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% -\n90%, significantly improving the efficiency of Reasoning Language Models. The\ncode is available at https://github.com/VainF/Thinkless"}
{"id": "2505.11878", "pdf": "https://arxiv.org/pdf/2505.11878", "abs": "https://arxiv.org/abs/2505.11878", "authors": ["Yifan Dai", "Xuanbai Ren", "Tengfei Ma", "Qipeng Yan", "Yiping Liu", "Yuansheng Liu", "Xiangxiang Zeng"], "title": "AdaptMol: Adaptive Fusion from Sequence String to Topological Structure for Few-shot Drug Discovery", "categories": ["cs.LG", "cs.AI", "q-bio.MN", "J.3; I.2.7"], "comment": "15 pages, 6 figures", "summary": "Accurate molecular property prediction (MPP) is a critical step in modern\ndrug development. However, the scarcity of experimental validation data poses a\nsignificant challenge to AI-driven research paradigms. Under few-shot learning\nscenarios, the quality of molecular representations directly dictates the\ntheoretical upper limit of model performance. We present AdaptMol, a\nprototypical network integrating Adaptive multimodal fusion for Molecular\nrepresentation. This framework employs a dual-level attention mechanism to\ndynamically integrate global and local molecular features derived from two\nmodalities: SMILES sequences and molecular graphs. (1) At the local level,\nstructural features such as atomic interactions and substructures are extracted\nfrom molecular graphs, emphasizing fine-grained topological information; (2) At\nthe global level, the SMILES sequence provides a holistic representation of the\nmolecule. To validate the necessity of multimodal adaptive fusion, we propose\nan interpretable approach based on identifying molecular active substructures\nto demonstrate that multimodal adaptive fusion can efficiently represent\nmolecules. Extensive experiments on three commonly used benchmarks under 5-shot\nand 10-shot settings demonstrate that AdaptMol achieves state-of-the-art\nperformance in most cases. The rationale-extracted method guides the fusion of\ntwo modalities and highlights the importance of both modalities."}
{"id": "2505.13318", "pdf": "https://arxiv.org/pdf/2505.13318", "abs": "https://arxiv.org/abs/2505.13318", "authors": ["Paula Feldman", "Martin Sinnona", "Viviana Siless", "Claudio Delrieux", "Emmanuel Iarussi"], "title": "VesselGPT: Autoregressive Modeling of Vascular Geometry", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "Anatomical trees are critical for clinical diagnosis and treatment planning,\nyet their complex and diverse geometry make accurate representation a\nsignificant challenge. Motivated by the latest advances in large language\nmodels, we introduce an autoregressive method for synthesizing anatomical\ntrees. Our approach first embeds vessel structures into a learned discrete\nvocabulary using a VQ-VAE architecture, then models their generation\nautoregressively with a GPT-2 model. This method effectively captures intricate\ngeometries and branching patterns, enabling realistic vascular tree synthesis.\nComprehensive qualitative and quantitative evaluations reveal that our\ntechnique achieves high-fidelity tree reconstruction with compact discrete\nrepresentations. Moreover, our B-spline representation of vessel cross-sections\npreserves critical morphological details that are often overlooked in previous'\nmethods parameterizations. To the best of our knowledge, this work is the first\nto generate blood vessels in an autoregressive manner. Code, data, and trained\nmodels will be made available."}
{"id": "2505.13388", "pdf": "https://arxiv.org/pdf/2505.13388", "abs": "https://arxiv.org/abs/2505.13388", "authors": ["David Anugraha", "Zilu Tang", "Lester James V. Miranda", "Hanyang Zhao", "Mohammad Rifqi Farhansyah", "Garry Kuwanto", "Derry Wijaya", "Genta Indra Winata"], "title": "R3: Robust Rubric-Agnostic Reward Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Preprint", "summary": "Reward models are essential for aligning language model outputs with human\npreferences, yet existing approaches often lack both controllability and\ninterpretability. These models are typically optimized for narrow objectives,\nlimiting their generalizability to broader downstream tasks. Moreover, their\nscalar outputs are difficult to interpret without contextual reasoning. To\naddress these limitations, we introduce R3, a novel reward modeling framework\nthat is rubric-agnostic, generalizable across evaluation dimensions, and\nprovides interpretable, reasoned score assignments. R3 enables more transparent\nand flexible evaluation of language models, supporting robust alignment with\ndiverse human values and use cases. Our models, data, and code are available as\nopen source at https://github.com/rubricreward/r3"}
{"id": "2505.11881", "pdf": "https://arxiv.org/pdf/2505.11881", "abs": "https://arxiv.org/abs/2505.11881", "authors": ["Giyeong Oh", "Woohyun Cho", "Siyeol Kim", "Suhwan Choi", "Younjae Yu"], "title": "Revisiting Residual Connections: Orthogonal Updates for Stable and Efficient Deep Networks", "categories": ["cs.CV", "cs.AI"], "comment": "27 pages, WIP", "summary": "Residual connections are pivotal for deep neural networks, enabling greater\ndepth by mitigating vanishing gradients. However, in standard residual updates,\nthe module's output is directly added to the input stream. This can lead to\nupdates that predominantly reinforce or modulate the existing stream direction,\npotentially underutilizing the module's capacity for learning entirely novel\nfeatures. In this work, we introduce Orthogonal Residual Update: we decompose\nthe module's output relative to the input stream and add only the component\northogonal to this stream. This design aims to guide modules to contribute\nprimarily new representational directions, fostering richer feature learning\nwhile promoting more efficient training. We demonstrate that our orthogonal\nupdate strategy improves generalization accuracy and training stability across\ndiverse architectures (ResNetV2, Vision Transformers) and datasets (CIFARs,\nTinyImageNet, ImageNet-1k), achieving, for instance, a +4.3\\%p top-1 accuracy\ngain for ViT-B on ImageNet-1k."}
{"id": "2505.13327", "pdf": "https://arxiv.org/pdf/2505.13327", "abs": "https://arxiv.org/abs/2505.13327", "authors": ["Ajian Liu", "Haocheng Yuan", "Xiao Guo", "Hui Ma", "Wanyi Zhuang", "Changtao Miao", "Yan Hong", "Chuanbiao Song", "Jun Lan", "Qi Chu", "Tao Gong", "Yanyan Liang", "Weiqiang Wang", "Jun Wan", "Xiaoming Liu", "Zhen Lei"], "title": "Benchmarking Unified Face Attack Detection via Hierarchical Prompt Tuning", "categories": ["cs.CV"], "comment": null, "summary": "Presentation Attack Detection and Face Forgery Detection are designed to\nprotect face data from physical media-based Presentation Attacks and digital\nediting-based DeepFakes respectively. But separate training of these two models\nmakes them vulnerable to unknown attacks and burdens deployment environments.\nThe lack of a Unified Face Attack Detection model to handle both types of\nattacks is mainly due to two factors. First, there's a lack of adequate\nbenchmarks for models to explore. Existing UAD datasets have limited attack\ntypes and samples, restricting the model's ability to address advanced threats.\nTo address this, we propose UniAttackDataPlus (UniAttackData+), the most\nextensive and sophisticated collection of forgery techniques to date. It\nincludes 2,875 identities and their 54 kinds of falsified samples, totaling\n697,347 videos. Second, there's a lack of a reliable classification criterion.\nCurrent methods try to find an arbitrary criterion within the same semantic\nspace, which fails when encountering diverse attacks. So, we present a novel\nVisual-Language Model-based Hierarchical Prompt Tuning Framework (HiPTune) that\nadaptively explores multiple classification criteria from different semantic\nspaces. We build a Visual Prompt Tree to explore various classification rules\nhierarchically. Then, by adaptively pruning the prompts, the model can select\nthe most suitable prompts to guide the encoder to extract discriminative\nfeatures at different levels in a coarse-to-fine way. Finally, to help the\nmodel understand the classification criteria in visual space, we propose a\nDynamically Prompt Integration module to project the visual prompts to the text\nencoder for more accurate semantics. Experiments on 12 datasets have shown the\npotential to inspire further innovations in the UAD field."}
{"id": "2505.13403", "pdf": "https://arxiv.org/pdf/2505.13403", "abs": "https://arxiv.org/abs/2505.13403", "authors": ["Renjie Pi", "Felix Bai", "Qibin Chen", "Simon Wang", "Jiulong Shan", "Kieran Liu", "Meng Cao"], "title": "MR. Judge: Multimodal Reasoner as a Judge", "categories": ["cs.CL"], "comment": null, "summary": "The paradigm of using Large Language Models (LLMs) and Multimodal Large\nLanguage Models (MLLMs) as evaluative judges has emerged as an effective\napproach in RLHF and inference-time scaling. In this work, we propose\nMultimodal Reasoner as a Judge (MR. Judge), a paradigm for empowering\ngeneral-purpose MLLMs judges with strong reasoning capabilities. Instead of\ndirectly assigning scores for each response, we formulate the judgement process\nas a reasoning-inspired multiple-choice problem. Specifically, the judge model\nfirst conducts deliberate reasoning covering different aspects of the responses\nand eventually selects the best response from them. This reasoning process not\nonly improves the interpretibility of the judgement, but also greatly enhances\nthe performance of MLLM judges. To cope with the lack of questions with scored\nresponses, we propose the following strategy to achieve automatic annotation:\n1) Reverse Response Candidates Synthesis: starting from a supervised\nfine-tuning (SFT) dataset, we treat the original response as the best candidate\nand prompt the MLLM to generate plausible but flawed negative candidates. 2)\nText-based reasoning extraction: we carefully design a data synthesis pipeline\nfor distilling the reasoning capability from a text-based reasoning model,\nwhich is adopted to enable the MLLM judges to regain complex reasoning ability\nvia warm up supervised fine-tuning. Experiments demonstrate that our MR. Judge\nis effective across a wide range of tasks. Specifically, our MR. Judge-7B\nsurpasses GPT-4o by 9.9% on VL-RewardBench, and improves performance on MM-Vet\nduring inference-time scaling by up to 7.7%."}
{"id": "2505.11889", "pdf": "https://arxiv.org/pdf/2505.11889", "abs": "https://arxiv.org/abs/2505.11889", "authors": ["Hanfang Cui", "Longfei Song", "Li Li", "Dongxing Xu", "Yanhua Long"], "title": "Exploring the Potential of SSL Models for Sound Event Detection", "categories": ["eess.AS", "cs.AI", "cs.SD", "I.5.4; I.2.10; H.5.5"], "comment": "27 pages, 5 figures, submitted to the Journal of King Saud University\n  - Computer and Information Sciences (under review)", "summary": "Self-supervised learning (SSL) models offer powerful representations for\nsound event detection (SED), yet their synergistic potential remains\nunderexplored. This study systematically evaluates state-of-the-art SSL models\nto guide optimal model selection and integration for SED. We propose a\nframework that combines heterogeneous SSL representations (e.g., BEATs, HuBERT,\nWavLM) through three fusion strategies: individual SSL embedding integration,\ndual-modal fusion, and full aggregation. Experiments on the DCASE 2023 Task 4\nChallenge reveal that dual-modal fusion (e.g., CRNN+BEATs+WavLM) achieves\ncomplementary performance gains, while CRNN+BEATs alone delivers the best\nresults among individual SSL models. We further introduce normalized sound\nevent bounding boxes (nSEBBs), an adaptive post-processing method that\ndynamically adjusts event boundary predictions, improving PSDS1 by up to 4% for\nstandalone SSL models. These findings highlight the compatibility and\ncomplementarity of SSL architectures, providing guidance for task-specific\nfusion and robust SED system design."}
{"id": "2505.13344", "pdf": "https://arxiv.org/pdf/2505.13344", "abs": "https://arxiv.org/abs/2505.13344", "authors": ["Ahmet Berke Gokmen", "Yigit Ekin", "Bahri Batuhan Bilecen", "Aysegul Dundar"], "title": "RoPECraft: Training-Free Motion Transfer with Trajectory-Guided RoPE Optimization on Diffusion Transformers", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "https://berkegokmen1.github.io/RoPECraft/", "summary": "We propose RoPECraft, a training-free video motion transfer method for\ndiffusion transformers that operates solely by modifying their rotary\npositional embeddings (RoPE). We first extract dense optical flow from a\nreference video, and utilize the resulting motion offsets to warp the\ncomplex-exponential tensors of RoPE, effectively encoding motion into the\ngeneration process. These embeddings are then further optimized during\ndenoising time steps via trajectory alignment between the predicted and target\nvelocities using a flow-matching objective. To keep the output faithful to the\ntext prompt and prevent duplicate generations, we incorporate a regularization\nterm based on the phase components of the reference video's Fourier transform,\nprojecting the phase angles onto a smooth manifold to suppress high-frequency\nartifacts. Experiments on benchmarks reveal that RoPECraft outperforms all\nrecently published methods, both qualitatively and quantitatively."}
{"id": "2505.13404", "pdf": "https://arxiv.org/pdf/2505.13404", "abs": "https://arxiv.org/abs/2505.13404", "authors": ["Nithin Rao Koluguri", "Monica Sekoyan", "George Zelenfroynd", "Sasha Meister", "Shuoyang Ding", "Sofia Kostandian", "He Huang", "Nikolay Karpov", "Jagadeesh Balam", "Vitaly Lavrukhin", "Yifan Peng", "Sara Papi", "Marco Gaido", "Alessio Brutti", "Boris Ginsburg"], "title": "Granary: Speech Recognition and Translation Dataset in 25 European Languages", "categories": ["cs.CL", "eess.AS"], "comment": "Accepted at Interspeech 2025", "summary": "Multi-task and multilingual approaches benefit large models, yet speech\nprocessing for low-resource languages remains underexplored due to data\nscarcity. To address this, we present Granary, a large-scale collection of\nspeech datasets for recognition and translation across 25 European languages.\nThis is the first open-source effort at this scale for both transcription and\ntranslation. We enhance data quality using a pseudo-labeling pipeline with\nsegmentation, two-pass inference, hallucination filtering, and punctuation\nrestoration. We further generate translation pairs from pseudo-labeled\ntranscriptions using EuroLLM, followed by a data filtration pipeline. Designed\nfor efficiency, our pipeline processes vast amount of data within hours. We\nassess models trained on processed data by comparing their performance on\npreviously curated datasets for both high- and low-resource languages. Our\nfindings show that these models achieve similar performance using approx. 50%\nless data. Dataset will be made available at\nhttps://hf.co/datasets/nvidia/Granary"}
{"id": "2505.11891", "pdf": "https://arxiv.org/pdf/2505.11891", "abs": "https://arxiv.org/abs/2505.11891", "authors": ["Weikai Xu", "Zhizheng Jiang", "Yuxuan Liu", "Wei Liu", "Jian Luan", "Yuanchun Li", "Yunxin Liu", "Bin Wang", "Bo An"], "title": "Mobile-Bench-v2: A More Realistic and Comprehensive Benchmark for VLM-based Mobile Agents", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "VLM-based mobile agents are increasingly popular due to their capabilities to\ninteract with smartphone GUIs and XML-structured texts and to complete daily\ntasks. However, existing online benchmarks struggle with obtaining stable\nreward signals due to dynamic environmental changes. Offline benchmarks\nevaluate the agents through single-path trajectories, which stands in contrast\nto the inherently multi-solution characteristics of GUI tasks. Additionally,\nboth types of benchmarks fail to assess whether mobile agents can handle noise\nor engage in proactive interactions due to a lack of noisy apps or overly full\ninstructions during the evaluation process. To address these limitations, we\nuse a slot-based instruction generation method to construct a more realistic\nand comprehensive benchmark named Mobile-Bench-v2. Mobile-Bench-v2 includes a\ncommon task split, with offline multi-path evaluation to assess the agent's\nability to obtain step rewards during task execution. It contains a noisy split\nbased on pop-ups and ads apps, and a contaminated split named AITZ-Noise to\nformulate a real noisy environment. Furthermore, an ambiguous instruction split\nwith preset Q\\&A interactions is released to evaluate the agent's proactive\ninteraction capabilities. We conduct evaluations on these splits using the\nsingle-agent framework AppAgent-v1, the multi-agent framework Mobile-Agent-v2,\nas well as other mobile agents such as UI-Tars and OS-Atlas. Code and data are\navailable at https://huggingface.co/datasets/xwk123/MobileBench-v2."}
{"id": "2505.13389", "pdf": "https://arxiv.org/pdf/2505.13389", "abs": "https://arxiv.org/abs/2505.13389", "authors": ["Peiyuan Zhang", "Haofeng Huang", "Yongqi Chen", "Will Lin", "Zhengzhong Liu", "Ion Stoica", "Eric P. Xing", "Hao Zhang"], "title": "Faster Video Diffusion with Trainable Sparse Attention", "categories": ["cs.CV"], "comment": null, "summary": "Scaling video diffusion transformers (DiTs) is limited by their quadratic 3D\nattention, even though most of the attention mass concentrates on a small\nsubset of positions. We turn this observation into VSA, a trainable,\nhardware-efficient sparse attention that replaces full attention at \\emph{both}\ntraining and inference. In VSA, a lightweight coarse stage pools tokens into\ntiles and identifies high-weight \\emph{critical tokens}; a fine stage computes\ntoken-level attention only inside those tiles subjecting to block computing\nlayout to ensure hard efficiency. This leads to a single differentiable kernel\nthat trains end-to-end, requires no post-hoc profiling, and sustains 85\\% of\nFlashAttention3 MFU. We perform a large sweep of ablation studies and\nscaling-law experiments by pretraining DiTs from 60M to 1.4B parameters. VSA\nreaches a Pareto point that cuts training FLOPS by 2.53$\\times$ with no drop in\ndiffusion loss. Retrofitting the open-source Wan-2.1 model speeds up attention\ntime by 6$\\times$ and lowers end-to-end generation time from 31s to 18s with\ncomparable quality. These results establish trainable sparse attention as a\npractical alternative to full attention and a key enabler for further scaling\nof video diffusion models."}
{"id": "2505.13417", "pdf": "https://arxiv.org/pdf/2505.13417", "abs": "https://arxiv.org/abs/2505.13417", "authors": ["Jiajie Zhang", "Nianyi Lin", "Lei Hou", "Ling Feng", "Juanzi Li"], "title": "AdaptThink: Reasoning Models Can Learn When to Think", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recently, large reasoning models have achieved impressive performance on\nvarious tasks by employing human-like deep thinking. However, the lengthy\nthinking process substantially increases inference overhead, making efficiency\na critical bottleneck. In this work, we first demonstrate that NoThinking,\nwhich prompts the reasoning model to skip thinking and directly generate the\nfinal solution, is a better choice for relatively simple tasks in terms of both\nperformance and efficiency. Motivated by this, we propose AdaptThink, a novel\nRL algorithm to teach reasoning models to choose the optimal thinking mode\nadaptively based on problem difficulty. Specifically, AdaptThink features two\ncore components: (1) a constrained optimization objective that encourages the\nmodel to choose NoThinking while maintaining the overall performance; (2) an\nimportance sampling strategy that balances Thinking and NoThinking samples\nduring on-policy training, thereby enabling cold start and allowing the model\nto explore and exploit both thinking modes throughout the training process. Our\nexperiments indicate that AdaptThink significantly reduces the inference costs\nwhile further enhancing performance. Notably, on three math datasets,\nAdaptThink reduces the average response length of DeepSeek-R1-Distill-Qwen-1.5B\nby 53% and improves its accuracy by 2.4%, highlighting the promise of adaptive\nthinking-mode selection for optimizing the balance between reasoning quality\nand efficiency. Our codes and models are available at\nhttps://github.com/THU-KEG/AdaptThink."}
{"id": "2505.11893", "pdf": "https://arxiv.org/pdf/2505.11893", "abs": "https://arxiv.org/abs/2505.11893", "authors": ["Zepeng Ding", "Dixuan Wang", "Ziqin Luo", "Guochao Jiang", "Deqing Yang", "Jiaqing Liang"], "title": "RLAP: A Reinforcement Learning Enhanced Adaptive Planning Framework for Multi-step NLP Task Solving", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multi-step planning has been widely employed to enhance the performance of\nlarge language models (LLMs) on downstream natural language processing (NLP)\ntasks, which decomposes the original task into multiple subtasks and guide LLMs\nto solve them sequentially without additional training. When addressing task\ninstances, existing methods either preset the order of steps or attempt\nmultiple paths at each step. However, these methods overlook instances'\nlinguistic features and rely on the intrinsic planning capabilities of LLMs to\nevaluate intermediate feedback and then select subtasks, resulting in\nsuboptimal outcomes. To better solve multi-step NLP tasks with LLMs, in this\npaper we propose a Reinforcement Learning enhanced Adaptive Planning framework\n(RLAP). In our framework, we model an NLP task as a Markov decision process\n(MDP) and employ an LLM directly into the environment. In particular, a\nlightweight Actor model is trained to estimate Q-values for natural language\nsequences consisting of states and actions through reinforcement learning.\nTherefore, during sequential planning, the linguistic features of each sequence\nin the MDP can be taken into account, and the Actor model interacts with the\nLLM to determine the optimal order of subtasks for each task instance. We apply\nRLAP on three different types of NLP tasks and conduct extensive experiments on\nmultiple datasets to verify RLAP's effectiveness and robustness."}
{"id": "2505.13419", "pdf": "https://arxiv.org/pdf/2505.13419", "abs": "https://arxiv.org/abs/2505.13419", "authors": ["Zhuozhao Hu", "Kaishen Yuan", "Xin Liu", "Zitong Yu", "Yuan Zong", "Jingang Shi", "Huanjing Yue", "Jingyu Yang"], "title": "FEALLM: Advancing Facial Emotion Analysis in Multimodal Large Language Models with Emotional Synergy and Reasoning", "categories": ["cs.CV"], "comment": "10 pages, 7 figures", "summary": "Facial Emotion Analysis (FEA) plays a crucial role in visual affective\ncomputing, aiming to infer a person's emotional state based on facial data.\nScientifically, facial expressions (FEs) result from the coordinated movement\nof facial muscles, which can be decomposed into specific action units (AUs)\nthat provide detailed emotional insights. However, traditional methods often\nstruggle with limited interpretability, constrained generalization and\nreasoning abilities. Recently, Multimodal Large Language Models (MLLMs) have\nshown exceptional performance in various visual tasks, while they still face\nsignificant challenges in FEA due to the lack of specialized datasets and their\ninability to capture the intricate relationships between FEs and AUs. To\naddress these issues, we introduce a novel FEA Instruction Dataset that\nprovides accurate and aligned FE and AU descriptions and establishes causal\nreasoning relationships between them, followed by constructing a new benchmark,\nFEABench. Moreover, we propose FEALLM, a novel MLLM architecture designed to\ncapture more detailed facial information, enhancing its capability in FEA\ntasks. Our model demonstrates strong performance on FEABench and impressive\ngeneralization capability through zero-shot evaluation on various datasets,\nincluding RAF-DB, AffectNet, BP4D, and DISFA, showcasing its robustness and\neffectiveness in FEA tasks. The dataset and code will be available at\nhttps://github.com/953206211/FEALLM."}
{"id": "2505.13418", "pdf": "https://arxiv.org/pdf/2505.13418", "abs": "https://arxiv.org/abs/2505.13418", "authors": ["Lotem Peled-Cohen", "Maya Zadok", "Nitay Calderon", "Hila Gonen", "Roi Reichart"], "title": "Dementia Through Different Eyes: Explainable Modeling of Human and LLM Perceptions for Early Awareness", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Cognitive decline often surfaces in language years before diagnosis. It is\nfrequently non-experts, such as those closest to the patient, who first sense a\nchange and raise concern. As LLMs become integrated into daily communication\nand used over prolonged periods, it may even be an LLM that notices something\nis off. But what exactly do they notice--and should be noticing--when making\nthat judgment? This paper investigates how dementia is perceived through\nlanguage by non-experts. We presented transcribed picture descriptions to\nnon-expert humans and LLMs, asking them to intuitively judge whether each text\nwas produced by someone healthy or with dementia. We introduce an explainable\nmethod that uses LLMs to extract high-level, expert-guided features\nrepresenting these picture descriptions, and use logistic regression to model\nhuman and LLM perceptions and compare with clinical diagnoses. Our analysis\nreveals that human perception of dementia is inconsistent and relies on a\nnarrow, and sometimes misleading, set of cues. LLMs, by contrast, draw on a\nricher, more nuanced feature set that aligns more closely with clinical\npatterns. Still, both groups show a tendency toward false negatives, frequently\noverlooking dementia cases. Through our interpretable framework and the\ninsights it provides, we hope to help non-experts better recognize the\nlinguistic signs that matter."}
{"id": "2505.11896", "pdf": "https://arxiv.org/pdf/2505.11896", "abs": "https://arxiv.org/abs/2505.11896", "authors": ["Chenwei Lou", "Zewei Sun", "Xinnian Liang", "Meng Qu", "Wei Shen", "Wenqi Wang", "Yuntao Li", "Qingping Yang", "Shuangzhi Wu"], "title": "AdaCoT: Pareto-Optimal Adaptive Chain-of-Thought Triggering via Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities but\noften face challenges with tasks requiring sophisticated reasoning. While\nChain-of-Thought (CoT) prompting significantly enhances reasoning, it\nindiscriminately generates lengthy reasoning steps for all queries, leading to\nsubstantial computational costs and inefficiency, especially for simpler\ninputs. To address this critical issue, we introduce AdaCoT (Adaptive\nChain-of-Thought), a novel framework enabling LLMs to adaptively decide when to\ninvoke CoT. AdaCoT framed adaptive reasoning as a Pareto optimization problem\nthat seeks to balance model performance with the costs associated with CoT\ninvocation (both frequency and computational overhead). We propose a\nreinforcement learning (RL) based method, specifically utilizing Proximal\nPolicy Optimization (PPO), to dynamically control the CoT triggering decision\nboundary by adjusting penalty coefficients, thereby allowing the model to\ndetermine CoT necessity based on implicit query complexity. A key technical\ncontribution is Selective Loss Masking (SLM), designed to counteract decision\nboundary collapse during multi-stage RL training, ensuring robust and stable\nadaptive triggering. Experimental results demonstrate that AdaCoT successfully\nnavigates the Pareto frontier, achieving substantial reductions in CoT usage\nfor queries not requiring elaborate reasoning. For instance, on our production\ntraffic testset, AdaCoT reduced CoT triggering rates to as low as 3.18\\% and\ndecreased average response tokens by 69.06%, while maintaining high performance\non complex tasks."}
{"id": "2505.13426", "pdf": "https://arxiv.org/pdf/2505.13426", "abs": "https://arxiv.org/abs/2505.13426", "authors": ["Liang Chen", "Hongcheng Gao", "Tianyu Liu", "Zhiqi Huang", "Flood Sung", "Xinyu Zhou", "Yuxin Wu", "Baobao Chang"], "title": "G1: Bootstrapping Perception and Reasoning Abilities of Vision-Language Model via Reinforcement Learning", "categories": ["cs.CV"], "comment": "21 pages, 14 figures, code released at\n  https://github.com/chenllliang/G1", "summary": "Vision-Language Models (VLMs) excel in many direct multimodal tasks but\nstruggle to translate this prowess into effective decision-making within\ninteractive, visually rich environments like games. This ``knowing-doing'' gap\nsignificantly limits their potential as autonomous agents, as leading VLMs\noften performing badly in simple games. To address this, we introduce VLM-Gym,\na curated reinforcement learning (RL) environment featuring diverse visual\ngames with unified interfaces and adjustable, compositional difficulty,\nspecifically designed for scalable multi-game parallel training. Leveraging\nVLM-Gym, we train G0 models using pure RL-driven self-evolution, which\ndemonstrate emergent perception and reasoning patterns. To further mitigate\nchallenges arising from game diversity, we develop G1 models. G1 incorporates a\nperception-enhanced cold start prior to RL fine-tuning. Our resulting G1 models\nconsistently surpass their teacher across all games and outperform leading\nproprietary models like Claude-3.7-Sonnet-Thinking. Systematic analysis reveals\nan intriguing finding: perception and reasoning abilities mutually bootstrap\neach other throughout the RL training process. Source code including VLM-Gym\nand RL training are released at https://github.com/chenllliang/G1 to foster\nfuture research in advancing VLMs as capable interactive agents."}
{"id": "2505.13434", "pdf": "https://arxiv.org/pdf/2505.13434", "abs": "https://arxiv.org/abs/2505.13434", "authors": ["Mateusz Bystroński", "Mikołaj Hołysz", "Grzegorz Piotrowski", "Nitesh V. Chawla", "Tomasz Kajdanowicz"], "title": "SMOTExT: SMOTE meets Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Data scarcity and class imbalance are persistent challenges in training\nrobust NLP models, especially in specialized domains or low-resource settings.\nWe propose a novel technique, SMOTExT, that adapts the idea of Synthetic\nMinority Over-sampling (SMOTE) to textual data. Our method generates new\nsynthetic examples by interpolating between BERT-based embeddings of two\nexisting examples and then decoding the resulting latent point into text with\nxRAG architecture. By leveraging xRAG's cross-modal retrieval-generation\nframework, we can effectively turn interpolated vectors into coherent text.\nWhile this is preliminary work supported by qualitative outputs only, the\nmethod shows strong potential for knowledge distillation and data augmentation\nin few-shot settings. Notably, our approach also shows promise for\nprivacy-preserving machine learning: in early experiments, training models\nsolely on generated data achieved comparable performance to models trained on\nthe original dataset. This suggests a viable path toward safe and effective\nlearning under data protection constraints."}
{"id": "2505.11904", "pdf": "https://arxiv.org/pdf/2505.11904", "abs": "https://arxiv.org/abs/2505.11904", "authors": ["Louis Mahon", "Mirella Lapata"], "title": "K*-Means: A Parameter-free Clustering Algorithm", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "Clustering is a widely used and powerful machine learning technique, but its\neffectiveness is often limited by the need to specify the number of clusters,\nk, or by relying on thresholds that implicitly determine k. We introduce\nk*-means, a novel clustering algorithm that eliminates the need to set k or any\nother parameters. Instead, it uses the minimum description length principle to\nautomatically determine the optimal number of clusters, k*, by splitting and\nmerging clusters while also optimising the standard k-means objective. We prove\nthat k*-means is guaranteed to converge and demonstrate experimentally that it\nsignificantly outperforms existing methods in scenarios where k is unknown. We\nalso show that it is accurate in estimating k, and that empirically its runtime\nis competitive with existing methods, and scales well with dataset size."}
{"id": "2505.13429", "pdf": "https://arxiv.org/pdf/2505.13429", "abs": "https://arxiv.org/abs/2505.13429", "authors": ["Cristobal Eyzaguirre", "Igor Vasiljevic", "Achal Dave", "Jiajun Wu", "Rares Andrei Ambrus", "Thomas Kollar", "Juan Carlos Niebles", "Pavel Tokmakov"], "title": "Understanding Complexity in VideoQA via Visual Program Generation", "categories": ["cs.CV"], "comment": null, "summary": "We propose a data-driven approach to analyzing query complexity in Video\nQuestion Answering (VideoQA). Previous efforts in benchmark design have relied\non human expertise to design challenging questions, yet we experimentally show\nthat humans struggle to predict which questions are difficult for machine\nlearning models. Our automatic approach leverages recent advances in code\ngeneration for visual question answering, using the complexity of generated\ncode as a proxy for question difficulty. We demonstrate that this measure\ncorrelates significantly better with model performance than human estimates. To\noperationalize this insight, we propose an algorithm for estimating question\ncomplexity from code. It identifies fine-grained primitives that correlate with\nthe hardest questions for any given set of models, making it easy to scale to\nnew approaches in the future. Finally, to further illustrate the utility of our\nmethod, we extend it to automatically generate complex questions, constructing\na new benchmark that is 1.9 times harder than the popular NExT-QA."}
{"id": "2505.13444", "pdf": "https://arxiv.org/pdf/2505.13444", "abs": "https://arxiv.org/abs/2505.13444", "authors": ["Liyan Tang", "Grace Kim", "Xinyu Zhao", "Thom Lake", "Wenxuan Ding", "Fangcong Yin", "Prasann Singhal", "Manya Wadhwa", "Zeyu Leo Liu", "Zayne Sprague", "Ramya Namuduri", "Bodun Hu", "Juan Diego Rodriguez", "Puyuan Peng", "Greg Durrett"], "title": "ChartMuseum: Testing Visual Reasoning Capabilities of Large Vision-Language Models", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Chart understanding presents a unique challenge for large vision-language\nmodels (LVLMs), as it requires the integration of sophisticated textual and\nvisual reasoning capabilities. However, current LVLMs exhibit a notable\nimbalance between these skills, falling short on visual reasoning that is\ndifficult to perform in text. We conduct a case study using a synthetic dataset\nsolvable only through visual reasoning and show that model performance degrades\nsignificantly with increasing visual complexity, while human performance\nremains robust. We then introduce ChartMuseum, a new Chart Question Answering\n(QA) benchmark containing 1,162 expert-annotated questions spanning multiple\nreasoning types, curated from real-world charts across 184 sources,\nspecifically built to evaluate complex visual and textual reasoning. Unlike\nprior chart understanding benchmarks -- where frontier models perform similarly\nand near saturation -- our benchmark exposes a substantial gap between model\nand human performance, while effectively differentiating model capabilities:\nalthough humans achieve 93% accuracy, the best-performing model Gemini-2.5-Pro\nattains only 63.0%, and the leading open-source LVLM Qwen2.5-VL-72B-Instruct\nachieves only 38.5%. Moreover, on questions requiring primarily visual\nreasoning, all models experience a 35%-55% performance drop from\ntext-reasoning-heavy question performance. Lastly, our qualitative error\nanalysis reveals specific categories of visual reasoning that are challenging\nfor current LVLMs."}
{"id": "2505.11912", "pdf": "https://arxiv.org/pdf/2505.11912", "abs": "https://arxiv.org/abs/2505.11912", "authors": ["Paul Saves", "Nicolas Verstaevel", "Benoît Gaudou"], "title": "Modèles de Substitution pour les Modèles à base d'Agents : Enjeux, Méthodes et Applications", "categories": ["cs.LG", "cs.AI", "cs.MA"], "comment": "12 pages, in French language. Les 33\\`emes Journ\\'ees Francophones\n  sur les Syst\\`emes Multi-Agents (JFSMA 2025). 2025", "summary": "Multi-agent simulations enables the modeling and analyses of the dynamic\nbehaviors and interactions of autonomous entities evolving in complex\nenvironments. Agent-based models (ABM) are widely used to study emergent\nphenomena arising from local interactions. However, their high computational\ncost poses a significant challenge, particularly for large-scale simulations\nrequiring extensive parameter exploration, optimization, or uncertainty\nquantification. The increasing complexity of ABM limits their feasibility for\nreal-time decision-making and large-scale scenario analysis. To address these\nlimitations, surrogate models offer an efficient alternative by learning\napproximations from sparse simulation data. These models provide\ncheap-to-evaluate predictions, significantly reducing computational costs while\nmaintaining accuracy. Various machine learning techniques, including regression\nmodels, neural networks, random forests and Gaussian processes, have been\napplied to construct robust surrogates. Moreover, uncertainty quantification\nand sensitivity analysis play a crucial role in enhancing model reliability and\ninterpretability.\n  This article explores the motivations, methods, and applications of surrogate\nmodeling for ABM, emphasizing the trade-offs between accuracy, computational\nefficiency, and interpretability. Through a case study on a segregation model,\nwe highlight the challenges associated with building and validating surrogate\nmodels, comparing different approaches and evaluating their performance.\nFinally, we discuss future perspectives on integrating surrogate models within\nABM to improve scalability, explainability, and real-time decision support\nacross various fields such as ecology, urban planning and economics."}
{"id": "2505.13436", "pdf": "https://arxiv.org/pdf/2505.13436", "abs": "https://arxiv.org/abs/2505.13436", "authors": ["R. James Cotton"], "title": "KinTwin: Imitation Learning with Torque and Muscle Driven Biomechanical Models Enables Precise Replication of Able-Bodied and Impaired Movement from Markerless Motion Capture", "categories": ["cs.CV"], "comment": null, "summary": "Broader access to high-quality movement analysis could greatly benefit\nmovement science and rehabilitation, such as allowing more detailed\ncharacterization of movement impairments and responses to interventions, or\neven enabling early detection of new neurological conditions or fall risk.\nWhile emerging technologies are making it easier to capture kinematics with\nbiomechanical models, or how joint angles change over time, inferring the\nunderlying physics that give rise to these movements, including ground reaction\nforces, joint torques, or even muscle activations, is still challenging. Here\nwe explore whether imitation learning applied to a biomechanical model from a\nlarge dataset of movements from able-bodied and impaired individuals can learn\nto compute these inverse dynamics. Although imitation learning in human pose\nestimation has seen great interest in recent years, our work differences in\nseveral ways: we focus on using an accurate biomechanical model instead of\nmodels adopted for computer vision, we test it on a dataset that contains\nparticipants with impaired movements, we reported detailed tracking metrics\nrelevant for the clinical measurement of movement including joint angles and\nground contact events, and finally we apply imitation learning to a\nmuscle-driven neuromusculoskeletal model. We show that our imitation learning\npolicy, KinTwin, can accurately replicate the kinematics of a wide range of\nmovements, including those with assistive devices or therapist assistance, and\nthat it can infer clinically meaningful differences in joint torques and muscle\nactivations. Our work demonstrates the potential for using imitation learning\nto enable high-quality movement analysis in clinical practice."}
{"id": "2505.13448", "pdf": "https://arxiv.org/pdf/2505.13448", "abs": "https://arxiv.org/abs/2505.13448", "authors": ["Vinay Samuel", "Harshita Diddee", "Yiming Zhang", "Daphne Ippolito"], "title": "CIE: Controlling Language Model Text Generations Using Continuous Signals", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, 3 figures", "summary": "Aligning language models with user intent is becoming increasingly relevant\nto enhance user experience. This calls for designing methods that can allow\nusers to control the properties of the language that LMs generate. For example,\ncontrolling the length of the generation, the complexity of the language that\ngets chosen, the sentiment, tone, etc. Most existing work attempts to integrate\nusers' control by conditioning LM generations on natural language prompts or\ndiscrete control signals, which are often brittle and hard to scale. In this\nwork, we are interested in \\textit{continuous} control signals, ones that exist\nalong a spectrum that can't easily be captured in a natural language prompt or\nvia existing techniques in conditional generation. Through a case study in\ncontrolling the precise response-length of generations produced by LMs, we\ndemonstrate how after fine-tuning, behaviors of language models can be\ncontrolled via continuous signals -- as vectors that are interpolated between a\n\"low\" and a \"high\" token embedding. Our method more reliably exerts\nresponse-length control than in-context learning methods or fine-tuning methods\nthat represent the control signal as a discrete signal. Our full open-sourced\ncode and datasets are available at https://github.com/vsamuel2003/CIE."}
{"id": "2505.11924", "pdf": "https://arxiv.org/pdf/2505.11924", "abs": "https://arxiv.org/abs/2505.11924", "authors": ["Yu-Ting Lee", "Hui-Ying Shih", "Fu-Chieh Chang", "Pei-Yuan Wu"], "title": "An Explanation of Intrinsic Self-Correction via Linear Representations and Latent Concepts", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We provide an explanation for the performance gains of intrinsic\nself-correction, a process where a language model iteratively refines its\noutputs without external feedback. More precisely, we investigate how prompting\ninduces interpretable changes in hidden states and thus affects the output\ndistributions. We hypothesize that each prompt-induced shift lies in a linear\nspan of some linear representation vectors, naturally separating tokens based\non individual concept alignment. Building around this idea, we give a\nmathematical formulation of self-correction and derive a concentration result\nfor output tokens based on alignment magnitudes. Our experiments on text\ndetoxification with zephyr-7b-sft reveal a substantial gap in the inner\nproducts of the prompt-induced shifts and the unembeddings of the top-100 most\ntoxic tokens vs. those of the unembeddings of the bottom-100 least toxic\ntokens, under toxic instructions. This suggests that self-correction prompts\nenhance a language model's capability of latent concept recognition. Our\nanalysis offers insights into the underlying mechanism of self-correction by\ncharacterizing how prompting works explainably. For reproducibility, our code\nis available."}
{"id": "2505.13437", "pdf": "https://arxiv.org/pdf/2505.13437", "abs": "https://arxiv.org/abs/2505.13437", "authors": ["Dian Shao", "Mingfei Shi", "Shengda Xu", "Haodong Chen", "Yongle Huang", "Binglu Wang"], "title": "FinePhys: Fine-grained Human Action Generation by Explicitly Incorporating Physical Laws for Effective Skeletal Guidance", "categories": ["cs.CV", "cs.AI"], "comment": "CVPR 2025", "summary": "Despite significant advances in video generation, synthesizing physically\nplausible human actions remains a persistent challenge, particularly in\nmodeling fine-grained semantics and complex temporal dynamics. For instance,\ngenerating gymnastics routines such as \"switch leap with 0.5 turn\" poses\nsubstantial difficulties for current methods, often yielding unsatisfactory\nresults. To bridge this gap, we propose FinePhys, a Fine-grained human action\ngeneration framework that incorporates Physics to obtain effective skeletal\nguidance. Specifically, FinePhys first estimates 2D poses in an online manner\nand then performs 2D-to-3D dimension lifting via in-context learning. To\nmitigate the instability and limited interpretability of purely data-driven 3D\nposes, we further introduce a physics-based motion re-estimation module\ngoverned by Euler-Lagrange equations, calculating joint accelerations via\nbidirectional temporal updating. The physically predicted 3D poses are then\nfused with data-driven ones, offering multi-scale 2D heatmap guidance for the\ndiffusion process. Evaluated on three fine-grained action subsets from FineGym\n(FX-JUMP, FX-TURN, and FX-SALTO), FinePhys significantly outperforms\ncompetitive baselines. Comprehensive qualitative results further demonstrate\nFinePhys's ability to generate more natural and plausible fine-grained human\nactions."}
{"id": "2505.11545", "pdf": "https://arxiv.org/pdf/2505.11545", "abs": "https://arxiv.org/abs/2505.11545", "authors": ["Xingyu Ji", "Parker Glenn", "Aditya G. Parameswaran", "Madelon Hulsebos"], "title": "TARGET: Benchmarking Table Retrieval for Generative Tasks", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.DB"], "comment": null, "summary": "The data landscape is rich with structured data, often of high value to\norganizations, driving important applications in data analysis and machine\nlearning. Recent progress in representation learning and generative models for\nsuch data has led to the development of natural language interfaces to\nstructured data, including those leveraging text-to-SQL. Contextualizing\ninteractions, either through conversational interfaces or agentic components,\nin structured data through retrieval-augmented generation can provide\nsubstantial benefits in the form of freshness, accuracy, and comprehensiveness\nof answers. The key question is: how do we retrieve the right table(s) for the\nanalytical query or task at hand? To this end, we introduce TARGET: a benchmark\nfor evaluating TAble Retrieval for GEnerative Tasks. With TARGET we analyze the\nretrieval performance of different retrievers in isolation, as well as their\nimpact on downstream tasks. We find that dense embedding-based retrievers far\noutperform a BM25 baseline which is less effective than it is for retrieval\nover unstructured text. We also surface the sensitivity of retrievers across\nvarious metadata (e.g., missing table titles), and demonstrate a stark\nvariation of retrieval performance across datasets and tasks. TARGET is\navailable at https://target-benchmark.github.io."}
{"id": "2505.11926", "pdf": "https://arxiv.org/pdf/2505.11926", "abs": "https://arxiv.org/abs/2505.11926", "authors": ["Yixu Wang", "Jiaxin Song", "Yifeng Gao", "Xin Wang", "Yang Yao", "Yan Teng", "Xingjun Ma", "Yingchun Wang", "Yu-Gang Jiang"], "title": "SafeVid: Toward Safety Aligned Video Large Multimodal Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "As Video Large Multimodal Models (VLMMs) rapidly advance, their inherent\ncomplexity introduces significant safety challenges, particularly the issue of\nmismatched generalization where static safety alignments fail to transfer to\ndynamic video contexts. We introduce SafeVid, a framework designed to instill\nvideo-specific safety principles in VLMMs. SafeVid uniquely transfers robust\ntextual safety alignment capabilities to the video domain by employing detailed\ntextual video descriptions as an interpretive bridge, facilitating LLM-based\nrule-driven safety reasoning. This is achieved through a closed-loop system\ncomprising: 1) generation of SafeVid-350K, a novel 350,000-pair video-specific\nsafety preference dataset; 2) targeted alignment of VLMMs using Direct\nPreference Optimization (DPO); and 3) comprehensive evaluation via our new\nSafeVidBench benchmark. Alignment with SafeVid-350K significantly enhances VLMM\nsafety, with models like LLaVA-NeXT-Video demonstrating substantial\nimprovements (e.g., up to 42.39%) on SafeVidBench. SafeVid provides critical\nresources and a structured approach, demonstrating that leveraging textual\ndescriptions as a conduit for safety reasoning markedly improves the safety\nalignment of VLMMs. We have made SafeVid-350K dataset\n(https://huggingface.co/datasets/yxwang/SafeVid-350K) publicly available."}
{"id": "2505.13439", "pdf": "https://arxiv.org/pdf/2505.13439", "abs": "https://arxiv.org/abs/2505.13439", "authors": ["Huawei Lin", "Tong Geng", "Zhaozhuo Xu", "Weijie Zhao"], "title": "VTBench: Evaluating Visual Tokenizers for Autoregressive Image Generation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "24 pages, 13 figures, 3 tables", "summary": "Autoregressive (AR) models have recently shown strong performance in image\ngeneration, where a critical component is the visual tokenizer (VT) that maps\ncontinuous pixel inputs to discrete token sequences. The quality of the VT\nlargely defines the upper bound of AR model performance. However, current\ndiscrete VTs fall significantly behind continuous variational autoencoders\n(VAEs), leading to degraded image reconstructions and poor preservation of\ndetails and text. Existing benchmarks focus on end-to-end generation quality,\nwithout isolating VT performance. To address this gap, we introduce VTBench, a\ncomprehensive benchmark that systematically evaluates VTs across three core\ntasks: Image Reconstruction, Detail Preservation, and Text Preservation, and\ncovers a diverse range of evaluation scenarios. We systematically assess\nstate-of-the-art VTs using a set of metrics to evaluate the quality of\nreconstructed images. Our findings reveal that continuous VAEs produce superior\nvisual representations compared to discrete VTs, particularly in retaining\nspatial structure and semantic detail. In contrast, the degraded\nrepresentations produced by discrete VTs often lead to distorted\nreconstructions, loss of fine-grained textures, and failures in preserving text\nand object integrity. Furthermore, we conduct experiments on GPT-4o image\ngeneration and discuss its potential AR nature, offering new insights into the\nrole of visual tokenization. We release our benchmark and codebase publicly to\nsupport further research and call on the community to develop strong,\ngeneral-purpose open-source VTs."}
{"id": "2505.11572", "pdf": "https://arxiv.org/pdf/2505.11572", "abs": "https://arxiv.org/abs/2505.11572", "authors": ["Anand Rai", "Satyam Rahangdale", "Utkarsh Anand", "Animesh Mukherjee"], "title": "ASR-FAIRBENCH: Measuring and Benchmarking Equity Across Speech Recognition Systems", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Paper accepted at INTERSPEECH 2025", "summary": "Automatic Speech Recognition (ASR) systems have become ubiquitous in everyday\napplications, yet significant disparities in performance across diverse\ndemographic groups persist. In this work, we introduce the ASR-FAIRBENCH\nleaderboard which is designed to assess both the accuracy and equity of ASR\nmodels in real-time. Leveraging the Meta's Fair-Speech dataset, which captures\ndiverse demographic characteristics, we employ a mixed-effects Poisson\nregression model to derive an overall fairness score. This score is integrated\nwith traditional metrics like Word Error Rate (WER) to compute the Fairness\nAdjusted ASR Score (FAAS), providing a comprehensive evaluation framework. Our\napproach reveals significant performance disparities in SOTA ASR models across\ndemographic groups and offers a benchmark to drive the development of more\ninclusive ASR technologies."}
{"id": "2505.11930", "pdf": "https://arxiv.org/pdf/2505.11930", "abs": "https://arxiv.org/abs/2505.11930", "authors": ["Marco Sälzer", "Przemysław Andrzej Wałęga", "Martin Lange"], "title": "The Logical Expressiveness of Temporal GNNs via Two-Dimensional Product Logics", "categories": ["cs.LG", "cs.AI", "cs.LO"], "comment": null, "summary": "In recent years, the expressive power of various neural architectures --\nincluding graph neural networks (GNNs), transformers, and recurrent neural\nnetworks -- has been characterised using tools from logic and formal language\ntheory. As the capabilities of basic architectures are becoming well\nunderstood, increasing attention is turning to models that combine multiple\narchitectural paradigms. Among them particularly important, and challenging to\nanalyse, are temporal extensions of GNNs, which integrate both spatial\n(graph-structure) and temporal (evolution over time) dimensions. In this paper,\nwe initiate the study of logical characterisation of temporal GNNs by\nconnecting them to two-dimensional product logics. We show that the expressive\npower of temporal GNNs depends on how graph and temporal components are\ncombined. In particular, temporal GNNs that apply static GNNs recursively over\ntime can capture all properties definable in the product logic of (past)\npropositional temporal logic PTL and the modal logic K. In contrast,\narchitectures such as graph-and-time TGNNs and global TGNNs can only express\nrestricted fragments of this logic, where the interaction between temporal and\nspatial operators is syntactically constrained. These results yield the first\nlogical characterisations of temporal GNNs and establish new relative\nexpressiveness results for temporal GNNs."}
{"id": "2505.13440", "pdf": "https://arxiv.org/pdf/2505.13440", "abs": "https://arxiv.org/abs/2505.13440", "authors": ["Ruoyu Wang", "Yi Ma", "Shenghua Gao"], "title": "Recollection from Pensieve: Novel View Synthesis via Learning from Uncalibrated Videos", "categories": ["cs.CV"], "comment": "13 pages, 4 figures", "summary": "Currently almost all state-of-the-art novel view synthesis and reconstruction\nmodels rely on calibrated cameras or additional geometric priors for training.\nThese prerequisites significantly limit their applicability to massive\nuncalibrated data. To alleviate this requirement and unlock the potential for\nself-supervised training on large-scale uncalibrated videos, we propose a novel\ntwo-stage strategy to train a view synthesis model from only raw video frames\nor multi-view images, without providing camera parameters or other priors. In\nthe first stage, we learn to reconstruct the scene implicitly in a latent space\nwithout relying on any explicit 3D representation. Specifically, we predict\nper-frame latent camera and scene context features, and employ a view synthesis\nmodel as a proxy for explicit rendering. This pretraining stage substantially\nreduces the optimization complexity and encourages the network to learn the\nunderlying 3D consistency in a self-supervised manner. The learned latent\ncamera and implicit scene representation have a large gap compared with the\nreal 3D world. To reduce this gap, we introduce the second stage training by\nexplicitly predicting 3D Gaussian primitives. We additionally apply explicit\nGaussian Splatting rendering loss and depth projection loss to align the\nlearned latent representations with physically grounded 3D geometry. In this\nway, Stage 1 provides a strong initialization and Stage 2 enforces 3D\nconsistency - the two stages are complementary and mutually beneficial.\nExtensive experiments demonstrate the effectiveness of our approach, achieving\nhigh-quality novel view synthesis and accurate camera pose estimation, compared\nto methods that employ supervision with calibration, pose, or depth\ninformation. The code is available at https://github.com/Dwawayu/Pensieve."}
{"id": "2505.11595", "pdf": "https://arxiv.org/pdf/2505.11595", "abs": "https://arxiv.org/abs/2505.11595", "authors": ["Peter Chen", "Xiaopeng Li", "Ziniu Li", "Xi Chen", "Tianyi Lin"], "title": "Spectral Policy Optimization: Coloring your Incorrect Reasoning in GRPO", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "28 pages", "summary": "Reinforcement learning (RL) has demonstrated significant success in enhancing\nreasoning capabilities in large language models (LLMs). One of the most widely\nused RL methods is Group Relative Policy Optimization\n(GRPO)~\\cite{Shao-2024-Deepseekmath}, known for its memory efficiency and\nsuccess in training DeepSeek-R1~\\cite{Guo-2025-Deepseek}. However, GRPO stalls\nwhen all sampled responses in a group are incorrect -- referred to as an\n\\emph{all-negative-sample} group -- as it fails to update the policy, hindering\nlearning progress. The contributions of this paper are two-fold. First, we\npropose a simple yet effective framework that introduces response diversity\nwithin all-negative-sample groups in GRPO using AI feedback. We also provide a\ntheoretical analysis, via a stylized model, showing how this diversification\nimproves learning dynamics. Second, we empirically validate our approach,\nshowing the improved performance across various model sizes (7B, 14B, 32B) in\nboth offline and online learning settings with 10 benchmarks, including base\nand distilled variants. Our findings highlight that learning from\nall-negative-sample groups is not only feasible but beneficial, advancing\nrecent insights from \\citet{Xiong-2025-Minimalist}."}
{"id": "2505.11933", "pdf": "https://arxiv.org/pdf/2505.11933", "abs": "https://arxiv.org/abs/2505.11933", "authors": ["Piyush Talegaonkar", "Siddhant Hole", "Shrinesh Kamble", "Prashil Gulechha", "Deepali Salapurkar"], "title": "Conversational Recommendation System using NLP and Sentiment Analysis", "categories": ["cs.IR", "cs.AI"], "comment": "Presented in ISETE conference (International Conference on Artificial\n  Intelligence, Machine Learning and Big Data Engineering 2024)", "summary": "In today's digitally-driven world, the demand for personalized and\ncontext-aware recommendations has never been greater. Traditional recommender\nsystems have made significant strides in this direction, but they often lack\nthe ability to tap into the richness of conversational data. This paper\nrepresents a novel approach to recommendation systems by integrating\nconversational insights into the recommendation process. The Conversational\nRecommender System integrates cutting-edge technologies such as deep learning,\nleveraging machine learning algorithms like Apriori for Association Rule\nMining, Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN),\nand Long Short-Term Memory (LTSM). Furthermore, sophisticated voice recognition\ntechnologies, including Hidden Markov Models (HMMs) and Dynamic Time Warping\n(DTW) algorithms, play a crucial role in accurate speech-to-text conversion,\nensuring robust performance in diverse environments. The methodology\nincorporates a fusion of content-based and collaborative recommendation\napproaches, enhancing them with NLP techniques. This innovative integration\nensures a more personalized and context-aware recommendation experience,\nparticularly in marketing applications."}
{"id": "2505.11518", "pdf": "https://arxiv.org/pdf/2505.11518", "abs": "https://arxiv.org/abs/2505.11518", "authors": ["Merham Fouladvand", "Peuroly Batra"], "title": "Deep Unrolled Meta-Learning for Multi-Coil and Multi-Modality MRI with Adaptive Optimization", "categories": ["math.OC", "cs.CV"], "comment": null, "summary": "We propose a unified deep meta-learning framework for accelerated magnetic\nresonance imaging (MRI) that jointly addresses multi-coil reconstruction and\ncross-modality synthesis. Motivated by the limitations of conventional methods\nin handling undersampled data and missing modalities, our approach unrolls a\nprovably convergent optimization algorithm into a structured neural network\narchitecture. Each phase of the network mimics a step of an adaptive\nforward-backward scheme with extrapolation, enabling the model to incorporate\nboth data fidelity and nonconvex regularization in a principled manner. To\nenhance generalization across different acquisition settings, we integrate\nmeta-learning, which enables the model to rapidly adapt to unseen sampling\npatterns and modality combinations using task-specific meta-knowledge. The\nproposed method is evaluated on the open source datasets, showing significant\nimprovements in PSNR and SSIM over conventional supervised learning, especially\nunder aggressive undersampling and domain shifts. Our results demonstrate the\nsynergy of unrolled optimization, task-aware meta-learning, and modality\nfusion, offering a scalable and generalizable solution for real-world clinical\nMRI reconstruction."}
{"id": "2505.11611", "pdf": "https://arxiv.org/pdf/2505.11611", "abs": "https://arxiv.org/abs/2505.11611", "authors": ["Bofan Gong", "Shiyang Lai", "Dawn Song"], "title": "Probing the Vulnerability of Large Language Models to Polysemantic Interventions", "categories": ["cs.AI", "cs.CL", "cs.CR"], "comment": null, "summary": "Polysemanticity -- where individual neurons encode multiple unrelated\nfeatures -- is a well-known characteristic of large neural networks and remains\na central challenge in the interpretability of language models. At the same\ntime, its implications for model safety are also poorly understood. Leveraging\nrecent advances in sparse autoencoders, we investigate the polysemantic\nstructure of two small models (Pythia-70M and GPT-2-Small) and evaluate their\nvulnerability to targeted, covert interventions at the prompt, feature, token,\nand neuron levels. Our analysis reveals a consistent polysemantic topology\nshared across both models. Strikingly, we demonstrate that this structure can\nbe exploited to mount effective interventions on two larger, black-box\ninstruction-tuned models (LLaMA3.1-8B-Instruct and Gemma-2-9B-Instruct). These\nfindings suggest not only the generalizability of the interventions but also\npoint to a stable and transferable polysemantic structure that could\npotentially persist across architectures and training regimes."}
{"id": "2505.11936", "pdf": "https://arxiv.org/pdf/2505.11936", "abs": "https://arxiv.org/abs/2505.11936", "authors": ["Jingren Liu", "Zhong Ji", "Xiangyu Chen"], "title": "How can Diffusion Models Evolve into Continual Generators?", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "While diffusion models have achieved remarkable success in static data\ngeneration, their deployment in streaming or continual learning (CL) scenarios\nfaces a major challenge: catastrophic forgetting (CF), where newly acquired\ngenerative capabilities overwrite previously learned ones. To systematically\naddress this, we introduce a formal Continual Diffusion Generation (CDG)\nparadigm that characterizes and redefines CL in the context of generative\ndiffusion models. Prior efforts often adapt heuristic strategies from continual\nclassification tasks but lack alignment with the underlying diffusion process.\nIn this work, we develop the first theoretical framework for CDG by analyzing\ncross-task dynamics in diffusion-based generative modeling. Our analysis\nreveals that the retention and stability of generative knowledge across tasks\nare governed by three key consistency criteria: inter-task knowledge\nconsistency (IKC), unconditional knowledge consistency (UKC), and label\nknowledge consistency (LKC). Building on these insights, we propose Continual\nConsistency Diffusion (CCD), a principled framework that integrates these\nconsistency objectives into training via hierarchical loss terms\n$\\mathcal{L}_{IKC}$, $\\mathcal{L}_{UKC}$, and $\\mathcal{L}_{LKC}$. This\npromotes effective knowledge retention while enabling the assimilation of new\ngenerative capabilities. Extensive experiments on four benchmark datasets\ndemonstrate that CCD achieves state-of-the-art performance under continual\nsettings, with substantial gains in Mean Fidelity (MF) and Incremental Mean\nFidelity (IMF), particularly in tasks with rich cross-task knowledge overlap."}
{"id": "2505.11535", "pdf": "https://arxiv.org/pdf/2505.11535", "abs": "https://arxiv.org/abs/2505.11535", "authors": ["Yuhang Wang", "Hao Zhou"], "title": "Bridging Human Oversight and Black-box Driver Assistance: Vision-Language Models for Predictive Alerting in Lane Keeping Assist Systems", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": null, "summary": "Lane Keeping Assist systems, while increasingly prevalent, often suffer from\nunpredictable real-world failures, largely due to their opaque, black-box\nnature, which limits driver anticipation and trust. To bridge the gap between\nautomated assistance and effective human oversight, we present LKAlert, a novel\nsupervisory alert system that leverages VLM to forecast potential LKA risk 1-3\nseconds in advance. LKAlert processes dash-cam video and CAN data, integrating\nsurrogate lane segmentation features from a parallel interpretable model as\nautomated guiding attention. Unlike traditional binary classifiers, LKAlert\nissues both predictive alert and concise natural language explanation,\nenhancing driver situational awareness and trust. To support the development\nand evaluation of such systems, we introduce OpenLKA-Alert, the first benchmark\ndataset designed for predictive and explainable LKA failure warnings. It\ncontains synchronized multimodal inputs and human-authored justifications\nacross annotated temporal windows. We further contribute a generalizable\nmethodological framework for VLM-based black-box behavior prediction, combining\nsurrogate feature guidance with LoRA. This framework enables VLM to reason over\nstructured visual context without altering its vision backbone, making it\nbroadly applicable to other complex, opaque systems requiring interpretable\noversight. Empirical results correctly predicts upcoming LKA failures with\n69.8% accuracy and a 58.6\\% F1-score. The system also generates high-quality\ntextual explanations for drivers (71.7 ROUGE-L) and operates efficiently at\napproximately 2 Hz, confirming its suitability for real-time, in-vehicle use.\nOur findings establish LKAlert as a practical solution for enhancing the safety\nand usability of current ADAS and offer a scalable paradigm for applying VLMs\nto human-centered supervision of black-box automation."}
{"id": "2505.11614", "pdf": "https://arxiv.org/pdf/2505.11614", "abs": "https://arxiv.org/abs/2505.11614", "authors": ["Jian-Qiao Zhu", "Hanbo Xie", "Dilip Arumugam", "Robert C. Wilson", "Thomas L. Griffiths"], "title": "Using Reinforcement Learning to Train Large Language Models to Explain Human Decisions", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "A central goal of cognitive modeling is to develop models that not only\npredict human behavior but also provide insight into the underlying cognitive\nmechanisms. While neural network models trained on large-scale behavioral data\noften achieve strong predictive performance, they typically fall short in\noffering interpretable explanations of the cognitive processes they capture. In\nthis work, we explore the potential of pretrained large language models (LLMs)\nto serve as dual-purpose cognitive models--capable of both accurate prediction\nand interpretable explanation in natural language. Specifically, we employ\nreinforcement learning with outcome-based rewards to guide LLMs toward\ngenerating explicit reasoning traces for explaining human risky choices. Our\nfindings demonstrate that this approach produces high-quality explanations\nalongside strong quantitative predictions of human decisions."}
{"id": "2505.11939", "pdf": "https://arxiv.org/pdf/2505.11939", "abs": "https://arxiv.org/abs/2505.11939", "authors": ["Haitao Li", "Che Liu", "Zhengyao Ding", "Ziyi Liu", "Zhengxing Huang"], "title": "Fine-Grained ECG-Text Contrastive Learning via Waveform Understanding Enhancement", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": null, "summary": "Electrocardiograms (ECGs) are essential for diagnosing cardiovascular\ndiseases. While previous ECG-text contrastive learning methods have shown\npromising results, they often overlook the incompleteness of the reports. Given\nan ECG, the report is generated by first identifying key waveform features and\nthen inferring the final diagnosis through these features. Despite their\nimportance, these waveform features are often not recorded in the report as\nintermediate results. Aligning ECGs with such incomplete reports impedes the\nmodel's ability to capture the ECG's waveform features and limits its\nunderstanding of diagnostic reasoning based on those features. To address this,\nwe propose FG-CLEP (Fine-Grained Contrastive Language ECG Pre-training), which\naims to recover these waveform features from incomplete reports with the help\nof large language models (LLMs), under the challenges of hallucinations and the\nnon-bijective relationship between waveform features and diagnoses.\nAdditionally, considering the frequent false negatives due to the prevalence of\ncommon diagnoses in ECGs, we introduce a semantic similarity matrix to guide\ncontrastive learning. Furthermore, we adopt a sigmoid-based loss function to\naccommodate the multi-label nature of ECG-related tasks. Experiments on six\ndatasets demonstrate that FG-CLEP outperforms state-of-the-art methods in both\nzero-shot prediction and linear probing across these datasets."}
{"id": "2505.11538", "pdf": "https://arxiv.org/pdf/2505.11538", "abs": "https://arxiv.org/abs/2505.11538", "authors": ["Jiacheng Hou", "Zhenjie Song", "Ercan Engin Kuruoglu"], "title": "BrainNetMLP: An Efficient and Effective Baseline for Functional Brain Network Classification", "categories": ["q-bio.NC", "cs.CV"], "comment": "V1.0", "summary": "Recent studies have made great progress in functional brain network\nclassification by modeling the brain as a network of Regions of Interest (ROIs)\nand leveraging their connections to understand brain functionality and diagnose\nmental disorders. Various deep learning architectures, including Convolutional\nNeural Networks, Graph Neural Networks, and the recent Transformer, have been\ndeveloped. However, despite the increasing complexity of these models, the\nperformance gain has not been as salient. This raises a question: Does\nincreasing model complexity necessarily lead to higher classification accuracy?\nIn this paper, we revisit the simplest deep learning architecture, the\nMulti-Layer Perceptron (MLP), and propose a pure MLP-based method, named\nBrainNetMLP, for functional brain network classification, which capitalizes on\nthe advantages of MLP, including efficient computation and fewer parameters.\nMoreover, BrainNetMLP incorporates a dual-branch structure to jointly capture\nboth spatial connectivity and spectral information, enabling precise\nspatiotemporal feature fusion. We evaluate our proposed BrainNetMLP on two\npublic and popular brain network classification datasets, the Human Connectome\nProject (HCP) and the Autism Brain Imaging Data Exchange (ABIDE). Experimental\nresults demonstrate pure MLP-based methods can achieve state-of-the-art\nperformance, revealing the potential of MLP-based models as more efficient yet\neffective alternatives in functional brain network classification. The code\nwill be available at https://github.com/JayceonHo/BrainNetMLP."}
{"id": "2505.11717", "pdf": "https://arxiv.org/pdf/2505.11717", "abs": "https://arxiv.org/abs/2505.11717", "authors": ["Xilong Wang", "John Bloch", "Zedian Shao", "Yuepeng Hu", "Shuyan Zhou", "Neil Zhenqiang Gong"], "title": "EnvInjection: Environmental Prompt Injection Attack to Multi-modal Web Agents", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Multi-modal large language model (MLLM)-based web agents interact with\nwebpage environments by generating actions based on screenshots of the\nwebpages. Environmental prompt injection attacks manipulate the environment to\ninduce the web agent to perform a specific, attacker-chosen action--referred to\nas the target action. However, existing attacks suffer from limited\neffectiveness or stealthiness, or are impractical in real-world settings. In\nthis work, we propose EnvInjection, a new attack that addresses these\nlimitations. Our attack adds a perturbation to the raw pixel values of the\nrendered webpage, which can be implemented by modifying the webpage's source\ncode. After these perturbed pixels are mapped into a screenshot, the\nperturbation induces the web agent to perform the target action. We formulate\nthe task of finding the perturbation as an optimization problem. A key\nchallenge in solving this problem is that the mapping between raw pixel values\nand screenshot is non-differentiable, making it difficult to backpropagate\ngradients to the perturbation. To overcome this, we train a neural network to\napproximate the mapping and apply projected gradient descent to solve the\nreformulated optimization problem. Extensive evaluation on multiple webpage\ndatasets shows that EnvInjection is highly effective and significantly\noutperforms existing baselines."}
{"id": "2505.11946", "pdf": "https://arxiv.org/pdf/2505.11946", "abs": "https://arxiv.org/abs/2505.11946", "authors": ["Adam Kovari", "Yasin Ghafourian", "Csaba Hegedus", "Belal Abu Naim", "Kitti Mezei", "Pal Varga", "Markus Tauber"], "title": "Let's have a chat with the EU AI Act", "categories": ["cs.IR", "cs.AI", "cs.CY", "cs.DL", "cs.LG"], "comment": null, "summary": "As artificial intelligence (AI) regulations evolve and the regulatory\nlandscape develops and becomes more complex, ensuring compliance with ethical\nguidelines and legal frameworks remains a challenge for AI developers. This\npaper introduces an AI-driven self-assessment chatbot designed to assist users\nin navigating the European Union AI Act and related standards. Leveraging a\nRetrieval-Augmented Generation (RAG) framework, the chatbot enables real-time,\ncontext-aware compliance verification by retrieving relevant regulatory texts\nand providing tailored guidance. By integrating both public and proprietary\nstandards, it streamlines regulatory adherence, reduces complexity, and fosters\nresponsible AI development. The paper explores the chatbot's architecture,\ncomparing naive and graph-based RAG models, and discusses its potential impact\non AI governance."}
{"id": "2505.11576", "pdf": "https://arxiv.org/pdf/2505.11576", "abs": "https://arxiv.org/abs/2505.11576", "authors": ["Shuchen Wu", "Stephan Alaniz", "Shyamgopal Karthik", "Peter Dayan", "Eric Schulz", "Zeynep Akata"], "title": "Concept-Guided Interpretability via Neural Chunking", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "35 pages, 32 figures. arXiv admin note: text overlap with\n  arXiv:2502.01803", "summary": "Neural networks are often black boxes, reflecting the significant challenge\nof understanding their internal workings. We propose a different perspective\nthat challenges the prevailing view: rather than being inscrutable, neural\nnetworks exhibit patterns in their raw population activity that mirror\nregularities in the training data. We refer to this as the Reflection\nHypothesis and provide evidence for this phenomenon in both simple recurrent\nneural networks (RNNs) and complex large language models (LLMs). Building on\nthis insight, we propose to leverage cognitively-inspired methods of chunking\nto segment high-dimensional neural population dynamics into interpretable units\nthat reflect underlying concepts. We propose three methods to extract these\nemerging entities, complementing each other based on label availability and\ndimensionality. Discrete sequence chunking (DSC) creates a dictionary of\nentities; population averaging (PA) extracts recurring entities that correspond\nto known labels; and unsupervised chunk discovery (UCD) can be used when labels\nare absent. We demonstrate the effectiveness of these methods in extracting\nentities across varying model sizes, ranging from inducing compositionality in\nRNNs to uncovering recurring neural population states in large models with\ndiverse architectures, and illustrate their advantage over other methods.\nThroughout, we observe a robust correspondence between the extracted entities\nand concrete or abstract concepts. Artificially inducing the extracted entities\nin neural populations effectively alters the network's generation of associated\nconcepts. Our work points to a new direction for interpretability, one that\nharnesses both cognitive principles and the structure of naturalistic data to\nreveal the hidden computations of complex learning systems, gradually\ntransforming them from black boxes into systems we can begin to understand."}
{"id": "2505.11731", "pdf": "https://arxiv.org/pdf/2505.11731", "abs": "https://arxiv.org/abs/2505.11731", "authors": ["Harshil Vejendla", "Haizhou Shi", "Yibin Wang", "Tunyu Zhang", "Huan Zhang", "Hao Wang"], "title": "Efficient Uncertainty Estimation via Distillation of Bayesian Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Preprint; work in progress", "summary": "Recent advances in uncertainty estimation for Large Language Models (LLMs)\nduring downstream adaptation have addressed key challenges of reliability and\nsimplicity. However, existing Bayesian methods typically require multiple\nsampling iterations during inference, creating significant efficiency issues\nthat limit practical deployment. In this paper, we investigate the possibility\nof eliminating the need for test-time sampling for LLM uncertainty estimation.\nSpecifically, when given an off-the-shelf Bayesian LLM, we distill its aligned\nconfidence into a non-Bayesian student LLM by minimizing the divergence between\ntheir predictive distributions. Unlike typical calibration methods, our\ndistillation is carried out solely on the training dataset without the need of\nan additional validation dataset. This simple yet effective approach achieves\nN-times more efficient uncertainty estimation during testing, where N is the\nnumber of samples traditionally required by Bayesian LLMs. Our extensive\nexperiments demonstrate that uncertainty estimation capabilities on training\ndata can successfully generalize to unseen test data through our distillation\ntechnique, consistently producing results comparable to (or even better than)\nstate-of-the-art Bayesian LLMs."}
{"id": "2505.11953", "pdf": "https://arxiv.org/pdf/2505.11953", "abs": "https://arxiv.org/abs/2505.11953", "authors": ["Puning Yang", "Qizhou Wang", "Zhuo Huang", "Tongliang Liu", "Chengqi Zhang", "Bo Han"], "title": "Exploring Criteria of Loss Reweighting to Enhance LLM Unlearning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Loss reweighting has shown significant benefits for machine unlearning with\nlarge language models (LLMs). However, their exact functionalities are left\nunclear and the optimal strategy remains an open question, thus impeding the\nunderstanding and improvement of existing methodologies. In this paper, we\nidentify two distinct goals of loss reweighting, namely, Saturation and\nImportance -- the former indicates that those insufficiently optimized data\nshould be emphasized, while the latter stresses some critical data that are\nmost influential for loss minimization. To study their usefulness, we design\nspecific reweighting strategies for each goal and evaluate their respective\neffects on unlearning. We conduct extensive empirical analyses on\nwell-established benchmarks, and summarize some important observations as\nfollows: (i) Saturation enhances efficacy more than importance-based\nreweighting, and their combination can yield additional improvements. (ii)\nSaturation typically allocates lower weights to data with lower likelihoods,\nwhereas importance-based reweighting does the opposite. (iii) The efficacy of\nunlearning is also largely influenced by the smoothness and granularity of the\nweight distributions. Based on these findings, we propose SatImp, a simple\nreweighting method that combines the advantages of both saturation and\nimportance. Empirical results on extensive datasets validate the efficacy of\nour method, potentially bridging existing research gaps and indicating\ndirections for future research. Our code is available at\nhttps://github.com/Puning97/SatImp-for-LLM-Unlearning."}
{"id": "2505.11594", "pdf": "https://arxiv.org/pdf/2505.11594", "abs": "https://arxiv.org/abs/2505.11594", "authors": ["Jintao Zhang", "Jia Wei", "Pengle Zhang", "Xiaoming Xu", "Haofeng Huang", "Haoxu Wang", "Kai Jiang", "Jun Zhu", "Jianfei Chen"], "title": "SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training", "categories": ["cs.LG", "cs.AI", "cs.AR", "cs.CV", "cs.PF"], "comment": null, "summary": "The efficiency of attention is important due to its quadratic time\ncomplexity. We enhance the efficiency of attention through two key\ncontributions: First, we leverage the new FP4 Tensor Cores in Blackwell GPUs to\naccelerate attention computation. Our implementation achieves 1038 TOPS on\nRTX5090, which is a 5x speedup over the fastest FlashAttention on RTX5090.\nExperiments show that our FP4 attention can accelerate inference of various\nmodels in a plug-and-play way. Second, we pioneer low-bit attention to training\ntasks. Existing low-bit attention works like FlashAttention3 and SageAttention\nfocus only on inference. However, the efficiency of training large models is\nalso important. To explore whether low-bit attention can be effectively applied\nto training tasks, we design an accurate and efficient 8-bit attention for both\nforward and backward propagation. Experiments indicate that 8-bit attention\nachieves lossless performance in fine-tuning tasks but exhibits slower\nconvergence in pretraining tasks. The code will be available at\nhttps://github.com/thu-ml/SageAttention."}
{"id": "2505.11737", "pdf": "https://arxiv.org/pdf/2505.11737", "abs": "https://arxiv.org/abs/2505.11737", "authors": ["Tunyu Zhang", "Haizhou Shi", "Yibin Wang", "Hengyi Wang", "Xiaoxiao He", "Zhuowei Li", "Haoxian Chen", "Ligong Han", "Kai Xu", "Huan Zhang", "Dimitris Metaxas", "Hao Wang"], "title": "Token-Level Uncertainty Estimation for Large Language Model Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Preprint; Work in progress", "summary": "While Large Language Models (LLMs) have demonstrated impressive capabilities,\ntheir output quality remains inconsistent across various application scenarios,\nmaking it difficult to identify trustworthy responses, especially in complex\ntasks requiring multi-step reasoning. In this paper, we propose a token-level\nuncertainty estimation framework to enable LLMs to self-assess and self-improve\ntheir generation quality in mathematical reasoning. Specifically, we introduce\nlow-rank random weight perturbation to LLM decoding, generating predictive\ndistributions that we use to estimate token-level uncertainties. We then\naggregate these uncertainties to reflect semantic uncertainty of the generated\nsequences. Experiments on mathematical reasoning datasets of varying difficulty\ndemonstrate that our token-level uncertainty metrics strongly correlate with\nanswer correctness and model robustness. Additionally, we explore using\nuncertainty to directly enhance the model's reasoning performance through\nmultiple generations and the particle filtering algorithm. Our approach\nconsistently outperforms existing uncertainty estimation methods, establishing\neffective uncertainty estimation as a valuable tool for both evaluating and\nimproving reasoning generation in LLMs."}
{"id": "2505.11963", "pdf": "https://arxiv.org/pdf/2505.11963", "abs": "https://arxiv.org/abs/2505.11963", "authors": ["Luca Collini", "Baleegh Ahmad", "Joey Ah-kiow", "Ramesh Karri"], "title": "MARVEL: Multi-Agent RTL Vulnerability Extraction using Large Language Models", "categories": ["cs.CR", "cs.AI"], "comment": "Submitted for Peer Review", "summary": "Hardware security verification is a challenging and time-consuming task. For\nthis purpose, design engineers may utilize tools such as formal verification,\nlinters, and functional simulation tests, coupled with analysis and a deep\nunderstanding of the hardware design being inspected. Large Language Models\n(LLMs) have been used to assist during this task, either directly or in\nconjunction with existing tools. We improve the state of the art by proposing\nMARVEL, a multi-agent LLM framework for a unified approach to decision-making,\ntool use, and reasoning. MARVEL mimics the cognitive process of a designer\nlooking for security vulnerabilities in RTL code. It consists of a supervisor\nagent that devises the security policy of the system-on-chips (SoCs) using its\nsecurity documentation. It delegates tasks to validate the security policy to\nindividual executor agents. Each executor agent carries out its assigned task\nusing a particular strategy. Each executor agent may use one or more tools to\nidentify potential security bugs in the design and send the results back to the\nsupervisor agent for further analysis and confirmation. MARVEL includes\nexecutor agents that leverage formal tools, linters, simulation tests,\nLLM-based detection schemes, and static analysis-based checks. We test our\napproach on a known buggy SoC based on OpenTitan from the Hack@DATE\ncompetition. We find that 20 of the 48 issues reported by MARVEL pose security\nvulnerabilities."}
{"id": "2505.11645", "pdf": "https://arxiv.org/pdf/2505.11645", "abs": "https://arxiv.org/abs/2505.11645", "authors": ["Jinzhou Cao", "Xiangxu Wang", "Jiashi Chen", "Wei Tu", "Zhenhui Li", "Xindong Yang", "Tianhong Zhao", "Qingquan Li"], "title": "Urban Representation Learning for Fine-grained Economic Mapping: A Semi-supervised Graph-based Approach", "categories": ["cs.LG", "cs.CV"], "comment": "Accepted for publication in International Society Journal of\n  Photogrammetry and Remote Sensing (ISPRS). 70 pages, 10 Figures, 15 Tables", "summary": "Fine-grained economic mapping through urban representation learning has\nemerged as a crucial tool for evidence-based economic decisions. While existing\nmethods primarily rely on supervised or unsupervised approaches, they often\noverlook semi-supervised learning in data-scarce scenarios and lack unified\nmulti-task frameworks for comprehensive sectoral economic analysis. To address\nthese gaps, we propose SemiGTX, an explainable semi-supervised graph learning\nframework for sectoral economic mapping. The framework is designed with\ndedicated fusion encoding modules for various geospatial data modalities,\nseamlessly integrating them into a cohesive graph structure. It introduces a\nsemi-information loss function that combines spatial self-supervision with\nlocally masked supervised regression, enabling more informative and effective\nregion representations. Through multi-task learning, SemiGTX concurrently maps\nGDP across primary, secondary, and tertiary sectors within a unified model.\nExtensive experiments conducted in the Pearl River Delta region of China\ndemonstrate the model's superior performance compared to existing methods,\nachieving R2 scores of 0.93, 0.96, and 0.94 for the primary, secondary and\ntertiary sectors, respectively. Cross-regional experiments in Beijing and\nChengdu further illustrate its generality. Systematic analysis reveals how\ndifferent data modalities influence model predictions, enhancing explainability\nwhile providing valuable insights for regional development planning. This\nrepresentation learning framework advances regional economic monitoring through\ndiverse urban data integration, providing a robust foundation for precise\neconomic forecasting."}
{"id": "2505.11756", "pdf": "https://arxiv.org/pdf/2505.11756", "abs": "https://arxiv.org/abs/2505.11756", "authors": ["David Chanin", "Tomáš Dulka", "Adrià Garriga-Alonso"], "title": "Feature Hedging: Correlated Features Break Narrow Sparse Autoencoders", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "It is assumed that sparse autoencoders (SAEs) decompose polysemantic\nactivations into interpretable linear directions, as long as the activations\nare composed of sparse linear combinations of underlying features. However, we\nfind that if an SAE is more narrow than the number of underlying \"true\nfeatures\" on which it is trained, and there is correlation between features,\nthe SAE will merge components of correlated features together, thus destroying\nmonosemanticity. In LLM SAEs, these two conditions are almost certainly true.\nThis phenomenon, which we call feature hedging, is caused by SAE reconstruction\nloss, and is more severe the narrower the SAE. In this work, we introduce the\nproblem of feature hedging and study it both theoretically in toy models and\nempirically in SAEs trained on LLMs. We suspect that feature hedging may be one\nof the core reasons that SAEs consistently underperform supervised baselines.\nFinally, we use our understanding of feature hedging to propose an improved\nvariant of matryoshka SAEs. Our work shows there remain fundamental issues with\nSAEs, but we are hopeful that that highlighting feature hedging will catalyze\nfuture advances that allow SAEs to achieve their full potential of interpreting\nLLMs at scale."}
{"id": "2505.11979", "pdf": "https://arxiv.org/pdf/2505.11979", "abs": "https://arxiv.org/abs/2505.11979", "authors": ["Tarik Houichime", "Younes El Amrani"], "title": "Introduction to Analytical Software Engineering Design Paradigm", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.MS", "cs.PL"], "comment": "The Conference's autorization to submit a preprint was granted", "summary": "As modern software systems expand in scale and complexity, the challenges\nassociated with their modeling and formulation grow increasingly intricate.\nTraditional approaches often fall short in effectively addressing these\ncomplexities, particularly in tasks such as design pattern detection for\nmaintenance and assessment, as well as code refactoring for optimization and\nlong-term sustainability. This growing inadequacy underscores the need for a\nparadigm shift in how such challenges are approached and resolved. This paper\npresents Analytical Software Engineering (ASE), a novel design paradigm aimed\nat balancing abstraction, tool accessibility, compatibility, and scalability.\nASE enables effective modeling and resolution of complex software engineering\nproblems. The paradigm is evaluated through two frameworks\nBehavioral-Structural Sequences (BSS) and Optimized Design Refactoring (ODR),\nboth developed in accordance with ASE principles. BSS offers a compact,\nlanguage-agnostic representation of codebases to facilitate precise design\npattern detection. ODR unifies artifact and solution representations to\noptimize code refactoring via heuristic algorithms while eliminating iterative\ncomputational overhead. By providing a structured approach to software design\nchallenges, ASE lays the groundwork for future research in encoding and\nanalyzing complex software metrics."}
{"id": "2505.11651", "pdf": "https://arxiv.org/pdf/2505.11651", "abs": "https://arxiv.org/abs/2505.11651", "authors": ["Radek Osmulsk", "Gabriel de Souza P. Moreira", "Ronay Ak", "Mengyao Xu", "Benedikt Schifferer", "Even Oldridge"], "title": "MIRACL-VISION: A Large, multilingual, visual document retrieval benchmark", "categories": ["cs.IR", "cs.CV"], "comment": null, "summary": "Document retrieval is an important task for search and Retrieval-Augmented\nGeneration (RAG) applications. Large Language Models (LLMs) have contributed to\nimproving the accuracy of text-based document retrieval. However, documents\nwith complex layout and visual elements like tables, charts and infographics\nare not perfectly represented in textual format. Recently, image-based document\nretrieval pipelines have become popular, which use visual large language models\n(VLMs) to retrieve relevant page images given a query. Current evaluation\nbenchmarks on visual document retrieval are limited, as they primarily focus\nonly English language, rely on synthetically generated questions and offer a\nsmall corpus size. Therefore, we introduce MIRACL-VISION, a multilingual visual\ndocument retrieval evaluation benchmark. MIRACL-VISION covers 18 languages, and\nis an extension of the MIRACL dataset, a popular benchmark to evaluate\ntext-based multilingual retrieval pipelines. MIRACL was built using a\nhuman-intensive annotation process to generate high-quality questions. In order\nto reduce MIRACL-VISION corpus size to make evaluation more compute friendly\nwhile keeping the datasets challenging, we have designed a method for\neliminating the \"easy\" negatives from the corpus. We conducted extensive\nexperiments comparing MIRACL-VISION with other benchmarks, using popular public\ntext and image models. We observe a gap in state-of-the-art VLM-based embedding\nmodels on multilingual capabilities, with up to 59.7% lower retrieval accuracy\nthan a text-based retrieval models. Even for the English language, the visual\nmodels retrieval accuracy is 12.1% lower compared to text-based models.\nMIRACL-VISION is a challenging, representative, multilingual evaluation\nbenchmark for visual retrieval pipelines and will help the community build\nrobust models for document retrieval."}
{"id": "2505.11770", "pdf": "https://arxiv.org/pdf/2505.11770", "abs": "https://arxiv.org/abs/2505.11770", "authors": ["Jing Huang", "Junyi Tao", "Thomas Icard", "Diyi Yang", "Christopher Potts"], "title": "Internal Causal Mechanisms Robustly Predict Language Model Out-of-Distribution Behaviors", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": "ICML 2025", "summary": "Interpretability research now offers a variety of techniques for identifying\nabstract internal mechanisms in neural networks. Can such techniques be used to\npredict how models will behave on out-of-distribution examples? In this work,\nwe provide a positive answer to this question. Through a diverse set of\nlanguage modeling tasks--including symbol manipulation, knowledge retrieval,\nand instruction following--we show that the most robust features for\ncorrectness prediction are those that play a distinctive causal role in the\nmodel's behavior. Specifically, we propose two methods that leverage causal\nmechanisms to predict the correctness of model outputs: counterfactual\nsimulation (checking whether key causal variables are realized) and value\nprobing (using the values of those variables to make predictions). Both achieve\nhigh AUC-ROC in distribution and outperform methods that rely on\ncausal-agnostic features in out-of-distribution settings, where predicting\nmodel behaviors is more crucial. Our work thus highlights a novel and\nsignificant application for internal causal analysis of language models."}
{"id": "2505.11980", "pdf": "https://arxiv.org/pdf/2505.11980", "abs": "https://arxiv.org/abs/2505.11980", "authors": ["Yi Chen", "Mu-Young Son", "Chuanbo Hua", "Joo-Young Kim"], "title": "AoP-SAM: Automation of Prompts for Efficient Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at AAAI 2025", "summary": "The Segment Anything Model (SAM) is a powerful foundation model for image\nsegmentation, showing robust zero-shot generalization through prompt\nengineering. However, relying on manual prompts is impractical for real-world\napplications, particularly in scenarios where rapid prompt provision and\nresource efficiency are crucial. In this paper, we propose the Automation of\nPrompts for SAM (AoP-SAM), a novel approach that learns to generate essential\nprompts in optimal locations automatically. AoP-SAM enhances SAM's efficiency\nand usability by eliminating manual input, making it better suited for\nreal-world tasks. Our approach employs a lightweight yet efficient Prompt\nPredictor model that detects key entities across images and identifies the\noptimal regions for placing prompt candidates. This method leverages SAM's\nimage embeddings, preserving its zero-shot generalization capabilities without\nrequiring fine-tuning. Additionally, we introduce a test-time instance-level\nAdaptive Sampling and Filtering mechanism that generates prompts in a\ncoarse-to-fine manner. This notably enhances both prompt and mask generation\nefficiency by reducing computational overhead and minimizing redundant mask\nrefinements. Evaluations of three datasets demonstrate that AoP-SAM\nsubstantially improves both prompt generation efficiency and mask generation\naccuracy, making SAM more effective for automated segmentation tasks."}
{"id": "2505.11717", "pdf": "https://arxiv.org/pdf/2505.11717", "abs": "https://arxiv.org/abs/2505.11717", "authors": ["Xilong Wang", "John Bloch", "Zedian Shao", "Yuepeng Hu", "Shuyan Zhou", "Neil Zhenqiang Gong"], "title": "EnvInjection: Environmental Prompt Injection Attack to Multi-modal Web Agents", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Multi-modal large language model (MLLM)-based web agents interact with\nwebpage environments by generating actions based on screenshots of the\nwebpages. Environmental prompt injection attacks manipulate the environment to\ninduce the web agent to perform a specific, attacker-chosen action--referred to\nas the target action. However, existing attacks suffer from limited\neffectiveness or stealthiness, or are impractical in real-world settings. In\nthis work, we propose EnvInjection, a new attack that addresses these\nlimitations. Our attack adds a perturbation to the raw pixel values of the\nrendered webpage, which can be implemented by modifying the webpage's source\ncode. After these perturbed pixels are mapped into a screenshot, the\nperturbation induces the web agent to perform the target action. We formulate\nthe task of finding the perturbation as an optimization problem. A key\nchallenge in solving this problem is that the mapping between raw pixel values\nand screenshot is non-differentiable, making it difficult to backpropagate\ngradients to the perturbation. To overcome this, we train a neural network to\napproximate the mapping and apply projected gradient descent to solve the\nreformulated optimization problem. Extensive evaluation on multiple webpage\ndatasets shows that EnvInjection is highly effective and significantly\noutperforms existing baselines."}
{"id": "2505.11812", "pdf": "https://arxiv.org/pdf/2505.11812", "abs": "https://arxiv.org/abs/2505.11812", "authors": ["Yang Tan", "Wenrui Gou", "Bozitao Zhong", "Liang Hong", "Huiqun Yu", "Bingxin Zhou"], "title": "VenusX: Unlocking Fine-Grained Functional Understanding of Proteins", "categories": ["cs.LG", "cs.CL", "q-bio.QM"], "comment": "29 pages, 3 figures, 17 tables", "summary": "Deep learning models have driven significant progress in predicting protein\nfunction and interactions at the protein level. While these advancements have\nbeen invaluable for many biological applications such as enzyme engineering and\nfunction annotation, a more detailed perspective is essential for understanding\nprotein functional mechanisms and evaluating the biological knowledge captured\nby models. To address this demand, we introduce VenusX, the first large-scale\nbenchmark for fine-grained functional annotation and function-based protein\npairing at the residue, fragment, and domain levels. VenusX comprises three\nmajor task categories across six types of annotations, including residue-level\nbinary classification, fragment-level multi-class classification, and pairwise\nfunctional similarity scoring for identifying critical active sites, binding\nsites, conserved sites, motifs, domains, and epitopes. The benchmark features\nover 878,000 samples curated from major open-source databases such as InterPro,\nBioLiP, and SAbDab. By providing mixed-family and cross-family splits at three\nsequence identity thresholds, our benchmark enables a comprehensive assessment\nof model performance on both in-distribution and out-of-distribution scenarios.\nFor baseline evaluation, we assess a diverse set of popular and open-source\nmodels, including pre-trained protein language models, sequence-structure\nhybrids, structure-based methods, and alignment-based techniques. Their\nperformance is reported across all benchmark datasets and evaluation settings\nusing multiple metrics, offering a thorough comparison and a strong foundation\nfor future research. Code and data are publicly available at\nhttps://github.com/ai4protein/VenusX."}
{"id": "2505.11983", "pdf": "https://arxiv.org/pdf/2505.11983", "abs": "https://arxiv.org/abs/2505.11983", "authors": ["Ting Xiao", "Lei Shi", "Yang Zhang", "HaoFeng Yang", "Zhe Wang", "Chenjia Bai"], "title": "Online Iterative Self-Alignment for Radiology Report Generation", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ACL 2025 Main", "summary": "Radiology Report Generation (RRG) is an important research topic for\nrelieving radiologist' heavy workload. Existing RRG models mainly rely on\nsupervised fine-tuning (SFT) based on different model architectures using data\npairs of radiological images and corresponding radiologist-annotated reports.\nRecent research has shifted focus to post-training improvements, aligning RRG\nmodel outputs with human preferences using reinforcement learning (RL).\nHowever, the limited data coverage of high-quality annotated data poses risks\nof overfitting and generalization. This paper proposes a novel Online Iterative\nSelf-Alignment (OISA) method for RRG that consists of four stages:\nself-generation of diverse data, self-evaluation for multi-objective preference\ndata,self-alignment for multi-objective optimization and self-iteration for\nfurther improvement. Our approach allows for generating varied reports tailored\nto specific clinical objectives, enhancing the overall performance of the RRG\nmodel iteratively. Unlike existing methods, our frame-work significantly\nincreases data quality and optimizes performance through iterative\nmulti-objective optimization. Experimental results demonstrate that our method\nsurpasses previous approaches, achieving state-of-the-art performance across\nmultiple evaluation metrics."}
{"id": "2505.11797", "pdf": "https://arxiv.org/pdf/2505.11797", "abs": "https://arxiv.org/abs/2505.11797", "authors": ["Hancan Zhu", "Jinhao Chen", "Guanghua He"], "title": "MedVKAN: Efficient Feature Extraction with Mamba and KAN for Medical Image Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Medical image segmentation relies heavily on convolutional neural networks\n(CNNs) and Transformer-based models. However, CNNs are constrained by limited\nreceptive fields, while Transformers suffer from scalability challenges due to\ntheir quadratic computational complexity. To address these limitations, recent\nadvances have explored alternative architectures. The state-space model Mamba\noffers near-linear complexity while capturing long-range dependencies, and the\nKolmogorov-Arnold Network (KAN) enhances nonlinear expressiveness by replacing\nfixed activation functions with learnable ones. Building on these strengths, we\npropose MedVKAN, an efficient feature extraction model integrating Mamba and\nKAN. Specifically, we introduce the EFC-KAN module, which enhances KAN with\nconvolutional operations to improve local pixel interaction. We further design\nthe VKAN module, integrating Mamba with EFC-KAN as a replacement for\nTransformer modules, significantly improving feature extraction. Extensive\nexperiments on five public medical image segmentation datasets show that\nMedVKAN achieves state-of-the-art performance on four datasets and ranks second\non the remaining one. These results validate the potential of Mamba and KAN for\nmedical image segmentation while introducing an innovative and computationally\nefficient feature extraction framework. The code is available at:\nhttps://github.com/beginner-cjh/MedVKAN."}
{"id": "2505.11842", "pdf": "https://arxiv.org/pdf/2505.11842", "abs": "https://arxiv.org/abs/2505.11842", "authors": ["Xuannan Liu", "Zekun Li", "Zheqi He", "Peipei Li", "Shuhan Xia", "Xing Cui", "Huaibo Huang", "Xi Yang", "Ran He"], "title": "Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs", "categories": ["cs.CV", "cs.CL"], "comment": "Project page:\n  https://liuxuannan.github.io/Video-SafetyBench.github.io/", "summary": "The increasing deployment of Large Vision-Language Models (LVLMs) raises\nsafety concerns under potential malicious inputs. However, existing multimodal\nsafety evaluations primarily focus on model vulnerabilities exposed by static\nimage inputs, ignoring the temporal dynamics of video that may induce distinct\nsafety risks. To bridge this gap, we introduce Video-SafetyBench, the first\ncomprehensive benchmark designed to evaluate the safety of LVLMs under\nvideo-text attacks. It comprises 2,264 video-text pairs spanning 48\nfine-grained unsafe categories, each pairing a synthesized video with either a\nharmful query, which contains explicit malice, or a benign query, which appears\nharmless but triggers harmful behavior when interpreted alongside the video. To\ngenerate semantically accurate videos for safety evaluation, we design a\ncontrollable pipeline that decomposes video semantics into subject images (what\nis shown) and motion text (how it moves), which jointly guide the synthesis of\nquery-relevant videos. To effectively evaluate uncertain or borderline harmful\noutputs, we propose RJScore, a novel LLM-based metric that incorporates the\nconfidence of judge models and human-aligned decision threshold calibration.\nExtensive experiments show that benign-query video composition achieves average\nattack success rates of 67.2%, revealing consistent vulnerabilities to\nvideo-induced attacks. We believe Video-SafetyBench will catalyze future\nresearch into video-based safety evaluation and defense strategies."}
{"id": "2505.12005", "pdf": "https://arxiv.org/pdf/2505.12005", "abs": "https://arxiv.org/abs/2505.12005", "authors": ["Dong Liu", "Yifan Yang", "Zixiong Huang", "Yuxin Gao", "Mingkui Tan"], "title": "CHRIS: Clothed Human Reconstruction with Side View Consistency", "categories": ["cs.CV", "cs.AI"], "comment": "ICME 2025", "summary": "Creating a realistic clothed human from a single-view RGB image is crucial\nfor applications like mixed reality and filmmaking. Despite some progress in\nrecent years, mainstream methods often fail to fully utilize side-view\ninformation, as the input single-view image contains front-view information\nonly. This leads to globally unrealistic topology and local surface\ninconsistency in side views. To address these, we introduce Clothed Human\nReconstruction with Side View Consistency, namely CHRIS, which consists of 1) A\nSide-View Normal Discriminator that enhances global visual reasonability by\ndistinguishing the generated side-view normals from the ground truth ones; 2) A\nMulti-to-One Gradient Computation (M2O) that ensures local surface consistency.\nM2O calculates the gradient of a sampling point by integrating the gradients of\nthe nearby points, effectively acting as a smooth operation. Experimental\nresults demonstrate that CHRIS achieves state-of-the-art performance on public\nbenchmarks and outperforms the prior work."}
{"id": "2505.11832", "pdf": "https://arxiv.org/pdf/2505.11832", "abs": "https://arxiv.org/abs/2505.11832", "authors": ["Yuxiang Lai", "Jike Zhong", "Vanessa Su", "Xiaofeng Yang"], "title": "Patient-Specific Autoregressive Models for Organ Motion Prediction in Radiotherapy", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Radiotherapy often involves a prolonged treatment period. During this time,\npatients may experience organ motion due to breathing and other physiological\nfactors. Predicting and modeling this motion before treatment is crucial for\nensuring precise radiation delivery. However, existing pre-treatment organ\nmotion prediction methods primarily rely on deformation analysis using\nprincipal component analysis (PCA), which is highly dependent on registration\nquality and struggles to capture periodic temporal dynamics for motion\nmodeling.In this paper, we observe that organ motion prediction closely\nresembles an autoregressive process, a technique widely used in natural\nlanguage processing (NLP). Autoregressive models predict the next token based\non previous inputs, naturally aligning with our objective of predicting future\norgan motion phases. Building on this insight, we reformulate organ motion\nprediction as an autoregressive process to better capture patient-specific\nmotion patterns. Specifically, we acquire 4D CT scans for each patient before\ntreatment, with each sequence comprising multiple 3D CT phases. These phases\nare fed into the autoregressive model to predict future phases based on prior\nphase motion patterns. We evaluate our method on a real-world test set of 4D CT\nscans from 50 patients who underwent radiotherapy at our institution and a\npublic dataset containing 4D CT scans from 20 patients (some with multiple\nscans), totaling over 1,300 3D CT phases. The performance in predicting the\nmotion of the lung and heart surpasses existing benchmarks, demonstrating its\neffectiveness in capturing motion dynamics from CT images. These results\nhighlight the potential of our method to improve pre-treatment planning in\nradiotherapy, enabling more precise and adaptive radiation delivery."}
{"id": "2505.11861", "pdf": "https://arxiv.org/pdf/2505.11861", "abs": "https://arxiv.org/abs/2505.11861", "authors": ["Qi Zhou", "Jie Zhang", "Dongxia Wang", "Qiang Liu", "Tianlin Li", "Jin Song Dong", "Wenhai Wang", "Qing Guo"], "title": "Fair-PP: A Synthetic Dataset for Aligning LLM with Personalized Preferences of Social Equity", "categories": ["cs.AI", "cs.CL", "91C99", "I.2.7; J.4"], "comment": "under review", "summary": "Human preference plays a crucial role in the refinement of large language\nmodels (LLMs). However, collecting human preference feedback is costly and most\nexisting datasets neglect the correlation between personalization and\npreferences. To address this issue, we introduce Fair-PP, a synthetic dataset\nof personalized preferences targeting social equity, derived from real-world\nsocial survey data, which includes 28 social groups, 98 equity topics, and 5\npersonal preference dimensions. Leveraging GPT-4o-mini, we engage in\nrole-playing based on seven representative persona portrayals guided by\nexisting social survey data, yielding a total of 238,623 preference records.\nThrough Fair-PP, we also contribute (i) An automated framework for generating\npreference data, along with a more fine-grained dataset of personalized\npreferences; (ii) analysis of the positioning of the existing mainstream LLMs\nacross five major global regions within the personalized preference space; and\n(iii) a sample reweighting method for personalized preference alignment,\nenabling alignment with a target persona while maximizing the divergence from\nother personas. Empirical experiments show our method outperforms the\nbaselines."}
{"id": "2505.12020", "pdf": "https://arxiv.org/pdf/2505.12020", "abs": "https://arxiv.org/abs/2505.12020", "authors": ["Xi Han", "Jingwei Zhang", "Dimitris Samaras", "Fei Hou", "Hong Qin"], "title": "GeoMaNO: Geometric Mamba Neural Operator for Partial Differential Equations", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The neural operator (NO) framework has emerged as a powerful tool for solving\npartial differential equations (PDEs). Recent NOs are dominated by the\nTransformer architecture, which offers NOs the capability to capture long-range\ndependencies in PDE dynamics. However, existing Transformer-based NOs suffer\nfrom quadratic complexity, lack geometric rigor, and thus suffer from\nsub-optimal performance on regular grids. As a remedy, we propose the Geometric\nMamba Neural Operator (GeoMaNO) framework, which empowers NOs with Mamba's\nmodeling capability, linear complexity, plus geometric rigor. We evaluate\nGeoMaNO's performance on multiple standard and popularly employed PDE\nbenchmarks, spanning from Darcy flow problems to Navier-Stokes problems.\nGeoMaNO improves existing baselines in solution operator approximation by as\nmuch as 58.9%."}
{"id": "2505.11865", "pdf": "https://arxiv.org/pdf/2505.11865", "abs": "https://arxiv.org/abs/2505.11865", "authors": ["Teli Ma", "Jia Zheng", "Zifan Wang", "Ziyao Gao", "Jiaming Zhou", "Junwei Liang"], "title": "GLOVER++: Unleashing the Potential of Affordance Learning from Human Behaviors for Robotic Manipulation", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Learning manipulation skills from human demonstration videos offers a\npromising path toward generalizable and interpretable robotic\nintelligence-particularly through the lens of actionable affordances. However,\ntransferring such knowledge remains challenging due to: 1) a lack of\nlarge-scale datasets with precise affordance annotations, and 2) insufficient\nexploration of affordances in diverse manipulation contexts. To address these\ngaps, we introduce HOVA-500K, a large-scale, affordance-annotated dataset\ncomprising 500,000 images across 1,726 object categories and 675 actions. We\nalso release a standardized benchmarking suite for multi-modal affordance\nreasoning. Built upon HOVA-500K, we present GLOVER++, a global-to-local\naffordance training framework that effectively transfers actionable affordance\nknowledge from human demonstrations to downstream open-vocabulary reasoning\ntasks. GLOVER++ achieves state-of-the-art results on the HOVA-500K benchmark\nand demonstrates strong generalization across diverse downstream robotic\nmanipulation tasks. By explicitly modeling actionable affordances, GLOVER++\nfacilitates robust transfer across scenes, modalities, and tasks. We hope that\nHOVA-500K and the GLOVER++ framework will serve as valuable resources for\nbridging the gap between human demonstrations and robotic manipulation\ncapabilities."}
{"id": "2505.11875", "pdf": "https://arxiv.org/pdf/2505.11875", "abs": "https://arxiv.org/abs/2505.11875", "authors": ["Chi-Min Chan", "Chunpu Xu", "Jiaming Ji", "Zhen Ye", "Pengcheng Wen", "Chunyang Jiang", "Yaodong Yang", "Wei Xue", "Sirui Han", "Yike Guo"], "title": "J1: Exploring Simple Test-Time Scaling for LLM-as-a-Judge", "categories": ["cs.LG", "cs.CL"], "comment": "33 pages, 27 figures", "summary": "The current focus of AI research is shifting from emphasizing model training\ntowards enhancing evaluation quality, a transition that is crucial for driving\nfurther advancements in AI systems. Traditional evaluation methods typically\nrely on reward models assigning scalar preference scores to outputs. Although\neffective, such approaches lack interpretability, leaving users often uncertain\nabout why a reward model rates a particular response as high or low. The advent\nof LLM-as-a-Judge provides a more scalable and interpretable method of\nsupervision, offering insights into the decision-making process. Moreover, with\nthe emergence of large reasoning models, which consume more tokens for deeper\nthinking and answer refinement, scaling test-time computation in the\nLLM-as-a-Judge paradigm presents an avenue for further boosting performance and\nproviding more interpretability through reasoning traces. In this paper, we\nintroduce $\\textbf{J1-7B}$, which is first supervised fine-tuned on\nreflection-enhanced datasets collected via rejection-sampling and subsequently\ntrained using Reinforcement Learning (RL) with verifiable rewards. At inference\ntime, we apply Simple Test-Time Scaling (STTS) strategies for additional\nperformance improvement. Experimental results demonstrate that $\\textbf{J1-7B}$\nsurpasses the previous state-of-the-art LLM-as-a-Judge by $ \\textbf{4.8}$\\% and\nexhibits a $ \\textbf{5.1}$\\% stronger scaling trend under STTS. Additionally,\nwe present three key findings: (1) Existing LLM-as-a-Judge does not inherently\nexhibit such scaling trend. (2) Model simply fine-tuned on reflection-enhanced\ndatasets continues to demonstrate similarly weak scaling behavior. (3)\nSignificant scaling trend emerges primarily during the RL phase, suggesting\nthat effective STTS capability is acquired predominantly through RL training."}
{"id": "2505.12038", "pdf": "https://arxiv.org/pdf/2505.12038", "abs": "https://arxiv.org/abs/2505.12038", "authors": ["Ning Lu", "Shengcai Liu", "Jiahao Wu", "Weiyu Chen", "Zhirui Zhang", "Yew-Soon Ong", "Qi Wang", "Ke Tang"], "title": "Safe Delta: Consistently Preserving Safety when Fine-Tuning LLMs on Diverse Datasets", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "ICML 2025 Camera Ready", "summary": "Large language models (LLMs) have shown great potential as general-purpose AI\nassistants across various domains. To fully leverage this potential in specific\napplications, many companies provide fine-tuning API services, enabling users\nto upload their own data for LLM customization. However, fine-tuning services\nintroduce a new safety threat: user-uploaded data, whether harmful or benign,\ncan break the model's alignment, leading to unsafe outputs. Moreover, existing\ndefense methods struggle to address the diversity of fine-tuning datasets\n(e.g., varying sizes, tasks), often sacrificing utility for safety or vice\nversa. To address this issue, we propose Safe Delta, a safety-aware\npost-training defense method that adjusts the delta parameters (i.e., the\nparameter change before and after fine-tuning). Specifically, Safe Delta\nestimates the safety degradation, selects delta parameters to maximize utility\nwhile limiting overall safety loss, and applies a safety compensation vector to\nmitigate residual safety loss. Through extensive experiments on four diverse\ndatasets with varying settings, our approach consistently preserves safety\nwhile ensuring that the utility gain from benign datasets remains unaffected."}
{"id": "2505.11879", "pdf": "https://arxiv.org/pdf/2505.11879", "abs": "https://arxiv.org/abs/2505.11879", "authors": ["Reihaneh Yourdkhani", "Arash Tavoosian", "Navid Asadi Khomami", "Mehdi Tale Masouleh"], "title": "Experimental Study on Automatically Assembling Custom Catering Packages With a 3-DOF Delta Robot Using Deep Learning Methods", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "This paper introduces a pioneering experimental study on the automated\npacking of a catering package using a two-fingered gripper affixed to a\n3-degree-of-freedom Delta parallel robot. A distinctive contribution lies in\nthe application of a deep learning approach to tackle this challenge. A custom\ndataset, comprising 1,500 images, is meticulously curated for this endeavor,\nrepresenting a noteworthy initiative as the first dataset focusing on\nPersian-manufactured products. The study employs the YOLOV5 model for object\ndetection, followed by segmentation using the FastSAM model. Subsequently,\nrotation angle calculation is facilitated with segmentation masks, and a\nrotated rectangle encapsulating the object is generated. This rectangle forms\nthe basis for calculating two grasp points using a novel geometrical approach\ninvolving eigenvectors. An extensive experimental study validates the proposed\nmodel, where all pertinent information is seamlessly transmitted to the 3-DOF\nDelta parallel robot. The proposed algorithm ensures real-time detection,\ncalibration, and the fully autonomous packing process of a catering package,\nboasting an impressive over 80\\% success rate in automatic grasping. This study\nmarks a significant stride in advancing the capabilities of robotic systems for\npractical applications in packaging automation."}
{"id": "2505.11979", "pdf": "https://arxiv.org/pdf/2505.11979", "abs": "https://arxiv.org/abs/2505.11979", "authors": ["Tarik Houichime", "Younes El Amrani"], "title": "Introduction to Analytical Software Engineering Design Paradigm", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.MS", "cs.PL"], "comment": "The Conference's autorization to submit a preprint was granted", "summary": "As modern software systems expand in scale and complexity, the challenges\nassociated with their modeling and formulation grow increasingly intricate.\nTraditional approaches often fall short in effectively addressing these\ncomplexities, particularly in tasks such as design pattern detection for\nmaintenance and assessment, as well as code refactoring for optimization and\nlong-term sustainability. This growing inadequacy underscores the need for a\nparadigm shift in how such challenges are approached and resolved. This paper\npresents Analytical Software Engineering (ASE), a novel design paradigm aimed\nat balancing abstraction, tool accessibility, compatibility, and scalability.\nASE enables effective modeling and resolution of complex software engineering\nproblems. The paradigm is evaluated through two frameworks\nBehavioral-Structural Sequences (BSS) and Optimized Design Refactoring (ODR),\nboth developed in accordance with ASE principles. BSS offers a compact,\nlanguage-agnostic representation of codebases to facilitate precise design\npattern detection. ODR unifies artifact and solution representations to\noptimize code refactoring via heuristic algorithms while eliminating iterative\ncomputational overhead. By providing a structured approach to software design\nchallenges, ASE lays the groundwork for future research in encoding and\nanalyzing complex software metrics."}
{"id": "2505.12049", "pdf": "https://arxiv.org/pdf/2505.12049", "abs": "https://arxiv.org/abs/2505.12049", "authors": ["Mehran Shakerinava", "Siamak Ravanbakhsh", "Adam Oberman"], "title": "Beyond Scalar Rewards: An Axiomatic Framework for Lexicographic MDPs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recent work has formalized the reward hypothesis through the lens of expected\nutility theory, by interpreting reward as utility. Hausner's foundational work\nshowed that dropping the continuity axiom leads to a generalization of expected\nutility theory where utilities are lexicographically ordered vectors of\narbitrary dimension. In this paper, we extend this result by identifying a\nsimple and practical condition under which preferences cannot be represented by\nscalar rewards, necessitating a 2-dimensional reward function. We provide a\nfull characterization of such reward functions, as well as the general\nd-dimensional case, in Markov Decision Processes (MDPs) under a memorylessness\nassumption on preferences. Furthermore, we show that optimal policies in this\nsetting retain many desirable properties of their scalar-reward counterparts,\nwhile in the Constrained MDP (CMDP) setting -- another common multiobjective\nsetting -- they do not."}
{"id": "2505.11883", "pdf": "https://arxiv.org/pdf/2505.11883", "abs": "https://arxiv.org/abs/2505.11883", "authors": ["Zihuan Qiu", "Yi Xu", "Chiyuan He", "Fanman Meng", "Linfeng Xu", "Qingbo Wu", "Hongliang Li"], "title": "MINGLE: Mixtures of Null-Space Gated Low-Rank Experts for Test-Time Continual Model Merging", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Continual model merging integrates independently fine-tuned models\nsequentially without access to original training data, providing a scalable and\nefficient solution to continual learning. However, current methods still face\ncritical challenges, notably parameter interference among tasks and limited\nadaptability to evolving test distributions. The former causes catastrophic\nforgetting of integrated tasks, while the latter hinders effective adaptation\nto new tasks. To address these, we propose MINGLE, a novel framework for\ntest-time continual model merging, which leverages test-time adaptation using a\nsmall set of unlabeled test samples from the current task to dynamically guide\nthe merging process. MINGLE employs a mixture-of-experts architecture composed\nof parameter-efficient, low-rank experts, enabling efficient adaptation and\nimproving robustness to distribution shifts. To mitigate catastrophic\nforgetting, we propose Null-Space Constrained Gating, which restricts gating\nupdates to subspaces orthogonal to prior task representations. This suppresses\nactivations on old task inputs and preserves model behavior on past tasks. To\nfurther balance stability and adaptability, we design an Adaptive Relaxation\nStrategy, which dynamically adjusts the constraint strength based on\ninterference signals captured during test-time adaptation. Extensive\nexperiments on standard continual merging benchmarks demonstrate that MINGLE\nachieves robust generalization, reduces forgetting significantly, and\nconsistently surpasses previous state-of-the-art methods by 7-9\\% on average\nacross diverse task orders."}
{"id": "2505.12039", "pdf": "https://arxiv.org/pdf/2505.12039", "abs": "https://arxiv.org/abs/2505.12039", "authors": ["Renqi Chen", "Haoyang Su", "Shixiang Tang", "Zhenfei Yin", "Qi Wu", "Hui Li", "Ye Sun", "Nanqing Dong", "Wanli Ouyang", "Philip Torr"], "title": "AI-Driven Automation Can Become the Foundation of Next-Era Science of Science Research", "categories": ["cs.AI", "cs.CL", "physics.soc-ph"], "comment": null, "summary": "The Science of Science (SoS) explores the mechanisms underlying scientific\ndiscovery, and offers valuable insights for enhancing scientific efficiency and\nfostering innovation. Traditional approaches often rely on simplistic\nassumptions and basic statistical tools, such as linear regression and\nrule-based simulations, which struggle to capture the complexity and scale of\nmodern research ecosystems. The advent of artificial intelligence (AI) presents\na transformative opportunity for the next generation of SoS, enabling the\nautomation of large-scale pattern discovery and uncovering insights previously\nunattainable. This paper offers a forward-looking perspective on the\nintegration of Science of Science with AI for automated research pattern\ndiscovery and highlights key open challenges that could greatly benefit from\nAI. We outline the advantages of AI over traditional methods, discuss potential\nlimitations, and propose pathways to overcome them. Additionally, we present a\npreliminary multi-agent system as an illustrative example to simulate research\nsocieties, showcasing AI's ability to replicate real-world research patterns\nand accelerate progress in Science of Science research."}
{"id": "2505.12050", "pdf": "https://arxiv.org/pdf/2505.12050", "abs": "https://arxiv.org/abs/2505.12050", "authors": ["Vinod Raman", "Hilal Asi", "Satyen Kale"], "title": "ABoN: Adaptive Best-of-N Alignment", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "23 pages", "summary": "Recent advances in test-time alignment methods, such as Best-of-N sampling,\noffer a simple and effective way to steer language models (LMs) toward\npreferred behaviors using reward models (RM). However, these approaches can be\ncomputationally expensive, especially when applied uniformly across prompts\nwithout accounting for differences in alignment difficulty. In this work, we\npropose a prompt-adaptive strategy for Best-of-N alignment that allocates\ninference-time compute more efficiently. Motivated by latency concerns, we\ndevelop a two-stage algorithm: an initial exploratory phase estimates the\nreward distribution for each prompt using a small exploration budget, and a\nsecond stage adaptively allocates the remaining budget using these estimates.\nOur method is simple, practical, and compatible with any LM/RM combination.\nEmpirical results on the AlpacaEval dataset for 12 LM/RM pairs and 50 different\nbatches of prompts show that our adaptive strategy consistently outperforms the\nuniform allocation with the same inference budget. Moreover, our experiments\nshow that our adaptive strategy remains competitive against uniform allocations\nwith 20% larger inference budgets and even improves in performance as the batch\nsize grows."}
{"id": "2505.11909", "pdf": "https://arxiv.org/pdf/2505.11909", "abs": "https://arxiv.org/abs/2505.11909", "authors": ["Pengfei Lyu", "Pak-Hei Yeung", "Xiaosheng Yu", "Jing Xia", "Jianning Chi", "Chengdong Wu", "Jagath C. Rajapakse"], "title": "Bridging the Inter-Domain Gap through Low-Level Features for Cross-Modal Medical Image Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": "11 pages, 2 figures", "summary": "This paper addresses the task of cross-modal medical image segmentation by\nexploring unsupervised domain adaptation (UDA) approaches. We propose a\nmodel-agnostic UDA framework, LowBridge, which builds on a simple observation\nthat cross-modal images share some similar low-level features (e.g., edges) as\nthey are depicting the same structures. Specifically, we first train a\ngenerative model to recover the source images from their edge features,\nfollowed by training a segmentation model on the generated source images,\nseparately. At test time, edge features from the target images are input to the\npretrained generative model to generate source-style target domain images,\nwhich are then segmented using the pretrained segmentation network. Despite its\nsimplicity, extensive experiments on various publicly available datasets\ndemonstrate that \\proposed achieves state-of-the-art performance, outperforming\neleven existing UDA approaches under different settings. Notably, further\nablation studies show that \\proposed is agnostic to different types of\ngenerative and segmentation models, suggesting its potential to be seamlessly\nplugged with the most advanced models to achieve even more outstanding results\nin the future. The code is available at https://github.com/JoshuaLPF/LowBridge."}
{"id": "2505.12058", "pdf": "https://arxiv.org/pdf/2505.12058", "abs": "https://arxiv.org/abs/2505.12058", "authors": ["Vincent Koc"], "title": "Tiny QA Benchmark++: Ultra-Lightweight, Synthetic Multilingual Dataset Generation & Smoke-Tests for Continuous LLM Evaluation", "categories": ["cs.AI", "cs.CL", "I.2.7; I.2.6; H.2.8"], "comment": "28 pages, 7 figures, 3 tables. Includes expanded appendix & full\n  score matrices. Dataset & code: HF Hub + GitHub + Pypi links in abstract.\n  Core data and code Apache-2.0; synthetic packs eval-only", "summary": "Tiny QA Benchmark++ (TQB++) presents an ultra-lightweight, multilingual\nsmoke-test suite designed to give large-language-model (LLM) pipelines a\nunit-test style safety net dataset that runs in seconds with minimal cost. Born\nout of the tight feedback-loop demands building the Comet Opik\nprompt-optimization SDK, where waiting on heavyweight benchmarks breaks\ndeveloper flow. TQB++ couples a 52-item English gold set (less than 20 kB) with\na tiny synthetic-data generator pypi package built on provider-agnostic\nLiteLLM. The generator lets practitioners mint their own tiny packs in any\nlanguage, domain, or difficulty, while ten ready-made packs already cover\nArabic, Chinese, French, German, Japanese, Korean, Portuguese, Russian,\nSpanish, and Turkish. Every dataset ships with Croissant metadata and\nplug-and-play files for OpenAI-Evals, LangChain, and standard CI tools, so\nteams can drop deterministic micro-benchmarks directly into pull-request gates,\nprompt-engineering loops, and production dashboards without touching GPU\nbudgets. A complete TQB++ run adds only a few seconds to pipeline latency yet\nreliably flags prompt-template errors, tokenizer drift, and fine-tuning\nside-effects long before full-scale suites like MMLU or BIG-Bench would finish\nconfiguring. The entire framework is released to accelerate continuous,\nresource-efficient quality assurance across the generative-AI ecosystem."}
{"id": "2505.12051", "pdf": "https://arxiv.org/pdf/2505.12051", "abs": "https://arxiv.org/abs/2505.12051", "authors": ["Yinghui Zhang", "Tailin Chen", "Yuchen Zhang", "Zeyu Fu"], "title": "Enhanced Multimodal Hate Video Detection via Channel-wise and Modality-wise Fusion", "categories": ["cs.MM", "cs.AI", "cs.CV"], "comment": "ICDMW 2024, Github: https://github.com/EvelynZ10/cmfusion", "summary": "The rapid rise of video content on platforms such as TikTok and YouTube has\ntransformed information dissemination, but it has also facilitated the spread\nof harmful content, particularly hate videos. Despite significant efforts to\ncombat hate speech, detecting these videos remains challenging due to their\noften implicit nature. Current detection methods primarily rely on unimodal\napproaches, which inadequately capture the complementary features across\ndifferent modalities. While multimodal techniques offer a broader perspective,\nmany fail to effectively integrate temporal dynamics and modality-wise\ninteractions essential for identifying nuanced hate content. In this paper, we\npresent CMFusion, an enhanced multimodal hate video detection model utilizing a\nnovel Channel-wise and Modality-wise Fusion Mechanism. CMFusion first extracts\nfeatures from text, audio, and video modalities using pre-trained models and\nthen incorporates a temporal cross-attention mechanism to capture dependencies\nbetween video and audio streams. The learned features are then processed by\nchannel-wise and modality-wise fusion modules to obtain informative\nrepresentations of videos. Our extensive experiments on a real-world dataset\ndemonstrate that CMFusion significantly outperforms five widely used baselines\nin terms of accuracy, precision, recall, and F1 score. Comprehensive ablation\nstudies and parameter analyses further validate our design choices,\nhighlighting the model's effectiveness in detecting hate videos. The source\ncodes will be made publicly available at https://github.com/EvelynZ10/cmfusion."}
{"id": "2505.11913", "pdf": "https://arxiv.org/pdf/2505.11913", "abs": "https://arxiv.org/abs/2505.11913", "authors": ["Sven Dummer", "Puru Vaish", "Christoph Brune"], "title": "Joint Manifold Learning and Optimal Transport for Dynamic Imaging", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Dynamic imaging is critical for understanding and visualizing dynamic\nbiological processes in medicine and cell biology. These applications often\nencounter the challenge of a limited amount of time series data and time\npoints, which hinders learning meaningful patterns. Regularization methods\nprovide valuable prior knowledge to address this challenge, enabling the\nextraction of relevant information despite the scarcity of time-series data and\ntime points. In particular, low-dimensionality assumptions on the image\nmanifold address sample scarcity, while time progression models, such as\noptimal transport (OT), provide priors on image development to mitigate the\nlack of time points. Existing approaches using low-dimensionality assumptions\ndisregard a temporal prior but leverage information from multiple time series.\nOT-prior methods, however, incorporate the temporal prior but regularize only\nindividual time series, ignoring information from other time series of the same\nimage modality. In this work, we investigate the effect of integrating a\nlow-dimensionality assumption of the underlying image manifold with an OT\nregularizer for time-evolving images. In particular, we propose a latent model\nrepresentation of the underlying image manifold and promote consistency between\nthis representation, the time series data, and the OT prior on the\ntime-evolving images. We discuss the advantages of enriching OT interpolations\nwith latent models and integrating OT priors into latent models."}
{"id": "2505.12065", "pdf": "https://arxiv.org/pdf/2505.12065", "abs": "https://arxiv.org/abs/2505.12065", "authors": ["Tiannuo Yang", "Zebin Yao", "Bowen Jin", "Lixiao Cui", "Yusen Li", "Gang Wang", "Xiaoguang Liu"], "title": "Demystifying and Enhancing the Efficiency of Large Language Model Based Search Agents", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG"], "comment": null, "summary": "Large Language Model (LLM)-based search agents have shown remarkable\ncapabilities in solving complex tasks by dynamically decomposing problems and\naddressing them through interleaved reasoning and retrieval. However, this\ninterleaved paradigm introduces substantial efficiency bottlenecks. First, we\nobserve that both highly accurate and overly approximate retrieval methods\ndegrade system efficiency: exact search incurs significant retrieval overhead,\nwhile coarse retrieval requires additional reasoning steps during generation.\nSecond, we identify inefficiencies in system design, including improper\nscheduling and frequent retrieval stalls, which lead to cascading latency --\nwhere even minor delays in retrieval amplify end-to-end inference time. To\naddress these challenges, we introduce SearchAgent-X, a high-efficiency\ninference framework for LLM-based search agents. SearchAgent-X leverages\nhigh-recall approximate retrieval and incorporates two key techniques:\npriority-aware scheduling and non-stall retrieval. Extensive experiments\ndemonstrate that SearchAgent-X consistently outperforms state-of-the-art\nsystems such as vLLM and HNSW-based retrieval across diverse tasks, achieving\nup to 3.4$\\times$ higher throughput and 5$\\times$ lower latency, without\ncompromising generation quality. SearchAgent-X is available at\nhttps://github.com/tiannuo-yang/SearchAgent-X."}
{"id": "2505.12053", "pdf": "https://arxiv.org/pdf/2505.12053", "abs": "https://arxiv.org/abs/2505.12053", "authors": ["Tianxiong Zhong", "Xingye Tian", "Boyuan Jiang", "Xuebo Wang", "Xin Tao", "Pengfei Wan", "Zhiwei Zhang"], "title": "VFRTok: Variable Frame Rates Video Tokenizer with Duration-Proportional Information Assumption", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 10 figures", "summary": "Modern video generation frameworks based on Latent Diffusion Models suffer\nfrom inefficiencies in tokenization due to the Frame-Proportional Information\nAssumption. Existing tokenizers provide fixed temporal compression rates,\ncausing the computational cost of the diffusion model to scale linearly with\nthe frame rate. The paper proposes the Duration-Proportional Information\nAssumption: the upper bound on the information capacity of a video is\nproportional to the duration rather than the number of frames. Based on this\ninsight, the paper introduces VFRTok, a Transformer-based video tokenizer, that\nenables variable frame rate encoding and decoding through asymmetric frame rate\ntraining between the encoder and decoder. Furthermore, the paper proposes\nPartial Rotary Position Embeddings (RoPE) to decouple position and content\nmodeling, which groups correlated patches into unified tokens. The Partial RoPE\neffectively improves content-awareness, enhancing the video generation\ncapability. Benefiting from the compact and continuous spatio-temporal\nrepresentation, VFRTok achieves competitive reconstruction quality and\nstate-of-the-art generation fidelity while using only 1/8 tokens compared to\nexisting tokenizers."}
{"id": "2505.11998", "pdf": "https://arxiv.org/pdf/2505.11998", "abs": "https://arxiv.org/abs/2505.11998", "authors": ["Prashant Shivaram Bhat", "Shakib Yazdani", "Elahe Arani", "Bahram Zonooz"], "title": "Parameter Efficient Continual Learning with Dynamic Low-Rank Adaptation", "categories": ["cs.LG", "cs.CV"], "comment": "27 pages, 5 figures", "summary": "Catastrophic forgetting has remained a critical challenge for deep neural\nnetworks in Continual Learning (CL) as it undermines consolidated knowledge\nwhen learning new tasks. Parameter efficient fine tuning CL techniques are\ngaining traction for their effectiveness in addressing catastrophic forgetting\nwith a lightweight training schedule while avoiding degradation of consolidated\nknowledge in pre-trained models. However, low rank adapters (LoRA) in these\napproaches are highly sensitive to rank selection which can lead to sub-optimal\nresource allocation and performance. To this end, we introduce PEARL, a\nrehearsal-free CL framework that entails dynamic rank allocation for LoRA\ncomponents during CL training. Specifically, PEARL leverages reference task\nweights and adaptively determines the rank of task-specific LoRA components\nbased on the current tasks' proximity to reference task weights in parameter\nspace. To demonstrate the versatility of PEARL, we evaluate it across three\nvision architectures (ResNet, Separable Convolutional Network and Vision\nTransformer) and a multitude of CL scenarios, and show that PEARL outperforms\nall considered baselines by a large margin."}
{"id": "2505.12135", "pdf": "https://arxiv.org/pdf/2505.12135", "abs": "https://arxiv.org/abs/2505.12135", "authors": ["Omar Choukrani", "Idriss Malek", "Daniil Orel", "Zhuohan Xie", "Zangir Iklassov", "Martin Takáč", "Salem Lahlou"], "title": "LLM-BABYBENCH: Understanding and Evaluating Grounded Planning and Reasoning in LLMs", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Assessing the capacity of Large Language Models (LLMs) to plan and reason\nwithin the constraints of interactive environments is crucial for developing\ncapable AI agents. We introduce $\\textbf{LLM-BabyBench}$, a new benchmark suite\ndesigned specifically for this purpose. Built upon a textual adaptation of the\nprocedurally generated BabyAI grid world, this suite evaluates LLMs on three\nfundamental aspects of grounded intelligence: (1) predicting the consequences\nof actions on the environment state ($\\textbf{Predict}$ task), (2) generating\nsequences of low-level actions to achieve specified objectives ($\\textbf{Plan}$\ntask), and (3) decomposing high-level instructions into coherent subgoal\nsequences ($\\textbf{Decompose}$ task). We detail the methodology for generating\nthe three corresponding datasets ($\\texttt{LLM-BabyBench-Predict}$,\n$\\texttt{-Plan}$, $\\texttt{-Decompose}$) by extracting structured information\nfrom an expert agent operating within the text-based environment. Furthermore,\nwe provide a standardized evaluation harness and metrics, including environment\ninteraction for validating generated plans, to facilitate reproducible\nassessment of diverse LLMs. Initial baseline results highlight the challenges\nposed by these grounded reasoning tasks. The benchmark suite, datasets, data\ngeneration code, and evaluation code are made publicly available\n($\\href{https://github.com/choukrani/llm-babybench}{\\text{GitHub}}$,\n$\\href{https://huggingface.co/datasets/salem-mbzuai/LLM-BabyBench}{\\text{HuggingFace}}$)."}
{"id": "2505.12069", "pdf": "https://arxiv.org/pdf/2505.12069", "abs": "https://arxiv.org/abs/2505.12069", "authors": ["Shenzhou Liu", "Di Wang", "Haonan Guo", "Chengxi Han", "Wenzhi Zeng"], "title": "MT-CYP-Net: Multi-Task Network for Pixel-Level Crop Yield Prediction Under Very Few Samples", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate and fine-grained crop yield prediction plays a crucial role in\nadvancing global agriculture. However, the accuracy of pixel-level yield\nestimation based on satellite remote sensing data has been constrained by the\nscarcity of ground truth data. To address this challenge, we propose a novel\napproach called the Multi-Task Crop Yield Prediction Network (MT-CYP-Net). This\nframework introduces an effective multi-task feature-sharing strategy, where\nfeatures extracted from a shared backbone network are simultaneously utilized\nby both crop yield prediction decoders and crop classification decoders with\nthe ability to fuse information between them. This design allows MT-CYP-Net to\nbe trained with extremely sparse crop yield point labels and crop type labels,\nwhile still generating detailed pixel-level crop yield maps. Concretely, we\ncollected 1,859 yield point labels along with corresponding crop type labels\nand satellite images from eight farms in Heilongjiang Province, China, in 2023,\ncovering soybean, maize, and rice crops, and constructed a sparse crop yield\nlabel dataset. MT-CYP-Net is compared with three classical machine learning and\ndeep learning benchmark methods in this dataset. Experimental results not only\nindicate the superiority of MT-CYP-Net compared to previous methods on multiple\ntypes of crops but also demonstrate the potential of deep networks on precise\npixel-level crop yield prediction, especially with limited data labels."}
{"id": "2505.12051", "pdf": "https://arxiv.org/pdf/2505.12051", "abs": "https://arxiv.org/abs/2505.12051", "authors": ["Yinghui Zhang", "Tailin Chen", "Yuchen Zhang", "Zeyu Fu"], "title": "Enhanced Multimodal Hate Video Detection via Channel-wise and Modality-wise Fusion", "categories": ["cs.MM", "cs.AI", "cs.CV"], "comment": "ICDMW 2024, Github: https://github.com/EvelynZ10/cmfusion", "summary": "The rapid rise of video content on platforms such as TikTok and YouTube has\ntransformed information dissemination, but it has also facilitated the spread\nof harmful content, particularly hate videos. Despite significant efforts to\ncombat hate speech, detecting these videos remains challenging due to their\noften implicit nature. Current detection methods primarily rely on unimodal\napproaches, which inadequately capture the complementary features across\ndifferent modalities. While multimodal techniques offer a broader perspective,\nmany fail to effectively integrate temporal dynamics and modality-wise\ninteractions essential for identifying nuanced hate content. In this paper, we\npresent CMFusion, an enhanced multimodal hate video detection model utilizing a\nnovel Channel-wise and Modality-wise Fusion Mechanism. CMFusion first extracts\nfeatures from text, audio, and video modalities using pre-trained models and\nthen incorporates a temporal cross-attention mechanism to capture dependencies\nbetween video and audio streams. The learned features are then processed by\nchannel-wise and modality-wise fusion modules to obtain informative\nrepresentations of videos. Our extensive experiments on a real-world dataset\ndemonstrate that CMFusion significantly outperforms five widely used baselines\nin terms of accuracy, precision, recall, and F1 score. Comprehensive ablation\nstudies and parameter analyses further validate our design choices,\nhighlighting the model's effectiveness in detecting hate videos. The source\ncodes will be made publicly available at https://github.com/EvelynZ10/cmfusion."}
{"id": "2505.12185", "pdf": "https://arxiv.org/pdf/2505.12185", "abs": "https://arxiv.org/abs/2505.12185", "authors": ["Sen Fang", "Weiyuan Ding", "Bowen Xu"], "title": "EVALOOP: Assessing LLM Robustness in Programming from a Self-consistency Perspective", "categories": ["cs.SE", "cs.CL", "cs.LG"], "comment": "19 pages, 11 figures", "summary": "Assessing the programming capabilities of Large Language Models (LLMs) is\ncrucial for their effective use in software engineering. Current evaluations,\nhowever, predominantly measure the accuracy of generated code on static\nbenchmarks, neglecting the critical aspect of model robustness during\nprogramming tasks. While adversarial attacks offer insights on model\nrobustness, their effectiveness is limited and evaluation could be constrained.\nCurrent adversarial attack methods for robustness evaluation yield inconsistent\nresults, struggling to provide a unified evaluation across different LLMs. We\nintroduce EVALOOP, a novel assessment framework that evaluate the robustness\nfrom a self-consistency perspective, i.e., leveraging the natural duality\ninherent in popular software engineering tasks, e.g., code generation and code\nsummarization. EVALOOP initiates a self-contained feedback loop: an LLM\ngenerates output (e.g., code) from an input (e.g., natural language\nspecification), and then use the generated output as the input to produce a new\noutput (e.g., summarizes that code into a new specification). EVALOOP repeats\nthe process to assess the effectiveness of EVALOOP in each loop. This cyclical\nstrategy intrinsically evaluates robustness without rely on any external attack\nsetups, providing a unified metric to evaluate LLMs' robustness in programming.\nWe evaluate 16 prominent LLMs (e.g., GPT-4.1, O4-mini) on EVALOOP and found\nthat EVALOOP typically induces a 5.01%-19.31% absolute drop in pass@1\nperformance within ten loops. Intriguingly, robustness does not always align\nwith initial performance (i.e., one-time query); for instance, GPT-3.5-Turbo,\ndespite superior initial code generation compared to DeepSeek-V2, demonstrated\nlower robustness over repeated evaluation loop."}
{"id": "2505.12079", "pdf": "https://arxiv.org/pdf/2505.12079", "abs": "https://arxiv.org/abs/2505.12079", "authors": ["Yuqi Li", "Kai Li", "Xin Yin", "Zhifei Yang", "Junhao Dong", "Zeyu Dong", "Chuanguang Yang", "Yingli Tian", "Yao Lu"], "title": "SepPrune: Structured Pruning for Efficient Deep Speech Separation", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "Although deep learning has substantially advanced speech separation in recent\nyears, most existing studies continue to prioritize separation quality while\noverlooking computational efficiency, an essential factor for low-latency\nspeech processing in real-time applications. In this paper, we propose\nSepPrune, the first structured pruning framework specifically designed to\ncompress deep speech separation models and reduce their computational cost.\nSepPrune begins by analyzing the computational structure of a given model to\nidentify layers with the highest computational burden. It then introduces a\ndifferentiable masking strategy to enable gradient-driven channel selection.\nBased on the learned masks, SepPrune prunes redundant channels and fine-tunes\nthe remaining parameters to recover performance. Extensive experiments\ndemonstrate that this learnable pruning paradigm yields substantial advantages\nfor channel pruning in speech separation models, outperforming existing\nmethods. Notably, a model pruned with SepPrune can recover 85% of the\nperformance of a pre-trained model (trained over hundreds of epochs) with only\none epoch of fine-tuning, and achieves convergence 36$\\times$ faster than\ntraining from scratch. Code is available at\nhttps://github.com/itsnotacie/SepPrune."}
{"id": "2505.12061", "pdf": "https://arxiv.org/pdf/2505.12061", "abs": "https://arxiv.org/abs/2505.12061", "authors": ["Samuel T. M. Ball"], "title": "Bayesian Deep Learning Approaches for Uncertainty-Aware Retinal OCT Image Segmentation for Multiple Sclerosis", "categories": ["eess.IV", "cs.CV", "68U10, 92C55", "I.2.10; I.4.6; J.3"], "comment": null, "summary": "Optical Coherence Tomography (OCT) provides valuable insights in\nophthalmology, cardiology, and neurology due to high-resolution,\ncross-sectional images of the retina. One critical task for ophthalmologists\nusing OCT is delineation of retinal layers within scans. This process is\ntime-consuming and prone to human bias, affecting the accuracy and reliability\nof diagnoses. Previous efforts to automate delineation using deep learning face\nchallenges in uptake from clinicians and statisticians due to the absence of\nuncertainty estimation, leading to \"confidently wrong\" models via\nhallucinations. In this study, we address these challenges by applying Bayesian\nconvolutional neural networks (BCNNs) to segment an openly available OCT\nimaging dataset containing 35 human retina OCTs split between healthy controls\nand patients with multiple sclerosis. Our findings demonstrate that Bayesian\nmodels can be used to provide uncertainty maps of the segmentation, which can\nfurther be used to identify highly uncertain samples that exhibit recording\nartefacts such as noise or miscalibration at inference time. Our method also\nallows for uncertainty-estimation for important secondary measurements such as\nlayer thicknesses, that are medically relevant for patients. We show that these\nfeatures come in addition to greater performance compared to similar work over\nall delineations; with an overall Dice score of 95.65%. Our work brings greater\nclinical applicability, statistical robustness, and performance to retinal OCT\nsegmentation."}
{"id": "2505.12189", "pdf": "https://arxiv.org/pdf/2505.12189", "abs": "https://arxiv.org/abs/2505.12189", "authors": ["Marco Valentino", "Geonhee Kim", "Dhairya Dalal", "Zhixue Zhao", "André Freitas"], "title": "Mitigating Content Effects on Reasoning in Language Models through Fine-Grained Activation Steering", "categories": ["cs.AI", "cs.CL"], "comment": "Work in progress", "summary": "Large language models (LLMs) frequently demonstrate reasoning limitations,\noften conflating content plausibility (i.e., material inference) with logical\nvalidity (i.e., formal inference). This can result in biased inferences, where\nplausible arguments are incorrectly deemed logically valid or vice versa.\nMitigating this limitation is critical, as it undermines the trustworthiness\nand generalizability of LLMs in applications that demand rigorous logical\nconsistency. This paper investigates the problem of mitigating content biases\non formal reasoning through activation steering. Specifically, we curate a\ncontrolled syllogistic reasoning dataset to disentangle formal validity from\ncontent plausibility. After localising the layers responsible for formal and\nmaterial inference, we investigate contrastive activation steering methods for\ntest-time interventions. An extensive empirical analysis on different LLMs\nreveals that contrastive steering consistently supports linear control over\ncontent biases. However, we observe that a static approach is insufficient for\nimproving all the tested models. We then leverage the possibility to control\ncontent effects by dynamically determining the value of the steering parameters\nvia fine-grained conditional methods. We found that conditional steering is\neffective on unresponsive models, achieving up to 15% absolute improvement in\nformal reasoning accuracy with a newly introduced kNN-based method (K-CAST).\nFinally, additional experiments reveal that steering for content effects is\nrobust to prompt variations, incurs minimal side effects on language modeling\ncapabilities, and can partially generalize to out-of-distribution reasoning\ntasks. Practically, this paper demonstrates that activation-level interventions\ncan offer a scalable strategy for enhancing the robustness of LLMs,\ncontributing towards more systematic and unbiased formal reasoning."}
{"id": "2505.12089", "pdf": "https://arxiv.org/pdf/2505.12089", "abs": "https://arxiv.org/abs/2505.12089", "authors": ["Sangmin Lee", "Eunpil Park", "Angel Canelo", "Hyunhee Park", "Youngjo Kim", "Hyung-Ju Chun", "Xin Jin", "Chongyi Li", "Chun-Le Guo", "Radu Timofte", "Qi Wu", "Tianheng Qiu", "Yuchun Dong", "Shenglin Ding", "Guanghua Pan", "Weiyu Zhou", "Tao Hu", "Yixu Feng", "Duwei Dai", "Yu Cao", "Peng Wu", "Wei Dong", "Yanning Zhang", "Qingsen Yan", "Simon J. Larsen", "Ruixuan Jiang", "Senyan Xu", "Xingbo Wang", "Xin Lu", "Marcos V. Conde", "Javier Abad-Hernandez", "Alvaro Garcıa-Lara", "Daniel Feijoo", "Alvaro Garcıa", "Zeyu Xiao", "Zhuoyuan Li"], "title": "NTIRE 2025 Challenge on Efficient Burst HDR and Restoration: Datasets, Methods, and Results", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "This paper reviews the NTIRE 2025 Efficient Burst HDR and Restoration\nChallenge, which aims to advance efficient multi-frame high dynamic range (HDR)\nand restoration techniques. The challenge is based on a novel RAW multi-frame\nfusion dataset, comprising nine noisy and misaligned RAW frames with various\nexposure levels per scene. Participants were tasked with developing solutions\ncapable of effectively fusing these frames while adhering to strict efficiency\nconstraints: fewer than 30 million model parameters and a computational budget\nunder 4.0 trillion FLOPs. A total of 217 participants registered, with six\nteams finally submitting valid solutions. The top-performing approach achieved\na PSNR of 43.22 dB, showcasing the potential of novel methods in this domain.\nThis paper provides a comprehensive overview of the challenge, compares the\nproposed solutions, and serves as a valuable reference for researchers and\npractitioners in efficient burst HDR and restoration."}
{"id": "2505.12089", "pdf": "https://arxiv.org/pdf/2505.12089", "abs": "https://arxiv.org/abs/2505.12089", "authors": ["Sangmin Lee", "Eunpil Park", "Angel Canelo", "Hyunhee Park", "Youngjo Kim", "Hyung-Ju Chun", "Xin Jin", "Chongyi Li", "Chun-Le Guo", "Radu Timofte", "Qi Wu", "Tianheng Qiu", "Yuchun Dong", "Shenglin Ding", "Guanghua Pan", "Weiyu Zhou", "Tao Hu", "Yixu Feng", "Duwei Dai", "Yu Cao", "Peng Wu", "Wei Dong", "Yanning Zhang", "Qingsen Yan", "Simon J. Larsen", "Ruixuan Jiang", "Senyan Xu", "Xingbo Wang", "Xin Lu", "Marcos V. Conde", "Javier Abad-Hernandez", "Alvaro Garcıa-Lara", "Daniel Feijoo", "Alvaro Garcıa", "Zeyu Xiao", "Zhuoyuan Li"], "title": "NTIRE 2025 Challenge on Efficient Burst HDR and Restoration: Datasets, Methods, and Results", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "This paper reviews the NTIRE 2025 Efficient Burst HDR and Restoration\nChallenge, which aims to advance efficient multi-frame high dynamic range (HDR)\nand restoration techniques. The challenge is based on a novel RAW multi-frame\nfusion dataset, comprising nine noisy and misaligned RAW frames with various\nexposure levels per scene. Participants were tasked with developing solutions\ncapable of effectively fusing these frames while adhering to strict efficiency\nconstraints: fewer than 30 million model parameters and a computational budget\nunder 4.0 trillion FLOPs. A total of 217 participants registered, with six\nteams finally submitting valid solutions. The top-performing approach achieved\na PSNR of 43.22 dB, showcasing the potential of novel methods in this domain.\nThis paper provides a comprehensive overview of the challenge, compares the\nproposed solutions, and serves as a valuable reference for researchers and\npractitioners in efficient burst HDR and restoration."}
{"id": "2505.12225", "pdf": "https://arxiv.org/pdf/2505.12225", "abs": "https://arxiv.org/abs/2505.12225", "authors": ["Jizhou Guo", "Zhaomin Wu", "Philip S. Yu"], "title": "Reward Inside the Model: A Lightweight Hidden-State Reward Model for LLM's Best-of-N sampling", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "High-quality reward models are crucial for unlocking the reasoning potential\nof large language models (LLMs), with best-of-N voting demonstrating\nsignificant performance gains. However, current reward models, which typically\noperate on the textual output of LLMs, are computationally expensive and\nparameter-heavy, limiting their real-world applications. We introduce the\nEfficient Linear Hidden State Reward (ELHSR) model - a novel, highly\nparameter-efficient approach that leverages the rich information embedded in\nLLM hidden states to address these issues. ELHSR systematically outperform\nbaselines with less than 0.005% of the parameters of baselines, requiring only\na few samples for training. ELHSR also achieves orders-of-magnitude efficiency\nimprovement with significantly less time and fewer FLOPs per sample than\nbaseline reward models. Moreover, ELHSR exhibits robust performance even when\ntrained only on logits, extending its applicability to some closed-source LLMs.\nIn addition, ELHSR can also be combined with traditional reward models to\nachieve additional performance gains."}
{"id": "2505.12090", "pdf": "https://arxiv.org/pdf/2505.12090", "abs": "https://arxiv.org/abs/2505.12090", "authors": ["Mohammad Shokri", "Sarah Ita Levitan", "Rivka Levitan"], "title": "Personalized Author Obfuscation with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this paper, we investigate the efficacy of large language models (LLMs) in\nobfuscating authorship by paraphrasing and altering writing styles. Rather than\nadopting a holistic approach that evaluates performance across the entire\ndataset, we focus on user-wise performance to analyze how obfuscation\neffectiveness varies across individual authors. While LLMs are generally\neffective, we observe a bimodal distribution of efficacy, with performance\nvarying significantly across users. To address this, we propose a personalized\nprompting method that outperforms standard prompting techniques and partially\nmitigates the bimodality issue."}
{"id": "2505.12114", "pdf": "https://arxiv.org/pdf/2505.12114", "abs": "https://arxiv.org/abs/2505.12114", "authors": ["Dena F. Mujtaba", "Nihar R. Mahapatra"], "title": "Behind the Screens: Uncovering Bias in AI-Driven Video Interview Assessments Using Counterfactuals", "categories": ["cs.HC", "cs.CV", "eess.IV"], "comment": null, "summary": "AI-enhanced personality assessments are increasingly shaping hiring\ndecisions, using affective computing to predict traits from the Big Five\n(OCEAN) model. However, integrating AI into these assessments raises ethical\nconcerns, especially around bias amplification rooted in training data. These\nbiases can lead to discriminatory outcomes based on protected attributes like\ngender, ethnicity, and age. To address this, we introduce a\ncounterfactual-based framework to systematically evaluate and quantify bias in\nAI-driven personality assessments. Our approach employs generative adversarial\nnetworks (GANs) to generate counterfactual representations of job applicants by\naltering protected attributes, enabling fairness analysis without access to the\nunderlying model. Unlike traditional bias assessments that focus on unimodal or\nstatic data, our method supports multimodal evaluation-spanning visual, audio,\nand textual features. This comprehensive approach is particularly important in\nhigh-stakes applications like hiring, where third-party vendors often provide\nAI systems as black boxes. Applied to a state-of-the-art personality prediction\nmodel, our method reveals significant disparities across demographic groups. We\nalso validate our framework using a protected attribute classifier to confirm\nthe effectiveness of our counterfactual generation. This work provides a\nscalable tool for fairness auditing of commercial AI hiring platforms,\nespecially in black-box settings where training data and model internals are\ninaccessible. Our results highlight the importance of counterfactual approaches\nin improving ethical transparency in affective computing."}
{"id": "2505.12260", "pdf": "https://arxiv.org/pdf/2505.12260", "abs": "https://arxiv.org/abs/2505.12260", "authors": ["Guangyuan Ma", "Yongliang Ma", "Xuanrui Gou", "Zhenpeng Su", "Ming Zhou", "Songlin Hu"], "title": "LightRetriever: A LLM-based Hybrid Retrieval Architecture with 1000x Faster Query Inference", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs)-based hybrid retrieval uses LLMs to encode\nqueries and documents into low-dimensional dense or high-dimensional sparse\nvectors. It retrieves documents relevant to search queries based on vector\nsimilarities. Documents are pre-encoded offline, while queries arrive in\nreal-time, necessitating an efficient online query encoder. Although LLMs\nsignificantly enhance retrieval capabilities, serving deeply parameterized LLMs\nslows down query inference throughput and increases demands for online\ndeployment resources. In this paper, we propose LightRetriever, a novel\nLLM-based hybrid retriever with extremely lightweight query encoders. Our\nmethod retains a full-sized LLM for document encoding, but reduces the workload\nof query encoding to no more than an embedding lookup. Compared to serving a\nfull-sized LLM on an H800 GPU, our approach achieves over a 1000x speedup for\nquery inference with GPU acceleration, and even a 20x speedup without GPU.\nExperiments on large-scale retrieval benchmarks demonstrate that our method\ngeneralizes well across diverse retrieval tasks, retaining an average of 95%\nfull-sized performance."}
{"id": "2505.12094", "pdf": "https://arxiv.org/pdf/2505.12094", "abs": "https://arxiv.org/abs/2505.12094", "authors": ["M Ruhul Amin"], "title": "Attribution Projection Calculus: A Novel Framework for Causal Inference in Bayesian Networks", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT", "stat.ML", "60E10, 62R07, 68Q32, 68T07, 94A16", "F.2.2; G.3; I.1.2; I.2.6"], "comment": "*AI was used to improve Text and collecting Citations", "summary": "This paper introduces Attribution Projection Calculus (AP-Calculus), a novel\nmathematical framework for determining causal relationships in structured\nBayesian networks. We investigate a specific network architecture with source\nnodes connected to destination nodes through intermediate nodes, where each\ninput maps to a single label with maximum marginal probability. We prove that\nfor each label, exactly one intermediate node acts as a deconfounder while\nothers serve as confounders, enabling optimal attribution of features to their\ncorresponding labels. The framework formalizes the dual nature of intermediate\nnodes as both confounders and deconfounders depending on the context, and\nestablishes separation functions that maximize distinctions between\nintermediate representations. We demonstrate that the proposed network\narchitecture is optimal for causal inference compared to alternative\nstructures, including those based on Pearl's causal framework. AP-Calculus\nprovides a comprehensive mathematical foundation for analyzing feature-label\nattributions, managing spurious correlations, quantifying information gain,\nensuring fairness, and evaluating uncertainty in prediction models, including\nlarge language models. Theoretical verification shows that AP-Calculus not only\nextends but can also subsume traditional do-calculus for many practical\napplications, offering a more direct approach to causal inference in supervised\nlearning contexts."}
{"id": "2505.12120", "pdf": "https://arxiv.org/pdf/2505.12120", "abs": "https://arxiv.org/abs/2505.12120", "authors": ["Dmitry Nechaev", "Alexey Pchelnikov", "Ekaterina Ivanova"], "title": "HISTAI: An Open-Source, Large-Scale Whole Slide Image Dataset for Computational Pathology", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Recent advancements in Digital Pathology (DP), particularly through\nartificial intelligence and Foundation Models, have underscored the importance\nof large-scale, diverse, and richly annotated datasets. Despite their critical\nrole, publicly available Whole Slide Image (WSI) datasets often lack sufficient\nscale, tissue diversity, and comprehensive clinical metadata, limiting the\nrobustness and generalizability of AI models. In response, we introduce the\nHISTAI dataset, a large, multimodal, open-access WSI collection comprising over\n60,000 slides from various tissue types. Each case in the HISTAI dataset is\naccompanied by extensive clinical metadata, including diagnosis, demographic\ninformation, detailed pathological annotations, and standardized diagnostic\ncoding. The dataset aims to fill gaps identified in existing resources,\npromoting innovation, reproducibility, and the development of clinically\nrelevant computational pathology solutions. The dataset can be accessed at\nhttps://github.com/HistAI/HISTAI."}
{"id": "2505.12269", "pdf": "https://arxiv.org/pdf/2505.12269", "abs": "https://arxiv.org/abs/2505.12269", "authors": ["Kerry Xiao", "Amy Zang"], "title": "Vague Knowledge: Evidence from Analyst Reports", "categories": ["econ.GN", "cs.AI", "cs.CL", "math.LO", "q-fin.EC", "q-fin.GN", "03B48, 03B65, 03E02, 03E15, 03E72, 18E45, 28A05, 62F15, 68T01,\n  68T35, 68T50, 91G30,", "F.4; I.2.3; I.2.4; I.2.7; J.1; J.4; J.5"], "comment": null, "summary": "People in the real world often possess vague knowledge of future payoffs, for\nwhich quantification is not feasible or desirable. We argue that language, with\ndiffering ability to convey vague information, plays an important but less\nknown-role in subjective expectations. Empirically, we find that in their\nreports, analysts include useful information in linguistic expressions but not\nnumerical forecasts. Specifically, the textual tone of analyst reports has\npredictive power for forecast errors and subsequent revisions in numerical\nforecasts, and this relation becomes stronger when analyst's language is\nvaguer, when uncertainty is higher, and when analysts are busier. Overall, our\ntheory and evidence suggest that some useful information is vaguely known and\nonly communicated through language."}
{"id": "2505.12096", "pdf": "https://arxiv.org/pdf/2505.12096", "abs": "https://arxiv.org/abs/2505.12096", "authors": ["Alberto Bassi", "Carlo Albert", "Aurelien Lucchi", "Marco Baity-Jesi", "Emanuele Francazi"], "title": "When the Left Foot Leads to the Right Path: Bridging Initial Prejudice and Trainability", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Understanding the statistical properties of deep neural networks (DNNs) at\ninitialization is crucial for elucidating both their trainability and the\nintrinsic architectural biases they encode prior to data exposure. Mean-field\n(MF) analyses have demonstrated that the parameter distribution in randomly\ninitialized networks dictates whether gradients vanish or explode.\nConcurrently, untrained DNNs were found to exhibit an initial-guessing bias\n(IGB), in which large regions of the input space are assigned to a single\nclass. In this work, we derive a theoretical proof establishing the\ncorrespondence between IGB and previous MF theories, thereby connecting a\nnetwork prejudice toward specific classes with the conditions for fast and\naccurate learning. This connection yields the counter-intuitive conclusion: the\ninitialization that optimizes trainability is necessarily biased, rather than\nneutral. Furthermore, we extend the MF/IGB framework to multi-node activation\nfunctions, offering practical guidelines for designing initialization schemes\nthat ensure stable optimization in architectures employing max- and\naverage-pooling layers."}
{"id": "2505.12203", "pdf": "https://arxiv.org/pdf/2505.12203", "abs": "https://arxiv.org/abs/2505.12203", "authors": ["Zhiting Zheng", "Shuqi Wu", "Wen Ding"], "title": "CTLformer: A Hybrid Denoising Model Combining Convolutional Layers and Self-Attention for Enhanced CT Image Reconstruction", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Low-dose CT (LDCT) images are often accompanied by significant noise, which\nnegatively impacts image quality and subsequent diagnostic accuracy. To address\nthe challenges of multi-scale feature fusion and diverse noise distribution\npatterns in LDCT denoising, this paper introduces an innovative model,\nCTLformer, which combines convolutional structures with transformer\narchitecture. Two key innovations are proposed: a multi-scale attention\nmechanism and a dynamic attention control mechanism. The multi-scale attention\nmechanism, implemented through the Token2Token mechanism and self-attention\ninteraction modules, effectively captures both fine details and global\nstructures at different scales, enhancing relevant features and suppressing\nnoise. The dynamic attention control mechanism adapts the attention\ndistribution based on the noise characteristics of the input image, focusing on\nhigh-noise regions while preserving details in low-noise areas, thereby\nenhancing robustness and improving denoising performance. Furthermore,\nCTLformer integrates convolutional layers for efficient feature extraction and\nuses overlapping inference to mitigate boundary artifacts, further\nstrengthening its denoising capability. Experimental results on the 2016\nNational Institutes of Health AAPM Mayo Clinic LDCT Challenge dataset\ndemonstrate that CTLformer significantly outperforms existing methods in both\ndenoising performance and model efficiency, greatly improving the quality of\nLDCT images. The proposed CTLformer not only provides an efficient solution for\nLDCT denoising but also shows broad potential in medical image analysis,\nespecially for clinical applications dealing with complex noise patterns."}
{"id": "2505.12284", "pdf": "https://arxiv.org/pdf/2505.12284", "abs": "https://arxiv.org/abs/2505.12284", "authors": ["Danlong Yuan", "Tian Xie", "Shaohan Huang", "Zhuocheng Gong", "Huishuai Zhang", "Chong Luo", "Furu Wei", "Dongyan Zhao"], "title": "Efficient RL Training for Reasoning Models via Length-Aware Optimization", "categories": ["cs.AI", "cs.CL"], "comment": "Under review", "summary": "Large reasoning models, such as OpenAI o1 or DeepSeek R1, have demonstrated\nremarkable performance on reasoning tasks but often incur a long reasoning path\nwith significant memory and time costs. Existing methods primarily aim to\nshorten reasoning paths by introducing additional training data and stages. In\nthis paper, we propose three critical reward designs integrated directly into\nthe reinforcement learning process of large reasoning models, which reduce the\nresponse length without extra training stages. Experiments on four settings\nshow that our method significantly decreases response length while maintaining\nor even improving performance. Specifically, in a logic reasoning setting, we\nachieve a 40% reduction in response length averaged by steps alongside a 14%\ngain in performance. For math problems, we reduce response length averaged by\nsteps by 33% while preserving performance."}
{"id": "2505.12100", "pdf": "https://arxiv.org/pdf/2505.12100", "abs": "https://arxiv.org/abs/2505.12100", "authors": ["Isabela Pereira Gregio", "Ian Pons", "Anna Helena Reali Costa", "Artur Jordão"], "title": "Improving Fairness in LLMs Through Testing-Time Adversaries", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) push the bound-aries in natural language\nprocessing and generative AI, driving progress across various aspects of modern\nsociety. Unfortunately, the pervasive issue of bias in LLMs responses (i.e.,\npredictions) poses a significant and open challenge, hindering their\napplication in tasks involving ethical sensitivity and responsible\ndecision-making. In this work, we propose a straightforward, user-friendly and\npractical method to mitigate such biases, enhancing the reliability and\ntrustworthiness of LLMs. Our method creates multiple variations of a given\nsentence by modifying specific attributes and evaluates the corresponding\nprediction behavior compared to the original, unaltered, prediction/sentence.\nThe idea behind this process is that critical ethical predictions often exhibit\nnotable inconsistencies, indicating the presence of bias. Unlike previous\napproaches, our method relies solely on forward passes (i.e., testing-time\nadversaries), eliminating the need for training, fine-tuning, or prior\nknowledge of the training data distribution. Through extensive experiments on\nthe popular Llama family, we demonstrate the effectiveness of our method in\nimproving various fairness metrics, focusing on the reduction of disparities in\nhow the model treats individuals from different racial groups. Specifically,\nusing standard metrics, we improve the fairness in Llama3 in up to 27\npercentage points. Overall, our approach significantly enhances fairness,\nequity, and reliability in LLM-generated results without parameter tuning or\ntraining data modifications, confirming its effectiveness in practical\nscenarios. We believe our work establishes an important step toward enabling\nthe use of LLMs in tasks that require ethical considerations and responsible\ndecision-making."}
{"id": "2505.12233", "pdf": "https://arxiv.org/pdf/2505.12233", "abs": "https://arxiv.org/abs/2505.12233", "authors": ["Yeonkyung Lee", "Woojung Han", "Youngjun Jun", "Hyeonmin Kim", "Jungkyung Cho", "Seong Jae Hwang"], "title": "PRETI: Patient-Aware Retinal Foundation Model via Metadata-Guided Representation Learning", "categories": ["eess.IV", "cs.CV"], "comment": "MICCAI2025 early accept", "summary": "Retinal foundation models have significantly advanced retinal image analysis\nby leveraging self-supervised learning to reduce dependence on labeled data\nwhile achieving strong generalization. Many recent approaches enhance retinal\nimage understanding using report supervision, but obtaining clinical reports is\noften costly and challenging. In contrast, metadata (e.g., age, gender) is\nwidely available and serves as a valuable resource for analyzing disease\nprogression. To effectively incorporate patient-specific information, we\npropose PRETI, a retinal foundation model that integrates metadata-aware\nlearning with robust self-supervised representation learning. We introduce\nLearnable Metadata Embedding (LME), which dynamically refines metadata\nrepresentations. Additionally, we construct patient-level data pairs,\nassociating images from the same individual to improve robustness against\nnon-clinical variations. To further optimize retinal image representation, we\npropose Retina-Aware Adaptive Masking (RAAM), a strategy that selectively\napplies masking within the retinal region and dynamically adjusts the masking\nratio during training. PRETI captures both global structures and fine-grained\npathological details, resulting in superior diagnostic performance. Extensive\nexperiments demonstrate that PRETI achieves state-of-the-art results across\ndiverse diseases and biomarker predictions using in-house and public data,\nindicating the importance of metadata-guided foundation models in retinal\ndisease analysis. Our code and pretrained model are available at\nhttps://github.com/MICV-yonsei/PRETI"}
{"id": "2505.12301", "pdf": "https://arxiv.org/pdf/2505.12301", "abs": "https://arxiv.org/abs/2505.12301", "authors": ["Luyu Chen", "Zeyu Zhang", "Haoran Tan", "Quanyu Dai", "Hao Yang", "Zhenhua Dong", "Xu Chen"], "title": "Beyond Single-Point Judgment: Distribution Alignment for LLM-as-a-Judge", "categories": ["cs.AI", "cs.CL"], "comment": "19 pages, 3 tables, 3 figures", "summary": "LLMs have emerged as powerful evaluators in the LLM-as-a-Judge paradigm,\noffering significant efficiency and flexibility compared to human judgments.\nHowever, previous methods primarily rely on single-point evaluations,\noverlooking the inherent diversity and uncertainty in human evaluations. This\napproach leads to information loss and decreases the reliability of\nevaluations. To address this limitation, we propose a novel training framework\nthat explicitly aligns the LLM-generated judgment distribution with empirical\nhuman distributions. Specifically, we propose a distributional alignment\nobjective based on KL divergence, combined with an auxiliary cross-entropy\nregularization to stabilize the training process. Furthermore, considering that\nempirical distributions may derive from limited human annotations, we\nincorporate adversarial training to enhance model robustness against\ndistribution perturbations. Extensive experiments across various LLM backbones\nand evaluation tasks demonstrate that our framework significantly outperforms\nexisting closed-source LLMs and conventional single-point alignment methods,\nwith improved alignment quality, evaluation accuracy, and robustness."}
{"id": "2505.12107", "pdf": "https://arxiv.org/pdf/2505.12107", "abs": "https://arxiv.org/abs/2505.12107", "authors": ["Rajarshi Roy", "Yash Pote", "David Parker", "Marta Kwiatkowska"], "title": "Learning Probabilistic Temporal Logic Specifications for Stochastic Systems", "categories": ["cs.LO", "cs.AI", "cs.FL"], "comment": "Full version of the paper that appears in IJCAI'25", "summary": "There has been substantial progress in the inference of formal behavioural\nspecifications from sample trajectories, for example, using Linear Temporal\nLogic (LTL). However, these techniques cannot handle specifications that\ncorrectly characterise systems with stochastic behaviour, which occur commonly\nin reinforcement learning and formal verification. We consider the passive\nlearning problem of inferring a Boolean combination of probabilistic LTL (PLTL)\nformulas from a set of Markov chains, classified as either positive or\nnegative. We propose a novel learning algorithm that infers concise PLTL\nspecifications, leveraging grammar-based enumeration, search heuristics,\nprobabilistic model checking and Boolean set-cover procedures. We demonstrate\nthe effectiveness of our algorithm in two use cases: learning from policies\ninduced by RL algorithms and learning from variants of a probabilistic model.\nIn both cases, our method automatically and efficiently extracts PLTL\nspecifications that succinctly characterise the temporal differences between\nthe policies or model variants."}
{"id": "2505.12261", "pdf": "https://arxiv.org/pdf/2505.12261", "abs": "https://arxiv.org/abs/2505.12261", "authors": ["Hanchen Wang", "Yixuan Wu", "Yinan Feng", "Peng Jin", "Shihang Feng", "Yiming Mao", "James Wiskin", "Baris Turkbey", "Peter A. Pinto", "Bradford J. Wood", "Songting Luo", "Yinpeng Chen", "Emad Boctor", "Youzuo Lin"], "title": "OpenPros: A Large-Scale Dataset for Limited View Prostate Ultrasound Computed Tomography", "categories": ["physics.med-ph", "cs.CV"], "comment": null, "summary": "Prostate cancer is one of the most common and lethal cancers among men,\nmaking its early detection critically important. Although ultrasound imaging\noffers greater accessibility and cost-effectiveness compared to MRI,\ntraditional transrectal ultrasound methods suffer from low sensitivity,\nespecially in detecting anteriorly located tumors. Ultrasound computed\ntomography provides quantitative tissue characterization, but its clinical\nimplementation faces significant challenges, particularly under anatomically\nconstrained limited-angle acquisition conditions specific to prostate imaging.\nTo address these unmet needs, we introduce OpenPros, the first large-scale\nbenchmark dataset explicitly developed for limited-view prostate USCT. Our\ndataset includes over 280,000 paired samples of realistic 2D speed-of-sound\n(SOS) phantoms and corresponding ultrasound full-waveform data, generated from\nanatomically accurate 3D digital prostate models derived from real clinical\nMRI/CT scans and ex vivo ultrasound measurements, annotated by medical experts.\nSimulations are conducted under clinically realistic configurations using\nadvanced finite-difference time-domain and Runge-Kutta acoustic wave solvers,\nboth provided as open-source components. Through comprehensive baseline\nexperiments, we demonstrate that state-of-the-art deep learning methods surpass\ntraditional physics-based approaches in both inference efficiency and\nreconstruction accuracy. Nevertheless, current deep learning models still fall\nshort of delivering clinically acceptable high-resolution images with\nsufficient accuracy. By publicly releasing OpenPros, we aim to encourage the\ndevelopment of advanced machine learning algorithms capable of bridging this\nperformance gap and producing clinically usable, high-resolution, and highly\naccurate prostate ultrasound images. The dataset is publicly accessible at\nhttps://open-pros.github.io/."}
{"id": "2505.12307", "pdf": "https://arxiv.org/pdf/2505.12307", "abs": "https://arxiv.org/abs/2505.12307", "authors": ["Maoyuan Ye", "Jing Zhang", "Juhua Liu", "Bo Du", "Dacheng Tao"], "title": "LogicOCR: Do Your Large Multimodal Models Excel at Logical Reasoning on Text-Rich Images?", "categories": ["cs.CV", "cs.CL"], "comment": "GitHub: \\url{https://github.com/MiliLab/LogicOCR}", "summary": "Recent advances in Large Multimodal Models (LMMs) have significantly improved\ntheir reasoning and Optical Character Recognition (OCR) capabilities. However,\ntheir performance on complex logical reasoning tasks involving text-rich images\nremains underexplored. To bridge this gap, we introduce LogicOCR, a benchmark\ncomprising 1,100 multiple-choice questions designed to evaluate LMMs' logical\nreasoning abilities on text-rich images, while minimizing reliance on\ndomain-specific knowledge (e.g., mathematics). We construct LogicOCR by\ncurating a text corpus from the Chinese National Civil Servant Examination and\ndevelop a scalable, automated pipeline to convert it into multimodal samples.\nFirst, we design prompt templates to steer GPT-Image-1 to generate images with\ndiverse backgrounds, interleaved text-illustration layouts, and varied fonts,\nensuring contextual relevance and visual realism. Then, the generated images\nare manually verified, with low-quality examples discarded. We evaluate a range\nof representative open-source and proprietary LMMs under both Chain-of-Thought\n(CoT) and direct-answer settings. Our multi-dimensional analysis reveals key\ninsights, such as the impact of test-time scaling, input modality differences,\nand sensitivity to visual-text orientation. Notably, LMMs still lag in\nmultimodal reasoning compared to text-only inputs, indicating that they have\nnot fully bridged visual reading with reasoning. We hope LogicOCR will serve as\na valuable resource for advancing multimodal reasoning research. The dataset is\navailable at https://github.com/MiliLab/LogicOCR."}
{"id": "2505.12108", "pdf": "https://arxiv.org/pdf/2505.12108", "abs": "https://arxiv.org/abs/2505.12108", "authors": ["Jiancheng Pan", "Shiye Lei", "Yuqian Fu", "Jiahao Li", "Yanxing Liu", "Yuze Sun", "Xiao He", "Long Peng", "Xiaomeng Huang", "Bo Zhao"], "title": "EarthSynth: Generating Informative Earth Observation with Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": "23 pages", "summary": "Remote sensing image (RSI) interpretation typically faces challenges due to\nthe scarcity of labeled data, which limits the performance of RSI\ninterpretation tasks. To tackle this challenge, we propose EarthSynth, a\ndiffusion-based generative foundation model that enables synthesizing\nmulti-category, cross-satellite labeled Earth observation for downstream RSI\ninterpretation tasks. To the best of our knowledge, EarthSynth is the first to\nexplore multi-task generation for remote sensing. EarthSynth, trained on the\nEarthSynth-180K dataset, employs the Counterfactual Composition training\nstrategy to improve training data diversity and enhance category control.\nFurthermore, a rule-based method of R-Filter is proposed to filter more\ninformative synthetic data for downstream tasks. We evaluate our EarthSynth on\nscene classification, object detection, and semantic segmentation in open-world\nscenarios, offering a practical solution for advancing RSI interpretation."}
{"id": "2505.12278", "pdf": "https://arxiv.org/pdf/2505.12278", "abs": "https://arxiv.org/abs/2505.12278", "authors": ["Zhengyi Luo", "Chen Tessler", "Toru Lin", "Ye Yuan", "Tairan He", "Wenli Xiao", "Yunrong Guo", "Gal Chechik", "Kris Kitani", "Linxi Fan", "Yuke Zhu"], "title": "Emergent Active Perception and Dexterity of Simulated Humanoids from Visual Reinforcement Learning", "categories": ["cs.RO", "cs.CV"], "comment": "Project page: https://zhengyiluo.github.io/PDC", "summary": "Human behavior is fundamentally shaped by visual perception -- our ability to\ninteract with the world depends on actively gathering relevant information and\nadapting our movements accordingly. Behaviors like searching for objects,\nreaching, and hand-eye coordination naturally emerge from the structure of our\nsensory system. Inspired by these principles, we introduce Perceptive Dexterous\nControl (PDC), a framework for vision-driven dexterous whole-body control with\nsimulated humanoids. PDC operates solely on egocentric vision for task\nspecification, enabling object search, target placement, and skill selection\nthrough visual cues, without relying on privileged state information (e.g., 3D\nobject positions and geometries). This perception-as-interface paradigm enables\nlearning a single policy to perform multiple household tasks, including\nreaching, grasping, placing, and articulated object manipulation. We also show\nthat training from scratch with reinforcement learning can produce emergent\nbehaviors such as active search. These results demonstrate how vision-driven\ncontrol and complex tasks induce human-like behaviors and can serve as the key\ningredients in closing the perception-action loop for animation, robotics, and\nembodied AI."}
{"id": "2505.12312", "pdf": "https://arxiv.org/pdf/2505.12312", "abs": "https://arxiv.org/abs/2505.12312", "authors": ["Qi Feng", "Hidetoshi Shimodaira"], "title": "Visuospatial Cognitive Assistant", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "comment": "31 pages, 10 figures, 6 tables. The implementation and fine-tuned\n  model (ViCA-7B) are publicly available at https://huggingface.co/nkkbr/ViCA.\n  The ViCA-322K dataset can be found at\n  https://huggingface.co/datasets/nkkbr/ViCA-322K, and the ViCA-Thinking-2.68K\n  dataset is at https://huggingface.co/datasets/nkkbr/ViCA-thinking-2.68k", "summary": "Video-based spatial cognition is vital for robotics and embodied AI but\nchallenges current Vision-Language Models (VLMs). This paper makes two key\ncontributions. First, we introduce ViCA (Visuospatial Cognitive\nAssistant)-322K, a diverse dataset of 322,003 QA pairs from real-world indoor\nvideos (ARKitScenes, ScanNet, ScanNet++), offering supervision for 3D\nmetadata-grounded queries and video-based complex reasoning. Second, we develop\nViCA-7B, fine-tuned on ViCA-322K, which achieves new state-of-the-art on all\neight VSI-Bench tasks, outperforming existing models, including larger ones\n(e.g., +26.1 on Absolute Distance). For interpretability, we present\nViCA-Thinking-2.68K, a dataset with explicit reasoning chains, and fine-tune\nViCA-7B to create ViCA-7B-Thinking, a model that articulates its spatial\nreasoning. Our work highlights the importance of targeted data and suggests\npaths for improved temporal-spatial modeling. We release all resources to\nfoster research in robust visuospatial intelligence."}
{"id": "2505.12109", "pdf": "https://arxiv.org/pdf/2505.12109", "abs": "https://arxiv.org/abs/2505.12109", "authors": ["Matthew Landers", "Taylor W. Killian", "Thomas Hartvigsen", "Afsaneh Doryab"], "title": "SAINT: Attention-Based Modeling of Sub-Action Dependencies in Multi-Action Policies", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The combinatorial structure of many real-world action spaces leads to\nexponential growth in the number of possible actions, limiting the\neffectiveness of conventional reinforcement learning algorithms. Recent\napproaches for combinatorial action spaces impose factorized or sequential\nstructures over sub-actions, failing to capture complex joint behavior. We\nintroduce the Sub-Action Interaction Network using Transformers (SAINT), a\nnovel policy architecture that represents multi-component actions as unordered\nsets and models their dependencies via self-attention conditioned on the global\nstate. SAINT is permutation-invariant, sample-efficient, and compatible with\nstandard policy optimization algorithms. In 15 distinct combinatorial\nenvironments across three task domains, including environments with nearly 17\nmillion joint actions, SAINT consistently outperforms strong baselines."}
{"id": "2505.12298", "pdf": "https://arxiv.org/pdf/2505.12298", "abs": "https://arxiv.org/abs/2505.12298", "authors": ["Amal Lahchim", "Lazar Davic"], "title": "Attention-Enhanced U-Net for Accurate Segmentation of COVID-19 Infected Lung Regions in CT Scans", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "14 pages, 9 figures, created using Google Colab and PyTorch. Compares\n  segmentation models for COVID-19 CT data", "summary": "In this study, we propose a robust methodology for automatic segmentation of\ninfected lung regions in COVID-19 CT scans using convolutional neural networks.\nThe approach is based on a modified U-Net architecture enhanced with attention\nmechanisms, data augmentation, and postprocessing techniques. It achieved a\nDice coefficient of 0.8658 and mean IoU of 0.8316, outperforming other methods.\nThe dataset was sourced from public repositories and augmented for diversity.\nResults demonstrate superior segmentation performance. Future work includes\nexpanding the dataset, exploring 3D segmentation, and preparing the model for\nclinical deployment."}
{"id": "2505.12363", "pdf": "https://arxiv.org/pdf/2505.12363", "abs": "https://arxiv.org/abs/2505.12363", "authors": ["Qi Feng", "Hidetoshi Shimodaira"], "title": "Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "comment": "26 pages, 19 figures, 4 tables. Code, models, and dataset are\n  available at our project page: https://github.com/nkkbr/ViCA", "summary": "While Multimodal Large Language Models (MLLMs) excel at general\nvision-language tasks, visuospatial cognition - reasoning about spatial\nlayouts, relations, and dynamics - remains a significant challenge. Existing\nmodels often lack the necessary architectural components and specialized\ntraining data for fine-grained spatial understanding. We introduce ViCA2\n(Visuospatial Cognitive Assistant 2), a novel MLLM designed to enhance spatial\nreasoning. ViCA2 features a dual vision encoder architecture integrating SigLIP\nfor semantics and Hiera for spatial structure, coupled with a token ratio\ncontrol mechanism for efficiency. We also developed ViCA-322K, a new\nlarge-scale dataset with over 322,000 spatially grounded question-answer pairs\nfor targeted instruction tuning. On the challenging VSI-Bench benchmark, our\nViCA2-7B model achieves a state-of-the-art average score of 56.8, significantly\nsurpassing larger open-source models (e.g., LLaVA-NeXT-Video-72B, 40.9) and\nleading proprietary models (Gemini-1.5 Pro, 45.4). This demonstrates the\neffectiveness of our approach in achieving strong visuospatial intelligence\nwith a compact model. We release ViCA2, its codebase, and the ViCA-322K dataset\nto facilitate further research."}
{"id": "2505.12130", "pdf": "https://arxiv.org/pdf/2505.12130", "abs": "https://arxiv.org/abs/2505.12130", "authors": ["Niaz Ahmad", "Jawad Khan", "Kang G. Shin", "Youngmoon Lee", "Guanghui Wang"], "title": "Keypoints as Dynamic Centroids for Unified Human Pose and Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The dynamic movement of the human body presents a fundamental challenge for\nhuman pose estimation and body segmentation. State-of-the-art approaches\nprimarily rely on combining keypoint heatmaps with segmentation masks but often\nstruggle in scenarios involving overlapping joints or rapidly changing poses\nduring instance-level segmentation. To address these limitations, we propose\nKeypoints as Dynamic Centroid (KDC), a new centroid-based representation for\nunified human pose estimation and instance-level segmentation. KDC adopts a\nbottom-up paradigm to generate keypoint heatmaps for both easily\ndistinguishable and complex keypoints and improves keypoint detection and\nconfidence scores by introducing KeyCentroids using a keypoint disk. It\nleverages high-confidence keypoints as dynamic centroids in the embedding space\nto generate MaskCentroids, allowing for swift clustering of pixels to specific\nhuman instances during rapid body movements in live environments. Our\nexperimental evaluations on the CrowdPose, OCHuman, and COCO benchmarks\ndemonstrate KDC's effectiveness and generalizability in challenging scenarios\nin terms of both accuracy and runtime performance. The implementation is\navailable at: https://sites.google.com/view/niazahmad/projects/kdc."}
{"id": "2505.12322", "pdf": "https://arxiv.org/pdf/2505.12322", "abs": "https://arxiv.org/abs/2505.12322", "authors": ["Ali Gholamzadeh", "Noor Sajid"], "title": "Model alignment using inter-modal bridges", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Foundation models have demonstrated remarkable performance across modalities\nsuch as language and vision. However, model reuse across distinct modalities\n(e.g., text and vision) remains limited due to the difficulty of aligning\ninternal representations. Existing methods require extensive paired training\ndata or are constrained to specific domains. We introduce a semi-supervised\napproach for model alignment via conditional flow matching. The conditional\nflow between latent spaces of different modalities (e.g., text-to-image or\nbiological-to-artificial neuronal activity) can be learned in two settings:\n($1$) solving a (balanced or unbalanced) optimal transport problem with an\ninter-space bridge cost, and ($2$) performing memory-efficient alignment using\nlabelled exemplars. Despite being constrained by the original models' capacity,\nour method--under both settings--matches downstream task performance of\nend-to-end trained models on object recognition and image generation tasks\nacross MNIST, ImageNet, and \\cite{majaj2015simple} datasets, particularly when\nlabelled training data is scarce ($<20\\%$). Our method provides a\ndata-efficient solution for inter-modal model alignment with minimal\nsupervision."}
{"id": "2505.12371", "pdf": "https://arxiv.org/pdf/2505.12371", "abs": "https://arxiv.org/abs/2505.12371", "authors": ["Yinghao Zhu", "Ziyi He", "Haoran Hu", "Xiaochen Zheng", "Xichen Zhang", "Zixiang Wang", "Junyi Gao", "Liantao Ma", "Lequan Yu"], "title": "MedAgentBoard: Benchmarking Multi-Agent Collaboration with Conventional Methods for Diverse Medical Tasks", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has stimulated interest\nin multi-agent collaboration for addressing complex medical tasks. However, the\npractical advantages of multi-agent collaboration approaches remain\ninsufficiently understood. Existing evaluations often lack generalizability,\nfailing to cover diverse tasks reflective of real-world clinical practice, and\nfrequently omit rigorous comparisons against both single-LLM-based and\nestablished conventional methods. To address this critical gap, we introduce\nMedAgentBoard, a comprehensive benchmark for the systematic evaluation of\nmulti-agent collaboration, single-LLM, and conventional approaches.\nMedAgentBoard encompasses four diverse medical task categories: (1) medical\n(visual) question answering, (2) lay summary generation, (3) structured\nElectronic Health Record (EHR) predictive modeling, and (4) clinical workflow\nautomation, across text, medical images, and structured EHR data. Our extensive\nexperiments reveal a nuanced landscape: while multi-agent collaboration\ndemonstrates benefits in specific scenarios, such as enhancing task\ncompleteness in clinical workflow automation, it does not consistently\noutperform advanced single LLMs (e.g., in textual medical QA) or, critically,\nspecialized conventional methods that generally maintain better performance in\ntasks like medical VQA and EHR-based prediction. MedAgentBoard offers a vital\nresource and actionable insights, emphasizing the necessity of a task-specific,\nevidence-based approach to selecting and developing AI solutions in medicine.\nIt underscores that the inherent complexity and overhead of multi-agent\ncollaboration must be carefully weighed against tangible performance gains. All\ncode, datasets, detailed prompts, and experimental results are open-sourced at\nhttps://medagentboard.netlify.app/."}
{"id": "2505.12143", "pdf": "https://arxiv.org/pdf/2505.12143", "abs": "https://arxiv.org/abs/2505.12143", "authors": ["Arun Kumar", "Paul Schrater"], "title": "Structured Representation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Invariant representations are core to representation learning, yet a central\nchallenge remains: uncovering invariants that are stable and transferable\nwithout suppressing task-relevant signals. This raises fundamental questions,\nrequiring further inquiry, about the appropriate level of abstraction at which\nsuch invariants should be defined, and which aspects of a system they should\ncharacterize. Interpretation of the environment relies on abstract knowledge\nstructures to make sense of the current state, which leads to interactions,\nessential drivers of learning and knowledge acquisition. We posit that\ninterpretation operates at the level of higher-order relational knowledge;\nhence, invariant structures must be where knowledge resides, specifically, as\npartitions defined by the closure of relational paths within an abstract\nknowledge space. These partitions serve as the core invariant representations,\nforming the structural substrate where knowledge is stored and learning occurs.\nOn the other hand, inter-partition connectors enable the deployment of these\nknowledge partitions encoding task-relevant transitions. Thus, invariant\npartitions provide the foundational primitives of structured representation. We\nformalize the computational foundations for structured representation of the\ninvariant partitions based on closed semiring, a relational algebraic\nstructure."}
{"id": "2505.12332", "pdf": "https://arxiv.org/pdf/2505.12332", "abs": "https://arxiv.org/abs/2505.12332", "authors": ["Qianyue Hu", "Junyan Wu", "Wei Lu", "Xiangyang Luo"], "title": "VoiceCloak: A Multi-Dimensional Defense Framework against Unauthorized Diffusion-based Voice Cloning", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.MM", "eess.AS"], "comment": null, "summary": "Diffusion Models (DMs) have achieved remarkable success in realistic voice\ncloning (VC), while they also increase the risk of malicious misuse. Existing\nproactive defenses designed for traditional VC models aim to disrupt the\nforgery process, but they have been proven incompatible with DMs due to the\nintricate generative mechanisms of diffusion. To bridge this gap, we introduce\nVoiceCloak, a multi-dimensional proactive defense framework with the goal of\nobfuscating speaker identity and degrading perceptual quality in potential\nunauthorized VC. To achieve these goals, we conduct a focused analysis to\nidentify specific vulnerabilities within DMs, allowing VoiceCloak to disrupt\nthe cloning process by introducing adversarial perturbations into the reference\naudio. Specifically, to obfuscate speaker identity, VoiceCloak first targets\nspeaker identity by distorting representation learning embeddings to maximize\nidentity variation, which is guided by auditory perception principles.\nAdditionally, VoiceCloak disrupts crucial conditional guidance processes,\nparticularly attention context, thereby preventing the alignment of vocal\ncharacteristics that are essential for achieving convincing cloning. Then, to\naddress the second objective, VoiceCloak introduces score magnitude\namplification to actively steer the reverse trajectory away from the generation\nof high-quality speech. Noise-guided semantic corruption is further employed to\ndisrupt structural speech semantics captured by DMs, degrading output quality.\nExtensive experiments highlight VoiceCloak's outstanding defense success rate\nagainst unauthorized diffusion-based voice cloning. Audio samples of VoiceCloak\nare available at https://voice-cloak.github.io/VoiceCloak/."}
{"id": "2505.12442", "pdf": "https://arxiv.org/pdf/2505.12442", "abs": "https://arxiv.org/abs/2505.12442", "authors": ["Liwen Wang", "Wenxuan Wang", "Shuai Wang", "Zongjie Li", "Zhenlan Ji", "Zongyi Lyu", "Daoyuan Wu", "Shing-Chi Cheung"], "title": "IP Leakage Attacks Targeting LLM-Based Multi-Agent Systems", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has led to the\nemergence of Multi-Agent Systems (MAS) to perform complex tasks through\ncollaboration. However, the intricate nature of MAS, including their\narchitecture and agent interactions, raises significant concerns regarding\nintellectual property (IP) protection. In this paper, we introduce MASLEAK, a\nnovel attack framework designed to extract sensitive information from MAS\napplications. MASLEAK targets a practical, black-box setting, where the\nadversary has no prior knowledge of the MAS architecture or agent\nconfigurations. The adversary can only interact with the MAS through its public\nAPI, submitting attack query $q$ and observing outputs from the final agent.\nInspired by how computer worms propagate and infect vulnerable network hosts,\nMASLEAK carefully crafts adversarial query $q$ to elicit, propagate, and retain\nresponses from each MAS agent that reveal a full set of proprietary components,\nincluding the number of agents, system topology, system prompts, task\ninstructions, and tool usages. We construct the first synthetic dataset of MAS\napplications with 810 applications and also evaluate MASLEAK against real-world\nMAS applications, including Coze and CrewAI. MASLEAK achieves high accuracy in\nextracting MAS IP, with an average attack success rate of 87% for system\nprompts and task instructions, and 92% for system architecture in most cases.\nWe conclude by discussing the implications of our findings and the potential\ndefenses."}
{"id": "2505.12151", "pdf": "https://arxiv.org/pdf/2505.12151", "abs": "https://arxiv.org/abs/2505.12151", "authors": ["Alex Heyman", "Joel Zylberberg"], "title": "Reasoning Large Language Model Errors Arise from Hallucinating Critical Problem Features", "categories": ["cs.LG", "cs.AI", "I.2.6; I.2.7"], "comment": "13 pages (9 excluding references and appendices); 7 figures (6\n  excluding appendices)", "summary": "Large language models have recently made great strides in reasoning task\nperformance through chain-of-thought (CoT) strategies trained via reinforcement\nlearning; however, these \"reasoning large language models\" (RLLMs) remain\nimperfect reasoners, and understanding the frequencies and causes of their\nfailure modes is important for both users and developers. We test o1-mini,\no3-mini, DeepSeek-R1, Claude 3.7 Sonnet, Gemini 2.5 Pro Preview, and Grok 3\nMini Beta on graph coloring as a variable-complexity constraint-satisfaction\nlogic problem, and find evidence from both error rate comparisons and\nCoT/explanation text analysis that RLLMs are prone to hallucinate edges not\nspecified in the prompt's description of the graph. This phenomenon persists\nacross multiple problem complexity levels and semantic frames, and it appears\nto account for a significant fraction of the incorrect answers from every\ntested model, and the vast majority of them for some models. Our results\nindicate that RLLMs may possess broader issues with misrepresentation of\nproblem specifics, and we offer suggestions for design choices to mitigate this\nweakness."}
{"id": "2505.12337", "pdf": "https://arxiv.org/pdf/2505.12337", "abs": "https://arxiv.org/abs/2505.12337", "authors": ["Junlin Song", "Miguel Olivares-Mendez"], "title": "Structureless VIO", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Visual odometry (VO) is typically considered as a chicken-and-egg problem, as\nthe localization and mapping modules are tightly-coupled. The estimation of\nvisual map relies on accurate localization information. Meanwhile, localization\nrequires precise map points to provide motion constraints. This classical\ndesign principle is naturally inherited by visual-inertial odometry (VIO).\nEfficient localization solution that does not require a map has not been fully\ninvestigated. To this end, we propose a novel structureless VIO, where the\nvisual map is removed from the odometry framework. Experimental results\ndemonstrated that, compared to the structure-based VIO baseline, our\nstructureless VIO not only substantially improves computational efficiency but\nalso has advantages in accuracy."}
{"id": "2505.12457", "pdf": "https://arxiv.org/pdf/2505.12457", "abs": "https://arxiv.org/abs/2505.12457", "authors": ["Yang Zhao", "Kai Xiong", "Xiao Ding", "Li Du", "YangouOuyang", "Zhouhao Sun", "Jiannan Guan", "Wenbin Zhang", "Bin Liu", "Dong Hu", "Bing Qin", "Ting Liu"], "title": "UFO-RL: Uncertainty-Focused Optimization for Efficient Reinforcement Learning Data Selection", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Scaling RL for LLMs is computationally expensive, largely due to\nmulti-sampling for policy optimization and evaluation, making efficient data\nselection crucial. Inspired by the Zone of Proximal Development (ZPD) theory,\nwe hypothesize LLMs learn best from data within their potential comprehension\nzone. Addressing the limitation of conventional, computationally intensive\nmulti-sampling methods for data assessment, we introduce UFO-RL. This novel\nframework uses a computationally efficient single-pass uncertainty estimation\nto identify informative data instances, achieving up to 185x faster data\nevaluation. UFO-RL leverages this metric to select data within the estimated\nZPD for training. Experiments show that training with just 10% of data selected\nby UFO-RL yields performance comparable to or surpassing full-data training,\nreducing overall training time by up to 16x while enhancing stability and\ngeneralization. UFO-RL offers a practical and highly efficient strategy for\nscaling RL fine-tuning of LLMs by focusing learning on valuable data."}
{"id": "2505.12155", "pdf": "https://arxiv.org/pdf/2505.12155", "abs": "https://arxiv.org/abs/2505.12155", "authors": ["Ranit Karmakar", "Simon F. Nørrelykke"], "title": "SoftPQ: Robust Instance Segmentation Evaluation via Soft Matching and Tunable Thresholds", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Segmentation evaluation metrics traditionally rely on binary decision logic:\npredictions are either correct or incorrect, based on rigid IoU thresholds.\nDetection--based metrics such as F1 and mAP determine correctness at the object\nlevel using fixed overlap cutoffs, while overlap--based metrics like\nIntersection over Union (IoU) and Dice operate at the pixel level, often\noverlooking instance--level structure. Panoptic Quality (PQ) attempts to unify\ndetection and segmentation assessment, but it remains dependent on\nhard-threshold matching--treating predictions below the threshold as entirely\nincorrect. This binary framing obscures important distinctions between\nqualitatively different errors and fails to reward gradual model improvements.\nWe propose SoftPQ, a flexible and interpretable instance segmentation metric\nthat redefines evaluation as a graded continuum rather than a binary\nclassification. SoftPQ introduces tunable upper and lower IoU thresholds to\ndefine a partial matching region and applies a sublinear penalty function to\nambiguous or fragmented predictions. These extensions allow SoftPQ to exhibit\nsmoother score behavior, greater robustness to structural segmentation errors,\nand more informative feedback for model development and evaluation. Through\ncontrolled perturbation experiments, we show that SoftPQ captures meaningful\ndifferences in segmentation quality that existing metrics overlook, making it a\npractical and principled alternative for both benchmarking and iterative model\nrefinement."}
{"id": "2505.12343", "pdf": "https://arxiv.org/pdf/2505.12343", "abs": "https://arxiv.org/abs/2505.12343", "authors": ["Kai Tang", "Jinhao You", "Xiuqi Ge", "Hanze Li", "Yichen Guo", "Xiande Huang"], "title": "Mitigating Hallucinations via Inter-Layer Consistency Aggregation in Large Vision-Language Models", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Despite the impressive capabilities of Large Vision-Language Models (LVLMs),\nthey remain susceptible to hallucinations-generating content that is\ninconsistent with the input image. Existing training-free hallucination\nmitigation methods often suffer from unstable performance and high sensitivity\nto hyperparameter settings, limiting their practicality and broader adoption.\nIn this paper, we propose a novel decoding mechanism, Decoding with Inter-layer\nConsistency via Layer Aggregation (DCLA), which requires no retraining,\nfine-tuning, or access to external knowledge bases. Specifically, our approach\nconstructs a dynamic semantic reference by aggregating representations from\nprevious layers, and corrects semantically deviated layers to enforce\ninter-layer consistency. The method allows DCLA to robustly mitigate\nhallucinations across multiple LVLMs. Experiments on hallucination benchmarks\nsuch as MME and POPE demonstrate that DCLA effectively reduces hallucinations\nwhile enhancing the reliability and performance of LVLMs."}
{"id": "2505.12565", "pdf": "https://arxiv.org/pdf/2505.12565", "abs": "https://arxiv.org/abs/2505.12565", "authors": ["Carl Edwards", "Chi Han", "Gawon Lee", "Thao Nguyen", "Bowen Jin", "Chetan Kumar Prasad", "Sara Szymkuć", "Bartosz A. Grzybowski", "Ying Diao", "Jiawei Han", "Ge Liu", "Hao Peng", "Martin D. Burke", "Heng Ji"], "title": "mCLM: A Function-Infused and Synthesis-Friendly Modular Chemical Language Model", "categories": ["cs.AI", "cs.CL", "cs.LG", "q-bio.QM"], "comment": null, "summary": "Despite their ability to understand chemical knowledge and accurately\ngenerate sequential representations, large language models (LLMs) remain\nlimited in their capacity to propose novel molecules with drug-like properties.\nIn addition, the molecules that LLMs propose can often be challenging to make\nin the lab. To more effectively enable the discovery of functional small\nmolecules, LLMs need to learn a molecular language. However, LLMs are currently\nlimited by encoding molecules from atoms. In this paper, we argue that just\nlike tokenizing texts into (sub-)word tokens instead of characters, molecules\nshould be decomposed and reassembled at the level of functional building\nblocks, i.e., parts of molecules that bring unique functions and serve as\neffective building blocks for real-world automated laboratory synthesis. This\nmotivates us to propose mCLM, a modular Chemical-Language Model tokenizing\nmolecules into building blocks and learning a bilingual language model of both\nnatural language descriptions of functions and molecule building blocks. By\nreasoning on such functional building blocks, mCLM guarantees to generate\nefficiently synthesizable molecules thanks to recent progress in block-based\nchemistry, while also improving the functions of molecules in a principled\nmanner. In experiments on 430 FDA-approved drugs, we find mCLM capable of\nsignificantly improving 5 out of 6 chemical functions critical to determining\ndrug potentials. More importantly, mCLM can reason on multiple functions and\nimprove the FDA-rejected drugs (``fallen angels'') over multiple iterations to\ngreatly improve their shortcomings."}
{"id": "2505.12183", "pdf": "https://arxiv.org/pdf/2505.12183", "abs": "https://arxiv.org/abs/2505.12183", "authors": ["Manari Hirose", "Masato Uchida"], "title": "Decoding the Mind of Large Language Models: A Quantitative Evaluation of Ideology and Biases", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": "23 pages, 5 figures, 17 tables", "summary": "The widespread integration of Large Language Models (LLMs) across various\nsectors has highlighted the need for empirical research to understand their\nbiases, thought patterns, and societal implications to ensure ethical and\neffective use. In this study, we propose a novel framework for evaluating LLMs,\nfocusing on uncovering their ideological biases through a quantitative analysis\nof 436 binary-choice questions, many of which have no definitive answer. By\napplying our framework to ChatGPT and Gemini, findings revealed that while LLMs\ngenerally maintain consistent opinions on many topics, their ideologies differ\nacross models and languages. Notably, ChatGPT exhibits a tendency to change\ntheir opinion to match the questioner's opinion. Both models also exhibited\nproblematic biases, unethical or unfair claims, which might have negative\nsocietal impacts. These results underscore the importance of addressing both\nideological and ethical considerations when evaluating LLMs. The proposed\nframework offers a flexible, quantitative method for assessing LLM behavior,\nproviding valuable insights for the development of more socially aligned AI\nsystems."}
{"id": "2505.12359", "pdf": "https://arxiv.org/pdf/2505.12359", "abs": "https://arxiv.org/abs/2505.12359", "authors": ["Yichen Guo", "Hanze Li", "Zonghao Zhang", "Jinhao You", "Kai Tang", "Xiande Huang"], "title": "STAR: Stage-Wise Attention-Guided Token Reduction for Efficient Large Vision-Language Models Inference", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Although large vision-language models (LVLMs) leverage rich visual token\nrepresentations to achieve strong performance on multimodal tasks, these tokens\nalso introduce significant computational overhead during inference. Existing\ntraining-free token pruning methods typically adopt a single-stage strategy,\nfocusing either on visual self-attention or visual-textual cross-attention.\nHowever, such localized perspectives often overlook the broader information\nflow across the model, leading to substantial performance degradation,\nespecially under high pruning ratios. In this work, we propose STAR (Stage-wise\nAttention-guided token Reduction), a training-free, plug-and-play framework\nthat approaches token pruning from a global perspective. Instead of pruning at\na single point, STAR performs attention-guided reduction in two complementary\nstages: an early-stage pruning based on visual self-attention to remove\nredundant low-level features, and a later-stage pruning guided by cross-modal\nattention to discard task-irrelevant tokens. This holistic approach allows STAR\nto significantly reduce computational cost while better preserving\ntask-critical information. Extensive experiments across multiple LVLM\narchitectures and benchmarks show that STAR achieves strong acceleration while\nmaintaining comparable, and in some cases even improved performance."}
{"id": "2505.12629", "pdf": "https://arxiv.org/pdf/2505.12629", "abs": "https://arxiv.org/abs/2505.12629", "authors": ["Yuchang Sun", "Yanxi Chen", "Yaliang Li", "Bolin Ding"], "title": "Enhancing Latent Computation in Transformers with Latent Tokens", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Augmenting large language models (LLMs) with auxiliary tokens has emerged as\na promising strategy for enhancing model performance. In this work, we\nintroduce a lightweight method termed latent tokens; these are dummy tokens\nthat may be non-interpretable in natural language but steer the autoregressive\ndecoding process of a Transformer-based LLM via the attention mechanism. The\nproposed latent tokens can be seamlessly integrated with a pre-trained\nTransformer, trained in a parameter-efficient manner, and applied flexibly at\ninference time, while adding minimal complexity overhead to the existing\ninfrastructure of standard Transformers. We propose several hypotheses about\nthe underlying mechanisms of latent tokens and design synthetic tasks\naccordingly to verify them. Numerical results confirm that the proposed method\nnoticeably outperforms the baselines, particularly in the out-of-distribution\ngeneralization scenarios, highlighting its potential in improving the\nadaptability of LLMs."}
{"id": "2505.12186", "pdf": "https://arxiv.org/pdf/2505.12186", "abs": "https://arxiv.org/abs/2505.12186", "authors": ["Yuhui Wang", "Rongyi Zhu", "Ting Wang"], "title": "Self-Destructive Language Model", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Harmful fine-tuning attacks pose a major threat to the security of large\nlanguage models (LLMs), allowing adversaries to compromise safety guardrails\nwith minimal harmful data. While existing defenses attempt to reinforce LLM\nalignment, they fail to address models' inherent \"trainability\" on harmful\ndata, leaving them vulnerable to stronger attacks with increased learning rates\nor larger harmful datasets. To overcome this critical limitation, we introduce\nSEAM, a novel alignment-enhancing defense that transforms LLMs into\nself-destructive models with intrinsic resilience to misalignment attempts.\nSpecifically, these models retain their capabilities for legitimate tasks while\nexhibiting substantial performance degradation when fine-tuned on harmful data.\nThe protection is achieved through a novel loss function that couples the\noptimization trajectories of benign and harmful data, enhanced with adversarial\ngradient ascent to amplify the self-destructive effect. To enable practical\ntraining, we develop an efficient Hessian-free gradient estimate with\ntheoretical error bounds. Extensive evaluation across LLMs and datasets\ndemonstrates that SEAM creates a no-win situation for adversaries: the\nself-destructive models achieve state-of-the-art robustness against\nlow-intensity attacks and undergo catastrophic performance collapse under\nhigh-intensity attacks, rendering them effectively unusable. (warning: this\npaper contains potentially harmful content generated by LLMs.)"}
{"id": "2505.12373", "pdf": "https://arxiv.org/pdf/2505.12373", "abs": "https://arxiv.org/abs/2505.12373", "authors": ["Kapil Dev"], "title": "Modeling Aesthetic Preferences in 3D Shapes: A Large-Scale Paired Comparison Study Across Object Categories", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "11 pages, 8 figures, submitted to IEEE Transactions on Visualization\n  and Computer Graphics (TVCG)", "summary": "Human aesthetic preferences for 3D shapes are central to industrial design,\nvirtual reality, and consumer product development. However, most computational\nmodels of 3D aesthetics lack empirical grounding in large-scale human\njudgments, limiting their practical relevance. We present a large-scale study\nof human preferences. We collected 22,301 pairwise comparisons across five\nobject categories (chairs, tables, mugs, lamps, and dining chairs) via Amazon\nMechanical Turk. Building on a previously published\ndataset~\\cite{dev2020learning}, we introduce new non-linear modeling and\ncross-category analysis to uncover the geometric drivers of aesthetic\npreference. We apply the Bradley-Terry model to infer latent aesthetic scores\nand use Random Forests with SHAP analysis to identify and interpret the most\ninfluential geometric features (e.g., symmetry, curvature, compactness). Our\ncross-category analysis reveals both universal principles and domain-specific\ntrends in aesthetic preferences. We focus on human interpretable geometric\nfeatures to ensure model transparency and actionable design insights, rather\nthan relying on black-box deep learning approaches. Our findings bridge\ncomputational aesthetics and cognitive science, providing practical guidance\nfor designers and a publicly available dataset to support reproducibility. This\nwork advances the understanding of 3D shape aesthetics through a human-centric,\ndata-driven framework."}
{"id": "2505.12632", "pdf": "https://arxiv.org/pdf/2505.12632", "abs": "https://arxiv.org/abs/2505.12632", "authors": ["Yunseok Jang", "Yeda Song", "Sungryull Sohn", "Lajanugen Logeswaran", "Tiange Luo", "Dong-Ki Kim", "Kyunghoon Bae", "Honglak Lee"], "title": "Scalable Video-to-Dataset Generation for Cross-Platform Mobile Agents", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "CVPR 2025", "summary": "Recent advancements in Large Language Models (LLMs) and Vision-Language\nModels (VLMs) have sparked significant interest in developing GUI visual\nagents. We introduce MONDAY (Mobile OS Navigation Task Dataset for Agents from\nYouTube), a large-scale dataset of 313K annotated frames from 20K instructional\nvideos capturing diverse real-world mobile OS navigation across multiple\nplatforms. Models that include MONDAY in their pre-training phases demonstrate\nrobust cross-platform generalization capabilities, consistently outperforming\nmodels trained on existing single OS datasets while achieving an average\nperformance gain of 18.11%p on an unseen mobile OS platform. To enable\ncontinuous dataset expansion as mobile platforms evolve, we present an\nautomated framework that leverages publicly available video content to create\ncomprehensive task datasets without manual annotation. Our framework comprises\nrobust OCR-based scene detection (95.04% F1score), near-perfect UI element\ndetection (99.87% hit ratio), and novel multi-step action identification to\nextract reliable action sequences across diverse interface configurations. We\ncontribute both the MONDAY dataset and our automated collection framework to\nfacilitate future research in mobile OS navigation."}
{"id": "2505.12188", "pdf": "https://arxiv.org/pdf/2505.12188", "abs": "https://arxiv.org/abs/2505.12188", "authors": ["Hanyu Wang", "Xinrui Wu", "Zijian Ding", "Su Zheng", "Chengyue Wang", "Tony Nowatzki", "Yizhou Sun", "Jason Cong"], "title": "LLM-DSE: Searching Accelerator Parameters with LLM Agents", "categories": ["cs.AR", "cs.AI"], "comment": null, "summary": "Even though high-level synthesis (HLS) tools mitigate the challenges of\nprogramming domain-specific accelerators (DSAs) by raising the abstraction\nlevel, optimizing hardware directive parameters remains a significant hurdle.\nExisting heuristic and learning-based methods struggle with adaptability and\nsample efficiency.We present LLM-DSE, a multi-agent framework designed\nspecifically for optimizing HLS directives. Combining LLM with design space\nexploration (DSE), our explorer coordinates four agents: Router, Specialists,\nArbitrator, and Critic. These multi-agent components interact with various\ntools to accelerate the optimization process. LLM-DSE leverages essential\ndomain knowledge to identify efficient parameter combinations while maintaining\nadaptability through verbal learning from online interactions. Evaluations on\nthe HLSyn dataset demonstrate that LLM-DSE achieves substantial $2.55\\times$\nperformance gains over state-of-the-art methods, uncovering novel designs while\nreducing runtime. Ablation studies validate the effectiveness and necessity of\nthe proposed agent interactions. Our code is open-sourced here:\nhttps://github.com/Nozidoali/LLM-DSE."}
{"id": "2505.12418", "pdf": "https://arxiv.org/pdf/2505.12418", "abs": "https://arxiv.org/abs/2505.12418", "authors": ["Yuanpeng He", "Yali Bi", "Lijian Li", "Chi-Man Pun", "Wenpin Jiao", "Zhi Jin"], "title": "Mutual Evidential Deep Learning for Medical Image Segmentation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Existing semi-supervised medical segmentation co-learning frameworks have\nrealized that model performance can be diminished by the biases in model\nrecognition caused by low-quality pseudo-labels. Due to the averaging nature of\ntheir pseudo-label integration strategy, they fail to explore the reliability\nof pseudo-labels from different sources. In this paper, we propose a mutual\nevidential deep learning (MEDL) framework that offers a potentially viable\nsolution for pseudo-label generation in semi-supervised learning from two\nperspectives. First, we introduce networks with different architectures to\ngenerate complementary evidence for unlabeled samples and adopt an improved\nclass-aware evidential fusion to guide the confident synthesis of evidential\npredictions sourced from diverse architectural networks. Second, utilizing the\nuncertainty in the fused evidence, we design an asymptotic Fisher\ninformation-based evidential learning strategy. This strategy enables the model\nto initially focus on unlabeled samples with more reliable pseudo-labels,\ngradually shifting attention to samples with lower-quality pseudo-labels while\navoiding over-penalization of mislabeled classes in high data uncertainty\nsamples. Additionally, for labeled data, we continue to adopt an\nuncertainty-driven asymptotic learning strategy, gradually guiding the model to\nfocus on challenging voxels. Extensive experiments on five mainstream datasets\nhave demonstrated that MEDL achieves state-of-the-art performance."}
{"id": "2505.12680", "pdf": "https://arxiv.org/pdf/2505.12680", "abs": "https://arxiv.org/abs/2505.12680", "authors": ["Haoyu Zhao", "Yihan Geng", "Shange Tang", "Yong Lin", "Bohan Lyu", "Hongzhou Lin", "Chi Jin", "Sanjeev Arora"], "title": "Ineq-Comp: Benchmarking Human-Intuitive Compositional Reasoning in Automated Theorem Proving on Inequalities", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "27 pages", "summary": "LLM-based formal proof assistants (e.g., in Lean) hold great promise for\nautomating mathematical discovery. But beyond syntactic correctness, do these\nsystems truly understand mathematical structure as humans do? We investigate\nthis question through the lens of mathematical inequalities -- a fundamental\ntool across many domains. While modern provers can solve basic inequalities, we\nprobe their ability to handle human-intuitive compositionality. We introduce\nIneq-Comp, a benchmark built from elementary inequalities through systematic\ntransformations, including variable duplication, algebraic rewriting, and\nmulti-step composition. Although these problems remain easy for humans, we find\nthat most provers -- including Goedel, STP, and Kimina-7B -- struggle\nsignificantly. DeepSeek-Prover-V2-7B shows relative robustness -- possibly\nbecause it is trained to decompose the problems into sub-problems -- but still\nsuffers a 20\\% performance drop (pass@32). Strikingly, performance remains poor\nfor all models even when formal proofs of the constituent parts are provided in\ncontext, revealing that the source of weakness is indeed in compositional\nreasoning. Our results expose a persisting gap between the generalization\nbehavior of current AI provers and human mathematical intuition."}
{"id": "2505.12191", "pdf": "https://arxiv.org/pdf/2505.12191", "abs": "https://arxiv.org/abs/2505.12191", "authors": ["Wenquan Lu", "Jiaqi Zhang", "Hugues Van Assel", "Randall Balestriero"], "title": "Ditch the Denoiser: Emergence of Noise Robustness in Self-Supervised Learning from Data Curriculum", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Self-Supervised Learning (SSL) has become a powerful solution to extract rich\nrepresentations from unlabeled data. Yet, SSL research is mostly focused on\nclean, curated and high-quality datasets. As a result, applying SSL on noisy\ndata remains a challenge, despite being crucial to applications such as\nastrophysics, medical imaging, geophysics or finance. In this work, we present\na fully self-supervised framework that enables noise-robust representation\nlearning without requiring a denoiser at inference or downstream fine-tuning.\nOur method first trains an SSL denoiser on noisy data, then uses it to\nconstruct a denoised-to-noisy data curriculum (i.e., training first on\ndenoised, then noisy samples) for pretraining a SSL backbone (e.g., DINOv2),\ncombined with a teacher-guided regularization that anchors noisy embeddings to\ntheir denoised counterparts. This process encourages the model to internalize\nnoise robustness. Notably, the denoiser can be discarded after pretraining,\nsimplifying deployment. On ImageNet-1k with ViT-B under extreme Gaussian noise\n($\\sigma=255$, SNR = 0.72 dB), our method improves linear probing accuracy by\n4.8% over DINOv2, demonstrating that denoiser-free robustness can emerge from\nnoise-aware pretraining. The code is available at\nhttps://github.com/wenquanlu/noisy_dinov2."}
{"id": "2505.12432", "pdf": "https://arxiv.org/pdf/2505.12432", "abs": "https://arxiv.org/abs/2505.12432", "authors": ["Zirun Guo", "Minjie Hong", "Tao Jin"], "title": "Observe-R1: Unlocking Reasoning Abilities of MLLMs with Dynamic Progressive Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Reinforcement Learning (RL) has shown promise in improving the reasoning\nabilities of Large Language Models (LLMs). However, the specific challenges of\nadapting RL to multimodal data and formats remain relatively unexplored. In\nthis work, we present Observe-R1, a novel framework aimed at enhancing the\nreasoning capabilities of multimodal large language models (MLLMs). We draw\ninspirations from human learning progression--from simple to complex and easy\nto difficult, and propose a gradual learning paradigm for MLLMs. To this end,\nwe construct the NeuraLadder dataset, which is organized and sampled according\nto the difficulty and complexity of data samples for RL training. To tackle\nmultimodal tasks, we introduce a multimodal format constraint that encourages\ncareful observation of images, resulting in enhanced visual abilities and\nclearer and more structured responses. Additionally, we implement a bonus\nreward system that favors concise, correct answers within a length constraint,\nalongside a dynamic weighting mechanism that prioritizes uncertain and\nmedium-difficulty problems, ensuring that more informative samples have a\ngreater impact on training. Our experiments with the Qwen2.5-VL-3B and\nQwen2.5-VL-7B models on 20k samples from the NeuraLadder dataset show that\nObserve-R1 outperforms a series of larger reasoning models on both reasoning\nand general benchmarks, achieving superior clarity and conciseness in reasoning\nchains. Ablation studies validate the effectiveness of our strategies,\nhighlighting the robustness and generalization of our approach. The dataset and\ncode will be released at https://github.com/zrguo/Observe-R1."}
{"id": "2505.12692", "pdf": "https://arxiv.org/pdf/2505.12692", "abs": "https://arxiv.org/abs/2505.12692", "authors": ["Ziwei Xu", "Udit Sanghi", "Mohan Kankanhalli"], "title": "Bullying the Machine: How Personas Increase LLM Vulnerability", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in interactions where\nthey are prompted to adopt personas. This paper investigates whether such\npersona conditioning affects model safety under bullying, an adversarial\nmanipulation that applies psychological pressures in order to force the victim\nto comply to the attacker. We introduce a simulation framework in which an\nattacker LLM engages a victim LLM using psychologically grounded bullying\ntactics, while the victim adopts personas aligned with the Big Five personality\ntraits. Experiments using multiple open-source LLMs and a wide range of\nadversarial goals reveal that certain persona configurations -- such as\nweakened agreeableness or conscientiousness -- significantly increase victim's\nsusceptibility to unsafe outputs. Bullying tactics involving emotional or\nsarcastic manipulation, such as gaslighting and ridicule, are particularly\neffective. These findings suggest that persona-driven interaction introduces a\nnovel vector for safety risks in LLMs and highlight the need for persona-aware\nsafety evaluation and alignment strategies."}
{"id": "2505.12199", "pdf": "https://arxiv.org/pdf/2505.12199", "abs": "https://arxiv.org/abs/2505.12199", "authors": ["Kui Jiang", "Jing Cao", "Zhaocheng Yu", "Junjun Jiang", "Jingchun Zhou"], "title": "Always Clear Depth: Robust Monocular Depth Estimation under Adverse Weather", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Monocular depth estimation is critical for applications such as autonomous\ndriving and scene reconstruction. While existing methods perform well under\nnormal scenarios, their performance declines in adverse weather, due to\nchallenging domain shifts and difficulties in extracting scene information. To\naddress this issue, we present a robust monocular depth estimation method\ncalled \\textbf{ACDepth} from the perspective of high-quality training data\ngeneration and domain adaptation. Specifically, we introduce a one-step\ndiffusion model for generating samples that simulate adverse weather\nconditions, constructing a multi-tuple degradation dataset during training. To\nensure the quality of the generated degradation samples, we employ LoRA\nadapters to fine-tune the generation weights of diffusion model. Additionally,\nwe integrate circular consistency loss and adversarial training to guarantee\nthe fidelity and naturalness of the scene contents. Furthermore, we elaborate\non a multi-granularity knowledge distillation strategy (MKD) that encourages\nthe student network to absorb knowledge from both the teacher model and\npretrained Depth Anything V2. This strategy guides the student model in\nlearning degradation-agnostic scene information from various degradation\ninputs. In particular, we introduce an ordinal guidance distillation mechanism\n(OGD) that encourages the network to focus on uncertain regions through\ndifferential ranking, leading to a more precise depth estimation. Experimental\nresults demonstrate that our ACDepth surpasses md4all-DD by 2.50\\% for night\nscene and 2.61\\% for rainy scene on the nuScenes dataset in terms of the absRel\nmetric."}
{"id": "2505.12477", "pdf": "https://arxiv.org/pdf/2505.12477", "abs": "https://arxiv.org/abs/2505.12477", "authors": ["Hugues Van Assel", "Mark Ibrahim", "Tommaso Biancalani", "Aviv Regev", "Randall Balestriero"], "title": "Joint Embedding vs Reconstruction: Provable Benefits of Latent Space Prediction for Self Supervised Learning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "33 pages, 9 figures", "summary": "Reconstruction and joint embedding have emerged as two leading paradigms in\nSelf Supervised Learning (SSL). Reconstruction methods focus on recovering the\noriginal sample from a different view in input space. On the other hand, joint\nembedding methods align the representations of different views in latent space.\nBoth approaches offer compelling advantages, yet practitioners lack clear\nguidelines for choosing between them. In this work, we unveil the core\nmechanisms that distinguish each paradigm. By leveraging closed form solutions\nfor both approaches, we precisely characterize how the view generation process,\ne.g. data augmentation, impacts the learned representations. We then\ndemonstrate that, unlike supervised learning, both SSL paradigms require a\nminimal alignment between augmentations and irrelevant features to achieve\nasymptotic optimality with increasing sample size. Our findings indicate that\nin scenarios where these irrelevant features have a large magnitude, joint\nembedding methods are preferable because they impose a strictly weaker\nalignment condition compared to reconstruction based methods. These results not\nonly clarify the trade offs between the two paradigms but also substantiate the\nempirical success of joint embedding approaches on real world challenging\ndatasets."}
{"id": "2505.12763", "pdf": "https://arxiv.org/pdf/2505.12763", "abs": "https://arxiv.org/abs/2505.12763", "authors": ["Sunghwan Kim", "Dongjin Kang", "Taeyoon Kwon", "Hyungjoo Chae", "Dongha Lee", "Jinyoung Yeo"], "title": "Rethinking Reward Model Evaluation Through the Lens of Reward Overoptimization", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted to ACL 2025", "summary": "Reward models (RMs) play a crucial role in reinforcement learning from human\nfeedback (RLHF), aligning model behavior with human preferences. However,\nexisting benchmarks for reward models show a weak correlation with the\nperformance of optimized policies, suggesting that they fail to accurately\nassess the true capabilities of RMs. To bridge this gap, we explore several\nevaluation designs through the lens of reward overoptimization\\textemdash a\nphenomenon that captures both how well the reward model aligns with human\npreferences and the dynamics of the learning signal it provides to the policy.\nThe results highlight three key findings on how to construct a reliable\nbenchmark: (i) it is important to minimize differences between chosen and\nrejected responses beyond correctness, (ii) evaluating reward models requires\nmultiple comparisons across a wide range of chosen and rejected responses, and\n(iii) given that reward models encounter responses with diverse\nrepresentations, responses should be sourced from a variety of models. However,\nwe also observe that a extremely high correlation with degree of\noveroptimization leads to comparatively lower correlation with certain\ndownstream performance. Thus, when designing a benchmark, it is desirable to\nuse the degree of overoptimization as a useful tool, rather than the end goal."}
{"id": "2505.12207", "pdf": "https://arxiv.org/pdf/2505.12207", "abs": "https://arxiv.org/abs/2505.12207", "authors": ["Qingmei Li", "Yang Zhang", "Zurong Mai", "Yuhang Chen", "Shuohong Lou", "Henglian Huang", "Jiarui Zhang", "Zhiwei Zhang", "Yibin Wen", "Weijia Li", "Haohuan Fu", "Jianxi Huang", "Juepeng Zheng"], "title": "Can Large Multimodal Models Understand Agricultural Scenes? Benchmarking with AgroMind", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large Multimodal Models (LMMs) has demonstrated capabilities across various\ndomains, but comprehensive benchmarks for agricultural remote sensing (RS)\nremain scarce. Existing benchmarks designed for agricultural RS scenarios\nexhibit notable limitations, primarily in terms of insufficient scene diversity\nin the dataset and oversimplified task design. To bridge this gap, we introduce\nAgroMind, a comprehensive agricultural remote sensing benchmark covering four\ntask dimensions: spatial perception, object understanding, scene understanding,\nand scene reasoning, with a total of 13 task types, ranging from crop\nidentification and health monitoring to environmental analysis. We curate a\nhigh-quality evaluation set by integrating eight public datasets and one\nprivate farmland plot dataset, containing 25,026 QA pairs and 15,556 images.\nThe pipeline begins with multi-source data preprocessing, including collection,\nformat standardization, and annotation refinement. We then generate a diverse\nset of agriculturally relevant questions through the systematic definition of\ntasks. Finally, we employ LMMs for inference, generating responses, and\nperforming detailed examinations. We evaluated 18 open-source LMMs and 3\nclosed-source models on AgroMind. Experiments reveal significant performance\ngaps, particularly in spatial reasoning and fine-grained recognition, it is\nnotable that human performance lags behind several leading LMMs. By\nestablishing a standardized evaluation framework for agricultural RS, AgroMind\nreveals the limitations of LMMs in domain knowledge and highlights critical\nchallenges for future work. Data and code can be accessed at\nhttps://rssysu.github.io/AgroMind/."}
{"id": "2505.12512", "pdf": "https://arxiv.org/pdf/2505.12512", "abs": "https://arxiv.org/abs/2505.12512", "authors": ["Truman Hickok"], "title": "Scalable Strategies for Continual Learning with Replay", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Future deep learning models will be distinguished by systems that perpetually\nlearn through interaction, imagination, and cooperation, blurring the line\nbetween training and inference. This makes continual learning a critical\nchallenge, as methods that efficiently maximize bidirectional transfer across\nlearning trajectories will be essential. Replay is on track to play a\nfoundational role in continual learning, allowing models to directly reconcile\nnew information with past knowledge. In practice, however, replay is quite\nunscalable, doubling the cost of continual learning when applied naively.\nMoreover, the continual learning literature has not fully synchronized with the\nmulti-task fine-tuning literature, having not fully integrated highly scalable\ntechniques like model merging and low rank adaptation into a replay-enabled\ntoolset that can produce a unified model in the face of many sequential tasks.\nIn this paper, we begin by applying and analyzing low rank adaptation in a\ncontinual learning setting. Next, we introduce consolidation, a phasic approach\nto replay which leads to up to 55\\% less replay samples being needed for a\ngiven performance target. Then, we propose sequential merging, an offshoot of\ntask arithmetic which is tailored to the continual learning setting and is\nshown to work well in combination with replay. Finally, we demonstrate that the\ndeveloped strategies can operate synergistically, resulting in a highly\nscalable toolset that outperforms standalone variants."}
{"id": "2505.12842", "pdf": "https://arxiv.org/pdf/2505.12842", "abs": "https://arxiv.org/abs/2505.12842", "authors": ["Zheng Wu", "Pengzhou Cheng", "Zongru Wu", "Lingzhong Dong", "Zhuosheng Zhang"], "title": "GEM: Gaussian Embedding Modeling for Out-of-Distribution Detection in GUI Agents", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Graphical user interface (GUI) agents have recently emerged as an intriguing\nparadigm for human-computer interaction, capable of automatically executing\nuser instructions to operate intelligent terminal devices. However, when\nencountering out-of-distribution (OOD) instructions that violate environmental\nconstraints or exceed the current capabilities of agents, GUI agents may suffer\ntask breakdowns or even pose security threats. Therefore, effective OOD\ndetection for GUI agents is essential. Traditional OOD detection methods\nperform suboptimally in this domain due to the complex embedding space and\nevolving GUI environments. In this work, we observe that the in-distribution\ninput semantic space of GUI agents exhibits a clustering pattern with respect\nto the distance from the centroid. Based on the finding, we propose GEM, a\nnovel method based on fitting a Gaussian mixture model over input embedding\ndistances extracted from the GUI Agent that reflect its capability boundary.\nEvaluated on eight datasets spanning smartphones, computers, and web browsers,\nour method achieves an average accuracy improvement of 23.70\\% over the\nbest-performing baseline. Analysis verifies the generalization ability of our\nmethod through experiments on nine different backbones. The codes are available\nat https://github.com/Wuzheng02/GEM-OODforGUIagents."}
{"id": "2505.12211", "pdf": "https://arxiv.org/pdf/2505.12211", "abs": "https://arxiv.org/abs/2505.12211", "authors": ["Wenhui Liu", "Zhijian Wu", "Jingchao Wang", "Dingjiang Huang", "Shuigeng Zhou"], "title": "Imagination-Limited Q-Learning for Offline Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by IJCAI 2025", "summary": "Offline reinforcement learning seeks to derive improved policies entirely\nfrom historical data but often struggles with over-optimistic value estimates\nfor out-of-distribution (OOD) actions. This issue is typically mitigated via\npolicy constraint or conservative value regularization methods. However, these\napproaches may impose overly constraints or biased value estimates, potentially\nlimiting performance improvements. To balance exploitation and restriction, we\npropose an Imagination-Limited Q-learning (ILQ) method, which aims to maintain\nthe optimism that OOD actions deserve within appropriate limits. Specifically,\nwe utilize the dynamics model to imagine OOD action-values, and then clip the\nimagined values with the maximum behavior values. Such design maintains\nreasonable evaluation of OOD actions to the furthest extent, while avoiding its\nover-optimism. Theoretically, we prove the convergence of the proposed ILQ\nunder tabular Markov decision processes. Particularly, we demonstrate that the\nerror bound between estimated values and optimality values of OOD state-actions\npossesses the same magnitude as that of in-distribution ones, thereby\nindicating that the bias in value estimates is effectively mitigated.\nEmpirically, our method achieves state-of-the-art performance on a wide range\nof tasks in the D4RL benchmark."}
{"id": "2505.12552", "pdf": "https://arxiv.org/pdf/2505.12552", "abs": "https://arxiv.org/abs/2505.12552", "authors": ["Junliang Ye", "Lei Wang", "Md Zakir Hossain"], "title": "FreqSelect: Frequency-Aware fMRI-to-Image Reconstruction", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": "Research report", "summary": "Reconstructing natural images from functional magnetic resonance imaging\n(fMRI) data remains a core challenge in natural decoding due to the mismatch\nbetween the richness of visual stimuli and the noisy, low resolution nature of\nfMRI signals. While recent two-stage models, combining deep variational\nautoencoders (VAEs) with diffusion models, have advanced this task, they treat\nall spatial-frequency components of the input equally. This uniform treatment\nforces the model to extract meaning features and suppress irrelevant noise\nsimultaneously, limiting its effectiveness. We introduce FreqSelect, a\nlightweight, adaptive module that selectively filters spatial-frequency bands\nbefore encoding. By dynamically emphasizing frequencies that are most\npredictive of brain activity and suppressing those that are uninformative,\nFreqSelect acts as a content-aware gate between image features and natural\ndata. It integrates seamlessly into standard very deep VAE-diffusion pipelines\nand requires no additional supervision. Evaluated on the Natural Scenes\ndataset, FreqSelect consistently improves reconstruction quality across both\nlow- and high-level metrics. Beyond performance gains, the learned\nfrequency-selection patterns offer interpretable insights into how different\nvisual frequencies are represented in the brain. Our method generalizes across\nsubjects and scenes, and holds promise for extension to other neuroimaging\nmodalities, offering a principled approach to enhancing both decoding accuracy\nand neuroscientific interpretability."}
{"id": "2505.12871", "pdf": "https://arxiv.org/pdf/2505.12871", "abs": "https://arxiv.org/abs/2505.12871", "authors": ["Zi Liang", "Haibo Hu", "Qingqing Ye", "Yaxin Xiao", "Ronghua Li"], "title": "Does Low Rank Adaptation Lead to Lower Robustness against Training-Time Attacks?", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "comment": "To appear at ICML 25", "summary": "Low rank adaptation (LoRA) has emerged as a prominent technique for\nfine-tuning large language models (LLMs) thanks to its superb efficiency gains\nover previous methods. While extensive studies have examined the performance\nand structural properties of LoRA, its behavior upon training-time attacks\nremain underexplored, posing significant security risks. In this paper, we\ntheoretically investigate the security implications of LoRA's low-rank\nstructure during fine-tuning, in the context of its robustness against data\npoisoning and backdoor attacks. We propose an analytical framework that models\nLoRA's training dynamics, employs the neural tangent kernel to simplify the\nanalysis of the training process, and applies information theory to establish\nconnections between LoRA's low rank structure and its vulnerability against\ntraining-time attacks. Our analysis indicates that LoRA exhibits better\nrobustness to backdoor attacks than full fine-tuning, while becomes more\nvulnerable to untargeted data poisoning due to its over-simplified information\ngeometry. Extensive experimental evaluations have corroborated our theoretical\nfindings."}
{"id": "2505.12224", "pdf": "https://arxiv.org/pdf/2505.12224", "abs": "https://arxiv.org/abs/2505.12224", "authors": ["Weifeng Lu", "Minghao Ye", "Zewei Ye", "Ruihan Tao", "Shuo Yang", "Bo Zhao"], "title": "RoboFAC: A Comprehensive Framework for Robotic Failure Analysis and Correction", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Vision-Language-Action (VLA) models have recently advanced robotic\nmanipulation by translating natural-language instructions and image information\ninto sequential control actions. However, these models often underperform in\nopen-world scenarios, as they are predominantly trained on successful expert\ndemonstrations and exhibit a limited capacity for failure recovery. In this\nwork, we present a Robotic Failure Analysis and Correction (RoboFAC) framework\nto address this issue. Firstly, we construct RoboFAC dataset comprising 9,440\nerroneous manipulation trajectories and 78,623 QA pairs across 16 diverse tasks\nand 53 scenes in both simulation and real-world environments. Leveraging our\ndataset, we develop RoboFAC model, which is capable of Task Understanding,\nFailure Analysis and Failure Correction. Experimental results demonstrate that\nthe RoboFAC model outperforms GPT-4o by 34.1% on our evaluation benchmark.\nFurthermore, we integrate the RoboFAC model into a real-world VLA control\npipeline as an external supervision providing correction instructions, yielding\na 29.1% relative improvement on average on four real-world tasks. The results\nshow that our RoboFAC framework effectively handles robotic failures and\nassists the VLA model in recovering from failures."}
{"id": "2505.12581", "pdf": "https://arxiv.org/pdf/2505.12581", "abs": "https://arxiv.org/abs/2505.12581", "authors": ["Lucas M. Dorneles", "Luan Fonseca Garcia", "Joel Luís Carbonera"], "title": "An approach based on class activation maps for investigating the effects of data augmentation on neural networks for image classification", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Neural networks have become increasingly popular in the last few years as an\neffective tool for the task of image classification due to the impressive\nperformance they have achieved on this task. In image classification tasks, it\nis common to use data augmentation strategies to increase the robustness of\ntrained networks to changes in the input images and to avoid overfitting.\nAlthough data augmentation is a widely adopted technique, the literature lacks\na body of research analyzing the effects data augmentation methods have on the\npatterns learned by neural network models working on complex datasets. The\nprimary objective of this work is to propose a methodology and set of metrics\nthat may allow a quantitative approach to analyzing the effects of data\naugmentation in convolutional networks applied to image classification. An\nimportant tool used in the proposed approach lies in the concept of class\nactivation maps for said models, which allow us to identify and measure the\nimportance these models assign to each individual pixel in an image when\nexecuting the classification task. From these maps, we may then extract metrics\nover the similarities and differences between maps generated by these models\ntrained on a given dataset with different data augmentation strategies.\nExperiments made using this methodology suggest that the effects of these data\naugmentation techniques not only can be analyzed in this way but also allow us\nto identify different impact profiles over the trained models."}
{"id": "2505.12886", "pdf": "https://arxiv.org/pdf/2505.12886", "abs": "https://arxiv.org/abs/2505.12886", "authors": ["Zhongxiang Sun", "Qipeng Wang", "Haoyu Wang", "Xiao Zhang", "Jun Xu"], "title": "Detection and Mitigation of Hallucination in Large Reasoning Models: A Mechanistic Perspective", "categories": ["cs.AI", "cs.CL", "cs.CY"], "comment": "25 pages", "summary": "Large Reasoning Models (LRMs) have shown impressive capabilities in\nmulti-step reasoning tasks. However, alongside these successes, a more\ndeceptive form of model error has emerged--Reasoning Hallucination--where\nlogically coherent but factually incorrect reasoning traces lead to persuasive\nyet faulty conclusions. Unlike traditional hallucinations, these errors are\nembedded within structured reasoning, making them more difficult to detect and\npotentially more harmful. In this work, we investigate reasoning hallucinations\nfrom a mechanistic perspective. We propose the Reasoning Score, which\nquantifies the depth of reasoning by measuring the divergence between logits\nobtained from projecting late layers of LRMs to the vocabulary space,\neffectively distinguishing shallow pattern-matching from genuine deep\nreasoning. Using this score, we conduct an in-depth analysis on the ReTruthQA\ndataset and identify two key reasoning hallucination patterns: early-stage\nfluctuation in reasoning depth and incorrect backtracking to flawed prior\nsteps. These insights motivate our Reasoning Hallucination Detection (RHD)\nframework, which achieves state-of-the-art performance across multiple domains.\nTo mitigate reasoning hallucinations, we further introduce GRPO-R, an enhanced\nreinforcement learning algorithm that incorporates step-level deep reasoning\nrewards via potential-based shaping. Our theoretical analysis establishes\nstronger generalization guarantees, and experiments demonstrate improved\nreasoning quality and reduced hallucination rates."}
{"id": "2505.12225", "pdf": "https://arxiv.org/pdf/2505.12225", "abs": "https://arxiv.org/abs/2505.12225", "authors": ["Jizhou Guo", "Zhaomin Wu", "Philip S. Yu"], "title": "Reward Inside the Model: A Lightweight Hidden-State Reward Model for LLM's Best-of-N sampling", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "High-quality reward models are crucial for unlocking the reasoning potential\nof large language models (LLMs), with best-of-N voting demonstrating\nsignificant performance gains. However, current reward models, which typically\noperate on the textual output of LLMs, are computationally expensive and\nparameter-heavy, limiting their real-world applications. We introduce the\nEfficient Linear Hidden State Reward (ELHSR) model - a novel, highly\nparameter-efficient approach that leverages the rich information embedded in\nLLM hidden states to address these issues. ELHSR systematically outperform\nbaselines with less than 0.005% of the parameters of baselines, requiring only\na few samples for training. ELHSR also achieves orders-of-magnitude efficiency\nimprovement with significantly less time and fewer FLOPs per sample than\nbaseline reward models. Moreover, ELHSR exhibits robust performance even when\ntrained only on logits, extending its applicability to some closed-source LLMs.\nIn addition, ELHSR can also be combined with traditional reward models to\nachieve additional performance gains."}
{"id": "2505.12642", "pdf": "https://arxiv.org/pdf/2505.12642", "abs": "https://arxiv.org/abs/2505.12642", "authors": ["Jung Hoon Lee", "Sujith Vijayan"], "title": "Two out of Three (ToT): using self-consistency to make robust predictions", "categories": ["cs.LG", "cs.CV"], "comment": "12 pages, 7 main figures, 1 supplementary table and 2 supplementary\n  figures", "summary": "Deep learning (DL) can automatically construct intelligent agents, deep\nneural networks (alternatively, DL models), that can outperform humans in\ncertain tasks. However, the operating principles of DL remain poorly\nunderstood, making its decisions incomprehensible. As a result, it poses a\ngreat risk to deploy DL in high-stakes domains in which mistakes or errors may\nlead to critical consequences. Here, we aim to develop an algorithm that can\nhelp DL models make more robust decisions by allowing them to abstain from\nanswering when they are uncertain. Our algorithm, named `Two out of Three\n(ToT)', is inspired by the sensitivity of the human brain to conflicting\ninformation. ToT creates two alternative predictions in addition to the\noriginal model prediction and uses the alternative predictions to decide\nwhether it should provide an answer or not."}
{"id": "2505.12891", "pdf": "https://arxiv.org/pdf/2505.12891", "abs": "https://arxiv.org/abs/2505.12891", "authors": ["Shaohang Wei", "Wei Li", "Feifan Song", "Wen Luo", "Tianyi Zhuang", "Haochen Tan", "Zhijiang Guo", "Houfeng Wang"], "title": "TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in Real-World Scenarios", "categories": ["cs.AI", "cs.CL"], "comment": "First version. There are still some examples to be added into the\n  appendix", "summary": "Temporal reasoning is pivotal for Large Language Models (LLMs) to comprehend\nthe real world. However, existing works neglect the real-world challenges for\ntemporal reasoning: (1) intensive temporal information, (2) fast-changing event\ndynamics, and (3) complex temporal dependencies in social interactions. To\nbridge this gap, we propose a multi-level benchmark TIME, designed for temporal\nreasoning in real-world scenarios. TIME consists of 38,522 QA pairs, covering 3\nlevels with 11 fine-grained sub-tasks. This benchmark encompasses 3\nsub-datasets reflecting different real-world challenges: TIME-Wiki, TIME-News,\nand TIME-Dial. We conduct extensive experiments on reasoning models and\nnon-reasoning models. And we conducted an in-depth analysis of temporal\nreasoning performance across diverse real-world scenarios and tasks, and\nsummarized the impact of test-time scaling on temporal reasoning capabilities.\nAdditionally, we release TIME-Lite, a human-annotated subset to foster future\nresearch and standardized evaluation in temporal reasoning. The code is\navailable at https://github.com/sylvain-wei/TIME , and the dataset is available\nat https://huggingface.co/datasets/SylvainWei/TIME ."}
{"id": "2505.12226", "pdf": "https://arxiv.org/pdf/2505.12226", "abs": "https://arxiv.org/abs/2505.12226", "authors": ["Dong Yang", "Yiyi Cai", "Yuki Saito", "Lixu Wang", "Hiroshi Saruwatari"], "title": "Shallow Flow Matching for Coarse-to-Fine Text-to-Speech Synthesis", "categories": ["eess.AS", "cs.AI", "cs.SD"], "comment": null, "summary": "We propose a shallow flow matching (SFM) mechanism to enhance flow matching\n(FM)-based text-to-speech (TTS) models within a coarse-to-fine generation\nparadigm. SFM constructs intermediate states along the FM paths using coarse\noutput representations. During training, we introduce an orthogonal projection\nmethod to adaptively determine the temporal position of these states, and apply\na principled construction strategy based on a single-segment piecewise flow.\nThe SFM inference starts from the intermediate state rather than pure noise and\nfocuses computation on the latter stages of the FM paths. We integrate SFM into\nmultiple TTS models with a lightweight SFM head. Experiments show that SFM\nconsistently improves the naturalness of synthesized speech in both objective\nand subjective evaluations, while significantly reducing inference when using\nadaptive-step ODE solvers. Demo and codes are available at\nhttps://ydqmkkx.github.io/SFMDemo/."}
{"id": "2505.12681", "pdf": "https://arxiv.org/pdf/2505.12681", "abs": "https://arxiv.org/abs/2505.12681", "authors": ["Hana Satou", "Alan Mitkiy"], "title": "On the Mechanisms of Adversarial Data Augmentation for Robust and Adaptive Transfer Learning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Transfer learning across domains with distribution shift remains a\nfundamental challenge in building robust and adaptable machine learning\nsystems. While adversarial perturbations are traditionally viewed as threats\nthat expose model vulnerabilities, recent studies suggest that they can also\nserve as constructive tools for data augmentation. In this work, we\nsystematically investigate the role of adversarial data augmentation (ADA) in\nenhancing both robustness and adaptivity in transfer learning settings. We\nanalyze how adversarial examples, when used strategically during training,\nimprove domain generalization by enriching decision boundaries and reducing\noverfitting to source-domain-specific features. We further propose a unified\nframework that integrates ADA with consistency regularization and\ndomain-invariant representation learning. Extensive experiments across multiple\nbenchmark datasets -- including VisDA, DomainNet, and Office-Home --\ndemonstrate that our method consistently improves target-domain performance\nunder both unsupervised and few-shot domain adaptation settings. Our results\nhighlight a constructive perspective of adversarial learning, transforming\nperturbation from a destructive attack into a regularizing force for\ncross-domain transferability."}
{"id": "2505.12900", "pdf": "https://arxiv.org/pdf/2505.12900", "abs": "https://arxiv.org/abs/2505.12900", "authors": ["Shuyang Hou", "Zhangxiao Shen", "Huayi Wu", "Jianyuan Liang", "Haoyue Jiao", "Yaxian Qing", "Xiaopu Zhang", "Xu Li", "Zhipeng Gui", "Xuefeng Guan", "Longgang Xiang"], "title": "AutoGEEval: A Multimodal and Automated Framework for Geospatial Code Generation on GEE with Large Language Models", "categories": ["cs.SE", "cs.AI", "cs.CG", "cs.CL", "cs.DB"], "comment": null, "summary": "Geospatial code generation is emerging as a key direction in the integration\nof artificial intelligence and geoscientific analysis. However, there remains a\nlack of standardized tools for automatic evaluation in this domain. To address\nthis gap, we propose AutoGEEval, the first multimodal, unit-level automated\nevaluation framework for geospatial code generation tasks on the Google Earth\nEngine (GEE) platform powered by large language models (LLMs). Built upon the\nGEE Python API, AutoGEEval establishes a benchmark suite (AutoGEEval-Bench)\ncomprising 1325 test cases that span 26 GEE data types. The framework\nintegrates both question generation and answer verification components to\nenable an end-to-end automated evaluation pipeline-from function invocation to\nexecution validation. AutoGEEval supports multidimensional quantitative\nanalysis of model outputs in terms of accuracy, resource consumption, execution\nefficiency, and error types. We evaluate 18 state-of-the-art LLMs-including\ngeneral-purpose, reasoning-augmented, code-centric, and geoscience-specialized\nmodels-revealing their performance characteristics and potential optimization\npathways in GEE code generation. This work provides a unified protocol and\nfoundational resource for the development and assessment of geospatial code\ngeneration models, advancing the frontier of automated natural language to\ndomain-specific code translation."}
{"id": "2505.12236", "pdf": "https://arxiv.org/pdf/2505.12236", "abs": "https://arxiv.org/abs/2505.12236", "authors": ["Quanjiang Guo", "Jinchuan Zhang", "Sijie Wang", "Ling Tian", "Zhao Kang", "Bin Yan", "Weidong Xiao"], "title": "Bridging Generative and Discriminative Learning: Few-Shot Relation Extraction via Two-Stage Knowledge-Guided Pre-training", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "13 pages, 6 figures, Appear on IJCAI 2025", "summary": "Few-Shot Relation Extraction (FSRE) remains a challenging task due to the\nscarcity of annotated data and the limited generalization capabilities of\nexisting models. Although large language models (LLMs) have demonstrated\npotential in FSRE through in-context learning (ICL), their general-purpose\ntraining objectives often result in suboptimal performance for task-specific\nrelation extraction. To overcome these challenges, we propose TKRE (Two-Stage\nKnowledge-Guided Pre-training for Relation Extraction), a novel framework that\nsynergistically integrates LLMs with traditional relation extraction models,\nbridging generative and discriminative learning paradigms. TKRE introduces two\nkey innovations: (1) leveraging LLMs to generate explanation-driven knowledge\nand schema-constrained synthetic data, addressing the issue of data scarcity;\nand (2) a two-stage pre-training strategy combining Masked Span Language\nModeling (MSLM) and Span-Level Contrastive Learning (SCL) to enhance relational\nreasoning and generalization. Together, these components enable TKRE to\neffectively tackle FSRE tasks. Comprehensive experiments on benchmark datasets\ndemonstrate the efficacy of TKRE, achieving new state-of-the-art performance in\nFSRE and underscoring its potential for broader application in low-resource\nscenarios. \\footnote{The code and data are released on\nhttps://github.com/UESTC-GQJ/TKRE."}
{"id": "2505.12748", "pdf": "https://arxiv.org/pdf/2505.12748", "abs": "https://arxiv.org/abs/2505.12748", "authors": ["Hangyu Li", "Qin Zhao", "Haoran Xu", "Xinyu Jiang", "Qingwei Ben", "Feiyu Jia", "Haoyu Zhao", "Liang Xu", "Jia Zeng", "Hanqing Wang", "Bo Dai", "Junting Dong", "Jiangmiao Pang"], "title": "TeleOpBench: A Simulator-Centric Benchmark for Dual-Arm Dexterous Teleoperation", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "13 pages", "summary": "Teleoperation is a cornerstone of embodied-robot learning, and bimanual\ndexterous teleoperation in particular provides rich demonstrations that are\ndifficult to obtain with fully autonomous systems. While recent studies have\nproposed diverse hardware pipelines-ranging from inertial motion-capture gloves\nto exoskeletons and vision-based interfaces-there is still no unified benchmark\nthat enables fair, reproducible comparison of these systems. In this paper, we\nintroduce TeleOpBench, a simulator-centric benchmark tailored to bimanual\ndexterous teleoperation. TeleOpBench contains 30 high-fidelity task\nenvironments that span pick-and-place, tool use, and collaborative\nmanipulation, covering a broad spectrum of kinematic and force-interaction\ndifficulty. Within this benchmark we implement four representative\nteleoperation modalities-(i) MoCap, (ii) VR device, (iii) arm-hand\nexoskeletons, and (iv) monocular vision tracking-and evaluate them with a\ncommon protocol and metric suite. To validate that performance in simulation is\npredictive of real-world behavior, we conduct mirrored experiments on a\nphysical dual-arm platform equipped with two 6-DoF dexterous hands. Across 10\nheld-out tasks we observe a strong correlation between simulator and hardware\nperformance, confirming the external validity of TeleOpBench. TeleOpBench\nestablishes a common yardstick for teleoperation research and provides an\nextensible platform for future algorithmic and hardware innovation."}
{"id": "2505.12938", "pdf": "https://arxiv.org/pdf/2505.12938", "abs": "https://arxiv.org/abs/2505.12938", "authors": ["Uri Dalal", "Meirav Segal", "Zvika Ben-Haim", "Dan Lahav", "Omer Nevo"], "title": "Leveraging LLM Inconsistency to Boost Pass@k Performance", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) achieve impressive abilities in numerous\ndomains, but exhibit inconsistent performance in response to minor input\nchanges. Rather than view this as a drawback, in this paper we introduce a\nnovel method for leveraging models' inconsistency to boost Pass@k performance.\nSpecifically, we present a \"Variator\" agent that generates k variants of a\ngiven task and submits one candidate solution for each one. Our variant\ngeneration approach is applicable to a wide range of domains as it is task\nagnostic and compatible with free-form inputs. We demonstrate the efficacy of\nour agent theoretically using a probabilistic model of the inconsistency\neffect, and show empirically that it outperforms the baseline on the APPS\ndataset. Furthermore, we establish that inconsistency persists even in frontier\nreasoning models across coding and cybersecurity domains, suggesting our method\nis likely to remain relevant for future model generations."}
{"id": "2505.12238", "pdf": "https://arxiv.org/pdf/2505.12238", "abs": "https://arxiv.org/abs/2505.12238", "authors": ["Sriram Selvam", "Anneswa Ghosh"], "title": "PANORAMA: A synthetic PII-laced dataset for studying sensitive data memorization in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The memorization of sensitive and personally identifiable information (PII)\nby large language models (LLMs) poses growing privacy risks as models scale and\nare increasingly deployed in real-world applications. Existing efforts to study\nsensitive and PII data memorization and develop mitigation strategies are\nhampered by the absence of comprehensive, realistic, and ethically sourced\ndatasets reflecting the diversity of sensitive information found on the web. We\nintroduce PANORAMA - Profile-based Assemblage for Naturalistic Online\nRepresentation and Attribute Memorization Analysis, a large-scale synthetic\ncorpus of 384,789 samples derived from 9,674 synthetic profiles designed to\nclosely emulate the distribution, variety, and context of PII and sensitive\ndata as it naturally occurs in online environments. Our data generation\npipeline begins with the construction of internally consistent, multi-attribute\nhuman profiles using constrained selection to reflect real-world demographics\nsuch as education, health attributes, financial status, etc. Using a\ncombination of zero-shot prompting and OpenAI o3-mini, we generate diverse\ncontent types - including wiki-style articles, social media posts, forum\ndiscussions, online reviews, comments, and marketplace listings - each\nembedding realistic, contextually appropriate PII and other sensitive\ninformation. We validate the utility of PANORAMA by fine-tuning the Mistral-7B\nmodel on 1x, 5x, 10x, and 25x data replication rates with a subset of data and\nmeasure PII memorization rates - revealing not only consistent increases with\nrepetition but also variation across content types, highlighting PANORAMA's\nability to model how memorization risks differ by context. Our dataset and code\nare publicly available, providing a much-needed resource for privacy risk\nassessment, model auditing, and the development of privacy-preserving LLMs."}
{"id": "2505.12751", "pdf": "https://arxiv.org/pdf/2505.12751", "abs": "https://arxiv.org/abs/2505.12751", "authors": ["Filippo Leveni"], "title": "Structure-based Anomaly Detection and Clustering", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "Doctoral dissertation at Politecnico di Milano", "summary": "Anomaly detection is a fundamental problem in domains such as healthcare,\nmanufacturing, and cybersecurity. This thesis proposes new unsupervised methods\nfor anomaly detection in both structured and streaming data settings. In the\nfirst part, we focus on structure-based anomaly detection, where normal data\nfollows low-dimensional manifolds while anomalies deviate from them. We\nintroduce Preference Isolation Forest (PIF), which embeds data into a\nhigh-dimensional preference space via manifold fitting, and isolates outliers\nusing two variants: Voronoi-iForest, based on geometric distances, and\nRuzHash-iForest, leveraging Locality Sensitive Hashing for scalability. We also\npropose Sliding-PIF, which captures local manifold information for streaming\nscenarios. Our methods outperform existing techniques on synthetic and real\ndatasets. We extend this to structure-based clustering with MultiLink, a novel\nmethod for recovering multiple geometric model families in noisy data.\nMultiLink merges clusters via a model-aware linkage strategy, enabling robust\nmulti-class structure recovery. It offers key advantages over existing\napproaches, such as speed, reduced sensitivity to thresholds, and improved\nrobustness to poor initial sampling. The second part of the thesis addresses\nonline anomaly detection in evolving data streams. We propose Online Isolation\nForest (Online-iForest), which uses adaptive, multi-resolution histograms and\ndynamically updates tree structures to track changes over time. It avoids\nretraining while achieving accuracy comparable to offline models, with superior\nefficiency for real-time applications. Finally, we tackle anomaly detection in\ncybersecurity via open-set recognition for malware classification. We enhance a\nGradient Boosting classifier with MaxLogit to detect unseen malware families, a\nmethod now integrated into Cleafy's production system."}
{"id": "2505.12992", "pdf": "https://arxiv.org/pdf/2505.12992", "abs": "https://arxiv.org/abs/2505.12992", "authors": ["Baohao Liao", "Hanze Dong", "Yuhui Xu", "Doyen Sahoo", "Christof Monz", "Junnan Li", "Caiming Xiong"], "title": "Fractured Chain-of-Thought Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Inference-time scaling techniques have significantly bolstered the reasoning\ncapabilities of large language models (LLMs) by harnessing additional\ncomputational effort at inference without retraining. Similarly,\nChain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy\nby generating rich intermediate reasoning trajectories, but these approaches\nincur substantial token costs that impede their deployment in latency-sensitive\nsettings. In this work, we first show that truncated CoT, which stops reasoning\nbefore completion and directly generates the final answer, often matches full\nCoT sampling while using dramatically fewer tokens. Building on this insight,\nwe introduce Fractured Sampling, a unified inference-time strategy that\ninterpolates between full CoT and solution-only sampling along three orthogonal\naxes: (1) the number of reasoning trajectories, (2) the number of final\nsolutions per trajectory, and (3) the depth at which reasoning traces are\ntruncated. Through extensive experiments on five diverse reasoning benchmarks\nand several model scales, we demonstrate that Fractured Sampling consistently\nachieves superior accuracy-cost trade-offs, yielding steep log-linear scaling\ngains in Pass@k versus token budget. Our analysis reveals how to allocate\ncomputation across these dimensions to maximize performance, paving the way for\nmore efficient and scalable LLM reasoning."}
{"id": "2505.12239", "pdf": "https://arxiv.org/pdf/2505.12239", "abs": "https://arxiv.org/abs/2505.12239", "authors": ["Jianheng Tang", "Huiping Zhuang", "Di Fang", "Jiaxu Li", "Feijiang Han", "Yajiang Huang", "Kejia Fan", "Leye Wang", "Zhanxing Zhu", "Shanghang Zhang", "Houbing Herbert Song", "Yunhuai Liu"], "title": "ACU: Analytic Continual Unlearning for Efficient and Exact Forgetting with Privacy Preservation", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "21 pages, 4 figures, 2 tables", "summary": "The development of artificial intelligence demands that models incrementally\nupdate knowledge by Continual Learning (CL) to adapt to open-world\nenvironments. To meet privacy and security requirements, Continual Unlearning\n(CU) emerges as an important problem, aiming to sequentially forget particular\nknowledge acquired during the CL phase. However, existing unlearning methods\nprimarily focus on single-shot joint forgetting and face significant\nlimitations when applied to CU. First, most existing methods require access to\nthe retained dataset for re-training or fine-tuning, violating the inherent\nconstraint in CL that historical data cannot be revisited. Second, these\nmethods often suffer from a poor trade-off between system efficiency and model\nfidelity, making them vulnerable to being overwhelmed or degraded by\nadversaries through deliberately frequent requests. In this paper, we identify\nthat the limitations of existing unlearning methods stem fundamentally from\ntheir reliance on gradient-based updates. To bridge the research gap at its\nroot, we propose a novel gradient-free method for CU, named Analytic Continual\nUnlearning (ACU), for efficient and exact forgetting with historical data\nprivacy preservation. In response to each unlearning request, our ACU\nrecursively derives an analytical (i.e., closed-form) solution in an\ninterpretable manner using the least squares method. Theoretical and\nexperimental evaluations validate the superiority of our ACU on unlearning\neffectiveness, model fidelity, and system efficiency."}
{"id": "2505.12774", "pdf": "https://arxiv.org/pdf/2505.12774", "abs": "https://arxiv.org/abs/2505.12774", "authors": ["Zichen Geng", "Zeeshan Hayder", "Wei Liu", "Ajmal Mian"], "title": "UniHM: Universal Human Motion Generation with Object Interactions in Indoor Scenes", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": null, "summary": "Human motion synthesis in complex scenes presents a fundamental challenge,\nextending beyond conventional Text-to-Motion tasks by requiring the integration\nof diverse modalities such as static environments, movable objects, natural\nlanguage prompts, and spatial waypoints. Existing language-conditioned motion\nmodels often struggle with scene-aware motion generation due to limitations in\nmotion tokenization, which leads to information loss and fails to capture the\ncontinuous, context-dependent nature of 3D human movement. To address these\nissues, we propose UniHM, a unified motion language model that leverages\ndiffusion-based generation for synthesizing scene-aware human motion. UniHM is\nthe first framework to support both Text-to-Motion and Text-to-Human-Object\nInteraction (HOI) in complex 3D scenes. Our approach introduces three key\ncontributions: (1) a mixed-motion representation that fuses continuous 6DoF\nmotion with discrete local motion tokens to improve motion realism; (2) a novel\nLook-Up-Free Quantization VAE (LFQ-VAE) that surpasses traditional VQ-VAEs in\nboth reconstruction accuracy and generative performance; and (3) an enriched\nversion of the Lingo dataset augmented with HumanML3D annotations, providing\nstronger supervision for scene-specific motion learning. Experimental results\ndemonstrate that UniHM achieves comparative performance on the OMOMO benchmark\nfor text-to-HOI synthesis and yields competitive results on HumanML3D for\ngeneral text-conditioned motion generation."}
{"id": "2505.13028", "pdf": "https://arxiv.org/pdf/2505.13028", "abs": "https://arxiv.org/abs/2505.13028", "authors": ["Sayon Palit", "Daniel Woods"], "title": "Evaluatiing the efficacy of LLM Safety Solutions : The Palit Benchmark Dataset", "categories": ["cs.CR", "cs.AI", "cs.CL", "F.2.2, I.2.7; F.2.2, I.2.7; F.2.2, I.2.7"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly integrated into critical\nsystems in industries like healthcare and finance. Users can often submit\nqueries to LLM-enabled chatbots, some of which can enrich responses with\ninformation retrieved from internal databases storing sensitive data. This\ngives rise to a range of attacks in which a user submits a malicious query and\nthe LLM-system outputs a response that creates harm to the owner, such as\nleaking internal data or creating legal liability by harming a third-party.\nWhile security tools are being developed to counter these threats, there is\nlittle formal evaluation of their effectiveness and usability. This study\naddresses this gap by conducting a thorough comparative analysis of LLM\nsecurity tools. We identified 13 solutions (9 closed-source, 4 open-source),\nbut only 7 were evaluated due to a lack of participation by proprietary model\nowners.To evaluate, we built a benchmark dataset of malicious prompts, and\nevaluate these tools performance against a baseline LLM model\n(ChatGPT-3.5-Turbo). Our results show that the baseline model has too many\nfalse positives to be used for this task. Lakera Guard and ProtectAI LLM Guard\nemerged as the best overall tools showcasing the tradeoff between usability and\nperformance. The study concluded with recommendations for greater transparency\namong closed source providers, improved context-aware detections, enhanced\nopen-source engagement, increased user awareness, and the adoption of more\nrepresentative performance metrics."}
{"id": "2505.12245", "pdf": "https://arxiv.org/pdf/2505.12245", "abs": "https://arxiv.org/abs/2505.12245", "authors": ["Jianheng Tang", "Huiping Zhuang", "Jingyu He", "Run He", "Jingchao Wang", "Kejia Fan", "Anfeng Liu", "Tian Wang", "Leye Wang", "Zhanxing Zhu", "Shanghang Zhang", "Houbing Herbert Song", "Yunhuai Liu"], "title": "AFCL: Analytic Federated Continual Learning for Spatio-Temporal Invariance of Non-IID Data", "categories": ["cs.LG", "cs.AI"], "comment": "23 pages, 5 figures, 5 tables", "summary": "Federated Continual Learning (FCL) enables distributed clients to\ncollaboratively train a global model from online task streams in dynamic\nreal-world scenarios. However, existing FCL methods face challenges of both\nspatial data heterogeneity among distributed clients and temporal data\nheterogeneity across online tasks. Such data heterogeneity significantly\ndegrades the model performance with severe spatial-temporal catastrophic\nforgetting of local and past knowledge. In this paper, we identify that the\nroot cause of this issue lies in the inherent vulnerability and sensitivity of\ngradients to non-IID data. To fundamentally address this issue, we propose a\ngradient-free method, named Analytic Federated Continual Learning (AFCL), by\nderiving analytical (i.e., closed-form) solutions from frozen extracted\nfeatures. In local training, our AFCL enables single-epoch learning with only a\nlightweight forward-propagation process for each client. In global aggregation,\nthe server can recursively and efficiently update the global model with\nsingle-round aggregation. Theoretical analyses validate that our AFCL achieves\nspatio-temporal invariance of non-IID data. This ideal property implies that,\nregardless of how heterogeneous the data are distributed across local clients\nand online tasks, the aggregated model of our AFCL remains invariant and\nidentical to that of centralized joint learning. Extensive experiments show the\nconsistent superiority of our AFCL over state-of-the-art baselines across\nvarious benchmark datasets and settings."}
{"id": "2505.12782", "pdf": "https://arxiv.org/pdf/2505.12782", "abs": "https://arxiv.org/abs/2505.12782", "authors": ["Kai Zhang", "Xingyu Chen", "Xiaofeng Zhang"], "title": "AdaToken-3D: Dynamic Spatial Gating for Efficient 3D Large Multimodal-Models Reasoning", "categories": ["cs.GR", "cs.CV", "cs.IR", "cs.IT", "math.IT"], "comment": null, "summary": "Large Multimodal Models (LMMs) have become a pivotal research focus in deep\nlearning, demonstrating remarkable capabilities in 3D scene understanding.\nHowever, current 3D LMMs employing thousands of spatial tokens for multimodal\nreasoning suffer from critical inefficiencies: excessive computational overhead\nand redundant information flows. Unlike 2D VLMs processing single images, 3D\nLMMs exhibit inherent architectural redundancy due to the heterogeneous\nmechanisms between spatial tokens and visual tokens. To address this challenge,\nwe propose AdaToken-3D, an adaptive spatial token optimization framework that\ndynamically prunes redundant tokens through spatial contribution analysis. Our\nmethod automatically tailors pruning strategies to different 3D LMM\narchitectures by quantifying token-level information flows via attention\npattern mining. Extensive experiments on LLaVA-3D (a 7B parameter 3D-LMM)\ndemonstrate that AdaToken-3D achieves 21\\% faster inference speed and 63\\%\nFLOPs reduction while maintaining original task accuracy. Beyond efficiency\ngains, this work systematically investigates redundancy patterns in multimodal\nspatial information flows through quantitative token interaction analysis. Our\nfindings reveal that over 60\\% of spatial tokens contribute minimally ($<$5\\%)\nto the final predictions, establishing theoretical foundations for efficient 3D\nmultimodal learning."}
{"id": "2505.13032", "pdf": "https://arxiv.org/pdf/2505.13032", "abs": "https://arxiv.org/abs/2505.13032", "authors": ["Ziyang Ma", "Yinghao Ma", "Yanqiao Zhu", "Chen Yang", "Yi-Wen Chao", "Ruiyang Xu", "Wenxi Chen", "Yuanzhe Chen", "Zhuo Chen", "Jian Cong", "Kai Li", "Keliang Li", "Siyou Li", "Xinfeng Li", "Xiquan Li", "Zheng Lian", "Yuzhe Liang", "Minghao Liu", "Zhikang Niu", "Tianrui Wang", "Yuping Wang", "Yuxuan Wang", "Yihao Wu", "Guanrou Yang", "Jianwei Yu", "Ruibin Yuan", "Zhisheng Zheng", "Ziya Zhou", "Haina Zhu", "Wei Xue", "Emmanouil Benetos", "Kai Yu", "Eng-Siong Chng", "Xie Chen"], "title": "MMAR: A Challenging Benchmark for Deep Reasoning in Speech, Audio, Music, and Their Mix", "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "comment": "Open-source at https://github.com/ddlBoJack/MMAR", "summary": "We introduce MMAR, a new benchmark designed to evaluate the deep reasoning\ncapabilities of Audio-Language Models (ALMs) across massive multi-disciplinary\ntasks. MMAR comprises 1,000 meticulously curated audio-question-answer\ntriplets, collected from real-world internet videos and refined through\niterative error corrections and quality checks to ensure high quality. Unlike\nexisting benchmarks that are limited to specific domains of sound, music, or\nspeech, MMAR extends them to a broad spectrum of real-world audio scenarios,\nincluding mixed-modality combinations of sound, music, and speech. Each\nquestion in MMAR is hierarchically categorized across four reasoning layers:\nSignal, Perception, Semantic, and Cultural, with additional sub-categories\nwithin each layer to reflect task diversity and complexity. To further foster\nresearch in this area, we annotate every question with a Chain-of-Thought (CoT)\nrationale to promote future advancements in audio reasoning. Each item in the\nbenchmark demands multi-step deep reasoning beyond surface-level understanding.\nMoreover, a part of the questions requires graduate-level perceptual and\ndomain-specific knowledge, elevating the benchmark's difficulty and depth. We\nevaluate MMAR using a broad set of models, including Large Audio-Language\nModels (LALMs), Large Audio Reasoning Models (LARMs), Omni Language Models\n(OLMs), Large Language Models (LLMs), and Large Reasoning Models (LRMs), with\naudio caption inputs. The performance of these models on MMAR highlights the\nbenchmark's challenging nature, and our analysis further reveals critical\nlimitations of understanding and reasoning capabilities among current models.\nWe hope MMAR will serve as a catalyst for future advances in this important but\nlittle-explored area."}
{"id": "2505.12247", "pdf": "https://arxiv.org/pdf/2505.12247", "abs": "https://arxiv.org/abs/2505.12247", "authors": ["Yinqiu Liu", "Guangyuan Liu", "Jiacheng Wang", "Ruichen Zhang", "Dusit Niyato", "Geng Sun", "Zehui Xiong", "Zhu Han"], "title": "LAMeTA: Intent-Aware Agentic Network Optimization via a Large AI Model-Empowered Two-Stage Approach", "categories": ["cs.NI", "cs.AI"], "comment": "13 pages", "summary": "Nowadays, Generative AI (GenAI) reshapes numerous domains by enabling\nmachines to create content across modalities. As GenAI evolves into autonomous\nagents capable of reasoning, collaboration, and interaction, they are\nincreasingly deployed on network infrastructures to serve humans automatically.\nThis emerging paradigm, known as the agentic network, presents new optimization\nchallenges due to the demand to incorporate subjective intents of human users\nexpressed in natural language. Traditional generic Deep Reinforcement Learning\n(DRL) struggles to capture intent semantics and adjust policies dynamically,\nthus leading to suboptimality. In this paper, we present LAMeTA, a Large AI\nModel (LAM)-empowered Two-stage Approach for intent-aware agentic network\noptimization. First, we propose Intent-oriented Knowledge Distillation (IoKD),\nwhich efficiently distills intent-understanding capabilities from\nresource-intensive LAMs to lightweight edge LAMs (E-LAMs) to serve end users.\nSecond, we develop Symbiotic Reinforcement Learning (SRL), integrating E-LAMs\nwith a policy-based DRL framework. In SRL, E-LAMs translate natural language\nuser intents into structured preference vectors that guide both state\nrepresentation and reward design. The DRL, in turn, optimizes the generative\nservice function chain composition and E-LAM selection based on real-time\nnetwork conditions, thus optimizing the subjective Quality-of-Experience (QoE).\nExtensive experiments conducted in an agentic network with 81 agents\ndemonstrate that IoKD reduces mean squared error in intent prediction by up to\n22.5%, while SRL outperforms conventional generic DRL by up to 23.5% in\nmaximizing intent-aware QoE."}
{"id": "2505.12835", "pdf": "https://arxiv.org/pdf/2505.12835", "abs": "https://arxiv.org/abs/2505.12835", "authors": ["Hengxing Cai", "Jinhan Dong", "Jingjun Tan", "Jingcheng Deng", "Sihang Li", "Zhifeng Gao", "Haidong Wang", "Zicheng Su", "Agachai Sumalee", "Renxin Zhong"], "title": "FlightGPT: Towards Generalizable and Interpretable UAV Vision-and-Language Navigation with Vision-Language Models", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Unmanned Aerial Vehicle (UAV) Vision-and-Language Navigation (VLN) is vital\nfor applications such as disaster response, logistics delivery, and urban\ninspection. However, existing methods often struggle with insufficient\nmultimodal fusion, weak generalization, and poor interpretability. To address\nthese challenges, we propose FlightGPT, a novel UAV VLN framework built upon\nVision-Language Models (VLMs) with powerful multimodal perception capabilities.\nWe design a two-stage training pipeline: first, Supervised Fine-Tuning (SFT)\nusing high-quality demonstrations to improve initialization and structured\nreasoning; then, Group Relative Policy Optimization (GRPO) algorithm, guided by\na composite reward that considers goal accuracy, reasoning quality, and format\ncompliance, to enhance generalization and adaptability. Furthermore, FlightGPT\nintroduces a Chain-of-Thought (CoT)-based reasoning mechanism to improve\ndecision interpretability. Extensive experiments on the city-scale dataset\nCityNav demonstrate that FlightGPT achieves state-of-the-art performance across\nall scenarios, with a 9.22\\% higher success rate than the strongest baseline in\nunseen environments. Our implementation is publicly available."}
{"id": "2505.13098", "pdf": "https://arxiv.org/pdf/2505.13098", "abs": "https://arxiv.org/abs/2505.13098", "authors": ["Lars-Peter Meyer", "Johannes Frey", "Desiree Heim", "Felix Brei", "Claus Stadler", "Kurt Junghanns", "Michael Martin"], "title": "LLM-KG-Bench 3.0: A Compass for SemanticTechnology Capabilities in the Ocean of LLMs", "categories": ["cs.AI", "cs.CL", "cs.DB"], "comment": "Peer reviewed publication at ESWC 2025 Resources Track", "summary": "Current Large Language Models (LLMs) can assist developing program code\nbeside many other things, but can they support working with Knowledge Graphs\n(KGs) as well? Which LLM is offering the best capabilities in the field of\nSemantic Web and Knowledge Graph Engineering (KGE)? Is this possible to\ndetermine without checking many answers manually? The LLM-KG-Bench framework in\nVersion 3.0 is designed to answer these questions. It consists of an extensible\nset of tasks for automated evaluation of LLM answers and covers different\naspects of working with semantic technologies. In this paper the LLM-KG-Bench\nframework is presented in Version 3 along with a dataset of prompts, answers\nand evaluations generated with it and several state-of-the-art LLMs.\nSignificant enhancements have been made to the framework since its initial\nrelease, including an updated task API that offers greater flexibility in\nhandling evaluation tasks, revised tasks, and extended support for various open\nmodels through the vllm library, among other improvements. A comprehensive\ndataset has been generated using more than 30 contemporary open and proprietary\nLLMs, enabling the creation of exemplary model cards that demonstrate the\nmodels' capabilities in working with RDF and SPARQL, as well as comparing their\nperformance on Turtle and JSON-LD RDF serialization tasks."}
{"id": "2505.12250", "pdf": "https://arxiv.org/pdf/2505.12250", "abs": "https://arxiv.org/abs/2505.12250", "authors": ["Chi Zhang", "Huaping Zhong", "Hongtao Li", "Chengliang Chai", "Jiawei Hong", "Yuhao Deng", "Jiacheng Wang", "Tian Tan", "Yizhou Yan", "Jiantao Qiu", "Ye Yuan", "Guoren Wang", "Conghui He", "Lei Cao"], "title": "Not All Documents Are What You Need for Extracting Instruction Tuning Data", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Instruction tuning improves the performance of large language models (LLMs),\nbut it heavily relies on high-quality training data. Recently, LLMs have been\nused to synthesize instruction data using seed question-answer (QA) pairs.\nHowever, these synthesized instructions often lack diversity and tend to be\nsimilar to the input seeds, limiting their applicability in real-world\nscenarios. To address this, we propose extracting instruction tuning data from\nweb corpora that contain rich and diverse knowledge. A naive solution is to\nretrieve domain-specific documents and extract all QA pairs from them, but this\nfaces two key challenges: (1) extracting all QA pairs using LLMs is\nprohibitively expensive, and (2) many extracted QA pairs may be irrelevant to\nthe downstream tasks, potentially degrading model performance. To tackle these\nissues, we introduce EQUAL, an effective and scalable data extraction framework\nthat iteratively alternates between document selection and high-quality QA pair\nextraction to enhance instruction tuning. EQUAL first clusters the document\ncorpus based on embeddings derived from contrastive learning, then uses a\nmulti-armed bandit strategy to efficiently identify clusters that are likely to\ncontain valuable QA pairs. This iterative approach significantly reduces\ncomputational cost while boosting model performance. Experiments on\nAutoMathText and StackOverflow across four downstream tasks show that EQUAL\nreduces computational costs by 5-10x and improves accuracy by 2.5 percent on\nLLaMA-3.1-8B and Mistral-7B"}
{"id": "2505.12836", "pdf": "https://arxiv.org/pdf/2505.12836", "abs": "https://arxiv.org/abs/2505.12836", "authors": ["Muhamed Kuric", "Martin Zach", "Andreas Habring", "Michael Unser", "Thomas Pock"], "title": "The Gaussian Latent Machine: Efficient Prior and Posterior Sampling for Inverse Problems", "categories": ["eess.IV", "cs.CV", "cs.LG", "stat.ML", "65C40, 65C05, 68U10, 65C60"], "comment": null, "summary": "We consider the problem of sampling from a product-of-experts-type model that\nencompasses many standard prior and posterior distributions commonly found in\nBayesian imaging. We show that this model can be easily lifted into a novel\nlatent variable model, which we refer to as a Gaussian latent machine. This\nleads to a general sampling approach that unifies and generalizes many existing\nsampling algorithms in the literature. Most notably, it yields a highly\nefficient and effective two-block Gibbs sampling approach in the general case,\nwhile also specializing to direct sampling algorithms in particular cases.\nFinally, we present detailed numerical experiments that demonstrate the\nefficiency and effectiveness of our proposed sampling approach across a wide\nrange of prior and posterior sampling problems from Bayesian imaging."}
{"id": "2505.13109", "pdf": "https://arxiv.org/pdf/2505.13109", "abs": "https://arxiv.org/abs/2505.13109", "authors": ["Guangda Liu", "Chengwei Li", "Zhenyu Ning", "Jing Lin", "Yiwu Yao", "Danning Ke", "Minyi Guo", "Jieru Zhao"], "title": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods."}
{"id": "2505.12254", "pdf": "https://arxiv.org/pdf/2505.12254", "abs": "https://arxiv.org/abs/2505.12254", "authors": ["Yiwei Ou", "Xiaobin Ren", "Ronggui Sun", "Guansong Gao", "Ziyi Jiang", "Kaiqi Zhao", "Manfredo Manfredini"], "title": "MMS-VPR: Multimodal Street-Level Visual Place Recognition Dataset and Benchmark", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Existing visual place recognition (VPR) datasets predominantly rely on\nvehicle-mounted imagery, lack multimodal diversity and underrepresent dense,\nmixed-use street-level spaces, especially in non-Western urban contexts. To\naddress these gaps, we introduce MMS-VPR, a large-scale multimodal dataset for\nstreet-level place recognition in complex, pedestrian-only environments. The\ndataset comprises 78,575 annotated images and 2,512 video clips captured across\n207 locations in a ~70,800 $\\mathrm{m}^2$ open-air commercial district in\nChengdu, China. Each image is labeled with precise GPS coordinates, timestamp,\nand textual metadata, and covers varied lighting conditions, viewpoints, and\ntimeframes. MMS-VPR follows a systematic and replicable data collection\nprotocol with minimal device requirements, lowering the barrier for scalable\ndataset creation. Importantly, the dataset forms an inherent spatial graph with\n125 edges, 81 nodes, and 1 subgraph, enabling structure-aware place\nrecognition. We further define two application-specific subsets --\nDataset_Edges and Dataset_Points -- to support fine-grained and graph-based\nevaluation tasks. Extensive benchmarks using conventional VPR models, graph\nneural networks, and multimodal baselines show substantial improvements when\nleveraging multimodal and structural cues. MMS-VPR facilitates future research\nat the intersection of computer vision, geospatial understanding, and\nmultimodal reasoning. The dataset is publicly available at\nhttps://huggingface.co/datasets/Yiwei-Ou/MMS-VPR."}
{"id": "2505.12863", "pdf": "https://arxiv.org/pdf/2505.12863", "abs": "https://arxiv.org/abs/2505.12863", "authors": ["Jongmin Jung", "Dongmin Kim", "Sihun Lee", "Seola Cho", "Hyungjoon Soh", "Irmak Bukey", "Chris Donahue", "Dasaem Jeong"], "title": "Unified Cross-modal Translation of Score Images, Symbolic Music, and Performance Audio", "categories": ["cs.SD", "cs.AI", "cs.CV", "eess.AS"], "comment": "Submitted to IEEE Transactions on Audio, Speech and Language\n  Processing (TASLPRO)", "summary": "Music exists in various modalities, such as score images, symbolic scores,\nMIDI, and audio. Translations between each modality are established as core\ntasks of music information retrieval, such as automatic music transcription\n(audio-to-MIDI) and optical music recognition (score image to symbolic score).\nHowever, most past work on multimodal translation trains specialized models on\nindividual translation tasks. In this paper, we propose a unified approach,\nwhere we train a general-purpose model on many translation tasks\nsimultaneously. Two key factors make this unified approach viable: a new\nlarge-scale dataset and the tokenization of each modality. Firstly, we propose\na new dataset that consists of more than 1,300 hours of paired audio-score\nimage data collected from YouTube videos, which is an order of magnitude larger\nthan any existing music modal translation datasets. Secondly, our unified\ntokenization framework discretizes score images, audio, MIDI, and MusicXML into\na sequence of tokens, enabling a single encoder-decoder Transformer to tackle\nmultiple cross-modal translation as one coherent sequence-to-sequence task.\nExperimental results confirm that our unified multitask model improves upon\nsingle-task baselines in several key areas, notably reducing the symbol error\nrate for optical music recognition from 24.58% to a state-of-the-art 13.67%,\nwhile similarly substantial improvements are observed across the other\ntranslation tasks. Notably, our approach achieves the first successful\nscore-image-conditioned audio generation, marking a significant breakthrough in\ncross-modal music generation."}
{"id": "2505.13126", "pdf": "https://arxiv.org/pdf/2505.13126", "abs": "https://arxiv.org/abs/2505.13126", "authors": ["Liancheng Gong", "Wang Zhu", "Jesse Thomason", "Li Zhang"], "title": "Zero-Shot Iterative Formalization and Planning in Partially Observable Environments", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "In planning, using LLMs not to predict plans but to formalize an environment\ninto the Planning Domain Definition Language (PDDL) has been shown to greatly\nimprove performance and control. While most work focused on fully observable\nenvironments, we tackle the more realistic and challenging partially observable\nenvironments where existing methods are incapacitated by the lack of complete\ninformation. We propose PDDLego+, a framework to iteratively formalize, plan,\ngrow, and refine PDDL representations in a zero-shot manner, without needing\naccess to any existing trajectories. On two textual simulated environments, we\nshow that PDDLego+ not only achieves superior performance, but also shows\nrobustness against problem complexity. We also show that the domain knowledge\ncaptured after a successful trial is interpretable and benefits future tasks."}
{"id": "2505.12260", "pdf": "https://arxiv.org/pdf/2505.12260", "abs": "https://arxiv.org/abs/2505.12260", "authors": ["Guangyuan Ma", "Yongliang Ma", "Xuanrui Gou", "Zhenpeng Su", "Ming Zhou", "Songlin Hu"], "title": "LightRetriever: A LLM-based Hybrid Retrieval Architecture with 1000x Faster Query Inference", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs)-based hybrid retrieval uses LLMs to encode\nqueries and documents into low-dimensional dense or high-dimensional sparse\nvectors. It retrieves documents relevant to search queries based on vector\nsimilarities. Documents are pre-encoded offline, while queries arrive in\nreal-time, necessitating an efficient online query encoder. Although LLMs\nsignificantly enhance retrieval capabilities, serving deeply parameterized LLMs\nslows down query inference throughput and increases demands for online\ndeployment resources. In this paper, we propose LightRetriever, a novel\nLLM-based hybrid retriever with extremely lightweight query encoders. Our\nmethod retains a full-sized LLM for document encoding, but reduces the workload\nof query encoding to no more than an embedding lookup. Compared to serving a\nfull-sized LLM on an H800 GPU, our approach achieves over a 1000x speedup for\nquery inference with GPU acceleration, and even a 20x speedup without GPU.\nExperiments on large-scale retrieval benchmarks demonstrate that our method\ngeneralizes well across diverse retrieval tasks, retaining an average of 95%\nfull-sized performance."}
{"id": "2505.12884", "pdf": "https://arxiv.org/pdf/2505.12884", "abs": "https://arxiv.org/abs/2505.12884", "authors": ["Yuanze Hu", "Zhaoxin Fan", "Xinyu Wang", "Gen Li", "Ye Qiu", "Zhichao Yang", "Wenjun Wu", "Kejian Wu", "Yifan Sun", "Xiaotie Deng", "Jin Dong"], "title": "TinyAlign: Boosting Lightweight Vision-Language Models by Mitigating Modal Alignment Bottlenecks", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Lightweight Vision-Language Models (VLMs) are indispensable for\nresource-constrained applications. The prevailing approach to aligning vision\nand language models involves freezing both the vision encoder and the language\nmodel while training small connector modules. However, this strategy heavily\ndepends on the intrinsic capabilities of the language model, which can be\nsuboptimal for lightweight models with limited representational capacity. In\nthis work, we investigate this alignment bottleneck through the lens of mutual\ninformation, demonstrating that the constrained capacity of the language model\ninherently limits the Effective Mutual Information (EMI) between multimodal\ninputs and outputs, thereby compromising alignment quality. To address this\nchallenge, we propose TinyAlign, a novel framework inspired by\nRetrieval-Augmented Generation, which strategically retrieves relevant context\nfrom a memory bank to enrich multimodal inputs and enhance their alignment.\nExtensive empirical evaluations reveal that TinyAlign significantly reduces\ntraining loss, accelerates convergence, and enhances task performance.\nRemarkably, it allows models to achieve baseline-level performance with only\n40\\% of the fine-tuning data, highlighting exceptional data efficiency. Our\nwork thus offers a practical pathway for developing more capable lightweight\nVLMs while introducing a fresh theoretical lens to better understand and\naddress alignment bottlenecks in constrained multimodal systems."}
{"id": "2505.13208", "pdf": "https://arxiv.org/pdf/2505.13208", "abs": "https://arxiv.org/abs/2505.13208", "authors": ["Colin Krawchuk", "Nikhil Khatri", "Neil John Ortega", "Dimitri Kartsaklis"], "title": "Efficient Generation of Parameterised Quantum Circuits from Large Texts", "categories": ["quant-ph", "cs.AI", "cs.CL"], "comment": null, "summary": "Quantum approaches to natural language processing (NLP) are redefining how\nlinguistic information is represented and processed. While traditional hybrid\nquantum-classical models rely heavily on classical neural networks, recent\nadvancements propose a novel framework, DisCoCirc, capable of directly encoding\nentire documents as parameterised quantum circuits (PQCs), besides enjoying\nsome additional interpretability and compositionality benefits. Following these\nideas, this paper introduces an efficient methodology for converting\nlarge-scale texts into quantum circuits using tree-like representations of\npregroup diagrams. Exploiting the compositional parallels between language and\nquantum mechanics, grounded in symmetric monoidal categories, our approach\nenables faithful and efficient encoding of syntactic and discourse\nrelationships in long and complex texts (up to 6410 words in our experiments)\nto quantum circuits. The developed system is provided to the community as part\nof the augmented open-source quantum NLP package lambeq Gen II."}
{"id": "2505.12269", "pdf": "https://arxiv.org/pdf/2505.12269", "abs": "https://arxiv.org/abs/2505.12269", "authors": ["Kerry Xiao", "Amy Zang"], "title": "Vague Knowledge: Evidence from Analyst Reports", "categories": ["econ.GN", "cs.AI", "cs.CL", "math.LO", "q-fin.EC", "q-fin.GN", "03B48, 03B65, 03E02, 03E15, 03E72, 18E45, 28A05, 62F15, 68T01,\n  68T35, 68T50, 91G30,", "F.4; I.2.3; I.2.4; I.2.7; J.1; J.4; J.5"], "comment": null, "summary": "People in the real world often possess vague knowledge of future payoffs, for\nwhich quantification is not feasible or desirable. We argue that language, with\ndiffering ability to convey vague information, plays an important but less\nknown-role in subjective expectations. Empirically, we find that in their\nreports, analysts include useful information in linguistic expressions but not\nnumerical forecasts. Specifically, the textual tone of analyst reports has\npredictive power for forecast errors and subsequent revisions in numerical\nforecasts, and this relation becomes stronger when analyst's language is\nvaguer, when uncertainty is higher, and when analysts are busier. Overall, our\ntheory and evidence suggest that some useful information is vaguely known and\nonly communicated through language."}
{"id": "2505.12887", "pdf": "https://arxiv.org/pdf/2505.12887", "abs": "https://arxiv.org/abs/2505.12887", "authors": ["Junzhi Ning", "Cheng Tang", "Kaijin Zhou", "Diping Song", "Lihao Liu", "Ming Hu", "Wei Li", "Yanzhou Su", "Tianbing Li", "Jiyao Liu", "Yejin", "Sheng Zhang", "Yuanfeng Ji", "Junjun He"], "title": "RetinaLogos: Fine-Grained Synthesis of High-Resolution Retinal Images Through Captions", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "The scarcity of high-quality, labelled retinal imaging data, which presents a\nsignificant challenge in the development of machine learning models for\nophthalmology, hinders progress in the field. To synthesise Colour Fundus\nPhotographs (CFPs), existing methods primarily relying on predefined disease\nlabels face significant limitations. However, current methods remain limited,\nthus failing to generate images for broader categories with diverse and\nfine-grained anatomical structures. To overcome these challenges, we first\nintroduce an innovative pipeline that creates a large-scale, synthetic\nCaption-CFP dataset comprising 1.4 million entries, called RetinaLogos-1400k.\nSpecifically, RetinaLogos-1400k uses large language models (LLMs) to describe\nretinal conditions and key structures, such as optic disc configuration,\nvascular distribution, nerve fibre layers, and pathological features.\nFurthermore, based on this dataset, we employ a novel three-step training\nframework, called RetinaLogos, which enables fine-grained semantic control over\nretinal images and accurately captures different stages of disease progression,\nsubtle anatomical variations, and specific lesion types. Extensive experiments\ndemonstrate state-of-the-art performance across multiple datasets, with 62.07%\nof text-driven synthetic images indistinguishable from real ones by\nophthalmologists. Moreover, the synthetic data improves accuracy by 10%-25% in\ndiabetic retinopathy grading and glaucoma detection, thereby providing a\nscalable solution to augment ophthalmic datasets."}
{"id": "2505.13227", "pdf": "https://arxiv.org/pdf/2505.13227", "abs": "https://arxiv.org/abs/2505.13227", "authors": ["Tianbao Xie", "Jiaqi Deng", "Xiaochuan Li", "Junlin Yang", "Haoyuan Wu", "Jixuan Chen", "Wenjing Hu", "Xinyuan Wang", "Yuhui Xu", "Zekun Wang", "Yiheng Xu", "Junli Wang", "Doyen Sahoo", "Tao Yu", "Caiming Xiong"], "title": "Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": "49 pages, 13 figures", "summary": "Graphical user interface (GUI) grounding, the ability to map natural language\ninstructions to specific actions on graphical user interfaces, remains a\ncritical bottleneck in computer use agent development. Current benchmarks\noversimplify grounding tasks as short referring expressions, failing to capture\nthe complexity of real-world interactions that require software commonsense,\nlayout understanding, and fine-grained manipulation capabilities. To address\nthese limitations, we introduce OSWorld-G, a comprehensive benchmark comprising\n564 finely annotated samples across diverse task types including text matching,\nelement recognition, layout understanding, and precise manipulation.\nAdditionally, we synthesize and release the largest computer use grounding\ndataset Jedi, which contains 4 million examples through multi-perspective\ndecoupling of tasks. Our multi-scale models trained on Jedi demonstrate its\neffectiveness by outperforming existing approaches on ScreenSpot-v2,\nScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved\ngrounding with Jedi directly enhances agentic capabilities of general\nfoundation models on complex computer tasks, improving from 5% to 27% on\nOSWorld. Through detailed ablation studies, we identify key factors\ncontributing to grounding performance and verify that combining specialized\ndata for different interface elements enables compositional generalization to\nnovel interfaces. All benchmark, data, checkpoints, and code are open-sourced\nand available at https://osworld-grounding.github.io."}
{"id": "2505.12275", "pdf": "https://arxiv.org/pdf/2505.12275", "abs": "https://arxiv.org/abs/2505.12275", "authors": ["Wen-Chao Hu", "Qi-Jie Li", "Lin-Han Jia", "Cunjing Ge", "Yu-Feng Li", "Yuan Jiang", "Zhi-Hua Zhou"], "title": "Curriculum Abductive Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Abductive Learning (ABL) integrates machine learning with logical reasoning\nin a loop: a learning model predicts symbolic concept labels from raw inputs,\nwhich are revised through abduction using domain knowledge and then fed back\nfor retraining. However, due to the nondeterminism of abduction, the training\nprocess often suffers from instability, especially when the knowledge base is\nlarge and complex, resulting in a prohibitively large abduction space. While\nprior works focus on improving candidate selection within this space, they\ntypically treat the knowledge base as a static black box. In this work, we\npropose Curriculum Abductive Learning (C-ABL), a method that explicitly\nleverages the internal structure of the knowledge base to address the ABL\ntraining challenges. C-ABL partitions the knowledge base into a sequence of\nsub-bases, progressively introduced during training. This reduces the abduction\nspace throughout training and enables the model to incorporate logic in a\nstepwise, smooth way. Experiments across multiple tasks show that C-ABL\noutperforms previous ABL implementations, significantly improves training\nstability, convergence speed, and final accuracy, especially under complex\nknowledge setting."}
{"id": "2505.12944", "pdf": "https://arxiv.org/pdf/2505.12944", "abs": "https://arxiv.org/abs/2505.12944", "authors": ["Jan Hagnberger", "Daniel Musekamp", "Mathias Niepert"], "title": "CALM-PDE: Continuous and Adaptive Convolutions for Latent Space Modeling of Time-dependent PDEs", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.NE", "physics.comp-ph"], "comment": null, "summary": "Solving time-dependent Partial Differential Equations (PDEs) using a densely\ndiscretized spatial domain is a fundamental problem in various scientific and\nengineering disciplines, including modeling climate phenomena and fluid\ndynamics. However, performing these computations directly in the physical space\noften incurs significant computational costs. To address this issue, several\nneural surrogate models have been developed that operate in a compressed latent\nspace to solve the PDE. While these approaches reduce computational complexity,\nthey often use Transformer-based attention mechanisms to handle irregularly\nsampled domains, resulting in increased memory consumption. In contrast,\nconvolutional neural networks allow memory-efficient encoding and decoding but\nare limited to regular discretizations. Motivated by these considerations, we\npropose CALM-PDE, a model class that efficiently solves arbitrarily discretized\nPDEs in a compressed latent space. We introduce a novel continuous\nconvolution-based encoder-decoder architecture that uses an\nepsilon-neighborhood-constrained kernel and learns to apply the convolution\noperator to adaptive and optimized query points. We demonstrate the\neffectiveness of CALM-PDE on a diverse set of PDEs with both regularly and\nirregularly sampled spatial domains. CALM-PDE is competitive with or\noutperforms existing baseline methods while offering significant improvements\nin memory and inference time efficiency compared to Transformer-based methods."}
{"id": "2505.13237", "pdf": "https://arxiv.org/pdf/2505.13237", "abs": "https://arxiv.org/abs/2505.13237", "authors": ["Chih-Kai Yang", "Neo Ho", "Yen-Ting Piao", "Hung-yi Lee"], "title": "SAKURA: On the Multi-hop Reasoning of Large Audio-Language Models Based on Speech and Audio Information", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted to Interspeech 2025", "summary": "Large audio-language models (LALMs) extend the large language models with\nmultimodal understanding in speech, audio, etc. While their performances on\nspeech and audio-processing tasks are extensively studied, their reasoning\nabilities remain underexplored. Particularly, their multi-hop reasoning, the\nability to recall and integrate multiple facts, lacks systematic evaluation.\nExisting benchmarks focus on general speech and audio-processing tasks,\nconversational abilities, and fairness but overlook this aspect. To bridge this\ngap, we introduce SAKURA, a benchmark assessing LALMs' multi-hop reasoning\nbased on speech and audio information. Results show that LALMs struggle to\nintegrate speech/audio representations for multi-hop reasoning, even when they\nextract the relevant information correctly, highlighting a fundamental\nchallenge in multimodal reasoning. Our findings expose a critical limitation in\nLALMs, offering insights and resources for future research."}
{"id": "2505.12287", "pdf": "https://arxiv.org/pdf/2505.12287", "abs": "https://arxiv.org/abs/2505.12287", "authors": ["Linghan Huang", "Haolin Jin", "Zhaoge Bi", "Pengyue Yang", "Peizhou Zhao", "Taozhao Chen", "Xiongfei Wu", "Lei Ma", "Huaming Chen"], "title": "The Tower of Babel Revisited: Multilingual Jailbreak Prompts on Closed-Source Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have seen widespread applications across various\ndomains, yet remain vulnerable to adversarial prompt injections. While most\nexisting research on jailbreak attacks and hallucination phenomena has focused\nprimarily on open-source models, we investigate the frontier of closed-source\nLLMs under multilingual attack scenarios. We present a first-of-its-kind\nintegrated adversarial framework that leverages diverse attack techniques to\nsystematically evaluate frontier proprietary solutions, including GPT-4o,\nDeepSeek-R1, Gemini-1.5-Pro, and Qwen-Max. Our evaluation spans six categories\nof security contents in both English and Chinese, generating 38,400 responses\nacross 32 types of jailbreak attacks. Attack success rate (ASR) is utilized as\nthe quantitative metric to assess performance from three dimensions: prompt\ndesign, model architecture, and language environment. Our findings suggest that\nQwen-Max is the most vulnerable, while GPT-4o shows the strongest defense.\nNotably, prompts in Chinese consistently yield higher ASRs than their English\ncounterparts, and our novel Two-Sides attack technique proves to be the most\neffective across all models. This work highlights a dire need for\nlanguage-aware alignment and robust cross-lingual defenses in LLMs, and we hope\nit will inspire researchers, developers, and policymakers toward more robust\nand inclusive AI systems."}
{"id": "2505.12963", "pdf": "https://arxiv.org/pdf/2505.12963", "abs": "https://arxiv.org/abs/2505.12963", "authors": ["Maksim I. Ivanov", "Olga E. Mendybaeva", "Yuri E. Karyakin", "Igor N. Glukhikh", "Aleksey V. Lebedev"], "title": "Segmentation of temporomandibular joint structures on mri images using neural networks for diagnosis of pathologies", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "10 pages, 10 figures", "summary": "This article explores the use of artificial intelligence for the diagnosis of\npathologies of the temporomandibular joint (TMJ), in particular, for the\nsegmentation of the articular disc on MRI images. The relevance of the work is\ndue to the high prevalence of TMJ pathologies, as well as the need to improve\nthe accuracy and speed of diagnosis in medical institutions. During the study,\nthe existing solutions (Diagnocat, MandSeg) were analyzed, which, as a result,\nare not suitable for studying the articular disc due to the orientation towards\nbone structures. To solve the problem, an original dataset was collected from\n94 images with the classes \"temporomandibular joint\" and \"jaw\". To increase the\namount of data, augmentation methods were used. After that, the models of\nU-Net, YOLOv8n, YOLOv11n and Roboflow neural networks were trained and\ncompared. The evaluation was carried out according to the Dice Score,\nPrecision, Sensitivity, Specificity, and Mean Average Precision metrics. The\nresults confirm the potential of using the Roboflow model for segmentation of\nthe temporomandibular joint. In the future, it is planned to develop an\nalgorithm for measuring the distance between the jaws and determining the\nposition of the articular disc, which will improve the diagnosis of TMJ\npathologies."}
{"id": "2505.13308", "pdf": "https://arxiv.org/pdf/2505.13308", "abs": "https://arxiv.org/abs/2505.13308", "authors": ["Hengli Li", "Chenxi Li", "Tong Wu", "Xuekai Zhu", "Yuxuan Wang", "Zhaoxin Yu", "Eric Hanchen Jiang", "Song-Chun Zhu", "Zixia Jia", "Ying Nian Wu", "Zilong Zheng"], "title": "Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient in Latent Space", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reasoning ability, a core component of human intelligence, continues to pose\na significant challenge for Large Language Models (LLMs) in the pursuit of AGI.\nAlthough model performance has improved under the training scaling law,\nsignificant challenges remain, particularly with respect to training\nalgorithms, such as catastrophic forgetting, and the limited availability of\nnovel training data. As an alternative, test-time scaling enhances reasoning\nperformance by increasing test-time computation without parameter updating.\nUnlike prior methods in this paradigm focused on token space, we propose\nleveraging latent space for more effective reasoning and better adherence to\nthe test-time scaling law. We introduce LatentSeek, a novel framework that\nenhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA)\nwithin the model's latent space. Specifically, LatentSeek leverages policy\ngradient to iteratively update latent representations, guided by self-generated\nreward signals. LatentSeek is evaluated on a range of reasoning benchmarks,\nincluding GSM8K, MATH-500, and AIME2024, across multiple LLM architectures.\nResults show that LatentSeek consistently outperforms strong baselines, such as\nChain-of-Thought prompting and fine-tuning-based methods. Furthermore, our\nanalysis demonstrates that LatentSeek is highly efficient, typically converging\nwithin a few iterations for problems of average complexity, while also\nbenefiting from additional iterations, thereby highlighting the potential of\ntest-time scaling in the latent space. These findings position LatentSeek as a\nlightweight, scalable, and effective solution for enhancing the reasoning\ncapabilities of LLMs."}
{"id": "2505.12292", "pdf": "https://arxiv.org/pdf/2505.12292", "abs": "https://arxiv.org/abs/2505.12292", "authors": ["Boxun Xu", "Richard Boone", "Peng Li"], "title": "SpikeX: Exploring Accelerator Architecture and Network-Hardware Co-Optimization for Sparse Spiking Neural Networks", "categories": ["cs.NE", "cs.AI", "cs.AR"], "comment": "The paper has been accepted by IEEE TCAD", "summary": "Spiking Neural Networks (SNNs) are promising biologically plausible models of\ncomputation which utilize a spiking binary activation function similar to that\nof biological neurons. SNNs are well positioned to process spatiotemporal data,\nand are advantageous in ultra-low power and real-time processing. Despite a\nlarge body of work on conventional artificial neural network accelerators, much\nless attention has been given to efficient SNN hardware accelerator design. In\nparticular, SNNs exhibit inherent unstructured spatial and temporal firing\nsparsity, an opportunity yet to be fully explored for great hardware processing\nefficiency. In this work, we propose a novel systolic-array SNN accelerator\narchitecture, called SpikeX, to take on the challenges and opportunities\nstemming from unstructured sparsity while taking into account the unique\ncharacteristics of spike-based computation. By developing an efficient dataflow\ntargeting expensive multi-bit weight data movements, SpikeX reduces memory\naccess and increases data sharing and hardware utilization for computations\nspanning across both time and space, thereby significantly improving energy\nefficiency and inference latency. Furthermore, recognizing the importance of\nSNN network and hardware co-design, we develop a co-optimization methodology\nfacilitating not only hardware-aware SNN training but also hardware accelerator\narchitecture search, allowing joint network weight parameter optimization and\naccelerator architectural reconfiguration. This end-to-end network/accelerator\nco-design approach offers a significant reduction of 15.1x-150.87x in\nenergy-delay-product(EDP) without comprising model accuracy."}
{"id": "2505.12978", "pdf": "https://arxiv.org/pdf/2505.12978", "abs": "https://arxiv.org/abs/2505.12978", "authors": ["Yinzhe Wu", "Jiahao Huang", "Fanwen Wang", "Mengze Gao", "Congyu Liao", "Guang Yang", "Kawin Setsompop"], "title": "Enhancing Diffusion-Weighted Images (DWI) for Diffusion MRI: Is it Enough without Non-Diffusion-Weighted B=0 Reference?", "categories": ["eess.IV", "cs.CV"], "comment": "IEEE ISBI 2025", "summary": "Diffusion MRI (dMRI) is essential for studying brain microstructure, but\nhigh-resolution imaging remains challenging due to the inherent trade-offs\nbetween acquisition time and signal-to-noise ratio (SNR). Conventional methods\noften optimize only the diffusion-weighted images (DWIs) without considering\ntheir relationship with the non-diffusion-weighted (b=0) reference images.\nHowever, calculating diffusion metrics, such as the apparent diffusion\ncoefficient (ADC) and diffusion tensor with its derived metrics like fractional\nanisotropy (FA) and mean diffusivity (MD), relies on the ratio between each DWI\nand the b=0 image, which is crucial for clinical observation and diagnostics.\nIn this study, we demonstrate that solely enhancing DWIs using a conventional\npixel-wise mean squared error (MSE) loss is insufficient, as the error in ratio\nbetween generated DWIs and b=0 diverges. We propose a novel ratio loss, defined\nas the MSE loss between the predicted and ground-truth log of DWI/b=0 ratios.\nOur results show that incorporating the ratio loss significantly improves the\nconvergence of this ratio error, achieving lower ratio MSE and slightly\nenhancing the peak signal-to-noise ratio (PSNR) of generated DWIs. This leads\nto improved dMRI super-resolution and better preservation of b=0 ratio-based\nfeatures for the derivation of diffusion metrics."}
{"id": "2505.13380", "pdf": "https://arxiv.org/pdf/2505.13380", "abs": "https://arxiv.org/abs/2505.13380", "authors": ["Nam V. Nguyen", "Huy Nguyen", "Quang Pham", "Van Nguyen", "Savitha Ramasamy", "Nhat Ho"], "title": "CompeteSMoE -- Statistically Guaranteed Mixture of Experts Training via Competition", "categories": ["cs.AI", "cs.CL"], "comment": "52 pages. This work is an improved version of the previous study at\n  arXiv:2402.02526", "summary": "Sparse mixture of experts (SMoE) offers an appealing solution to scale up the\nmodel complexity beyond the mean of increasing the network's depth or width.\nHowever, we argue that effective SMoE training remains challenging because of\nthe suboptimal routing process where experts that perform computation do not\ndirectly contribute to the routing process. In this work, we propose\ncompetition, a novel mechanism to route tokens to experts with the highest\nneural response. Theoretically, we show that the competition mechanism enjoys a\nbetter sample efficiency than the traditional softmax routing. Furthermore, we\ndevelop CompeteSMoE, a simple yet effective algorithm to train large language\nmodels by deploying a router to learn the competition policy, thus enjoying\nstrong performances at a low training overhead. Our extensive empirical\nevaluations on both the visual instruction tuning and language pre-training\ntasks demonstrate the efficacy, robustness, and scalability of CompeteSMoE\ncompared to state-of-the-art SMoE strategies. We have made the implementation\navailable at: https://github.com/Fsoft-AIC/CompeteSMoE. This work is an\nimproved version of the previous study at arXiv:2402.02526"}
{"id": "2505.12296", "pdf": "https://arxiv.org/pdf/2505.12296", "abs": "https://arxiv.org/abs/2505.12296", "authors": ["Haiyu Deng", "Yanna Jiang", "Guangsheng Yu", "Qin Wang", "Xu Wang", "Baihe Ma", "Wei Ni", "Ren Ping Liu"], "title": "PoLO: Proof-of-Learning and Proof-of-Ownership at Once with Chained Watermarking", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "Machine learning models are increasingly shared and outsourced, raising\nrequirements of verifying training effort (Proof-of-Learning, PoL) to ensure\nclaimed performance and establishing ownership (Proof-of-Ownership, PoO) for\ntransactions. When models are trained by untrusted parties, PoL and PoO must be\nenforced together to enable protection, attribution, and compensation. However,\nexisting studies typically address them separately, which not only weakens\nprotection against forgery and privacy breaches but also leads to high\nverification overhead.\n  We propose PoLO, a unified framework that simultaneously achieves PoL and PoO\nusing chained watermarks. PoLO splits the training process into fine-grained\ntraining shards and embeds a dedicated watermark in each shard. Each watermark\nis generated using the hash of the preceding shard, certifying the training\nprocess of the preceding shard. The chained structure makes it computationally\ndifficult to forge any individual part of the whole training process. The\ncomplete set of watermarks serves as the PoL, while the final watermark\nprovides the PoO. PoLO offers more efficient and privacy-preserving\nverification compared to the vanilla PoL solutions that rely on gradient-based\ntrajectory tracing and inadvertently expose training data during verification,\nwhile maintaining the same level of ownership assurance of watermark-based PoO\nschemes. Our evaluation shows that PoLO achieves 99% watermark detection\naccuracy for ownership verification, while preserving data privacy and cutting\nverification costs to just 1.5-10% of traditional methods. Forging PoLO demands\n1.1-4x more resources than honest proof generation, with the original proof\nretaining over 90% detection accuracy even after attacks."}
{"id": "2505.12999", "pdf": "https://arxiv.org/pdf/2505.12999", "abs": "https://arxiv.org/abs/2505.12999", "authors": ["Lorena Garcia-Foncillas Macias", "Aaron Kujawa", "Aya Elshalakany", "Jonathan Shapey", "Tom Vercauteren"], "title": "A generalisable head MRI defacing pipeline: Evaluation on 2,566 meningioma scans", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Reliable MRI defacing techniques to safeguard patient privacy while\npreserving brain anatomy are critical for research collaboration. Existing\nmethods often struggle with incomplete defacing or degradation of brain tissue\nregions. We present a robust, generalisable defacing pipeline for\nhigh-resolution MRI that integrates atlas-based registration with brain\nmasking. Our method was evaluated on 2,566 heterogeneous clinical scans for\nmeningioma and achieved a 99.92 per cent success rate (2,564/2,566) upon visual\ninspection. Excellent anatomical preservation is demonstrated with a Dice\nsimilarity coefficient of 0.9975 plus or minus 0.0023 between brain masks\nautomatically extracted from the original and defaced volumes. Source code is\navailable at https://github.com/cai4cai/defacing_pipeline."}
{"id": "2505.13393", "pdf": "https://arxiv.org/pdf/2505.13393", "abs": "https://arxiv.org/abs/2505.13393", "authors": ["Christopher K. Frantz"], "title": "IG Parser: A Software Package for the Encoding of Institutional Statements using the Institutional Grammar", "categories": ["cs.MA", "cs.AI", "cs.CL", "68T30, 68T50", "E.2; H.1.0; I.7.2; I.6.5; K.4.1"], "comment": "24 pages", "summary": "This article provides an overview of IG Parser, a software that facilitates\nqualitative content analysis of formal (e.g., legal) rules or informal (e.g.,\nsocio-normative) norms, and strategies (such as conventions) -- referred to as\n\\emph{institutions} -- that govern social systems and operate configurally to\ndescribe \\emph{institutional systems}. To this end, the IG Parser employs a\ndistinctive syntax that ensures rigorous encoding of natural language, while\nautomating the transformation into various formats that support the downstream\nanalysis using diverse analytical techniques. The conceptual core of the IG\nParser is an associated syntax, IG Script, that operationalizes the conceptual\nfoundations of the Institutional Grammar, and more specifically Institutional\nGrammar 2.0, an analytical paradigm for institutional analysis. This article\npresents the IG Parser, including its conceptual foundations, syntactic\nspecification of IG Script, alongside architectural principles. This\nintroduction is augmented with selective illustrative examples that highlight\nthe use and benefit associated with the tool."}
{"id": "2505.12298", "pdf": "https://arxiv.org/pdf/2505.12298", "abs": "https://arxiv.org/abs/2505.12298", "authors": ["Amal Lahchim", "Lazar Davic"], "title": "Attention-Enhanced U-Net for Accurate Segmentation of COVID-19 Infected Lung Regions in CT Scans", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "14 pages, 9 figures, created using Google Colab and PyTorch. Compares\n  segmentation models for COVID-19 CT data", "summary": "In this study, we propose a robust methodology for automatic segmentation of\ninfected lung regions in COVID-19 CT scans using convolutional neural networks.\nThe approach is based on a modified U-Net architecture enhanced with attention\nmechanisms, data augmentation, and postprocessing techniques. It achieved a\nDice coefficient of 0.8658 and mean IoU of 0.8316, outperforming other methods.\nThe dataset was sourced from public repositories and augmented for diversity.\nResults demonstrate superior segmentation performance. Future work includes\nexpanding the dataset, exploring 3D segmentation, and preparing the model for\nclinical deployment."}
{"id": "2505.13081", "pdf": "https://arxiv.org/pdf/2505.13081", "abs": "https://arxiv.org/abs/2505.13081", "authors": ["Xiaoyu Yang", "Jie Lu", "En Yu"], "title": "Walking the Tightrope: Disentangling Beneficial and Detrimental Drifts in Non-Stationary Custom-Tuning", "categories": ["cs.LG", "cs.CV"], "comment": "17 pages, 5figures", "summary": "This paper uncovers a critical yet overlooked phenomenon in multi-modal large\nlanguage models (MLLMs): detrimental concept drift within chain-of-thought\n(CoT) reasoning during non-stationary reinforcement fine-tuning (RFT), where\nreasoning token distributions evolve unpredictably, thereby introducing\nsignificant biases in final predictions. To address this, we are pioneers in\nestablishing the theoretical bridge between concept drift theory and RFT\nprocesses by formalizing CoT's autoregressive token streams as non-stationary\ndistributions undergoing arbitrary temporal shifts. Leveraging this framework,\nwe propose a novel counterfact-aware RFT that systematically decouples\nbeneficial distribution adaptation from harmful concept drift through concept\ngraph-empowered LLM experts generating counterfactual reasoning trajectories.\nOur solution, Counterfactual Preference Optimization (CPO), enables stable RFT\nin non-stationary environments, particularly within the medical domain, through\ncustom-tuning of counterfactual-aware preference alignment. Extensive\nexperiments demonstrate our superior performance of robustness, generalization\nand coordination within RFT. Besides, we also contributed a large-scale dataset\nCXR-CounterFact (CCF), comprising 320,416 meticulously curated counterfactual\nreasoning trajectories derived from MIMIC-CXR. Our code and data are public."}
{"id": "2505.13398", "pdf": "https://arxiv.org/pdf/2505.13398", "abs": "https://arxiv.org/abs/2505.13398", "authors": ["Matan Abudy", "Orr Well", "Emmanuel Chemla", "Roni Katzir", "Nur Lan"], "title": "A Minimum Description Length Approach to Regularization in Neural Networks", "categories": ["cs.LG", "cs.CL"], "comment": "9 pages", "summary": "State-of-the-art neural networks can be trained to become remarkable\nsolutions to many problems. But while these architectures can express symbolic,\nperfect solutions, trained models often arrive at approximations instead. We\nshow that the choice of regularization method plays a crucial role: when\ntrained on formal languages with standard regularization ($L_1$, $L_2$, or\nnone), expressive architectures not only fail to converge to correct solutions\nbut are actively pushed away from perfect initializations. In contrast,\napplying the Minimum Description Length (MDL) principle to balance model\ncomplexity with data fit provides a theoretically grounded regularization\nmethod. Using MDL, perfect solutions are selected over approximations,\nindependently of the optimization algorithm. We propose that unlike existing\nregularization techniques, MDL introduces the appropriate inductive bias to\neffectively counteract overfitting and promote generalization."}
{"id": "2505.12299", "pdf": "https://arxiv.org/pdf/2505.12299", "abs": "https://arxiv.org/abs/2505.12299", "authors": ["Kun Huang", "Weikai Xu", "Yuxuan Liu", "Quandong Wang", "Pengzhi Gao", "Wei Liu", "Jian Luan", "Bin Wang", "Bo An"], "title": "Enhance Mobile Agents Thinking Process Via Iterative Preference Learning", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 8 figures, 7 tables", "summary": "The Chain of Action-Planning Thoughts (CoaT) paradigm has been shown to\nimprove the reasoning performance of VLM-based mobile agents in GUI tasks.\nHowever, the scarcity of diverse CoaT trajectories limits the expressiveness\nand generalization ability of such agents. While self-training is commonly\nemployed to address data scarcity, existing approaches either overlook the\ncorrectness of intermediate reasoning steps or depend on expensive\nprocess-level annotations to construct process reward models (PRM). To address\nthe above problems, we propose an Iterative Preference Learning (IPL) that\nconstructs a CoaT-tree through interative sampling, scores leaf nodes using\nrule-based reward, and backpropagates feedback to derive Thinking-level Direct\nPreference Optimization (T-DPO) pairs. To prevent overfitting during warm-up\nsupervised fine-tuning, we further introduce a three-stage instruction\nevolution, which leverages GPT-4o to generate diverse Q\\&A pairs based on real\nmobile UI screenshots, enhancing both generality and layout understanding.\nExperiments on three standard Mobile GUI-agent benchmarks demonstrate that our\nagent MobileIPL outperforms strong baselines, including continual pretraining\nmodels such as OS-ATLAS and UI-TARS. It achieves state-of-the-art performance\nacross three standard Mobile GUI-Agents benchmarks and shows strong\ngeneralization to out-of-domain scenarios."}
{"id": "2505.13152", "pdf": "https://arxiv.org/pdf/2505.13152", "abs": "https://arxiv.org/abs/2505.13152", "authors": ["Jonas Brenig", "Radu Timofte"], "title": "Higher fidelity perceptual image and video compression with a latent conditioned residual denoising diffusion model", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted at AIM Workshop 2024 at ECCV 2024", "summary": "Denoising diffusion models achieved impressive results on several image\ngeneration tasks often outperforming GAN based models. Recently, the generative\ncapabilities of diffusion models have been employed for perceptual image\ncompression, such as in CDC. A major drawback of these diffusion-based methods\nis that, while producing impressive perceptual quality images they are dropping\nin fidelity/increasing the distortion to the original uncompressed images when\ncompared with other traditional or learned image compression schemes aiming for\nfidelity. In this paper, we propose a hybrid compression scheme optimized for\nperceptual quality, extending the approach of the CDC model with a decoder\nnetwork in order to reduce the impact on distortion metrics such as PSNR. After\nusing the decoder network to generate an initial image, optimized for\ndistortion, the latent conditioned diffusion model refines the reconstruction\nfor perceptual quality by predicting the residual. On standard benchmarks, we\nachieve up to +2dB PSNR fidelity improvements while maintaining comparable\nLPIPS and FID perceptual scores when compared with CDC. Additionally, the\napproach is easily extensible to video compression, where we achieve similar\nresults."}
{"id": "2505.13408", "pdf": "https://arxiv.org/pdf/2505.13408", "abs": "https://arxiv.org/abs/2505.13408", "authors": ["Jinhe Bi", "Danqi Yan", "Yifan Wang", "Wenke Huang", "Haokun Chen", "Guancheng Wan", "Mang Ye", "Xun Xiao", "Hinrich Schuetze", "Volker Tresp", "Yunpu Ma"], "title": "CoT-Kinetics: A Theoretical Modeling Assessing LRM Reasoning Process", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recent Large Reasoning Models significantly improve the reasoning ability of\nLarge Language Models by learning to reason, exhibiting the promising\nperformance in solving complex tasks. LRMs solve tasks that require complex\nreasoning by explicitly generating reasoning trajectories together with\nanswers. Nevertheless, judging the quality of such an output answer is not easy\nbecause only considering the correctness of the answer is not enough and the\nsoundness of the reasoning trajectory part matters as well. Logically, if the\nsoundness of the reasoning part is poor, even if the answer is correct, the\nconfidence of the derived answer should be low. Existing methods did consider\njointly assessing the overall output answer by taking into account the\nreasoning part, however, their capability is still not satisfactory as the\ncausal relationship of the reasoning to the concluded answer cannot properly\nreflected. In this paper, inspired by classical mechanics, we present a novel\napproach towards establishing a CoT-Kinetics energy equation. Specifically, our\nCoT-Kinetics energy equation formulates the token state transformation process,\nwhich is regulated by LRM internal transformer layers, as like a particle\nkinetics dynamics governed in a mechanical field. Our CoT-Kinetics energy\nassigns a scalar score to evaluate specifically the soundness of the reasoning\nphase, telling how confident the derived answer could be given the evaluated\nreasoning. As such, the LRM's overall output quality can be accurately\nmeasured, rather than a coarse judgment (e.g., correct or incorrect) anymore."}
{"id": "2505.12304", "pdf": "https://arxiv.org/pdf/2505.12304", "abs": "https://arxiv.org/abs/2505.12304", "authors": ["Li Ni", "Hengkai Xu", "Lin Mu", "Yiwen Zhang", "Wenjian Luo"], "title": "Pre-trained Prompt-driven Community Search", "categories": ["cs.SI", "cs.AI"], "comment": "11 pages, 3 figures", "summary": "The \"pre-train, prompt\" paradigm is widely adopted in various graph-based\ntasks and has shown promising performance in community detection. Most existing\nsemi-supervised community detection algorithms detect communities based on\nknown ones, and the detected communities typically do not contain the given\nquery node. Therefore, they are not suitable for searching the community of a\ngiven node. Motivated by this, we adopt this paradigm into the semi-supervised\ncommunity search for the first time and propose Pre-trained Prompt-driven\nCommunity Search (PPCS), a novel model designed to enhance search accuracy and\nefficiency. PPCS consists of three main components: node encoding, sample\ngeneration, and prompt-driven fine-tuning. Specifically, the node encoding\ncomponent employs graph neural networks to learn local structural patterns of\nnodes in a graph, thereby obtaining representations for nodes and communities.\nNext, the sample generation component identifies an initial community for a\ngiven node and selects known communities that are structurally similar to the\ninitial one as training samples. Finally, the prompt-driven fine-tuning\ncomponent leverages these samples as prompts to guide the final community\nprediction. Experimental results on five real-world datasets demonstrate that\nPPCS performs better than baseline algorithms. It also achieves higher\ncommunity search efficiency than semi-supervised community search baseline\nmethods, with ablation studies verifying the effectiveness of each component of\nPPCS."}
{"id": "2505.13227", "pdf": "https://arxiv.org/pdf/2505.13227", "abs": "https://arxiv.org/abs/2505.13227", "authors": ["Tianbao Xie", "Jiaqi Deng", "Xiaochuan Li", "Junlin Yang", "Haoyuan Wu", "Jixuan Chen", "Wenjing Hu", "Xinyuan Wang", "Yuhui Xu", "Zekun Wang", "Yiheng Xu", "Junli Wang", "Doyen Sahoo", "Tao Yu", "Caiming Xiong"], "title": "Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": "49 pages, 13 figures", "summary": "Graphical user interface (GUI) grounding, the ability to map natural language\ninstructions to specific actions on graphical user interfaces, remains a\ncritical bottleneck in computer use agent development. Current benchmarks\noversimplify grounding tasks as short referring expressions, failing to capture\nthe complexity of real-world interactions that require software commonsense,\nlayout understanding, and fine-grained manipulation capabilities. To address\nthese limitations, we introduce OSWorld-G, a comprehensive benchmark comprising\n564 finely annotated samples across diverse task types including text matching,\nelement recognition, layout understanding, and precise manipulation.\nAdditionally, we synthesize and release the largest computer use grounding\ndataset Jedi, which contains 4 million examples through multi-perspective\ndecoupling of tasks. Our multi-scale models trained on Jedi demonstrate its\neffectiveness by outperforming existing approaches on ScreenSpot-v2,\nScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved\ngrounding with Jedi directly enhances agentic capabilities of general\nfoundation models on complex computer tasks, improving from 5% to 27% on\nOSWorld. Through detailed ablation studies, we identify key factors\ncontributing to grounding performance and verify that combining specialized\ndata for different interface elements enables compositional generalization to\nnovel interfaces. All benchmark, data, checkpoints, and code are open-sourced\nand available at https://osworld-grounding.github.io."}
{"id": "2505.13430", "pdf": "https://arxiv.org/pdf/2505.13430", "abs": "https://arxiv.org/abs/2505.13430", "authors": ["Sifeng Shang", "Jiayi Zhou", "Chenyu Lin", "Minxian Li", "Kaiyang Zhou"], "title": "Fine-tuning Quantized Neural Networks with Zeroth-order Optimization", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "As the size of large language models grows exponentially, GPU memory has\nbecome a bottleneck for adapting these models to downstream tasks. In this\npaper, we aim to push the limits of memory-efficient training by minimizing\nmemory usage on model weights, gradients, and optimizer states, within a\nunified framework. Our idea is to eliminate both gradients and optimizer states\nusing zeroth-order optimization, which approximates gradients by perturbing\nweights during forward passes to identify gradient directions. To minimize\nmemory usage on weights, we employ model quantization, e.g., converting from\nbfloat16 to int4. However, directly applying zeroth-order optimization to\nquantized weights is infeasible due to the precision gap between discrete\nweights and continuous gradients, which would otherwise require de-quantization\nand re-quantization. To overcome this challenge, we propose Quantized\nZeroth-order Optimization (QZO), a novel approach that perturbs the continuous\nquantization scale for gradient estimation and uses a directional derivative\nclipping method to stabilize training. QZO is orthogonal to both scalar-based\nand codebook-based post-training quantization methods. Compared to\nfull-parameter fine-tuning in bfloat16, QZO can reduce the total memory cost by\nmore than 18$\\times$ for 4-bit LLMs, and enables fine-tuning Llama-2-13B and\nStable Diffusion 3.5 Large within a single 24GB GPU."}
{"id": "2505.12309", "pdf": "https://arxiv.org/pdf/2505.12309", "abs": "https://arxiv.org/abs/2505.12309", "authors": ["Li Ni", "Hengkai Xu", "Lin Mu", "Yiwen Zhang", "Wenjian Luo"], "title": "Community Search in Time-dependent Road-social Attributed Networks", "categories": ["cs.SI", "cs.AI"], "comment": "12 pages, 7 figures", "summary": "Real-world networks often involve both keywords and locations, along with\ntravel time variations between locations due to traffic conditions. However,\nmost existing cohesive subgraph-based community search studies utilize a single\nattribute, either keywords or locations, to identify communities. They do not\nsimultaneously consider both keywords and locations, which results in low\nsemantic or spatial cohesiveness of the detected communities, and they fail to\naccount for variations in travel time. Additionally, these studies traverse the\nentire network to build efficient indexes, but the detected community only\ninvolves nodes around the query node, leading to the traversal of nodes that\nare not relevant to the community. Therefore, we propose the problem of\ndiscovering semantic-spatial aware k-core, which refers to a k-core with high\nsemantic and time-dependent spatial cohesiveness containing the query node. To\naddress this problem, we propose an exact and a greedy algorithm, both of which\ngradually expand outward from the query node. They are local methods that only\naccess the local part of the attributed network near the query node rather than\nthe entire network. Moreover, we design a method to calculate the semantic\nsimilarity between two keywords using large language models. This method\nalleviates the disadvantages of keyword-matching methods used in existing\ncommunity search studies, such as mismatches caused by differently expressed\nsynonyms and the presence of irrelevant words. Experimental results show that\nthe greedy algorithm outperforms baselines in terms of structural, semantic,\nand time-dependent spatial cohesiveness."}
{"id": "2505.13232", "pdf": "https://arxiv.org/pdf/2505.13232", "abs": "https://arxiv.org/abs/2505.13232", "authors": ["Younghyun Kim", "Jongheon Jeong", "Sangkyung Kwak", "Kyungmin Lee", "Juho Lee", "Jinwoo Shin"], "title": "StarFT: Robust Fine-tuning of Zero-shot Models via Spuriosity Alignment", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Learning robust representations from data often requires scale, which has led\nto the success of recent zero-shot models such as CLIP. However, the obtained\nrobustness can easily be deteriorated when these models are fine-tuned on other\ndownstream tasks (e.g., of smaller scales). Previous works often interpret this\nphenomenon in the context of domain shift, developing fine-tuning methods that\naim to preserve the original domain as much as possible. However, in a\ndifferent context, fine-tuned models with limited data are also prone to\nlearning features that are spurious to humans, such as background or texture.\nIn this paper, we propose StarFT (Spurious Textual Alignment Regularization), a\nnovel framework for fine-tuning zero-shot models to enhance robustness by\npreventing them from learning spuriosity. We introduce a regularization that\naligns the output distribution for spuriosity-injected labels with the original\nzero-shot model, ensuring that the model is not induced to extract irrelevant\nfeatures further from these descriptions.We leverage recent language models to\nget such spuriosity-injected labels by generating alternative textual\ndescriptions that highlight potentially confounding features.Extensive\nexperiments validate the robust generalization of StarFT and its emerging\nproperties: zero-shot group robustness and improved zero-shot classification.\nNotably, StarFT boosts both worst-group and average accuracy by 14.30% and\n3.02%, respectively, in the Waterbirds group shift scenario, where other robust\nfine-tuning baselines show even degraded performance."}
{"id": "2505.13438", "pdf": "https://arxiv.org/pdf/2505.13438", "abs": "https://arxiv.org/abs/2505.13438", "authors": ["Penghui Qi", "Zichen Liu", "Tianyu Pang", "Chao Du", "Wee Sun Lee", "Min Lin"], "title": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Scaling test-time compute is crucial for enhancing the reasoning capabilities\nof large language models (LLMs). Existing approaches typically employ\nreinforcement learning (RL) to maximize a verifiable reward obtained at the end\nof reasoning traces. However, such methods optimize only the final performance\nunder a large and fixed token budget, which hinders efficiency in both training\nand deployment. In this work, we present a novel framework, AnytimeReasoner, to\noptimize anytime reasoning performance, which aims to improve token efficiency\nand the flexibility of reasoning under varying token budget constraints. To\nachieve this, we truncate the complete thinking process to fit within sampled\ntoken budgets from a prior distribution, compelling the model to summarize the\noptimal answer for each truncated thinking for verification. This introduces\nverifiable dense rewards into the reasoning process, facilitating more\neffective credit assignment in RL optimization. We then optimize the thinking\nand summary policies in a decoupled manner to maximize the cumulative reward.\nAdditionally, we introduce a novel variance reduction technique, Budget\nRelative Policy Optimization (BRPO), to enhance the robustness and efficiency\nof the learning process when reinforcing the thinking policy. Empirical results\nin mathematical reasoning tasks demonstrate that our method consistently\noutperforms GRPO across all thinking budgets under various prior distributions,\nenhancing both training and token efficiency."}
{"id": "2505.12310", "pdf": "https://arxiv.org/pdf/2505.12310", "abs": "https://arxiv.org/abs/2505.12310", "authors": ["Shouyi Lu", "Huanyu Zhou", "Guirong Zhuo"], "title": "DNOI-4DRO: Deep 4D Radar Odometry with Differentiable Neural-Optimization Iterations", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "16 pages,10 figures", "summary": "A novel learning-optimization-combined 4D radar odometry model, named\nDNOI-4DRO, is proposed in this paper. The proposed model seamlessly integrates\ntraditional geometric optimization with end-to-end neural network training,\nleveraging an innovative differentiable neural-optimization iteration operator.\nIn this framework, point-wise motion flow is first estimated using a neural\nnetwork, followed by the construction of a cost function based on the\nrelationship between point motion and pose in 3D space. The radar pose is then\nrefined using Gauss-Newton updates. Additionally, we design a dual-stream 4D\nradar backbone that integrates multi-scale geometric features and\nclustering-based class-aware features to enhance the representation of sparse\n4D radar point clouds. Extensive experiments on the VoD and Snail-Radar\ndatasets demonstrate the superior performance of our model, which outperforms\nrecent classical and learning-based approaches. Notably, our method even\nachieves results comparable to A-LOAM with mapping optimization using LiDAR\npoint clouds as input. Our models and code will be publicly released."}
{"id": "2505.13289", "pdf": "https://arxiv.org/pdf/2505.13289", "abs": "https://arxiv.org/abs/2505.13289", "authors": ["Alonso Urbano", "David W. Romero", "Max Zimmer", "Sebastian Pokutta"], "title": "RECON: Robust symmetry discovery via Explicit Canonical Orientation Normalization", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Real-world data often exhibits unknown or approximate symmetries, yet\nexisting equivariant networks must commit to a fixed transformation group prior\nto training, e.g., continuous $SO(2)$ rotations. This mismatch degrades\nperformance when the actual data symmetries differ from those in the\ntransformation group. We introduce RECON, a framework to discover each input's\nintrinsic symmetry distribution from unlabeled data. RECON leverages class-pose\ndecompositions and applies a data-driven normalization to align arbitrary\nreference frames into a common natural pose, yielding directly comparable and\ninterpretable symmetry descriptors. We demonstrate effective symmetry discovery\non 2D image benchmarks and -- for the first time -- extend it to 3D\ntransformation groups, paving the way towards more flexible equivariant\nmodeling."}
{"id": "2505.13445", "pdf": "https://arxiv.org/pdf/2505.13445", "abs": "https://arxiv.org/abs/2505.13445", "authors": ["Xiaoyuan Liu", "Tian Liang", "Zhiwei He", "Jiahao Xu", "Wenxuan Wang", "Pinjia He", "Zhaopeng Tu", "Haitao Mi", "Dong Yu"], "title": "Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards", "categories": ["cs.AI", "cs.CL"], "comment": "code available at https://github.com/xyliu-cs/RISE", "summary": "Large Language Models (LLMs) show great promise in complex reasoning, with\nReinforcement Learning with Verifiable Rewards (RLVR) being a key enhancement\nstrategy. However, a prevalent issue is ``superficial self-reflection'', where\nmodels fail to robustly verify their own outputs. We introduce RISE\n(Reinforcing Reasoning with Self-Verification), a novel online RL framework\ndesigned to tackle this. RISE explicitly and simultaneously trains an LLM to\nimprove both its problem-solving and self-verification abilities within a\nsingle, integrated RL process. The core mechanism involves leveraging\nverifiable rewards from an outcome verifier to provide on-the-fly feedback for\nboth solution generation and self-verification tasks. In each iteration, the\nmodel generates solutions, then critiques its own on-policy generated\nsolutions, with both trajectories contributing to the policy update. Extensive\nexperiments on diverse mathematical reasoning benchmarks show that RISE\nconsistently improves model's problem-solving accuracy while concurrently\nfostering strong self-verification skills. Our analyses highlight the\nadvantages of online verification and the benefits of increased verification\ncompute. Additionally, RISE models exhibit more frequent and accurate\nself-verification behaviors during reasoning. These advantages reinforce RISE\nas a flexible and effective path towards developing more robust and self-aware\nreasoners."}
{"id": "2505.12312", "pdf": "https://arxiv.org/pdf/2505.12312", "abs": "https://arxiv.org/abs/2505.12312", "authors": ["Qi Feng", "Hidetoshi Shimodaira"], "title": "Visuospatial Cognitive Assistant", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "comment": "31 pages, 10 figures, 6 tables. The implementation and fine-tuned\n  model (ViCA-7B) are publicly available at https://huggingface.co/nkkbr/ViCA.\n  The ViCA-322K dataset can be found at\n  https://huggingface.co/datasets/nkkbr/ViCA-322K, and the ViCA-Thinking-2.68K\n  dataset is at https://huggingface.co/datasets/nkkbr/ViCA-thinking-2.68k", "summary": "Video-based spatial cognition is vital for robotics and embodied AI but\nchallenges current Vision-Language Models (VLMs). This paper makes two key\ncontributions. First, we introduce ViCA (Visuospatial Cognitive\nAssistant)-322K, a diverse dataset of 322,003 QA pairs from real-world indoor\nvideos (ARKitScenes, ScanNet, ScanNet++), offering supervision for 3D\nmetadata-grounded queries and video-based complex reasoning. Second, we develop\nViCA-7B, fine-tuned on ViCA-322K, which achieves new state-of-the-art on all\neight VSI-Bench tasks, outperforming existing models, including larger ones\n(e.g., +26.1 on Absolute Distance). For interpretability, we present\nViCA-Thinking-2.68K, a dataset with explicit reasoning chains, and fine-tune\nViCA-7B to create ViCA-7B-Thinking, a model that articulates its spatial\nreasoning. Our work highlights the importance of targeted data and suggests\npaths for improved temporal-spatial modeling. We release all resources to\nfoster research in robust visuospatial intelligence."}
{"id": "2505.13307", "pdf": "https://arxiv.org/pdf/2505.13307", "abs": "https://arxiv.org/abs/2505.13307", "authors": ["Qiguang Chen", "Libo Qin", "Jinhao Liu", "Yue Liao", "Jiaqi Wang", "Jingxuan Zhou", "Wanxiang Che"], "title": "RBF++: Quantifying and Optimizing Reasoning Boundaries across Measurable and Unmeasurable Capabilities for Chain-of-Thought Reasoning", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Manuscript", "summary": "Chain-of-Thought (CoT) reasoning has proven effective in enhancing large\nlanguage models (LLMs) on complex tasks, spurring research into its underlying\nmechanisms. However, two primary challenges remain for real-world applications:\n(1) the lack of quantitative metrics and actionable guidelines for evaluating\nand optimizing measurable boundaries of CoT capability, and (2) the absence of\nmethods to assess boundaries of unmeasurable CoT capability, such as multimodal\nperception. To address these gaps, we introduce the Reasoning Boundary\nFramework++ (RBF++). To tackle the first challenge, we define the reasoning\nboundary (RB) as the maximum limit of CoT performance. We also propose a\ncombination law for RBs, enabling quantitative analysis and offering actionable\nguidance across various CoT tasks. For the second challenge, particularly in\nmultimodal scenarios, we introduce a constant assumption, which replaces\nunmeasurable RBs with scenario-specific constants. Additionally, we propose the\nreasoning boundary division mechanism, which divides unmeasurable RBs into two\nsub-boundaries, facilitating the quantification and optimization of both\nunmeasurable domain knowledge and multimodal perception capabilities. Extensive\nexperiments involving 38 models across 13 tasks validate the feasibility of our\nframework in cross-modal settings. Additionally, we evaluate 10 CoT strategies,\noffer insights into optimization and decay from two complementary perspectives,\nand expand evaluation benchmarks for measuring RBs in LLM reasoning. We hope\nthis work advances the understanding of RBs and optimization strategies in\nLLMs. Code and data are available at\nhttps://github.com/LightChen233/reasoning-boundary."}
{"id": "2505.12327", "pdf": "https://arxiv.org/pdf/2505.12327", "abs": "https://arxiv.org/abs/2505.12327", "authors": ["Albert Zhao", "Stefano Soatto"], "title": "Robust Planning for Autonomous Driving via Mixed Adversarial Diffusion Predictions", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "IEEE International Conference on Robotics and Automation (ICRA) 2025", "summary": "We describe a robust planning method for autonomous driving that mixes normal\nand adversarial agent predictions output by a diffusion model trained for\nmotion prediction. We first train a diffusion model to learn an unbiased\ndistribution of normal agent behaviors. We then generate a distribution of\nadversarial predictions by biasing the diffusion model at test time to generate\npredictions that are likely to collide with a candidate plan. We score plans\nusing expected cost with respect to a mixture distribution of normal and\nadversarial predictions, leading to a planner that is robust against\nadversarial behaviors but not overly conservative when agents behave normally.\nUnlike current approaches, we do not use risk measures that over-weight\nadversarial behaviors while placing little to no weight on low-cost normal\nbehaviors or use hard safety constraints that may not be appropriate for all\ndriving scenarios. We show the effectiveness of our method on single-agent and\nmulti-agent jaywalking scenarios as well as a red light violation scenario."}
{"id": "2505.13391", "pdf": "https://arxiv.org/pdf/2505.13391", "abs": "https://arxiv.org/abs/2505.13391", "authors": ["Mikołaj Małkiński", "Jacek Mańdziuk"], "title": "Advancing Generalization Across a Variety of Abstract Visual Reasoning Tasks", "categories": ["cs.AI", "cs.CV", "cs.LG"], "comment": "Accepted to the 34th International Joint Conference on Artificial\n  Intelligence (IJCAI 2025)", "summary": "The abstract visual reasoning (AVR) domain presents a diverse suite of\nanalogy-based tasks devoted to studying model generalization. Recent years have\nbrought dynamic progress in the field, particularly in i.i.d. scenarios, in\nwhich models are trained and evaluated on the same data distributions.\nNevertheless, o.o.d. setups that assess model generalization to new test\ndistributions remain challenging even for the most recent models. To advance\ngeneralization in AVR tasks, we present the Pathways of Normalized Group\nConvolution model (PoNG), a novel neural architecture that features group\nconvolution, normalization, and a parallel design. We consider a wide set of\nAVR benchmarks, including Raven's Progressive Matrices and visual analogy\nproblems with both synthetic and real-world images. The experiments demonstrate\nstrong generalization capabilities of the proposed model, which in several\nsettings outperforms the existing literature methods."}
{"id": "2505.12332", "pdf": "https://arxiv.org/pdf/2505.12332", "abs": "https://arxiv.org/abs/2505.12332", "authors": ["Qianyue Hu", "Junyan Wu", "Wei Lu", "Xiangyang Luo"], "title": "VoiceCloak: A Multi-Dimensional Defense Framework against Unauthorized Diffusion-based Voice Cloning", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.MM", "eess.AS"], "comment": null, "summary": "Diffusion Models (DMs) have achieved remarkable success in realistic voice\ncloning (VC), while they also increase the risk of malicious misuse. Existing\nproactive defenses designed for traditional VC models aim to disrupt the\nforgery process, but they have been proven incompatible with DMs due to the\nintricate generative mechanisms of diffusion. To bridge this gap, we introduce\nVoiceCloak, a multi-dimensional proactive defense framework with the goal of\nobfuscating speaker identity and degrading perceptual quality in potential\nunauthorized VC. To achieve these goals, we conduct a focused analysis to\nidentify specific vulnerabilities within DMs, allowing VoiceCloak to disrupt\nthe cloning process by introducing adversarial perturbations into the reference\naudio. Specifically, to obfuscate speaker identity, VoiceCloak first targets\nspeaker identity by distorting representation learning embeddings to maximize\nidentity variation, which is guided by auditory perception principles.\nAdditionally, VoiceCloak disrupts crucial conditional guidance processes,\nparticularly attention context, thereby preventing the alignment of vocal\ncharacteristics that are essential for achieving convincing cloning. Then, to\naddress the second objective, VoiceCloak introduces score magnitude\namplification to actively steer the reverse trajectory away from the generation\nof high-quality speech. Noise-guided semantic corruption is further employed to\ndisrupt structural speech semantics captured by DMs, degrading output quality.\nExtensive experiments highlight VoiceCloak's outstanding defense success rate\nagainst unauthorized diffusion-based voice cloning. Audio samples of VoiceCloak\nare available at https://voice-cloak.github.io/VoiceCloak/."}
{"id": "2505.13414", "pdf": "https://arxiv.org/pdf/2505.13414", "abs": "https://arxiv.org/abs/2505.13414", "authors": ["Yaqian Chen", "Hanxue Gu", "Haoyu Dong", "Qihang Li", "Yuwen Chen", "Nicholas Konz", "Lin Li", "Maciej A. Mazurowski"], "title": "GuidedMorph: Two-Stage Deformable Registration for Breast MRI", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Accurately registering breast MR images from different time points enables\nthe alignment of anatomical structures and tracking of tumor progression,\nsupporting more effective breast cancer detection, diagnosis, and treatment\nplanning. However, the complexity of dense tissue and its highly non-rigid\nnature pose challenges for conventional registration methods, which primarily\nfocus on aligning general structures while overlooking intricate internal\ndetails. To address this, we propose \\textbf{GuidedMorph}, a novel two-stage\nregistration framework designed to better align dense tissue. In addition to a\nsingle-scale network for global structure alignment, we introduce a framework\nthat utilizes dense tissue information to track breast movement. The learned\ntransformation fields are fused by introducing the Dual Spatial Transformer\nNetwork (DSTN), improving overall alignment accuracy. A novel warping method\nbased on the Euclidean distance transform (EDT) is also proposed to accurately\nwarp the registered dense tissue and breast masks, preserving fine structural\ndetails during deformation. The framework supports paradigms that require\nexternal segmentation models and with image data only. It also operates\neffectively with the VoxelMorph and TransMorph backbones, offering a versatile\nsolution for breast registration. We validate our method on ISPY2 and internal\ndataset, demonstrating superior performance in dense tissue, overall breast\nalignment, and breast structural similarity index measure (SSIM), with notable\nimprovements by over 13.01% in dense tissue Dice, 3.13% in breast Dice, and\n1.21% in breast SSIM compared to the best learning-based baseline."}
{"id": "2505.12339", "pdf": "https://arxiv.org/pdf/2505.12339", "abs": "https://arxiv.org/abs/2505.12339", "authors": ["Midou Guo", "Qilin Yin", "Wei Lu", "Xiangyang Luo"], "title": "Towards Open-world Generalized Deepfake Detection: General Feature Extraction via Unsupervised Domain Adaptation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the development of generative artificial intelligence, new forgery\nmethods are rapidly emerging. Social platforms are flooded with vast amounts of\nunlabeled synthetic data and authentic data, making it increasingly challenging\nto distinguish real from fake. Due to the lack of labels, existing supervised\ndetection methods struggle to effectively address the detection of unknown\ndeepfake methods. Moreover, in open world scenarios, the amount of unlabeled\ndata greatly exceeds that of labeled data. Therefore, we define a new deepfake\ndetection generalization task which focuses on how to achieve efficient\ndetection of large amounts of unlabeled data based on limited labeled data to\nsimulate a open world scenario. To solve the above mentioned task, we propose a\nnovel Open-World Deepfake Detection Generalization Enhancement Training\nStrategy (OWG-DS) to improve the generalization ability of existing methods.\nOur approach aims to transfer deepfake detection knowledge from a small amount\nof labeled source domain data to large-scale unlabeled target domain data.\nSpecifically, we introduce the Domain Distance Optimization (DDO) module to\nalign different domain features by optimizing both inter-domain and\nintra-domain distances. Additionally, the Similarity-based Class Boundary\nSeparation (SCBS) module is used to enhance the aggregation of similar samples\nto ensure clearer class boundaries, while an adversarial training mechanism is\nadopted to learn the domain-invariant features. Extensive experiments show that\nthe proposed deepfake detection generalization enhancement training strategy\nexcels in cross-method and cross-dataset scenarios, improving the model's\ngeneralization."}
{"id": "2505.13427", "pdf": "https://arxiv.org/pdf/2505.13427", "abs": "https://arxiv.org/abs/2505.13427", "authors": ["Lingxiao Du", "Fanqing Meng", "Zongkai Liu", "Zhixiang Zhou", "Ping Luo", "Qiaosheng Zhang", "Wenqi Shao"], "title": "MM-PRM: Enhancing Multimodal Mathematical Reasoning with Scalable Step-Level Supervision", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "While Multimodal Large Language Models (MLLMs) have achieved impressive\nprogress in vision-language understanding, they still struggle with complex\nmulti-step reasoning, often producing logically inconsistent or partially\ncorrect solutions. A key limitation lies in the lack of fine-grained\nsupervision over intermediate reasoning steps. To address this, we propose\nMM-PRM, a process reward model trained within a fully automated, scalable\nframework. We first build MM-Policy, a strong multimodal model trained on\ndiverse mathematical reasoning data. Then, we construct MM-K12, a curated\ndataset of 10,000 multimodal math problems with verifiable answers, which\nserves as seed data. Leveraging a Monte Carlo Tree Search (MCTS)-based\npipeline, we generate over 700k step-level annotations without human labeling.\nThe resulting PRM is used to score candidate reasoning paths in the Best-of-N\ninference setup and achieves significant improvements across both in-domain\n(MM-K12 test set) and out-of-domain (OlympiadBench, MathVista, etc.)\nbenchmarks. Further analysis confirms the effectiveness of soft labels, smaller\nlearning rates, and path diversity in optimizing PRM performance. MM-PRM\ndemonstrates that process supervision is a powerful tool for enhancing the\nlogical robustness of multimodal reasoning systems. We release all our codes\nand data at https://github.com/ModalMinds/MM-PRM."}
{"id": "2505.12343", "pdf": "https://arxiv.org/pdf/2505.12343", "abs": "https://arxiv.org/abs/2505.12343", "authors": ["Kai Tang", "Jinhao You", "Xiuqi Ge", "Hanze Li", "Yichen Guo", "Xiande Huang"], "title": "Mitigating Hallucinations via Inter-Layer Consistency Aggregation in Large Vision-Language Models", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Despite the impressive capabilities of Large Vision-Language Models (LVLMs),\nthey remain susceptible to hallucinations-generating content that is\ninconsistent with the input image. Existing training-free hallucination\nmitigation methods often suffer from unstable performance and high sensitivity\nto hyperparameter settings, limiting their practicality and broader adoption.\nIn this paper, we propose a novel decoding mechanism, Decoding with Inter-layer\nConsistency via Layer Aggregation (DCLA), which requires no retraining,\nfine-tuning, or access to external knowledge bases. Specifically, our approach\nconstructs a dynamic semantic reference by aggregating representations from\nprevious layers, and corrects semantically deviated layers to enforce\ninter-layer consistency. The method allows DCLA to robustly mitigate\nhallucinations across multiple LVLMs. Experiments on hallucination benchmarks\nsuch as MME and POPE demonstrate that DCLA effectively reduces hallucinations\nwhile enhancing the reliability and performance of LVLMs."}
{"id": "2505.13430", "pdf": "https://arxiv.org/pdf/2505.13430", "abs": "https://arxiv.org/abs/2505.13430", "authors": ["Sifeng Shang", "Jiayi Zhou", "Chenyu Lin", "Minxian Li", "Kaiyang Zhou"], "title": "Fine-tuning Quantized Neural Networks with Zeroth-order Optimization", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "As the size of large language models grows exponentially, GPU memory has\nbecome a bottleneck for adapting these models to downstream tasks. In this\npaper, we aim to push the limits of memory-efficient training by minimizing\nmemory usage on model weights, gradients, and optimizer states, within a\nunified framework. Our idea is to eliminate both gradients and optimizer states\nusing zeroth-order optimization, which approximates gradients by perturbing\nweights during forward passes to identify gradient directions. To minimize\nmemory usage on weights, we employ model quantization, e.g., converting from\nbfloat16 to int4. However, directly applying zeroth-order optimization to\nquantized weights is infeasible due to the precision gap between discrete\nweights and continuous gradients, which would otherwise require de-quantization\nand re-quantization. To overcome this challenge, we propose Quantized\nZeroth-order Optimization (QZO), a novel approach that perturbs the continuous\nquantization scale for gradient estimation and uses a directional derivative\nclipping method to stabilize training. QZO is orthogonal to both scalar-based\nand codebook-based post-training quantization methods. Compared to\nfull-parameter fine-tuning in bfloat16, QZO can reduce the total memory cost by\nmore than 18$\\times$ for 4-bit LLMs, and enables fine-tuning Llama-2-13B and\nStable Diffusion 3.5 Large within a single 24GB GPU."}
{"id": "2505.12349", "pdf": "https://arxiv.org/pdf/2505.12349", "abs": "https://arxiv.org/abs/2505.12349", "authors": ["Axel Abels", "Tom Lenaerts"], "title": "Wisdom from Diversity: Bias Mitigation Through Hybrid Human-LLM Crowds", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "comment": "Accepted for publication in the Proceedings of the 34th International\n  Joint Conference on Artificial Intelligence (IJCAI 2025)", "summary": "Despite their performance, large language models (LLMs) can inadvertently\nperpetuate biases found in the data they are trained on. By analyzing LLM\nresponses to bias-eliciting headlines, we find that these models often mirror\nhuman biases. To address this, we explore crowd-based strategies for mitigating\nbias through response aggregation. We first demonstrate that simply averaging\nresponses from multiple LLMs, intended to leverage the \"wisdom of the crowd\",\ncan exacerbate existing biases due to the limited diversity within LLM crowds.\nIn contrast, we show that locally weighted aggregation methods more effectively\nleverage the wisdom of the LLM crowd, achieving both bias mitigation and\nimproved accuracy. Finally, recognizing the complementary strengths of LLMs\n(accuracy) and humans (diversity), we demonstrate that hybrid crowds containing\nboth significantly enhance performance and further reduce biases across ethnic\nand gender-related contexts."}
{"id": "2505.13444", "pdf": "https://arxiv.org/pdf/2505.13444", "abs": "https://arxiv.org/abs/2505.13444", "authors": ["Liyan Tang", "Grace Kim", "Xinyu Zhao", "Thom Lake", "Wenxuan Ding", "Fangcong Yin", "Prasann Singhal", "Manya Wadhwa", "Zeyu Leo Liu", "Zayne Sprague", "Ramya Namuduri", "Bodun Hu", "Juan Diego Rodriguez", "Puyuan Peng", "Greg Durrett"], "title": "ChartMuseum: Testing Visual Reasoning Capabilities of Large Vision-Language Models", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Chart understanding presents a unique challenge for large vision-language\nmodels (LVLMs), as it requires the integration of sophisticated textual and\nvisual reasoning capabilities. However, current LVLMs exhibit a notable\nimbalance between these skills, falling short on visual reasoning that is\ndifficult to perform in text. We conduct a case study using a synthetic dataset\nsolvable only through visual reasoning and show that model performance degrades\nsignificantly with increasing visual complexity, while human performance\nremains robust. We then introduce ChartMuseum, a new Chart Question Answering\n(QA) benchmark containing 1,162 expert-annotated questions spanning multiple\nreasoning types, curated from real-world charts across 184 sources,\nspecifically built to evaluate complex visual and textual reasoning. Unlike\nprior chart understanding benchmarks -- where frontier models perform similarly\nand near saturation -- our benchmark exposes a substantial gap between model\nand human performance, while effectively differentiating model capabilities:\nalthough humans achieve 93% accuracy, the best-performing model Gemini-2.5-Pro\nattains only 63.0%, and the leading open-source LVLM Qwen2.5-VL-72B-Instruct\nachieves only 38.5%. Moreover, on questions requiring primarily visual\nreasoning, all models experience a 35%-55% performance drop from\ntext-reasoning-heavy question performance. Lastly, our qualitative error\nanalysis reveals specific categories of visual reasoning that are challenging\nfor current LVLMs."}
{"id": "2505.12350", "pdf": "https://arxiv.org/pdf/2505.12350", "abs": "https://arxiv.org/abs/2505.12350", "authors": ["Georgiy Malaniya", "Anton Bolychev", "Grigory Yaremenko", "Anastasia Krasnaya", "Pavel Osinenko"], "title": "Multi-CALF: A Policy Combination Approach with Statistical Guarantees", "categories": ["cs.LG", "cs.AI", "cs.RO", "cs.SY", "eess.SY", "math.OC"], "comment": null, "summary": "We introduce Multi-CALF, an algorithm that intelligently combines\nreinforcement learning policies based on their relative value improvements. Our\napproach integrates a standard RL policy with a theoretically-backed\nalternative policy, inheriting formal stability guarantees while often\nachieving better performance than either policy individually. We prove that our\ncombined policy converges to a specified goal set with known probability and\nprovide precise bounds on maximum deviation and convergence time. Empirical\nvalidation on control tasks demonstrates enhanced performance while maintaining\nstability guarantees."}
{"id": "2505.13447", "pdf": "https://arxiv.org/pdf/2505.13447", "abs": "https://arxiv.org/abs/2505.13447", "authors": ["Zhengyang Geng", "Mingyang Deng", "Xingjian Bai", "J. Zico Kolter", "Kaiming He"], "title": "Mean Flows for One-step Generative Modeling", "categories": ["cs.LG", "cs.CV"], "comment": "Tech report", "summary": "We propose a principled and effective framework for one-step generative\nmodeling. We introduce the notion of average velocity to characterize flow\nfields, in contrast to instantaneous velocity modeled by Flow Matching methods.\nA well-defined identity between average and instantaneous velocities is derived\nand used to guide neural network training. Our method, termed the MeanFlow\nmodel, is self-contained and requires no pre-training, distillation, or\ncurriculum learning. MeanFlow demonstrates strong empirical performance: it\nachieves an FID of 3.43 with a single function evaluation (1-NFE) on ImageNet\n256x256 trained from scratch, significantly outperforming previous\nstate-of-the-art one-step diffusion/flow models. Our study substantially\nnarrows the gap between one-step diffusion/flow models and their multi-step\npredecessors, and we hope it will motivate future research to revisit the\nfoundations of these powerful models."}
{"id": "2505.12353", "pdf": "https://arxiv.org/pdf/2505.12353", "abs": "https://arxiv.org/abs/2505.12353", "authors": ["Prakash Palanivelu Rajmohan", "Fred Roosta"], "title": "Importance Sampling for Nonlinear Models", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "This work is accepted at ICML 2025", "summary": "While norm-based and leverage-score-based methods have been extensively\nstudied for identifying \"important\" data points in linear models, analogous\ntools for nonlinear models remain significantly underdeveloped. By introducing\nthe concept of the adjoint operator of a nonlinear map, we address this gap and\ngeneralize norm-based and leverage-score-based importance sampling to nonlinear\nsettings. We demonstrate that sampling based on these generalized notions of\nnorm and leverage scores provides approximation guarantees for the underlying\nnonlinear mapping, similar to linear subspace embeddings. As direct\napplications, these nonlinear scores not only reduce the computational\ncomplexity of training nonlinear models by enabling efficient sampling over\nlarge datasets but also offer a novel mechanism for model explainability and\noutlier detection. Our contributions are supported by both theoretical analyses\nand experimental results across a variety of supervised learning scenarios."}
{"id": "2505.12354", "pdf": "https://arxiv.org/pdf/2505.12354", "abs": "https://arxiv.org/abs/2505.12354", "authors": ["Anton Bolychev", "Georgiy Malaniya", "Grigory Yaremenko", "Anastasia Krasnaya", "Pavel Osinenko"], "title": "A universal policy wrapper with guarantees", "categories": ["cs.LG", "cs.AI", "cs.RO", "cs.SY", "eess.SY", "math.OC"], "comment": null, "summary": "We introduce a universal policy wrapper for reinforcement learning agents\nthat ensures formal goal-reaching guarantees. In contrast to standard\nreinforcement learning algorithms that excel in performance but lack rigorous\nsafety assurances, our wrapper selectively switches between a high-performing\nbase policy -- derived from any existing RL method -- and a fallback policy\nwith known convergence properties. Base policy's value function supervises this\nswitching process, determining when the fallback policy should override the\nbase policy to ensure the system remains on a stable path. The analysis proves\nthat our wrapper inherits the fallback policy's goal-reaching guarantees while\npreserving or improving upon the performance of the base policy. Notably, it\noperates without needing additional system knowledge or online constrained\noptimization, making it readily deployable across diverse reinforcement\nlearning architectures and tasks."}
{"id": "2505.12358", "pdf": "https://arxiv.org/pdf/2505.12358", "abs": "https://arxiv.org/abs/2505.12358", "authors": ["Abrar Rahman Abir", "Haz Sameen Shahgir", "Md Rownok Zahan Ratul", "Md Toki Tahmid", "Greg Ver Steeg", "Yue Dong"], "title": "AbFlowNet: Optimizing Antibody-Antigen Binding Energy via Diffusion-GFlowNet Fusion", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Complementarity Determining Regions (CDRs) are critical segments of an\nantibody that facilitate binding to specific antigens. Current computational\nmethods for CDR design utilize reconstruction losses and do not jointly\noptimize binding energy, a crucial metric for antibody efficacy. Rather,\nbinding energy optimization is done through computationally expensive Online\nReinforcement Learning (RL) pipelines rely heavily on unreliable binding energy\nestimators. In this paper, we propose AbFlowNet, a novel generative framework\nthat integrates GFlowNet with Diffusion models. By framing each diffusion step\nas a state in the GFlowNet framework, AbFlowNet jointly optimizes standard\ndiffusion losses and binding energy by directly incorporating energy signals\ninto the training process, thereby unifying diffusion and reward optimization\nin a single procedure. Experimental results show that AbFlowNet outperforms the\nbase diffusion model by 3.06% in amino acid recovery, 20.40% in geometric\nreconstruction (RMSD), and 3.60% in binding energy improvement ratio. ABFlowNet\nalso decreases Top-1 total energy and binding energy errors by 24.8% and 38.1%\nwithout pseudo-labeling the test dataset or using computationally expensive\nonline RL regimes."}
{"id": "2505.12361", "pdf": "https://arxiv.org/pdf/2505.12361", "abs": "https://arxiv.org/abs/2505.12361", "authors": ["Elizaveta Pestova", "Ilya Osokin", "Danil Belov", "Pavel Osinenko"], "title": "Adaptive MPC-based quadrupedal robot control under periodic disturbances", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "Recent advancements in adaptive control for reference trajectory tracking\nenable quadrupedal robots to perform locomotion tasks under challenging\nconditions. There are methods enabling the estimation of the external\ndisturbances in terms of forces and torques. However, a specific case of\ndisturbances that are periodic was not explicitly tackled in application to\nquadrupeds. This work is devoted to the estimation of the periodic disturbances\nwith a lightweight regressor using simplified robot dynamics and extracting the\ndisturbance properties in terms of the magnitude and frequency. Experimental\nevidence suggests performance improvement over the baseline static disturbance\ncompensation. All source files, including simulation setups, code, and\ncalculation scripts, are available on GitHub at\nhttps://github.com/aidagroup/quad-periodic-mpc."}
{"id": "2505.12363", "pdf": "https://arxiv.org/pdf/2505.12363", "abs": "https://arxiv.org/abs/2505.12363", "authors": ["Qi Feng", "Hidetoshi Shimodaira"], "title": "Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "comment": "26 pages, 19 figures, 4 tables. Code, models, and dataset are\n  available at our project page: https://github.com/nkkbr/ViCA", "summary": "While Multimodal Large Language Models (MLLMs) excel at general\nvision-language tasks, visuospatial cognition - reasoning about spatial\nlayouts, relations, and dynamics - remains a significant challenge. Existing\nmodels often lack the necessary architectural components and specialized\ntraining data for fine-grained spatial understanding. We introduce ViCA2\n(Visuospatial Cognitive Assistant 2), a novel MLLM designed to enhance spatial\nreasoning. ViCA2 features a dual vision encoder architecture integrating SigLIP\nfor semantics and Hiera for spatial structure, coupled with a token ratio\ncontrol mechanism for efficiency. We also developed ViCA-322K, a new\nlarge-scale dataset with over 322,000 spatially grounded question-answer pairs\nfor targeted instruction tuning. On the challenging VSI-Bench benchmark, our\nViCA2-7B model achieves a state-of-the-art average score of 56.8, significantly\nsurpassing larger open-source models (e.g., LLaVA-NeXT-Video-72B, 40.9) and\nleading proprietary models (Gemini-1.5 Pro, 45.4). This demonstrates the\neffectiveness of our approach in achieving strong visuospatial intelligence\nwith a compact model. We release ViCA2, its codebase, and the ViCA-322K dataset\nto facilitate further research."}
{"id": "2505.12366", "pdf": "https://arxiv.org/pdf/2505.12366", "abs": "https://arxiv.org/abs/2505.12366", "authors": ["Gang Li", "Ming Lin", "Tomer Galanti", "Zhengzhong Tu", "Tianbao Yang"], "title": "DisCO: Reinforcing Large Reasoning Models with Discriminative Constrained Optimization", "categories": ["cs.LG", "cs.AI"], "comment": "20 pages, 4 figures", "summary": "The recent success and openness of DeepSeek-R1 have brought widespread\nattention to Group Relative Policy Optimization (GRPO) as a reinforcement\nlearning method for large reasoning models (LRMs). In this work, we analyze the\nGRPO objective under a binary reward setting and reveal an inherent limitation\nof question-level difficulty bias. We also identify a connection between GRPO\nand traditional discriminative methods in supervised learning. Motivated by\nthese insights, we introduce a new Discriminative Constrained Optimization\n(DisCO) framework for reinforcing LRMs, grounded in the principle of\ndiscriminative learning. The main differences between DisCO and GRPO and its\nrecent variants are: (1) it replaces the group relative objective with a\ndiscriminative objective defined by a scoring function; (2) it abandons\nclipping-based surrogates in favor of non-clipping RL surrogate objectives used\nas scoring functions; (3) it employs a simple yet effective constrained\noptimization approach to enforce the KL divergence constraint, ensuring stable\ntraining. As a result, DisCO offers notable advantages over GRPO and its\nvariants: (i) it completely eliminates difficulty bias by adopting\ndiscriminative objectives; (ii) it addresses the entropy instability in GRPO\nand its variants through the use of non-clipping scoring functions and a\nconstrained optimization approach; (iii) it allows the incorporation of\nadvanced discriminative learning techniques to address data imbalance, where a\nsignificant number of questions have more negative than positive generated\nanswers during training. Our experiments on enhancing the mathematical\nreasoning capabilities of SFT-finetuned models show that DisCO significantly\noutperforms GRPO and its improved variants such as DAPO, achieving average\ngains of 7\\% over GRPO and 6\\% over DAPO across six benchmark tasks for an 1.5B\nmodel."}
{"id": "2505.12368", "pdf": "https://arxiv.org/pdf/2505.12368", "abs": "https://arxiv.org/abs/2505.12368", "authors": ["Gauri Kholkar", "Ratinder Ahuja"], "title": "CAPTURE: Context-Aware Prompt Injection Testing and Robustness Enhancement", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted in ACL LLMSec Workshop 2025", "summary": "Prompt injection remains a major security risk for large language models.\nHowever, the efficacy of existing guardrail models in context-aware settings\nremains underexplored, as they often rely on static attack benchmarks.\nAdditionally, they have over-defense tendencies. We introduce CAPTURE, a novel\ncontext-aware benchmark assessing both attack detection and over-defense\ntendencies with minimal in-domain examples. Our experiments reveal that current\nprompt injection guardrail models suffer from high false negatives in\nadversarial cases and excessive false positives in benign scenarios,\nhighlighting critical limitations."}
{"id": "2505.12381", "pdf": "https://arxiv.org/pdf/2505.12381", "abs": "https://arxiv.org/abs/2505.12381", "authors": ["Mohsinul Kabir", "Tasfia Tahsin", "Sophia Ananiadou"], "title": "From n-gram to Attention: How Model Architectures Learn and Propagate Bias in Language Modeling", "categories": ["cs.CL", "cs.AI"], "comment": "19 pages", "summary": "Current research on bias in language models (LMs) predominantly focuses on\ndata quality, with significantly less attention paid to model architecture and\ntemporal influences of data. Even more critically, few studies systematically\ninvestigate the origins of bias. We propose a methodology grounded in\ncomparative behavioral theory to interpret the complex interaction between\ntraining data and model architecture in bias propagation during language\nmodeling. Building on recent work that relates transformers to n-gram LMs, we\nevaluate how data, model design choices, and temporal dynamics affect bias\npropagation. Our findings reveal that: (1) n-gram LMs are highly sensitive to\ncontext window size in bias propagation, while transformers demonstrate\narchitectural robustness; (2) the temporal provenance of training data\nsignificantly affects bias; and (3) different model architectures respond\ndifferentially to controlled bias injection, with certain biases (e.g. sexual\norientation) being disproportionately amplified. As language models become\nubiquitous, our findings highlight the need for a holistic approach -- tracing\nbias to its origins across both data and model dimensions, not just symptoms,\nto mitigate harm."}
{"id": "2505.12386", "pdf": "https://arxiv.org/pdf/2505.12386", "abs": "https://arxiv.org/abs/2505.12386", "authors": ["Boaz Taitler", "Omer Madmon", "Moshe Tennenholtz", "Omer Ben-Porat"], "title": "Data Sharing with a Generative AI Competitor", "categories": ["cs.GT", "cs.AI"], "comment": null, "summary": "As GenAI platforms grow, their dependence on content from competing\nproviders, combined with access to alternative data sources, creates new\nchallenges for data-sharing decisions. In this paper, we provide a model of\ndata sharing between a content creation firm and a GenAI platform that can also\nacquire content from third-party experts. The interaction is modeled as a\nStackelberg game: the firm first decides how much of its proprietary dataset to\nshare with GenAI, and GenAI subsequently determines how much additional data to\nacquire from external experts. Their utilities depend on user traffic, monetary\ntransfers, and the cost of acquiring additional data from external experts. We\ncharacterize the unique subgame perfect equilibrium of the game and uncover a\nsurprising phenomenon: The firm may be willing to pay GenAI to share the firm's\nown data, leading to a costly data-sharing equilibrium. We further characterize\nthe set of Pareto improving data prices, and show that such improvements occur\nonly when the firm pays to share data. Finally, we study how the price can be\nset to optimize different design objectives, such as promoting firm data\nsharing, expert data acquisition, or a balance of both. Our results shed light\non the economic forces shaping data-sharing partnerships in the age of GenAI,\nand provide guidance for platforms, regulators and policymakers seeking to\ndesign effective data exchange mechanisms."}
{"id": "2505.12395", "pdf": "https://arxiv.org/pdf/2505.12395", "abs": "https://arxiv.org/abs/2505.12395", "authors": ["Udaya Shreyas", "L. N. Aadarsh"], "title": "Few-Shot Concept Unlearning with Low Rank Adaptation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Image Generation models are a trending topic nowadays, with many people\nutilizing Artificial Intelligence models in order to generate images. There are\nmany such models which, given a prompt of a text, will generate an image which\ndepicts said prompt. There are many image generation models, such as Latent\nDiffusion Models, Denoising Diffusion Probabilistic Models, Generative\nAdversarial Networks and many more. When generating images, these models can\ngenerate sensitive image data, which can be threatening to privacy or may\nviolate copyright laws of private entities. Machine unlearning aims at removing\nthe influence of specific data subsets from the trained models and in the case\nof image generation models, remove the influence of a concept such that the\nmodel is unable to generate said images of the concept when prompted.\nConventional retraining of the model can take upto days, hence fast algorithms\nare the need of the hour. In this paper we propose an algorithm that aims to\nremove the influence of concepts in diffusion models through updating the\ngradients of the final layers of the text encoders. Using a weighted loss\nfunction, we utilize backpropagation in order to update the weights of the\nfinal layers of the Text Encoder componet of the Stable Diffusion Model,\nremoving influence of the concept from the text-image embedding space, such\nthat when prompted, the result is an image not containing the concept. The\nweighted loss function makes use of Textual Inversion and Low-Rank\nAdaptation.We perform our experiments on Latent Diffusion Models, namely the\nStable Diffusion v2 model, with an average concept unlearning runtime of 50\nseconds using 4-5 images."}
{"id": "2505.12398", "pdf": "https://arxiv.org/pdf/2505.12398", "abs": "https://arxiv.org/abs/2505.12398", "authors": ["Yepeng Weng", "Qiao Hu", "Xujie Chen", "Li Liu", "Dianwen Mei", "Huishi Qiu", "Jiang Tian", "Zhongchao Shi"], "title": "Traversal Verification for Speculative Tree Decoding", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Under review", "summary": "Speculative decoding is a promising approach for accelerating large language\nmodels. The primary idea is to use a lightweight draft model to speculate the\noutput of the target model for multiple subsequent timesteps, and then verify\nthem in parallel to determine whether the drafted tokens should be accepted or\nrejected. To enhance acceptance rates, existing frameworks typically construct\ntoken trees containing multiple candidates in each timestep. However, their\nreliance on token-level verification mechanisms introduces two critical\nlimitations: First, the probability distribution of a sequence differs from\nthat of individual tokens, leading to suboptimal acceptance length. Second,\ncurrent verification schemes begin from the root node and proceed layer by\nlayer in a top-down manner. Once a parent node is rejected, all its child nodes\nshould be discarded, resulting in inefficient utilization of speculative\ncandidates. This paper introduces Traversal Verification, a novel speculative\ndecoding algorithm that fundamentally rethinks the verification paradigm\nthrough leaf-to-root traversal. Our approach considers the acceptance of the\nentire token sequence from the current node to the root, and preserves\npotentially valid subsequences that would be prematurely discarded by existing\nmethods. We theoretically prove that the probability distribution obtained\nthrough Traversal Verification is identical to that of the target model,\nguaranteeing lossless inference while achieving substantial acceleration gains.\nExperimental results across different large language models and multiple tasks\nshow that our method consistently improves acceptance length and throughput\nover existing methods"}
{"id": "2505.12404", "pdf": "https://arxiv.org/pdf/2505.12404", "abs": "https://arxiv.org/abs/2505.12404", "authors": ["Piotr Piękos", "Subhradeep Kayal", "Alexandros Karatzoglou"], "title": "Hyperbolic Residual Quantization: Discrete Representations for Data with Latent Hierarchies", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Hierarchical data arise in countless domains, from biological taxonomies and\norganizational charts to legal codes and knowledge graphs. Residual\nQuantization (RQ) is widely used to generate discrete, multitoken\nrepresentations for such data by iteratively quantizing residuals in a\nmultilevel codebook. However, its reliance on Euclidean geometry can introduce\nfundamental mismatches that hinder modeling of hierarchical branching,\nnecessary for faithful representation of hierarchical data. In this work, we\npropose Hyperbolic Residual Quantization (HRQ), which embeds data natively in a\nhyperbolic manifold and performs residual quantization using hyperbolic\noperations and distance metrics. By adapting the embedding network, residual\ncomputation, and distance metric to hyperbolic geometry, HRQ imparts an\ninductive bias that aligns naturally with hierarchical branching. We claim that\nHRQ in comparison to RQ can generate more useful for downstream tasks discrete\nhierarchical representations for data with latent hierarchies. We evaluate HRQ\non two tasks: supervised hierarchy modeling using WordNet hypernym trees, where\nthe model is supervised to learn the latent hierarchy - and hierarchy\ndiscovery, where, while latent hierarchy exists in the data, the model is not\ndirectly trained or evaluated on a task related to the hierarchy. Across both\nscenarios, HRQ hierarchical tokens yield better performance on downstream tasks\ncompared to Euclidean RQ with gains of up to $20\\%$ for the hierarchy modeling\ntask. Our results demonstrate that integrating hyperbolic geometry into\ndiscrete representation learning substantially enhances the ability to capture\nlatent hierarchies."}
{"id": "2505.12405", "pdf": "https://arxiv.org/pdf/2505.12405", "abs": "https://arxiv.org/abs/2505.12405", "authors": ["Konstantinos Xylogiannopoulos", "Petros Xanthopoulos", "Panagiotis Karampelas", "Georgios Bakamitsos"], "title": "The power of text similarity in identifying AI-LLM paraphrased documents: The case of BBC news articles and ChatGPT", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Generative AI paraphrased text can be used for copyright infringement and the\nAI paraphrased content can deprive substantial revenue from original content\ncreators. Despite this recent surge of malicious use of generative AI, there\nare few academic publications that research this threat. In this article, we\ndemonstrate the ability of pattern-based similarity detection for AI\nparaphrased news recognition. We propose an algorithmic scheme, which is not\nlimited to detect whether an article is an AI paraphrase, but, more\nimportantly, to identify that the source of infringement is the ChatGPT. The\nproposed method is tested with a benchmark dataset specifically created for\nthis task that incorporates real articles from BBC, incorporating a total of\n2,224 articles across five different news categories, as well as 2,224\nparaphrased articles created with ChatGPT. Results show that our pattern\nsimilarity-based method, that makes no use of deep learning, can detect ChatGPT\nassisted paraphrased articles at percentages 96.23% for accuracy, 96.25% for\nprecision, 96.21% for sensitivity, 96.25% for specificity and 96.23% for F1\nscore."}
{"id": "2505.12408", "pdf": "https://arxiv.org/pdf/2505.12408", "abs": "https://arxiv.org/abs/2505.12408", "authors": ["Minxu Liu", "Donghai Guan", "Chuhang Zheng", "Chunwei Tian", "Jie Wen", "Qi Zhu"], "title": "ViEEG: Hierarchical Neural Coding with Cross-Modal Progressive Enhancement for EEG-Based Visual Decoding", "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": "24 pages, 18 figures", "summary": "Understanding and decoding brain activity into visual representations is a\nfundamental challenge at the intersection of neuroscience and artificial\nintelligence. While EEG-based visual decoding has shown promise due to its\nnon-invasive, low-cost nature and millisecond-level temporal resolution,\nexisting methods are limited by their reliance on flat neural representations\nthat overlook the brain's inherent visual hierarchy. In this paper, we\nintroduce ViEEG, a biologically inspired hierarchical EEG decoding framework\nthat aligns with the Hubel-Wiesel theory of visual processing. ViEEG decomposes\neach visual stimulus into three biologically aligned components-contour,\nforeground object, and contextual scene-serving as anchors for a three-stream\nEEG encoder. These EEG features are progressively integrated via\ncross-attention routing, simulating cortical information flow from V1 to IT to\nthe association cortex. We further adopt hierarchical contrastive learning to\nalign EEG representations with CLIP embeddings, enabling zero-shot object\nrecognition. Extensive experiments on the THINGS-EEG dataset demonstrate that\nViEEG achieves state-of-the-art performance, with 40.9% Top-1 accuracy in\nsubject-dependent and 22.9% Top-1 accuracy in cross-subject settings,\nsurpassing existing methods by over 45%. Our framework not only advances the\nperformance frontier but also sets a new paradigm for biologically grounded\nbrain decoding in AI."}
{"id": "2505.12415", "pdf": "https://arxiv.org/pdf/2505.12415", "abs": "https://arxiv.org/abs/2505.12415", "authors": ["Zhenhe Wu", "Jian Yang", "Jiaheng Liu", "Xianjie Wu", "Changzai Pan", "Jie Zhang", "Yu Zhao", "Shuangyong Song", "Yongxiang Li", "Zhoujun Li"], "title": "Table-R1: Region-based Reinforcement Learning for Table Understanding", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Tables present unique challenges for language models due to their structured\nrow-column interactions, necessitating specialized approaches for effective\ncomprehension. While large language models (LLMs) have demonstrated potential\nin table reasoning through prompting and techniques like chain-of-thought (CoT)\nand program-of-thought (PoT), optimizing their performance for table question\nanswering remains underexplored. In this paper, we introduce region-based\nTable-R1, a novel reinforcement learning approach that enhances LLM table\nunderstanding by integrating region evidence into reasoning steps. Our method\nemploys Region-Enhanced Supervised Fine-Tuning (RE-SFT) to guide models in\nidentifying relevant table regions before generating answers, incorporating\ntextual, symbolic, and program-based reasoning. Additionally, Table-Aware Group\nRelative Policy Optimization (TARPO) introduces a mixed reward system to\ndynamically balance region accuracy and answer correctness, with decaying\nregion rewards and consistency penalties to align reasoning steps. Experiments\nshow that Table-R1 achieves an average performance improvement of 14.36 points\nacross multiple base models on three benchmark datasets, even outperforming\nbaseline models with ten times the parameters, while TARPO reduces response\ntoken consumption by 67.5% compared to GRPO, significantly advancing LLM\ncapabilities in efficient tabular reasoning."}
{"id": "2505.12418", "pdf": "https://arxiv.org/pdf/2505.12418", "abs": "https://arxiv.org/abs/2505.12418", "authors": ["Yuanpeng He", "Yali Bi", "Lijian Li", "Chi-Man Pun", "Wenpin Jiao", "Zhi Jin"], "title": "Mutual Evidential Deep Learning for Medical Image Segmentation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Existing semi-supervised medical segmentation co-learning frameworks have\nrealized that model performance can be diminished by the biases in model\nrecognition caused by low-quality pseudo-labels. Due to the averaging nature of\ntheir pseudo-label integration strategy, they fail to explore the reliability\nof pseudo-labels from different sources. In this paper, we propose a mutual\nevidential deep learning (MEDL) framework that offers a potentially viable\nsolution for pseudo-label generation in semi-supervised learning from two\nperspectives. First, we introduce networks with different architectures to\ngenerate complementary evidence for unlabeled samples and adopt an improved\nclass-aware evidential fusion to guide the confident synthesis of evidential\npredictions sourced from diverse architectural networks. Second, utilizing the\nuncertainty in the fused evidence, we design an asymptotic Fisher\ninformation-based evidential learning strategy. This strategy enables the model\nto initially focus on unlabeled samples with more reliable pseudo-labels,\ngradually shifting attention to samples with lower-quality pseudo-labels while\navoiding over-penalization of mislabeled classes in high data uncertainty\nsamples. Additionally, for labeled data, we continue to adopt an\nuncertainty-driven asymptotic learning strategy, gradually guiding the model to\nfocus on challenging voxels. Extensive experiments on five mainstream datasets\nhave demonstrated that MEDL achieves state-of-the-art performance."}
{"id": "2505.12421", "pdf": "https://arxiv.org/pdf/2505.12421", "abs": "https://arxiv.org/abs/2505.12421", "authors": ["Emanuele La Malfa", "Jon Vadillo", "Marco Molinari", "Michael Wooldridge"], "title": "Fixed Point Explainability", "categories": ["cs.LG", "cs.AI"], "comment": "Code: https://github.com/EmanueleLM/fixed-point-explainability", "summary": "This paper introduces a formal notion of fixed point explanations, inspired\nby the \"why regress\" principle, to assess, through recursive applications, the\nstability of the interplay between a model and its explainer. Fixed point\nexplanations satisfy properties like minimality, stability, and faithfulness,\nrevealing hidden model behaviours and explanatory weaknesses. We define\nconvergence conditions for several classes of explainers, from feature-based to\nmechanistic tools like Sparse AutoEncoders, and we report quantitative and\nqualitative results."}
{"id": "2505.12423", "pdf": "https://arxiv.org/pdf/2505.12423", "abs": "https://arxiv.org/abs/2505.12423", "authors": ["Wenqiao Zhu", "Chao Xu", "Lulu Wang", "Jun Wu"], "title": "PSC: Extending Context Window of Large Language Models via Phase Shift Calibration", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Rotary Position Embedding (RoPE) is an efficient position encoding approach\nand is widely utilized in numerous large language models (LLMs). Recently, a\nlot of methods have been put forward to further expand the context window based\non RoPE. The core concept of those methods is to predefine or search for a set\nof factors to rescale the base frequencies of RoPE. Nevertheless, it is quite a\nchallenge for existing methods to predefine an optimal factor due to the\nexponential search space. In view of this, we introduce PSC (Phase Shift\nCalibration), a small module for calibrating the frequencies predefined by\nexisting methods. With the employment of PSC, we demonstrate that many existing\nmethods can be further enhanced, like PI, YaRN, and LongRoPE. We conducted\nextensive experiments across multiple models and tasks. The results demonstrate\nthat (1) when PSC is enabled, the comparative reductions in perplexity increase\nas the context window size is varied from 16k, to 32k, and up to 64k. (2) Our\napproach is broadly applicable and exhibits robustness across a variety of\nmodels and tasks. The code can be found at https://github.com/WNQzhu/PSC."}
{"id": "2505.12424", "pdf": "https://arxiv.org/pdf/2505.12424", "abs": "https://arxiv.org/abs/2505.12424", "authors": ["Lior Broide", "Roni Stern"], "title": "EvoGPT: Enhancing Test Suite Robustness via LLM-Based Generation and Genetic Optimization", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have recently emerged as promising tools for\nautomated unit test generation. We introduce a hybrid framework called EvoGPT\nthat integrates LLM-based test generation with evolutionary search techniques\nto create diverse, fault-revealing unit tests. Unit tests are initially\ngenerated with diverse temperature sampling to maximize behavioral and test\nsuite diversity, followed by a generation-repair loop and coverage-guided\nassertion enhancement. The resulting test suites are evolved using genetic\nalgorithms, guided by a fitness function prioritizing mutation score over\ntraditional coverage metrics. This design emphasizes the primary objective of\nunit testing-fault detection. Evaluated on multiple open-source Java projects,\nEvoGPT achieves an average improvement of 10% in both code coverage and\nmutation score compared to LLMs and traditional search-based software testing\nbaselines. These results demonstrate that combining LLM-driven diversity,\ntargeted repair, and evolutionary optimization produces more effective and\nresilient test suites."}
{"id": "2505.12432", "pdf": "https://arxiv.org/pdf/2505.12432", "abs": "https://arxiv.org/abs/2505.12432", "authors": ["Zirun Guo", "Minjie Hong", "Tao Jin"], "title": "Observe-R1: Unlocking Reasoning Abilities of MLLMs with Dynamic Progressive Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Reinforcement Learning (RL) has shown promise in improving the reasoning\nabilities of Large Language Models (LLMs). However, the specific challenges of\nadapting RL to multimodal data and formats remain relatively unexplored. In\nthis work, we present Observe-R1, a novel framework aimed at enhancing the\nreasoning capabilities of multimodal large language models (MLLMs). We draw\ninspirations from human learning progression--from simple to complex and easy\nto difficult, and propose a gradual learning paradigm for MLLMs. To this end,\nwe construct the NeuraLadder dataset, which is organized and sampled according\nto the difficulty and complexity of data samples for RL training. To tackle\nmultimodal tasks, we introduce a multimodal format constraint that encourages\ncareful observation of images, resulting in enhanced visual abilities and\nclearer and more structured responses. Additionally, we implement a bonus\nreward system that favors concise, correct answers within a length constraint,\nalongside a dynamic weighting mechanism that prioritizes uncertain and\nmedium-difficulty problems, ensuring that more informative samples have a\ngreater impact on training. Our experiments with the Qwen2.5-VL-3B and\nQwen2.5-VL-7B models on 20k samples from the NeuraLadder dataset show that\nObserve-R1 outperforms a series of larger reasoning models on both reasoning\nand general benchmarks, achieving superior clarity and conciseness in reasoning\nchains. Ablation studies validate the effectiveness of our strategies,\nhighlighting the robustness and generalization of our approach. The dataset and\ncode will be released at https://github.com/zrguo/Observe-R1."}
{"id": "2505.12433", "pdf": "https://arxiv.org/pdf/2505.12433", "abs": "https://arxiv.org/abs/2505.12433", "authors": ["Haodong Yang", "Lei Wang", "Md Zakir Hossain"], "title": "SRLoRA: Subspace Recomposition in Low-Rank Adaptation via Importance-Based Fusion and Reinitialization", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Research report", "summary": "Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient\nfine-tuning (PEFT) method that injects two trainable low-rank matrices (A and\nB) into frozen pretrained models. While efficient, LoRA constrains updates to a\nfixed low-rank subspace (Delta W = BA), which can limit representational\ncapacity and hinder downstream performance. We introduce Subspace Recomposition\nin Low-Rank Adaptation (SRLoRA) via importance-based fusion and\nreinitialization, a novel approach that enhances LoRA's expressiveness without\ncompromising its lightweight structure. SRLoRA assigns importance scores to\neach LoRA pair (a column of B and the corresponding row of A), and dynamically\nrecomposes the subspace during training. Less important pairs are fused into\nthe frozen backbone, freeing capacity to reinitialize new pairs along unused\nprincipal directions derived from the pretrained weight's singular value\ndecomposition. This mechanism enables continual subspace refreshment and richer\nadaptation over time, without increasing the number of trainable parameters. We\nevaluate SRLoRA on both language and vision tasks, including the GLUE benchmark\nand various image classification datasets. SRLoRA consistently achieves faster\nconvergence and improved accuracy over standard LoRA, demonstrating its\ngenerality, efficiency, and potential for broader PEFT applications."}
{"id": "2505.12435", "pdf": "https://arxiv.org/pdf/2505.12435", "abs": "https://arxiv.org/abs/2505.12435", "authors": ["Wenqiao Zhu", "Ji Liu", "Lulu Wang", "Jun Wu", "Yulun Zhang"], "title": "SGDPO: Self-Guided Direct Preference Optimization for Language Model Alignment", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": "18 pages, to appear in ACL'25", "summary": "Direct Preference Optimization (DPO) is broadly utilized for aligning Large\nLanguage Models (LLMs) with human values because of its flexibility. Despite\nits effectiveness, it has been observed that the capability of DPO to generate\nhuman-preferred response is limited and the results of DPO are far from\nresilient. To address these limitations, in this paper we propose a novel\nSelf-Guided Direct Preference Optimization algorithm, i.e., SGDPO, which\nincorporates a pilot term to steer the gradient flow during the optimization\nprocess, allowing for fine-grained control over the updates of chosen and\nrejected rewards. We provide a detailed theoretical analysis of our proposed\nmethod and elucidate its operational mechanism. Furthermore, we conduct\ncomprehensive experiments on various models and benchmarks. The extensive\nexperimental results demonstrate the consistency between the empirical results\nand our theoretical analysis and confirm the effectiveness of our proposed\napproach (up to 9.19% higher score)."}
{"id": "2505.12437", "pdf": "https://arxiv.org/pdf/2505.12437", "abs": "https://arxiv.org/abs/2505.12437", "authors": ["Michele Fontanesi", "Alessio Micheli", "Marco Podda", "Domenico Tortorella"], "title": "Addressing the Scarcity of Benchmarks for Graph XAI", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "While Graph Neural Networks (GNNs) have become the de facto model for\nlearning from structured data, their decisional process remains opaque to the\nend user, undermining their deployment in safety-critical applications. In the\ncase of graph classification, Explainable Artificial Intelligence (XAI)\ntechniques address this major issue by identifying sub-graph motifs that\nexplain predictions. However, advancements in this field are hindered by a\nchronic scarcity of benchmark datasets with known ground-truth motifs to assess\nthe explanations' quality. Current graph XAI benchmarks are limited to\nsynthetic data or a handful of real-world tasks hand-curated by domain experts.\nIn this paper, we propose a general method to automate the construction of XAI\nbenchmarks for graph classification from real-world datasets. We provide both\n15 ready-made benchmarks, as well as the code to generate more than 2000\nadditional XAI benchmarks with our method. As a use case, we employ our\nbenchmarks to assess the effectiveness of some popular graph explainers."}
{"id": "2505.12442", "pdf": "https://arxiv.org/pdf/2505.12442", "abs": "https://arxiv.org/abs/2505.12442", "authors": ["Liwen Wang", "Wenxuan Wang", "Shuai Wang", "Zongjie Li", "Zhenlan Ji", "Zongyi Lyu", "Daoyuan Wu", "Shing-Chi Cheung"], "title": "IP Leakage Attacks Targeting LLM-Based Multi-Agent Systems", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has led to the\nemergence of Multi-Agent Systems (MAS) to perform complex tasks through\ncollaboration. However, the intricate nature of MAS, including their\narchitecture and agent interactions, raises significant concerns regarding\nintellectual property (IP) protection. In this paper, we introduce MASLEAK, a\nnovel attack framework designed to extract sensitive information from MAS\napplications. MASLEAK targets a practical, black-box setting, where the\nadversary has no prior knowledge of the MAS architecture or agent\nconfigurations. The adversary can only interact with the MAS through its public\nAPI, submitting attack query $q$ and observing outputs from the final agent.\nInspired by how computer worms propagate and infect vulnerable network hosts,\nMASLEAK carefully crafts adversarial query $q$ to elicit, propagate, and retain\nresponses from each MAS agent that reveal a full set of proprietary components,\nincluding the number of agents, system topology, system prompts, task\ninstructions, and tool usages. We construct the first synthetic dataset of MAS\napplications with 810 applications and also evaluate MASLEAK against real-world\nMAS applications, including Coze and CrewAI. MASLEAK achieves high accuracy in\nextracting MAS IP, with an average attack success rate of 87% for system\nprompts and task instructions, and 92% for system architecture in most cases.\nWe conclude by discussing the implications of our findings and the potential\ndefenses."}
{"id": "2505.12467", "pdf": "https://arxiv.org/pdf/2505.12467", "abs": "https://arxiv.org/abs/2505.12467", "authors": ["Haochun Wang", "Sendong Zhao", "Jingbo Wang", "Zewen Qiang", "Bing Qin", "Ting Liu"], "title": "Beyond Frameworks: Unpacking Collaboration Strategies in Multi-Agent Systems", "categories": ["cs.MA", "cs.AI"], "comment": "ACL 2025", "summary": "Multi-agent collaboration has emerged as a pivotal paradigm for addressing\ncomplex, distributed tasks in large language model (LLM)-driven applications.\nWhile prior research has focused on high-level architectural frameworks, the\ngranular mechanisms governing agents, critical to performance and scalability,\nremain underexplored. This study systematically investigates four dimensions of\ncollaboration strategies: (1) agent governance, (2) participation control, (3)\ninteraction dynamics, and (4) dialogue history management. Through rigorous\nexperimentation under two context-dependent scenarios: Distributed Evidence\nIntegration (DEI) and Structured Evidence Synthesis (SES), we quantify the\nimpact of these strategies on both task accuracy and computational efficiency.\nOur findings reveal that centralized governance, instructor-led participation,\nordered interaction patterns, and instructor-curated context summarization\ncollectively optimize the trade-off between decision quality and resource\nutilization with the support of the proposed Token-Accuracy Ratio (TAR). This\nwork establishes a foundation for designing adaptive, scalable multi-agent\nsystems, shifting the focus from structural novelty to strategic interaction\nmechanics."}
{"id": "2505.12476", "pdf": "https://arxiv.org/pdf/2505.12476", "abs": "https://arxiv.org/abs/2505.12476", "authors": ["Xiao Long", "Liansheng Zhuang", "Chen Shen", "Shaotian Yan", "Yifei Li", "Shafei Wang"], "title": "Enhancing Large Language Models with Reward-guided Tree Search for Knowledge Graph Question and Answering", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recently, large language models (LLMs) have demonstrated impressive\nperformance in Knowledge Graph Question Answering (KGQA) tasks, which aim to\nfind answers based on knowledge graphs (KGs) for natural language questions.\nExisting LLMs-based KGQA methods typically follow the Graph Retrieval-Augmented\nGeneration (GraphRAG) paradigm, which first retrieves reasoning paths from the\nlarge KGs, and then generates the answers based on them. However, these methods\nemphasize the exploration of new optimal reasoning paths in KGs while ignoring\nthe exploitation of historical reasoning paths, which may lead to sub-optimal\nreasoning paths. Additionally, the complex semantics contained in questions may\nlead to the retrieval of inaccurate reasoning paths. To address these issues,\nthis paper proposes a novel and training-free framework for KGQA tasks called\nReward-guided Tree Search on Graph (RTSoG). RTSoG decomposes an original\nquestion into a series of simpler and well-defined sub-questions to handle the\ncomplex semantics. Then, a Self-Critic Monte Carlo Tree Search (SC-MCTS) guided\nby a reward model is introduced to iteratively retrieve weighted reasoning\npaths as contextual knowledge. Finally, it stacks the weighted reasoning paths\naccording to their weights to generate the final answers. Extensive experiments\non four datasets demonstrate the effectiveness of RTSoG. Notably, it achieves\n8.7\\% and 7.0\\% performance improvement over the state-of-the-art method on the\nGrailQA and the WebQSP respectively."}
{"id": "2505.12477", "pdf": "https://arxiv.org/pdf/2505.12477", "abs": "https://arxiv.org/abs/2505.12477", "authors": ["Hugues Van Assel", "Mark Ibrahim", "Tommaso Biancalani", "Aviv Regev", "Randall Balestriero"], "title": "Joint Embedding vs Reconstruction: Provable Benefits of Latent Space Prediction for Self Supervised Learning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "33 pages, 9 figures", "summary": "Reconstruction and joint embedding have emerged as two leading paradigms in\nSelf Supervised Learning (SSL). Reconstruction methods focus on recovering the\noriginal sample from a different view in input space. On the other hand, joint\nembedding methods align the representations of different views in latent space.\nBoth approaches offer compelling advantages, yet practitioners lack clear\nguidelines for choosing between them. In this work, we unveil the core\nmechanisms that distinguish each paradigm. By leveraging closed form solutions\nfor both approaches, we precisely characterize how the view generation process,\ne.g. data augmentation, impacts the learned representations. We then\ndemonstrate that, unlike supervised learning, both SSL paradigms require a\nminimal alignment between augmentations and irrelevant features to achieve\nasymptotic optimality with increasing sample size. Our findings indicate that\nin scenarios where these irrelevant features have a large magnitude, joint\nembedding methods are preferable because they impose a strictly weaker\nalignment condition compared to reconstruction based methods. These results not\nonly clarify the trade offs between the two paradigms but also substantiate the\nempirical success of joint embedding approaches on real world challenging\ndatasets."}
{"id": "2505.12489", "pdf": "https://arxiv.org/pdf/2505.12489", "abs": "https://arxiv.org/abs/2505.12489", "authors": ["Shaobin Zhuang", "Zhipeng Huang", "Ying Zhang", "Fangyikang Wang", "Canmiao Fu", "Binxin Yang", "Chong Sun", "Chen Li", "Yali Wang"], "title": "Video-GPT via Next Clip Diffusion", "categories": ["cs.CV", "cs.AI"], "comment": "22 pages, 12 figures, 18 tables", "summary": "GPT has shown its remarkable success in natural language processing. However,\nthe language sequence is not sufficient to describe spatial-temporal details in\nthe visual world. Alternatively, the video sequence is good at capturing such\ndetails. Motivated by this fact, we propose a concise Video-GPT in this paper\nby treating video as new language for visual world modeling. By analogy to next\ntoken prediction in GPT, we introduce a novel next clip diffusion paradigm for\npretraining Video-GPT. Different from the previous works, this distinct\nparadigm allows Video-GPT to tackle both short-term generation and long-term\nprediction, by autoregressively denoising the noisy clip according to the clean\nclips in the history. Extensive experiments show our Video-GPT achieves the\nstate-of-the-art performance on video prediction, which is the key factor\ntowards world modeling (Physics-IQ Benchmark: Video-GPT 34.97 vs. Kling 23.64\nvs. Wan 20.89). Moreover, it can be well adapted on 6 mainstream video tasks in\nboth video generation and understanding, showing its great generalization\ncapacity in downstream. The project page is at https://Video-GPT.github.io."}
{"id": "2505.12492", "pdf": "https://arxiv.org/pdf/2505.12492", "abs": "https://arxiv.org/abs/2505.12492", "authors": ["Amit Cohen", "Lev Gloukhenki", "Ravid Hadar", "Eden Itah", "Yehuda Shvut", "Michael Schapira"], "title": "Unleashing Automated Congestion Control Customization in the Wild", "categories": ["cs.NI", "cs.AI", "cs.LG", "cs.PF", "cs.SY", "eess.SY"], "comment": null, "summary": "Congestion control (CC) crucially impacts user experience across Internet\nservices like streaming, gaming, AR/VR, and connected cars. Traditionally, CC\nalgorithm design seeks universal control rules that yield high performance\nacross diverse application domains and networks. However, varying service needs\nand network conditions challenge this approach. We share operational experience\nwith a system that automatically customizes congestion control logic to service\nneeds and network conditions. We discuss design, deployment challenges, and\nsolutions, highlighting performance benefits through case studies in streaming,\ngaming, connected cars, and more.\n  Our system leverages PCC Vivace, an online-learning based congestion control\nprotocol developed by researchers. Hence, along with insights from customizing\ncongestion control, we also discuss lessons learned and modifications made to\nadapt PCC Vivace for real-world deployment."}
{"id": "2505.12504", "pdf": "https://arxiv.org/pdf/2505.12504", "abs": "https://arxiv.org/abs/2505.12504", "authors": ["Zongkai Liu", "Fanqing Meng", "Lingxiao Du", "Zhixiang Zhou", "Chao Yu", "Wenqi Shao", "Qiaosheng Zhang"], "title": "CPGD: Toward Stable Rule-based Reinforcement Learning for Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recent advances in rule-based reinforcement learning (RL) have significantly\nimproved the reasoning capability of language models (LMs) with rule-based\nrewards. However, existing RL methods -- such as GRPO, REINFORCE++, and RLOO --\noften suffer from training instability, where large policy updates and improper\nclipping can lead to training collapse. To address this issue, we propose\nClipped Policy Gradient Optimization with Policy Drift (CPGD), a novel\nalgorithm designed to stabilize policy learning in LMs. CPGD introduces a\npolicy drift constraint based on KL divergence to dynamically regularize policy\nupdates, and leverages a clip mechanism on the logarithm of the ratio to\nprevent excessive policy updates. We provide theoretical justification for CPGD\nand demonstrate through empirical analysis that it mitigates the instability\nobserved in prior approaches. Furthermore, we show that CPGD significantly\nimproves performance while maintaining training stability. Our implementation\nbalances theoretical rigor with practical usability, offering a robust\nalternative for RL in the post-training of LMs. We release our code at\nhttps://github.com/ModalMinds/MM-EUREKA."}
{"id": "2505.12506", "pdf": "https://arxiv.org/pdf/2505.12506", "abs": "https://arxiv.org/abs/2505.12506", "authors": ["Yotam Norman", "Ron Meir"], "title": "Unsupervised Invariant Risk Minimization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We propose a novel unsupervised framework for \\emph{Invariant Risk\nMinimization} (IRM), extending the concept of invariance to settings where\nlabels are unavailable. Traditional IRM methods rely on labeled data to learn\nrepresentations that are robust to distributional shifts across environments.\nIn contrast, our approach redefines invariance through feature distribution\nalignment, enabling robust representation learning from unlabeled data. We\nintroduce two methods within this framework: Principal Invariant Component\nAnalysis (PICA), a linear method that extracts invariant directions under\nGaussian assumptions, and Variational Invariant Autoencoder (VIAE), a deep\ngenerative model that disentangles environment-invariant and\nenvironment-dependent latent factors. Our approach is based on a novel\n``unsupervised'' structural causal model and supports environment-conditioned\nsample-generation and intervention. Empirical evaluations on synthetic dataset\nand modified versions of MNIST demonstrate the effectiveness of our methods in\ncapturing invariant structure, preserving relevant information, and\ngeneralizing across environments without access to labels."}
{"id": "2505.12509", "pdf": "https://arxiv.org/pdf/2505.12509", "abs": "https://arxiv.org/abs/2505.12509", "authors": ["Junhao Liu", "Haonan Yu", "Xin Zhang"], "title": "Towards Budget-Friendly Model-Agnostic Explanation Generation for Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "With Large language models (LLMs) becoming increasingly prevalent in various\napplications, the need for interpreting their predictions has become a critical\nchallenge. As LLMs vary in architecture and some are closed-sourced,\nmodel-agnostic techniques show great promise without requiring access to the\nmodel's internal parameters. However, existing model-agnostic techniques need\nto invoke LLMs many times to gain sufficient samples for generating faithful\nexplanations, which leads to high economic costs. In this paper, we show that\nit is practical to generate faithful explanations for large-scale LLMs by\nsampling from some budget-friendly models through a series of empirical\nstudies. Moreover, we show that such proxy explanations also perform well on\ndownstream tasks. Our analysis provides a new paradigm of model-agnostic\nexplanation methods for LLMs, by including information from budget-friendly\nmodels."}
{"id": "2505.12512", "pdf": "https://arxiv.org/pdf/2505.12512", "abs": "https://arxiv.org/abs/2505.12512", "authors": ["Truman Hickok"], "title": "Scalable Strategies for Continual Learning with Replay", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Future deep learning models will be distinguished by systems that perpetually\nlearn through interaction, imagination, and cooperation, blurring the line\nbetween training and inference. This makes continual learning a critical\nchallenge, as methods that efficiently maximize bidirectional transfer across\nlearning trajectories will be essential. Replay is on track to play a\nfoundational role in continual learning, allowing models to directly reconcile\nnew information with past knowledge. In practice, however, replay is quite\nunscalable, doubling the cost of continual learning when applied naively.\nMoreover, the continual learning literature has not fully synchronized with the\nmulti-task fine-tuning literature, having not fully integrated highly scalable\ntechniques like model merging and low rank adaptation into a replay-enabled\ntoolset that can produce a unified model in the face of many sequential tasks.\nIn this paper, we begin by applying and analyzing low rank adaptation in a\ncontinual learning setting. Next, we introduce consolidation, a phasic approach\nto replay which leads to up to 55\\% less replay samples being needed for a\ngiven performance target. Then, we propose sequential merging, an offshoot of\ntask arithmetic which is tailored to the continual learning setting and is\nshown to work well in combination with replay. Finally, we demonstrate that the\ndeveloped strategies can operate synergistically, resulting in a highly\nscalable toolset that outperforms standalone variants."}
{"id": "2505.12532", "pdf": "https://arxiv.org/pdf/2505.12532", "abs": "https://arxiv.org/abs/2505.12532", "authors": ["Ahmet Bilican", "M. Akın Yılmaz", "A. Murat Tekalp", "R. Gökberk Cinbiş"], "title": "Exploring Sparsity for Parameter Efficient Fine Tuning Using Wavelets", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV", "eess.SP"], "comment": null, "summary": "Efficiently adapting large foundation models is critical, especially with\ntight compute and memory budgets. Parameter-Efficient Fine-Tuning (PEFT)\nmethods such as LoRA offer limited granularity and effectiveness in\nfew-parameter regimes. We propose Wavelet Fine-Tuning (WaveFT), a novel PEFT\nmethod that learns highly sparse updates in the wavelet domain of residual\nmatrices. WaveFT allows precise control of trainable parameters, offering\nfine-grained capacity adjustment and excelling with remarkably low parameter\ncount, potentially far fewer than LoRA's minimum -- ideal for extreme\nparameter-efficient scenarios. In order to demonstrate the effect of the\nwavelet transform, we compare WaveFT with a special case, called SHiRA, that\nentails applying sparse updates directly in the weight domain. Evaluated on\npersonalized text-to-image generation using Stable Diffusion XL as baseline,\nWaveFT significantly outperforms LoRA and other PEFT methods, especially at low\nparameter counts; achieving superior subject fidelity, prompt alignment, and\nimage diversity."}
{"id": "2505.12547", "pdf": "https://arxiv.org/pdf/2505.12547", "abs": "https://arxiv.org/abs/2505.12547", "authors": ["Florent Chiaroni", "Ali Ayub", "Ola Ahmad"], "title": "ProMi: An Efficient Prototype-Mixture Baseline for Few-Shot Segmentation with Bounding-Box Annotations", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "In robotics applications, few-shot segmentation is crucial because it allows\nrobots to perform complex tasks with minimal training data, facilitating their\nadaptation to diverse, real-world environments. However, pixel-level\nannotations of even small amount of images is highly time-consuming and costly.\nIn this paper, we present a novel few-shot binary segmentation method based on\nbounding-box annotations instead of pixel-level labels. We introduce, ProMi, an\nefficient prototype-mixture-based method that treats the background class as a\nmixture of distributions. Our approach is simple, training-free, and effective,\naccommodating coarse annotations with ease. Compared to existing baselines,\nProMi achieves the best results across different datasets with significant\ngains, demonstrating its effectiveness. Furthermore, we present qualitative\nexperiments tailored to real-world mobile robot tasks, demonstrating the\napplicability of our approach in such scenarios. Our code:\nhttps://github.com/ThalesGroup/promi."}
{"id": "2505.12552", "pdf": "https://arxiv.org/pdf/2505.12552", "abs": "https://arxiv.org/abs/2505.12552", "authors": ["Junliang Ye", "Lei Wang", "Md Zakir Hossain"], "title": "FreqSelect: Frequency-Aware fMRI-to-Image Reconstruction", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": "Research report", "summary": "Reconstructing natural images from functional magnetic resonance imaging\n(fMRI) data remains a core challenge in natural decoding due to the mismatch\nbetween the richness of visual stimuli and the noisy, low resolution nature of\nfMRI signals. While recent two-stage models, combining deep variational\nautoencoders (VAEs) with diffusion models, have advanced this task, they treat\nall spatial-frequency components of the input equally. This uniform treatment\nforces the model to extract meaning features and suppress irrelevant noise\nsimultaneously, limiting its effectiveness. We introduce FreqSelect, a\nlightweight, adaptive module that selectively filters spatial-frequency bands\nbefore encoding. By dynamically emphasizing frequencies that are most\npredictive of brain activity and suppressing those that are uninformative,\nFreqSelect acts as a content-aware gate between image features and natural\ndata. It integrates seamlessly into standard very deep VAE-diffusion pipelines\nand requires no additional supervision. Evaluated on the Natural Scenes\ndataset, FreqSelect consistently improves reconstruction quality across both\nlow- and high-level metrics. Beyond performance gains, the learned\nfrequency-selection patterns offer interpretable insights into how different\nvisual frequencies are represented in the brain. Our method generalizes across\nsubjects and scenes, and holds promise for extension to other neuroimaging\nmodalities, offering a principled approach to enhancing both decoding accuracy\nand neuroscientific interpretability."}
{"id": "2505.12556", "pdf": "https://arxiv.org/pdf/2505.12556", "abs": "https://arxiv.org/abs/2505.12556", "authors": ["Taniya Kapoor", "Abhishek Chandra", "Anastasios Stamou", "Stephen J Roberts"], "title": "Beyond Accuracy: EcoL2 Metric for Sustainable Neural PDE Solvers", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Real-world systems, from aerospace to railway engineering, are modeled with\npartial differential equations (PDEs) describing the physics of the system.\nEstimating robust solutions for such problems is essential. Deep learning-based\narchitectures, such as neural PDE solvers, have recently gained traction as a\nreliable solution method. The current state of development of these approaches,\nhowever, primarily focuses on improving accuracy. The environmental impact of\nexcessive computation, leading to increased carbon emissions, has largely been\noverlooked. This paper introduces a carbon emission measure for a range of PDE\nsolvers. Our proposed metric, EcoL2, balances model accuracy with emissions\nacross data collection, model training, and deployment. Experiments across both\nphysics-informed machine learning and operator learning architectures\ndemonstrate that the proposed metric presents a holistic assessment of model\nperformance and emission cost. As such solvers grow in scale and deployment,\nEcoL2 represents a step toward building performant scientific machine learning\nsystems with lower long-term environmental impact."}
{"id": "2505.12567", "pdf": "https://arxiv.org/pdf/2505.12567", "abs": "https://arxiv.org/abs/2505.12567", "authors": ["Wenrui Xu", "Keshab K. Parhi"], "title": "A Survey of Attacks on Large Language Models", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) and LLM-based agents have been widely deployed\nin a wide range of applications in the real world, including healthcare\ndiagnostics, financial analysis, customer support, robotics, and autonomous\ndriving, expanding their powerful capability of understanding, reasoning, and\ngenerating natural languages. However, the wide deployment of LLM-based\napplications exposes critical security and reliability risks, such as the\npotential for malicious misuse, privacy leakage, and service disruption that\nweaken user trust and undermine societal safety. This paper provides a\nsystematic overview of the details of adversarial attacks targeting both LLMs\nand LLM-based agents. These attacks are organized into three phases in LLMs:\nTraining-Phase Attacks, Inference-Phase Attacks, and Availability & Integrity\nAttacks. For each phase, we analyze the details of representative and recently\nintroduced attack methods along with their corresponding defenses. We hope our\nsurvey will provide a good tutorial and a comprehensive understanding of LLM\nsecurity, especially for attacks on LLMs. We desire to raise attention to the\nrisks inherent in widely deployed LLM-based applications and highlight the\nurgent need for robust mitigation strategies for evolving threats."}
{"id": "2505.12572", "pdf": "https://arxiv.org/pdf/2505.12572", "abs": "https://arxiv.org/abs/2505.12572", "authors": ["Hanwen Shen", "Ting Ying"], "title": "Measuring Information Distortion in Hierarchical Ultra long Novel Generation:The Optimal Expansion Ratio", "categories": ["cs.CL", "cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "Writing novels with Large Language Models (LLMs) raises a critical question:\nhow much human-authored outline is necessary to generate high-quality\nmillion-word novels? While frameworks such as DOME, Plan&Write, and Long Writer\nhave improved stylistic coherence and logical consistency, they primarily\ntarget shorter novels (10k--100k words), leaving ultra-long generation largely\nunexplored. Drawing on insights from recent text compression methods like\nLLMZip and LLM2Vec, we conduct an information-theoretic analysis that\nquantifies distortion occurring when LLMs compress and reconstruct ultra-long\nnovels under varying compression-expansion ratios. We introduce a hierarchical\ntwo-stage generation pipeline (outline -> detailed outline -> manuscript) and\nfind an optimal outline length that balances information preservation with\nhuman effort. Through extensive experimentation with Chinese novels, we\nestablish that a two-stage hierarchical outline approach significantly reduces\nsemantic distortion compared to single-stage methods. Our findings provide\nempirically-grounded guidance for authors and researchers collaborating with\nLLMs to create million-word novels."}
{"id": "2505.12576", "pdf": "https://arxiv.org/pdf/2505.12576", "abs": "https://arxiv.org/abs/2505.12576", "authors": ["Kiran Kokilepersaud", "Mohit Prabhushankar", "Ghassan AlRegib"], "title": "AdaDim: Dimensionality Adaptation for SSL Representational Dynamics", "categories": ["cs.LG", "cs.AI"], "comment": "Under Review", "summary": "A key factor in effective Self-Supervised learning (SSL) is preventing\ndimensional collapse, which is where higher-dimensional representation spaces\nspan a lower-dimensional subspace. Therefore, SSL optimization strategies\ninvolve guiding a model to produce representations ($R$) with a higher\ndimensionality. Dimensionality is either optimized through a\ndimension-contrastive approach that encourages feature decorrelation or through\na sample-contrastive method that promotes a uniform spread of sample\nrepresentations. Both families of SSL algorithms also utilize a projection head\nthat maps $R$ into a lower-dimensional embedding space $Z$. Recent work has\ncharacterized the projection head as a filter of irrelevant features from the\nSSL objective by reducing mutual information, $I(R;Z)$. Therefore, the current\nliterature's view is that a good SSL representation space should have a high\n$H(R)$ and a low $I(R;Z)$. However, this view of the problem is lacking in\nterms of an understanding of the underlying training dynamics that influences\nboth terms, as well as how the values of $H(R)$ and $I(R;Z)$ arrived at the end\nof training reflect the downstream performance of an SSL model. We address both\ngaps in the literature by demonstrating that increases in $H(R)$ due to feature\ndecorrelation at the start of training lead to a higher $I(R;Z)$, while\nincreases in $H(R)$ due to samples distributing uniformly in a high-dimensional\nspace at the end of training cause $I(R;Z)$ to plateau or decrease.\nFurthermore, our analysis shows that the best performing SSL models do not have\nthe highest $H(R)$ nor the lowest $I(R;Z)$, but arrive at an optimal\nintermediate point for both. We develop a method called AdaDim to exploit these\nobserved training dynamics by adaptively weighting between losses based on\nfeature decorrelation and uniform sample spread."}
{"id": "2505.12581", "pdf": "https://arxiv.org/pdf/2505.12581", "abs": "https://arxiv.org/abs/2505.12581", "authors": ["Lucas M. Dorneles", "Luan Fonseca Garcia", "Joel Luís Carbonera"], "title": "An approach based on class activation maps for investigating the effects of data augmentation on neural networks for image classification", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Neural networks have become increasingly popular in the last few years as an\neffective tool for the task of image classification due to the impressive\nperformance they have achieved on this task. In image classification tasks, it\nis common to use data augmentation strategies to increase the robustness of\ntrained networks to changes in the input images and to avoid overfitting.\nAlthough data augmentation is a widely adopted technique, the literature lacks\na body of research analyzing the effects data augmentation methods have on the\npatterns learned by neural network models working on complex datasets. The\nprimary objective of this work is to propose a methodology and set of metrics\nthat may allow a quantitative approach to analyzing the effects of data\naugmentation in convolutional networks applied to image classification. An\nimportant tool used in the proposed approach lies in the concept of class\nactivation maps for said models, which allow us to identify and measure the\nimportance these models assign to each individual pixel in an image when\nexecuting the classification task. From these maps, we may then extract metrics\nover the similarities and differences between maps generated by these models\ntrained on a given dataset with different data augmentation strategies.\nExperiments made using this methodology suggest that the effects of these data\naugmentation techniques not only can be analyzed in this way but also allow us\nto identify different impact profiles over the trained models."}
{"id": "2505.12583", "pdf": "https://arxiv.org/pdf/2505.12583", "abs": "https://arxiv.org/abs/2505.12583", "authors": ["Takeshi Kojima", "Yaonan Zhu", "Yusuke Iwasawa", "Toshinori Kitamura", "Gang Yan", "Shu Morikuni", "Ryosuke Takanami", "Alfredo Solano", "Tatsuya Matsushima", "Akiko Murakami", "Yutaka Matsuo"], "title": "A Comprehensive Survey on Physical Risk Control in the Era of Foundation Model-enabled Robotics", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Accepted to IJCAI 2025 Survey Track", "summary": "Recent Foundation Model-enabled robotics (FMRs) display greatly improved\ngeneral-purpose skills, enabling more adaptable automation than conventional\nrobotics. Their ability to handle diverse tasks thus creates new opportunities\nto replace human labor. However, unlike general foundation models, FMRs\ninteract with the physical world, where their actions directly affect the\nsafety of humans and surrounding objects, requiring careful deployment and\ncontrol. Based on this proposition, our survey comprehensively summarizes robot\ncontrol approaches to mitigate physical risks by covering all the lifespan of\nFMRs ranging from pre-deployment to post-accident stage. Specifically, we\nbroadly divide the timeline into the following three phases: (1) pre-deployment\nphase, (2) pre-incident phase, and (3) post-incident phase. Throughout this\nsurvey, we find that there is much room to study (i) pre-incident risk\nmitigation strategies, (ii) research that assumes physical interaction with\nhumans, and (iii) essential issues of foundation models themselves. We hope\nthat this survey will be a milestone in providing a high-resolution analysis of\nthe physical risks of FMRs and their control, contributing to the realization\nof a good human-robot relationship."}
{"id": "2505.12585", "pdf": "https://arxiv.org/pdf/2505.12585", "abs": "https://arxiv.org/abs/2505.12585", "authors": ["En Yu", "Jie Lu", "Xiaoyu Yang", "Guangquan Zhang", "Zhen Fang"], "title": "Learning Robust Spectral Dynamics for Temporal Domain Generalization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Modern machine learning models struggle to maintain performance in dynamic\nenvironments where temporal distribution shifts, \\emph{i.e., concept drift},\nare prevalent. Temporal Domain Generalization (TDG) seeks to enable model\ngeneralization across evolving domains, yet existing approaches typically\nassume smooth incremental changes, struggling with complex real-world drifts\ninvolving long-term structure (incremental evolution/periodicity) and local\nuncertainties. To overcome these limitations, we introduce FreKoo, which\ntackles these challenges via a novel frequency-domain analysis of parameter\ntrajectories. It leverages the Fourier transform to disentangle parameter\nevolution into distinct spectral bands. Specifically, low-frequency component\nwith dominant dynamics are learned and extrapolated using the Koopman operator,\nrobustly capturing diverse drift patterns including both incremental and\nperiodicity. Simultaneously, potentially disruptive high-frequency variations\nare smoothed via targeted temporal regularization, preventing overfitting to\ntransient noise and domain uncertainties. In addition, this dual spectral\nstrategy is rigorously grounded through theoretical analysis, providing\nstability guarantees for the Koopman prediction, a principled Bayesian\njustification for the high-frequency regularization, and culminating in a\nmultiscale generalization bound connecting spectral dynamics to improved\ngeneralization. Extensive experiments demonstrate FreKoo's significant\nsuperiority over SOTA TDG approaches, particularly excelling in real-world\nstreaming scenarios with complex drifts and uncertainties."}
{"id": "2505.12594", "pdf": "https://arxiv.org/pdf/2505.12594", "abs": "https://arxiv.org/abs/2505.12594", "authors": ["Tiankai Yang", "Junjun Liu", "Wingchun Siu", "Jiahang Wang", "Zhuangzhuang Qian", "Chanjuan Song", "Cheng Cheng", "Xiyang Hu", "Yue Zhao"], "title": "AD-AGENT: A Multi-agent Framework for End-to-end Anomaly Detection", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Anomaly detection (AD) is essential in areas such as fraud detection, network\nmonitoring, and scientific research. However, the diversity of data modalities\nand the increasing number of specialized AD libraries pose challenges for\nnon-expert users who lack in-depth library-specific knowledge and advanced\nprogramming skills. To tackle this, we present AD-AGENT, an LLM-driven\nmulti-agent framework that turns natural-language instructions into fully\nexecutable AD pipelines. AD-AGENT coordinates specialized agents for intent\nparsing, data preparation, library and model selection, documentation mining,\nand iterative code generation and debugging. Using a shared short-term\nworkspace and a long-term cache, the agents integrate popular AD libraries like\nPyOD, PyGOD, and TSLib into a unified workflow. Experiments demonstrate that\nAD-AGENT produces reliable scripts and recommends competitive models across\nlibraries. The system is open-sourced to support further research and practical\napplications in AD."}
{"id": "2505.12623", "pdf": "https://arxiv.org/pdf/2505.12623", "abs": "https://arxiv.org/abs/2505.12623", "authors": ["Keisuke Okumura", "Hiroki Nagai"], "title": "Lightweight and Effective Preference Construction in PIBT for Large-Scale Multi-Agent Pathfinding", "categories": ["cs.MA", "cs.AI"], "comment": "To be presented at SoCS-25", "summary": "PIBT is a computationally lightweight algorithm that can be applied to a\nvariety of multi-agent pathfinding (MAPF) problems, generating the next\ncollision-free locations of agents given another. Because of its simplicity and\nscalability, it is becoming a popular underlying scheme for recent large-scale\nMAPF methods involving several hundreds or thousands of agents. Vanilla PIBT\nmakes agents behave greedily towards their assigned goals, while agents\ntypically have multiple best actions, since the graph shortest path is not\nalways unique. Consequently, tiebreaking about how to choose between these\nactions significantly affects resulting solutions. This paper studies two\nsimple yet effective techniques for tiebreaking in PIBT, without compromising\nits computational advantage. The first technique allows an agent to\nintelligently dodge another, taking into account whether each action will\nhinder the progress of the next timestep. The second technique is to learn,\nthrough multiple PIBT runs, how an action causes regret in others and to use\nthis information to minimise regret collectively. Our empirical results\ndemonstrate that these techniques can reduce the solution cost of one-shot MAPF\nand improve the throughput of lifelong MAPF. For instance, in densely populated\none-shot cases, the combined use of these tiebreaks achieves improvements of\naround 10-20% in sum-of-costs, without significantly compromising the speed of\na PIBT-based planner."}
{"id": "2505.12626", "pdf": "https://arxiv.org/pdf/2505.12626", "abs": "https://arxiv.org/abs/2505.12626", "authors": ["Ping Xu", "Zhiyuan Ning", "Pengjiang Li", "Wenhao Liu", "Pengyang Wang", "Jiaxu Cui", "Yuanchun Zhou", "Pengfei Wang"], "title": "scSiameseClu: A Siamese Clustering Framework for Interpreting single-cell RNA Sequencing Data", "categories": ["q-bio.GN", "cs.AI", "cs.LG"], "comment": null, "summary": "Single-cell RNA sequencing (scRNA-seq) reveals cell heterogeneity, with cell\nclustering playing a key role in identifying cell types and marker genes.\nRecent advances, especially graph neural networks (GNNs)-based methods, have\nsignificantly improved clustering performance. However, the analysis of\nscRNA-seq data remains challenging due to noise, sparsity, and high\ndimensionality. Compounding these challenges, GNNs often suffer from\nover-smoothing, limiting their ability to capture complex biological\ninformation. In response, we propose scSiameseClu, a novel Siamese Clustering\nframework for interpreting single-cell RNA-seq data, comprising of 3 key steps:\n(1) Dual Augmentation Module, which applies biologically informed perturbations\nto the gene expression matrix and cell graph relationships to enhance\nrepresentation robustness; (2) Siamese Fusion Module, which combines\ncross-correlation refinement and adaptive information fusion to capture complex\ncellular relationships while mitigating over-smoothing; and (3) Optimal\nTransport Clustering, which utilizes Sinkhorn distance to efficiently align\ncluster assignments with predefined proportions while maintaining balance.\nComprehensive evaluations on seven real-world datasets demonstrate\nthat~\\methodname~outperforms state-of-the-art methods in single-cell\nclustering, cell type annotation, and cell type classification, providing a\npowerful tool for scRNA-seq data interpretation."}
{"id": "2505.12630", "pdf": "https://arxiv.org/pdf/2505.12630", "abs": "https://arxiv.org/abs/2505.12630", "authors": ["Xiangpeng Tian", "Xiangyu Liao", "Xiao Liu", "Meng Li", "Chao Ren"], "title": "Degradation-Aware Feature Perturbation for All-in-One Image Restoration", "categories": ["cs.CV", "cs.AI", "I.4.5"], "comment": "Accepted to CVPR 2025. 8 pages, 7 figures", "summary": "All-in-one image restoration aims to recover clear images from various\ndegradation types and levels with a unified model. Nonetheless, the significant\nvariations among degradation types present challenges for training a universal\nmodel, often resulting in task interference, where the gradient update\ndirections of different tasks may diverge due to shared parameters. To address\nthis issue, motivated by the routing strategy, we propose DFPIR, a novel\nall-in-one image restorer that introduces Degradation-aware Feature\nPerturbations(DFP) to adjust the feature space to align with the unified\nparameter space. In this paper, the feature perturbations primarily include\nchannel-wise perturbations and attention-wise perturbations. Specifically,\nchannel-wise perturbations are implemented by shuffling the channels in\nhigh-dimensional space guided by degradation types, while attention-wise\nperturbations are achieved through selective masking in the attention space. To\nachieve these goals, we propose a Degradation-Guided Perturbation Block (DGPB)\nto implement these two functions, positioned between the encoding and decoding\nstages of the encoder-decoder architecture. Extensive experimental results\ndemonstrate that DFPIR achieves state-of-the-art performance on several\nall-in-one image restoration tasks including image denoising, image dehazing,\nimage deraining, motion deblurring, and low-light image enhancement. Our codes\nare available at https://github.com/TxpHome/DFPIR."}
{"id": "2505.12632", "pdf": "https://arxiv.org/pdf/2505.12632", "abs": "https://arxiv.org/abs/2505.12632", "authors": ["Yunseok Jang", "Yeda Song", "Sungryull Sohn", "Lajanugen Logeswaran", "Tiange Luo", "Dong-Ki Kim", "Kyunghoon Bae", "Honglak Lee"], "title": "Scalable Video-to-Dataset Generation for Cross-Platform Mobile Agents", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "CVPR 2025", "summary": "Recent advancements in Large Language Models (LLMs) and Vision-Language\nModels (VLMs) have sparked significant interest in developing GUI visual\nagents. We introduce MONDAY (Mobile OS Navigation Task Dataset for Agents from\nYouTube), a large-scale dataset of 313K annotated frames from 20K instructional\nvideos capturing diverse real-world mobile OS navigation across multiple\nplatforms. Models that include MONDAY in their pre-training phases demonstrate\nrobust cross-platform generalization capabilities, consistently outperforming\nmodels trained on existing single OS datasets while achieving an average\nperformance gain of 18.11%p on an unseen mobile OS platform. To enable\ncontinuous dataset expansion as mobile platforms evolve, we present an\nautomated framework that leverages publicly available video content to create\ncomprehensive task datasets without manual annotation. Our framework comprises\nrobust OCR-based scene detection (95.04% F1score), near-perfect UI element\ndetection (99.87% hit ratio), and novel multi-step action identification to\nextract reliable action sequences across diverse interface configurations. We\ncontribute both the MONDAY dataset and our automated collection framework to\nfacilitate future research in mobile OS navigation."}
{"id": "2505.12638", "pdf": "https://arxiv.org/pdf/2505.12638", "abs": "https://arxiv.org/abs/2505.12638", "authors": ["Yifeng Jiao", "Yuchen Liu", "Yu Zhang", "Xin Guo", "Yushuai Wu", "Chen Jiang", "Jiyang Li", "Hongwei Zhang", "Limei Han", "Xin Gao", "Yuan Qi", "Yuan Cheng"], "title": "ChromFound: Towards A Universal Foundation Model for Single-Cell Chromatin Accessibility Data", "categories": ["q-bio.GN", "cs.AI", "cs.CE", "cs.LG"], "comment": null, "summary": "The advent of single-cell Assay for Transposase-Accessible Chromatin using\nsequencing (scATAC-seq) offers an innovative perspective for deciphering\nregulatory mechanisms by assembling a vast repository of single-cell chromatin\naccessibility data. While foundation models have achieved significant success\nin single-cell transcriptomics, there is currently no foundation model for\nscATAC-seq that supports zero-shot high-quality cell identification and\ncomprehensive multi-omics analysis simultaneously. Key challenges lie in the\nhigh dimensionality and sparsity of scATAC-seq data, as well as the lack of a\nstandardized schema for representing open chromatin regions (OCRs). Here, we\npresent \\textbf{ChromFound}, a foundation model tailored for scATAC-seq.\nChromFound utilizes a hybrid architecture and genome-aware tokenization to\neffectively capture genome-wide long contexts and regulatory signals from\ndynamic chromatin landscapes. Pretrained on 1.97 million cells from 30 tissues\nand 6 disease conditions, ChromFound demonstrates broad applicability across 6\ndiverse tasks. Notably, it achieves robust zero-shot performance in generating\nuniversal cell representations and exhibits excellent transferability in cell\ntype annotation and cross-omics prediction. By uncovering enhancer-gene links\nundetected by existing computational methods, ChromFound offers a promising\nframework for understanding disease risk variants in the noncoding genome."}
{"id": "2505.12641", "pdf": "https://arxiv.org/pdf/2505.12641", "abs": "https://arxiv.org/abs/2505.12641", "authors": ["Yue Huang", "Zi'ang Li", "Tianle Hu", "Jie Wen", "Guanbin Li", "Jinglin Zhang", "Guoxu Zhou", "Xiaozhao Fang"], "title": "Single Image Reflection Removal via inter-layer Complementarity", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Although dual-stream architectures have achieved remarkable success in single\nimage reflection removal, they fail to fully exploit inter-layer\ncomplementarity in their physical modeling and network design, which limits the\nquality of image separation. To address this fundamental limitation, we propose\ntwo targeted improvements to enhance dual-stream architectures: First, we\nintroduce a novel inter-layer complementarity model where low-frequency\ncomponents extracted from the residual layer interact with the transmission\nlayer through dual-stream architecture to enhance inter-layer complementarity.\nMeanwhile, high-frequency components from the residual layer provide inverse\nmodulation to both streams, improving the detail quality of the transmission\nlayer. Second, we propose an efficient inter-layer complementarity attention\nmechanism which first cross-reorganizes dual streams at the channel level to\nobtain reorganized streams with inter-layer complementary structures, then\nperforms attention computation on the reorganized streams to achieve better\ninter-layer separation, and finally restores the original stream structure for\noutput. Experimental results demonstrate that our method achieves\nstate-of-the-art separation quality on multiple public datasets while\nsignificantly reducing both computational cost and model complexity."}
{"id": "2505.12650", "pdf": "https://arxiv.org/pdf/2505.12650", "abs": "https://arxiv.org/abs/2505.12650", "authors": ["Yaotian Yang", "Yiwen Tang", "Yizhe Chen", "Xiao Chen", "Jiangjie Qiu", "Hao Xiong", "Haoyu Yin", "Zhiyao Luo", "Yifei Zhang", "Sijia Tao", "Wentao Li", "Qinghua Zhang", "Yuqiang Li", "Wanli Ouyang", "Bin Zhao", "Xiaonan Wang", "Fei Wei"], "title": "AutoMat: Enabling Automated Crystal Structure Reconstruction from Microscopy via Agentic Tool Use", "categories": ["cs.CV", "cs.AI"], "comment": "The code and dataset are publicly available at\n  https://github.com/yyt-2378/AutoMat and\n  https://huggingface.co/datasets/yaotianvector/STEM2Mat", "summary": "Machine learning-based interatomic potentials and force fields depend\ncritically on accurate atomic structures, yet such data are scarce due to the\nlimited availability of experimentally resolved crystals. Although\natomic-resolution electron microscopy offers a potential source of structural\ndata, converting these images into simulation-ready formats remains\nlabor-intensive and error-prone, creating a bottleneck for model training and\nvalidation. We introduce AutoMat, an end-to-end, agent-assisted pipeline that\nautomatically transforms scanning transmission electron microscopy (STEM)\nimages into atomic crystal structures and predicts their physical properties.\nAutoMat combines pattern-adaptive denoising, physics-guided template retrieval,\nsymmetry-aware atomic reconstruction, fast relaxation and property prediction\nvia MatterSim, and coordinated orchestration across all stages. We propose the\nfirst dedicated STEM2Mat-Bench for this task and evaluate performance using\nlattice RMSD, formation energy MAE, and structure-matching success rate. By\norchestrating external tool calls, AutoMat enables a text-only LLM to\noutperform vision-language models in this domain, achieving closed-loop\nreasoning throughout the pipeline. In large-scale experiments over 450\nstructure samples, AutoMat substantially outperforms existing multimodal large\nlanguage models and tools. These results validate both AutoMat and\nSTEM2Mat-Bench, marking a key step toward bridging microscopy and atomistic\nsimulation in materials science.The code and dataset are publicly available at\nhttps://github.com/yyt-2378/AutoMat and\nhttps://huggingface.co/datasets/yaotianvector/STEM2Mat."}
{"id": "2505.12654", "pdf": "https://arxiv.org/pdf/2505.12654", "abs": "https://arxiv.org/abs/2505.12654", "authors": ["Yuxin Lin", "Yinglin Zheng", "Ming Zeng", "Wangzheng Shi"], "title": "Predicting Turn-Taking and Backchannel in Human-Machine Conversations Using Linguistic, Acoustic, and Visual Signals", "categories": ["cs.CL", "cs.AI"], "comment": "Accepected by ACL 2025", "summary": "This paper addresses the gap in predicting turn-taking and backchannel\nactions in human-machine conversations using multi-modal signals (linguistic,\nacoustic, and visual). To overcome the limitation of existing datasets, we\npropose an automatic data collection pipeline that allows us to collect and\nannotate over 210 hours of human conversation videos. From this, we construct a\nMulti-Modal Face-to-Face (MM-F2F) human conversation dataset, including over\n1.5M words and corresponding turn-taking and backchannel annotations from\napproximately 20M frames. Additionally, we present an end-to-end framework that\npredicts the probability of turn-taking and backchannel actions from\nmulti-modal signals. The proposed model emphasizes the interrelation between\nmodalities and supports any combination of text, audio, and video inputs,\nmaking it adaptable to a variety of realistic scenarios. Our experiments show\nthat our approach achieves state-of-the-art performance on turn-taking and\nbackchannel prediction tasks, achieving a 10\\% increase in F1-score on\nturn-taking and a 33\\% increase on backchannel prediction. Our dataset and code\nare publicly available online to ease of subsequent research."}
{"id": "2505.12655", "pdf": "https://arxiv.org/pdf/2505.12655", "abs": "https://arxiv.org/abs/2505.12655", "authors": ["Yisheng Zhong", "Yizhu Wen", "Junfeng Guo", "Mehran Kafai", "Heng Huang", "Hanqing Guo", "Zhuangdi Zhu"], "title": "Web IP at Risk: Prevent Unauthorized Real-Time Retrieval by Large Language Models", "categories": ["cs.CR", "cs.AI"], "comment": "13 pages, 13 figures, 4 tables", "summary": "Protecting cyber Intellectual Property (IP) such as web content is an\nincreasingly critical concern. The rise of large language models (LLMs) with\nonline retrieval capabilities presents a double-edged sword that enables\nconvenient access to information but often undermines the rights of original\ncontent creators. As users increasingly rely on LLM-generated responses, they\ngradually diminish direct engagement with original information sources,\nsignificantly reducing the incentives for IP creators to contribute, and\nleading to a saturating cyberspace with more AI-generated content. In response,\nwe propose a novel defense framework that empowers web content creators to\nsafeguard their web-based IP from unauthorized LLM real-time extraction by\nleveraging the semantic understanding capability of LLMs themselves. Our method\nfollows principled motivations and effectively addresses an intractable\nblack-box optimization problem. Real-world experiments demonstrated that our\nmethods improve defense success rates from 2.5% to 88.6% on different LLMs,\noutperforming traditional defenses such as configuration-based restrictions."}
{"id": "2505.12662", "pdf": "https://arxiv.org/pdf/2505.12662", "abs": "https://arxiv.org/abs/2505.12662", "authors": ["Xukai Liu", "Ye Liu", "Shiwen Wu", "Yanghai Zhang", "Yihao Yuan", "Kai Zhang", "Qi Liu"], "title": "Know3-RAG: A Knowledge-aware RAG Framework with Adaptive Retrieval, Generation, and Filtering", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have led to impressive\nprogress in natural language generation, yet their tendency to produce\nhallucinated or unsubstantiated content remains a critical concern. To improve\nfactual reliability, Retrieval-Augmented Generation (RAG) integrates external\nknowledge during inference. However, existing RAG systems face two major\nlimitations: (1) unreliable adaptive control due to limited external knowledge\nsupervision, and (2) hallucinations caused by inaccurate or irrelevant\nreferences. To address these issues, we propose Know3-RAG, a knowledge-aware\nRAG framework that leverages structured knowledge from knowledge graphs (KGs)\nto guide three core stages of the RAG process, including retrieval, generation,\nand filtering. Specifically, we introduce a knowledge-aware adaptive retrieval\nmodule that employs KG embedding to assess the confidence of the generated\nanswer and determine retrieval necessity, a knowledge-enhanced reference\ngeneration strategy that enriches queries with KG-derived entities to improve\ngenerated reference relevance, and a knowledge-driven reference filtering\nmechanism that ensures semantic alignment and factual accuracy of references.\nExperiments on multiple open-domain QA benchmarks demonstrate that Know3-RAG\nconsistently outperforms strong baselines, significantly reducing\nhallucinations and enhancing answer reliability."}
{"id": "2505.12664", "pdf": "https://arxiv.org/pdf/2505.12664", "abs": "https://arxiv.org/abs/2505.12664", "authors": ["Ziqing Xing", "Zhaoyang Zhang", "Zirui Chen", "Hongning Ruan", "Zhaohui Yang"], "title": "Multi-View Wireless Sensing via Conditional Generative Learning: Framework and Model Design", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": "submitted to IEEE Transactions on Wireless Communications", "summary": "In this paper, we incorporate physical knowledge into learning-based\nhigh-precision target sensing using the multi-view channel state information\n(CSI) between multiple base stations (BSs) and user equipment (UEs). Such kind\nof multi-view sensing problem can be naturally cast into a conditional\ngeneration framework. To this end, we design a bipartite neural network\narchitecture, the first part of which uses an elaborately designed encoder to\nfuse the latent target features embedded in the multi-view CSI, and then the\nsecond uses them as conditioning inputs of a powerful generative model to guide\nthe target's reconstruction. Specifically, the encoder is designed to capture\nthe physical correlation between the CSI and the target, and also be adaptive\nto the numbers and positions of BS-UE pairs. Therein the view-specific nature\nof CSI is assimilated by introducing a spatial positional embedding scheme,\nwhich exploits the structure of electromagnetic(EM)-wave propagation channels.\nFinally, a conditional diffusion model with a weighted loss is employed to\ngenerate the target's point cloud from the fused features. Extensive numerical\nresults demonstrate that the proposed generative multi-view (Gen-MV) sensing\nframework exhibits excellent flexibility and significant performance\nimprovement on the reconstruction quality of target's shape and EM properties."}
{"id": "2505.12669", "pdf": "https://arxiv.org/pdf/2505.12669", "abs": "https://arxiv.org/abs/2505.12669", "authors": ["Abhinaba Roy", "Geeta Puri", "Dorien Herremans"], "title": "Text2midi-InferAlign: Improving Symbolic Music Generation with Inference-Time Alignment", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS", "68T07", "I.2.1"], "comment": "7 pages, 1 figure, 5 tables", "summary": "We present Text2midi-InferAlign, a novel technique for improving symbolic\nmusic generation at inference time. Our method leverages text-to-audio\nalignment and music structural alignment rewards during inference to encourage\nthe generated music to be consistent with the input caption. Specifically, we\nintroduce two objectives scores: a text-audio consistency score that measures\nrhythmic alignment between the generated music and the original text caption,\nand a harmonic consistency score that penalizes generated music containing\nnotes inconsistent with the key. By optimizing these alignment-based objectives\nduring the generation process, our model produces symbolic music that is more\nclosely tied to the input captions, thereby improving the overall quality and\ncoherence of the generated compositions. Our approach can extend any existing\nautoregressive model without requiring further training or fine-tuning. We\nevaluate our work on top of Text2midi - an existing text-to-midi generation\nmodel, demonstrating significant improvements in both objective and subjective\nevaluation metrics."}
{"id": "2505.12684", "pdf": "https://arxiv.org/pdf/2505.12684", "abs": "https://arxiv.org/abs/2505.12684", "authors": ["Yinlin Zhu", "Xunkai Li", "Jishuo Jia", "Miao Hu", "Di Wu", "Meikang Qiu"], "title": "Towards Effective Federated Graph Foundation Model via Mitigating Knowledge Entanglement", "categories": ["cs.LG", "cs.AI", "cs.DB", "cs.SI"], "comment": "Under Review", "summary": "Recent advances in graph machine learning have shifted to data-centric\nparadigms, driven by two emerging fields: (1) Federated graph learning (FGL)\nenables multi-client collaboration but faces challenges from data and task\nheterogeneity, limiting its practicality; (2) Graph foundation models (GFM)\noffer strong domain generalization but are usually trained on single machines,\nmissing out on cross-silo data and resources.\n  These paradigms are complementary, and their integration brings notable\nbenefits. Motivated by this, we propose FedGFM, a novel decentralized GFM\ntraining paradigm. However, a key challenge is knowledge entanglement, where\nmulti-domain knowledge merges into indistinguishable representations, hindering\ndownstream adaptation.\n  To address this, we present FedGFM+, an enhanced framework with two core\nmodules to reduce knowledge entanglement: (1) AncDAI: A global anchor-based\ndomain-aware initialization strategy. Before pre-training, each client encodes\nits local graph into domain-specific prototypes that serve as semantic anchors.\nSynthetic embeddings around these anchors initialize the global model. We\ntheoretically prove these prototypes are distinguishable across domains,\nproviding a strong inductive bias to disentangle domain-specific knowledge. (2)\nAdaDPP: A local adaptive domain-sensitive prompt pool. Each client learns a\nlightweight graph prompt capturing domain semantics during pre-training. During\nfine-tuning, prompts from all clients form a pool from which the GFM selects\nrelevant prompts to augment target graph attributes, improving downstream\nadaptation.\n  FedGFM+ is evaluated on 8 diverse benchmarks across multiple domains and\ntasks, outperforming 20 baselines from supervised learning, FGL, and federated\nGFM variants."}
{"id": "2505.12701", "pdf": "https://arxiv.org/pdf/2505.12701", "abs": "https://arxiv.org/abs/2505.12701", "authors": ["Shuyang Dong", "Shangtong Zhang", "Lu Feng"], "title": "Counterfactual Explanations for Continuous Action Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by International Joint Conference on Artificial Intelligence\n  (IJCAI) 2025", "summary": "Reinforcement Learning (RL) has shown great promise in domains like\nhealthcare and robotics but often struggles with adoption due to its lack of\ninterpretability. Counterfactual explanations, which address \"what if\"\nscenarios, provide a promising avenue for understanding RL decisions but remain\nunderexplored for continuous action spaces. We propose a novel approach for\ngenerating counterfactual explanations in continuous action RL by computing\nalternative action sequences that improve outcomes while minimizing deviations\nfrom the original sequence. Our approach leverages a distance metric for\ncontinuous actions and accounts for constraints such as adhering to predefined\npolicies in specific states. Evaluations in two RL domains, Diabetes Control\nand Lunar Lander, demonstrate the effectiveness, efficiency, and generalization\nof our approach, enabling more interpretable and trustworthy RL applications."}
{"id": "2505.12705", "pdf": "https://arxiv.org/pdf/2505.12705", "abs": "https://arxiv.org/abs/2505.12705", "authors": ["Joel Jang", "Seonghyeon Ye", "Zongyu Lin", "Jiannan Xiang", "Johan Bjorck", "Yu Fang", "Fengyuan Hu", "Spencer Huang", "Kaushil Kundalia", "Yen-Chen Lin", "Loic Magne", "Ajay Mandlekar", "Avnish Narayan", "You Liang Tan", "Guanzhi Wang", "Jing Wang", "Qi Wang", "Yinzhen Xu", "Xiaohui Zeng", "Kaiyuan Zheng", "Ruijie Zheng", "Ming-Yu Liu", "Luke Zettlemoyer", "Dieter Fox", "Jan Kautz", "Scott Reed", "Yuke Zhu", "Linxi Fan"], "title": "DreamGen: Unlocking Generalization in Robot Learning through Neural Trajectories", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "See website for videos:\n  https://research.nvidia.com/labs/gear/dreamgen", "summary": "We introduce DreamGen, a simple yet highly effective 4-stage pipeline for\ntraining robot policies that generalize across behaviors and environments\nthrough neural trajectories - synthetic robot data generated from video world\nmodels. DreamGen leverages state-of-the-art image-to-video generative models,\nadapting them to the target robot embodiment to produce photorealistic\nsynthetic videos of familiar or novel tasks in diverse environments. Since\nthese models generate only videos, we recover pseudo-action sequences using\neither a latent action model or an inverse-dynamics model (IDM). Despite its\nsimplicity, DreamGen unlocks strong behavior and environment generalization: a\nhumanoid robot can perform 22 new behaviors in both seen and unseen\nenvironments, while requiring teleoperation data from only a single\npick-and-place task in one environment. To evaluate the pipeline\nsystematically, we introduce DreamGen Bench, a video generation benchmark that\nshows a strong correlation between benchmark performance and downstream policy\nsuccess. Our work establishes a promising new axis for scaling robot learning\nwell beyond manual data collection."}
{"id": "2505.12707", "pdf": "https://arxiv.org/pdf/2505.12707", "abs": "https://arxiv.org/abs/2505.12707", "authors": ["Yingchen He", "Christian D. Weilbach", "Martyna E. Wojciechowska", "Yuxuan Zhang", "Frank Wood"], "title": "PLAICraft: Large-Scale Time-Aligned Vision-Speech-Action Dataset for Embodied AI", "categories": ["cs.LG", "cs.AI", "cs.MA"], "comment": "9 pages, 8 figures", "summary": "Advances in deep generative modelling have made it increasingly plausible to\ntrain human-level embodied agents. Yet progress has been limited by the absence\nof large-scale, real-time, multi-modal, and socially interactive datasets that\nreflect the sensory-motor complexity of natural environments. To address this,\nwe present PLAICraft, a novel data collection platform and dataset capturing\nmultiplayer Minecraft interactions across five time-aligned modalities: video,\ngame output audio, microphone input audio, mouse, and keyboard actions. Each\nmodality is logged with millisecond time precision, enabling the study of\nsynchronous, embodied behaviour in a rich, open-ended world. The dataset\ncomprises over 10,000 hours of gameplay from more than 10,000 global\nparticipants.\\footnote{We have done a privacy review for the public release of\nan initial 200-hour subset of the dataset, with plans to release most of the\ndataset over time.} Alongside the dataset, we provide an evaluation suite for\nbenchmarking model capabilities in object recognition, spatial awareness,\nlanguage grounding, and long-term memory. PLAICraft opens a path toward\ntraining and evaluating agents that act fluently and purposefully in real time,\npaving the way for truly embodied artificial intelligence."}
{"id": "2505.12711", "pdf": "https://arxiv.org/pdf/2505.12711", "abs": "https://arxiv.org/abs/2505.12711", "authors": ["Qichen Sun", "Zhengrui Guo", "Rui Peng", "Hao Chen", "Jinzhuo Wang"], "title": "Any-to-Any Learning in Computational Pathology via Triplet Multimodal Pretraining", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in computational pathology and artificial intelligence have\nsignificantly enhanced the utilization of gigapixel whole-slide images and and\nadditional modalities (e.g., genomics) for pathological diagnosis. Although\ndeep learning has demonstrated strong potential in pathology, several key\nchallenges persist: (1) fusing heterogeneous data types requires sophisticated\nstrategies beyond simple concatenation due to high computational costs; (2)\ncommon scenarios of missing modalities necessitate flexible strategies that\nallow the model to learn robustly in the absence of certain modalities; (3) the\ndownstream tasks in CPath are diverse, ranging from unimodal to multimodal,\ncnecessitating a unified model capable of handling all modalities. To address\nthese challenges, we propose ALTER, an any-to-any tri-modal pretraining\nframework that integrates WSIs, genomics, and pathology reports. The term \"any\"\nemphasizes ALTER's modality-adaptive design, enabling flexible pretraining with\nany subset of modalities, and its capacity to learn robust, cross-modal\nrepresentations beyond WSI-centric approaches. We evaluate ALTER across\nextensive clinical tasks including survival prediction, cancer subtyping, gene\nmutation prediction, and report generation, achieving superior or comparable\nperformance to state-of-the-art baselines."}
{"id": "2505.12716", "pdf": "https://arxiv.org/pdf/2505.12716", "abs": "https://arxiv.org/abs/2505.12716", "authors": ["Taiqiang Wu", "Runming Yang", "Jiayi Li", "Pengfei Hu", "Ngai Wong", "Yujiu Yang"], "title": "Shadow-FT: Tuning Instruct via Base", "categories": ["cs.CL", "cs.AI"], "comment": "Under review", "summary": "Large language models (LLMs) consistently benefit from further fine-tuning on\nvarious tasks. However, we observe that directly tuning the INSTRUCT (i.e.,\ninstruction tuned) models often leads to marginal improvements and even\nperformance degeneration. Notably, paired BASE models, the foundation for these\nINSTRUCT variants, contain highly similar weight values (i.e., less than 2% on\naverage for Llama 3.1 8B). Therefore, we propose a novel Shadow-FT framework to\ntune the INSTRUCT models by leveraging the corresponding BASE models. The key\ninsight is to fine-tune the BASE model, and then directly graft the learned\nweight updates to the INSTRUCT model. Our proposed Shadow-FT introduces no\nadditional parameters, is easy to implement, and significantly improves\nperformance. We conduct extensive experiments on tuning mainstream LLMs, such\nas Qwen 3 and Llama 3 series, and evaluate them across 19 benchmarks covering\ncoding, reasoning, and mathematical tasks. Experimental results demonstrate\nthat Shadow-FT consistently outperforms conventional full-parameter and\nparameter-efficient tuning approaches. Further analyses indicate that Shadow-FT\ncan be applied to multimodal large language models (MLLMs) and combined with\ndirect preference optimization (DPO). Codes and weights are available at\n\\href{https://github.com/wutaiqiang/Shadow-FT}{Github}."}
{"id": "2505.12734", "pdf": "https://arxiv.org/pdf/2505.12734", "abs": "https://arxiv.org/abs/2505.12734", "authors": ["Junbo Wang", "Haofeng Tan", "Bowen Liao", "Albert Jiang", "Teng Fei", "Qixing Huang", "Zhengzhong Tu", "Shan Ye", "Yuhao Kang"], "title": "SounDiT: Geo-Contextual Soundscape-to-Landscape Generation", "categories": ["cs.SD", "cs.AI", "cs.GR", "cs.HC", "eess.AS"], "comment": "14 pages, 5 figures", "summary": "We present a novel and practically significant problem-Geo-Contextual\nSoundscape-to-Landscape (GeoS2L) generation-which aims to synthesize\ngeographically realistic landscape images from environmental soundscapes. Prior\naudio-to-image generation methods typically rely on general-purpose datasets\nand overlook geographic and environmental contexts, resulting in unrealistic\nimages that are misaligned with real-world environmental settings. To address\nthis limitation, we introduce a novel geo-contextual computational framework\nthat explicitly integrates geographic knowledge into multimodal generative\nmodeling. We construct two large-scale geo-contextual multimodal datasets,\nSoundingSVI and SonicUrban, pairing diverse soundscapes with real-world\nlandscape images. We propose SounDiT, a novel Diffusion Transformer (DiT)-based\nmodel that incorporates geo-contextual scene conditioning to synthesize\ngeographically coherent landscape images. Furthermore, we propose a\npractically-informed geo-contextual evaluation framework, the Place Similarity\nScore (PSS), across element-, scene-, and human perception-levels to measure\nconsistency between input soundscapes and generated landscape images. Extensive\nexperiments demonstrate that SounDiT outperforms existing baselines in both\nvisual fidelity and geographic settings. Our work not only establishes\nfoundational benchmarks for GeoS2L generation but also highlights the\nimportance of incorporating geographic domain knowledge in advancing multimodal\ngenerative models, opening new directions at the intersection of generative AI,\ngeography, urban planning, and environmental sciences."}
{"id": "2505.12737", "pdf": "https://arxiv.org/pdf/2505.12737", "abs": "https://arxiv.org/abs/2505.12737", "authors": ["Hongjoon Ahn", "Heewoong Choi", "Jisu Han", "Taesup Moon"], "title": "Option-aware Temporally Abstracted Value for Offline Goal-Conditioned Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Offline goal-conditioned reinforcement learning (GCRL) offers a practical\nlearning paradigm where goal-reaching policies are trained from abundant\nunlabeled (reward-free) datasets without additional environment interaction.\nHowever, offline GCRL still struggles with long-horizon tasks, even with recent\nadvances that employ hierarchical policy structures, such as HIQL. By\nidentifying the root cause of this challenge, we observe the following\ninsights: First, performance bottlenecks mainly stem from the high-level\npolicy's inability to generate appropriate subgoals. Second, when learning the\nhigh-level policy in the long-horizon regime, the sign of the advantage signal\nfrequently becomes incorrect. Thus, we argue that improving the value function\nto produce a clear advantage signal for learning the high-level policy is\nessential. In this paper, we propose a simple yet effective solution:\nOption-aware Temporally Abstracted value learning, dubbed OTA, which\nincorporates temporal abstraction into the temporal-difference learning\nprocess. By modifying the value update to be option-aware, the proposed\nlearning scheme contracts the effective horizon length, enabling better\nadvantage estimates even in long-horizon regimes. We experimentally show that\nthe high-level policy extracted using the OTA value function achieves strong\nperformance on complex tasks from OGBench, a recently proposed offline GCRL\nbenchmark, including maze navigation and visual robotic manipulation\nenvironments."}
{"id": "2505.12738", "pdf": "https://arxiv.org/pdf/2505.12738", "abs": "https://arxiv.org/abs/2505.12738", "authors": ["Chenghua Gong", "Rui Sun", "Yuhao Zheng", "Juyuan Zhang", "Tianjun Gu", "Liming Pan", "Linyuan Lv"], "title": "EpiLLM: Unlocking the Potential of Large Language Models in Epidemic Forecasting", "categories": ["cs.LG", "cs.AI", "cs.SI"], "comment": "18 pages", "summary": "Advanced epidemic forecasting is critical for enabling precision containment\nstrategies, highlighting its strategic importance for public health security.\nWhile recent advances in Large Language Models (LLMs) have demonstrated\neffectiveness as foundation models for domain-specific tasks, their potential\nfor epidemic forecasting remains largely unexplored. In this paper, we\nintroduce EpiLLM, a novel LLM-based framework tailored for spatio-temporal\nepidemic forecasting. Considering the key factors in real-world epidemic\ntransmission: infection cases and human mobility, we introduce a dual-branch\narchitecture to achieve fine-grained token-level alignment between such complex\nepidemic patterns and language tokens for LLM adaptation. To unleash the\nmulti-step forecasting and generalization potential of LLM architectures, we\npropose an autoregressive modeling paradigm that reformulates the epidemic\nforecasting task into next-token prediction. To further enhance LLM perception\nof epidemics, we introduce spatio-temporal prompt learning techniques, which\nstrengthen forecasting capabilities from a data-driven perspective. Extensive\nexperiments show that EpiLLM significantly outperforms existing baselines on\nreal-world COVID-19 datasets and exhibits scaling behavior characteristic of\nLLMs."}
{"id": "2505.12745", "pdf": "https://arxiv.org/pdf/2505.12745", "abs": "https://arxiv.org/abs/2505.12745", "authors": ["Dong Kyu Cho", "Inwoo Hwang", "Sanghack Lee"], "title": "PEER pressure: Model-to-Model Regularization for Single Source Domain Generalization", "categories": ["cs.LG", "cs.AI"], "comment": "21 pages, 9 figures, Accepted at CVPR 2025", "summary": "Data augmentation is a popular tool for single source domain generalization,\nwhich expands the source domain by generating simulated ones, improving\ngeneralization on unseen target domains. In this work, we show that the\nperformance of such augmentation-based methods in the target domains\nuniversally fluctuates during training, posing challenges in model selection\nunder realistic scenarios. We argue that the fluctuation stems from the\ninability of the model to accumulate the knowledge learned from diverse\naugmentations, exacerbating feature distortion during training. Based on this\nobservation, we propose a novel generalization method, coined Parameter-Space\nEnsemble with Entropy Regularization (PEER), that uses a proxy model to learn\nthe augmented data on behalf of the main model. The main model is updated by\naveraging its parameters with the proxy model, progressively accumulating\nknowledge over the training steps. Maximizing the mutual information between\nthe output representations of the two models guides the learning process of the\nproxy model, mitigating feature distortion during training. Experimental\nresults demonstrate the effectiveness of PEER in reducing the OOD performance\nfluctuation and enhancing generalization across various datasets, including\nPACS, Digits, Office-Home, and VLCS. Notably, our method with simple random\naugmentation achieves state-of-the-art performance, surpassing prior approaches\non sDG that utilize complex data augmentation strategies."}
{"id": "2505.12748", "pdf": "https://arxiv.org/pdf/2505.12748", "abs": "https://arxiv.org/abs/2505.12748", "authors": ["Hangyu Li", "Qin Zhao", "Haoran Xu", "Xinyu Jiang", "Qingwei Ben", "Feiyu Jia", "Haoyu Zhao", "Liang Xu", "Jia Zeng", "Hanqing Wang", "Bo Dai", "Junting Dong", "Jiangmiao Pang"], "title": "TeleOpBench: A Simulator-Centric Benchmark for Dual-Arm Dexterous Teleoperation", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "13 pages", "summary": "Teleoperation is a cornerstone of embodied-robot learning, and bimanual\ndexterous teleoperation in particular provides rich demonstrations that are\ndifficult to obtain with fully autonomous systems. While recent studies have\nproposed diverse hardware pipelines-ranging from inertial motion-capture gloves\nto exoskeletons and vision-based interfaces-there is still no unified benchmark\nthat enables fair, reproducible comparison of these systems. In this paper, we\nintroduce TeleOpBench, a simulator-centric benchmark tailored to bimanual\ndexterous teleoperation. TeleOpBench contains 30 high-fidelity task\nenvironments that span pick-and-place, tool use, and collaborative\nmanipulation, covering a broad spectrum of kinematic and force-interaction\ndifficulty. Within this benchmark we implement four representative\nteleoperation modalities-(i) MoCap, (ii) VR device, (iii) arm-hand\nexoskeletons, and (iv) monocular vision tracking-and evaluate them with a\ncommon protocol and metric suite. To validate that performance in simulation is\npredictive of real-world behavior, we conduct mirrored experiments on a\nphysical dual-arm platform equipped with two 6-DoF dexterous hands. Across 10\nheld-out tasks we observe a strong correlation between simulator and hardware\nperformance, confirming the external validity of TeleOpBench. TeleOpBench\nestablishes a common yardstick for teleoperation research and provides an\nextensible platform for future algorithmic and hardware innovation."}
{"id": "2505.12750", "pdf": "https://arxiv.org/pdf/2505.12750", "abs": "https://arxiv.org/abs/2505.12750", "authors": ["Filippo Leveni", "Matteo Mistura", "Francesco Iubatti", "Carmine Giangregorio", "Nicolò Pastore", "Cesare Alippi", "Giacomo Boracchi"], "title": "Malware families discovery via Open-Set Recognition on Android manifest permissions", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": "Submitted to European Conference on Artificial Intelligence (ECAI\n  2025)", "summary": "Malware are malicious programs that are grouped into families based on their\npenetration technique, source code, and other characteristics. Classifying\nmalware programs into their respective families is essential for building\neffective defenses against cyber threats. Machine learning models have a huge\npotential in malware detection on mobile devices, as malware families can be\nrecognized by classifying permission data extracted from Android manifest\nfiles. Still, the malware classification task is challenging due to the\nhigh-dimensional nature of permission data and the limited availability of\ntraining samples. In particular, the steady emergence of new malware families\nmakes it impossible to acquire a comprehensive training set covering all the\nmalware classes. In this work, we present a malware classification system that,\non top of classifying known malware, detects new ones. In particular, we\ncombine an open-set recognition technique developed within the computer vision\ncommunity, namely MaxLogit, with a tree-based Gradient Boosting classifier,\nwhich is particularly effective in classifying high-dimensional data. Our\nsolution turns out to be very practical, as it can be seamlessly employed in a\nstandard classification workflow, and efficient, as it adds minimal\ncomputational overhead. Experiments on public and proprietary datasets\ndemonstrate the potential of our solution, which has been deployed in a\nbusiness environment."}
{"id": "2505.12751", "pdf": "https://arxiv.org/pdf/2505.12751", "abs": "https://arxiv.org/abs/2505.12751", "authors": ["Filippo Leveni"], "title": "Structure-based Anomaly Detection and Clustering", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "Doctoral dissertation at Politecnico di Milano", "summary": "Anomaly detection is a fundamental problem in domains such as healthcare,\nmanufacturing, and cybersecurity. This thesis proposes new unsupervised methods\nfor anomaly detection in both structured and streaming data settings. In the\nfirst part, we focus on structure-based anomaly detection, where normal data\nfollows low-dimensional manifolds while anomalies deviate from them. We\nintroduce Preference Isolation Forest (PIF), which embeds data into a\nhigh-dimensional preference space via manifold fitting, and isolates outliers\nusing two variants: Voronoi-iForest, based on geometric distances, and\nRuzHash-iForest, leveraging Locality Sensitive Hashing for scalability. We also\npropose Sliding-PIF, which captures local manifold information for streaming\nscenarios. Our methods outperform existing techniques on synthetic and real\ndatasets. We extend this to structure-based clustering with MultiLink, a novel\nmethod for recovering multiple geometric model families in noisy data.\nMultiLink merges clusters via a model-aware linkage strategy, enabling robust\nmulti-class structure recovery. It offers key advantages over existing\napproaches, such as speed, reduced sensitivity to thresholds, and improved\nrobustness to poor initial sampling. The second part of the thesis addresses\nonline anomaly detection in evolving data streams. We propose Online Isolation\nForest (Online-iForest), which uses adaptive, multi-resolution histograms and\ndynamically updates tree structures to track changes over time. It avoids\nretraining while achieving accuracy comparable to offline models, with superior\nefficiency for real-time applications. Finally, we tackle anomaly detection in\ncybersecurity via open-set recognition for malware classification. We enhance a\nGradient Boosting classifier with MaxLogit to detect unseen malware families, a\nmethod now integrated into Cleafy's production system."}
{"id": "2505.12761", "pdf": "https://arxiv.org/pdf/2505.12761", "abs": "https://arxiv.org/abs/2505.12761", "authors": ["Donghwa Shin", "Edwin Zhang"], "title": "Enhancing Channel-Independent Time-Series Forecasting via Cross-Variate Patch Embedding", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Transformers have recently gained popularity in time series forecasting due\nto their ability to capture long-term dependencies. However, many existing\nmodels focus only on capturing temporal dependencies while omitting intricate\nrelationships between variables. Recent models have tried tackling this by\nexplicitly modeling both cross-time and cross-variate dependencies through a\nsequential or unified attention mechanism, but they are entirely channel\ndependent (CD) across all layers, making them potentially susceptible to\noverfitting. To address this, we propose Cross-Variate Patch Embeddings (CVPE),\na lightweight CD module that injects cross-variate context into\nchannel-independent (CI) models by simply modifying the patch embedding\nprocess. We achieve this by adding a learnable positional encoding and a\nlightweight router-attention block to the vanilla patch embedding layer. We\nthen integrate CVPE into Time-LLM, a multimodal CI forecasting model, to\ndemonstrate its effectiveness in capturing cross-variate dependencies and\nenhance the CI model's performance. Extensive experimental results on seven\nreal-world datasets show that our enhanced Time-LLM outperforms the original\nbaseline model simply by incorporating the CVPE module, with no other changes."}
{"id": "2505.12763", "pdf": "https://arxiv.org/pdf/2505.12763", "abs": "https://arxiv.org/abs/2505.12763", "authors": ["Sunghwan Kim", "Dongjin Kang", "Taeyoon Kwon", "Hyungjoo Chae", "Dongha Lee", "Jinyoung Yeo"], "title": "Rethinking Reward Model Evaluation Through the Lens of Reward Overoptimization", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted to ACL 2025", "summary": "Reward models (RMs) play a crucial role in reinforcement learning from human\nfeedback (RLHF), aligning model behavior with human preferences. However,\nexisting benchmarks for reward models show a weak correlation with the\nperformance of optimized policies, suggesting that they fail to accurately\nassess the true capabilities of RMs. To bridge this gap, we explore several\nevaluation designs through the lens of reward overoptimization\\textemdash a\nphenomenon that captures both how well the reward model aligns with human\npreferences and the dynamics of the learning signal it provides to the policy.\nThe results highlight three key findings on how to construct a reliable\nbenchmark: (i) it is important to minimize differences between chosen and\nrejected responses beyond correctness, (ii) evaluating reward models requires\nmultiple comparisons across a wide range of chosen and rejected responses, and\n(iii) given that reward models encounter responses with diverse\nrepresentations, responses should be sourced from a variety of models. However,\nwe also observe that a extremely high correlation with degree of\noveroptimization leads to comparatively lower correlation with certain\ndownstream performance. Thus, when designing a benchmark, it is desirable to\nuse the degree of overoptimization as a useful tool, rather than the end goal."}
{"id": "2505.12774", "pdf": "https://arxiv.org/pdf/2505.12774", "abs": "https://arxiv.org/abs/2505.12774", "authors": ["Zichen Geng", "Zeeshan Hayder", "Wei Liu", "Ajmal Mian"], "title": "UniHM: Universal Human Motion Generation with Object Interactions in Indoor Scenes", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": null, "summary": "Human motion synthesis in complex scenes presents a fundamental challenge,\nextending beyond conventional Text-to-Motion tasks by requiring the integration\nof diverse modalities such as static environments, movable objects, natural\nlanguage prompts, and spatial waypoints. Existing language-conditioned motion\nmodels often struggle with scene-aware motion generation due to limitations in\nmotion tokenization, which leads to information loss and fails to capture the\ncontinuous, context-dependent nature of 3D human movement. To address these\nissues, we propose UniHM, a unified motion language model that leverages\ndiffusion-based generation for synthesizing scene-aware human motion. UniHM is\nthe first framework to support both Text-to-Motion and Text-to-Human-Object\nInteraction (HOI) in complex 3D scenes. Our approach introduces three key\ncontributions: (1) a mixed-motion representation that fuses continuous 6DoF\nmotion with discrete local motion tokens to improve motion realism; (2) a novel\nLook-Up-Free Quantization VAE (LFQ-VAE) that surpasses traditional VQ-VAEs in\nboth reconstruction accuracy and generative performance; and (3) an enriched\nversion of the Lingo dataset augmented with HumanML3D annotations, providing\nstronger supervision for scene-specific motion learning. Experimental results\ndemonstrate that UniHM achieves comparative performance on the OMOMO benchmark\nfor text-to-HOI synthesis and yields competitive results on HumanML3D for\ngeneral text-conditioned motion generation."}
{"id": "2505.12781", "pdf": "https://arxiv.org/pdf/2505.12781", "abs": "https://arxiv.org/abs/2505.12781", "authors": ["Jitai Hao", "Qiang Huang", "Hao Liu", "Xinyan Xiao", "Zhaochun Ren", "Jun Yu"], "title": "A Token is Worth over 1,000 Tokens: Efficient Knowledge Distillation through Low-Rank Clone", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Training high-performing Small Language Models (SLMs) remains costly, even\nwith knowledge distillation and pruning from larger teacher models. Existing\nwork often faces three key challenges: (1) information loss from hard pruning,\n(2) inefficient alignment of representations, and (3) underutilization of\ninformative activations, particularly from Feed-Forward Networks (FFNs). To\naddress these challenges, we introduce Low-Rank Clone (LRC), an efficient\npre-training method that constructs SLMs aspiring to behavioral equivalence\nwith strong teacher models. LRC trains a set of low-rank projection matrices\nthat jointly enable soft pruning by compressing teacher weights, and activation\nclone by aligning student activations, including FFN signals, with those of the\nteacher. This unified design maximizes knowledge transfer while removing the\nneed for explicit alignment modules. Extensive experiments with open-source\nteachers (e.g., Llama-3.2-3B-Instruct, Qwen2.5-3B/7B-Instruct) show that LRC\nmatches or surpasses state-of-the-art models trained on trillions of\ntokens--while using only 20B tokens, achieving over 1,000x training efficiency.\nOur codes and model checkpoints are available at\nhttps://github.com/CURRENTF/LowRankClone and\nhttps://huggingface.co/collections/JitaiHao/low-rank-clone-lrc-6828389e96a93f1d4219dfaf."}
{"id": "2505.12800", "pdf": "https://arxiv.org/pdf/2505.12800", "abs": "https://arxiv.org/abs/2505.12800", "authors": ["Hieu-Nghia Huynh-Nguyen", "Ngoc Son Nguyen", "Huynh Nguyen Dang", "Thieu Vo", "Truong-Son Hy", "Van Nguyen"], "title": "OZSpeech: One-step Zero-shot Speech Synthesis with Learned-Prior-Conditioned Flow Matching", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "Text-to-speech (TTS) systems have seen significant advancements in recent\nyears, driven by improvements in deep learning and neural network\narchitectures. Viewing the output speech as a data distribution, previous\napproaches often employ traditional speech representations, such as waveforms\nor spectrograms, within the Flow Matching framework. However, these methods\nhave limitations, including overlooking various speech attributes and incurring\nhigh computational costs due to additional constraints introduced during\ntraining. To address these challenges, we introduce OZSpeech, the first TTS\nmethod to explore optimal transport conditional flow matching with one-step\nsampling and a learned prior as the condition, effectively disregarding\npreceding states and reducing the number of sampling steps. Our approach\noperates on disentangled, factorized components of speech in token format,\nenabling accurate modeling of each speech attribute, which enhances the TTS\nsystem's ability to precisely clone the prompt speech. Experimental results\nshow that our method achieves promising performance over existing methods in\ncontent accuracy, naturalness, prosody generation, and speaker style\npreservation. Audio samples are available at our demo page\nhttps://ozspeech.github.io/OZSpeech_Web/."}
{"id": "2505.12805", "pdf": "https://arxiv.org/pdf/2505.12805", "abs": "https://arxiv.org/abs/2505.12805", "authors": ["Seanie Lee", "Sangwoo Park", "Dong Bok Lee", "Dominik Wagner", "Haebin Seong", "Tobias Bocklet", "Juho Lee", "Sung Ju Hwang"], "title": "FedSVD: Adaptive Orthogonalization for Private Federated Learning with LoRA", "categories": ["cs.LG", "cs.AI"], "comment": "preprint", "summary": "Low-Rank Adaptation (LoRA), which introduces a product of two trainable\nlow-rank matrices into frozen pre-trained weights, is widely used for efficient\nfine-tuning of language models in federated learning (FL). However, when\ncombined with differentially private stochastic gradient descent (DP-SGD), LoRA\nfaces substantial noise amplification: DP-SGD perturbs per-sample gradients,\nand the matrix multiplication of the LoRA update ($BA$) intensifies this\neffect. Freezing one matrix (e.g., $A$) reduces the noise but restricts model\nexpressiveness, often resulting in suboptimal adaptation. To address this, we\npropose FedSVD, a simple yet effective method that introduces a global\nreparameterization based on singular value decomposition (SVD). In our\napproach, each client optimizes only the $B$ matrix and transmits it to the\nserver. The server aggregates the $B$ matrices, computes the product $BA$ using\nthe previous $A$, and refactorizes the result via SVD. This yields a new\nadaptive $A$ composed of the orthonormal right singular vectors of $BA$, and an\nupdated $B$ containing the remaining SVD components. This reparameterization\navoids quadratic noise amplification, while allowing $A$ to better capture the\nprincipal directions of the aggregate updates. Moreover, the orthonormal\nstructure of $A$ bounds the gradient norms of $B$ and preserves more signal\nunder DP-SGD, as confirmed by our theoretical analysis. As a result, FedSVD\nconsistently improves stability and performance across a variety of privacy\nsettings and benchmarks, outperforming relevant baselines under both private\nand non-private regimes."}
{"id": "2505.12811", "pdf": "https://arxiv.org/pdf/2505.12811", "abs": "https://arxiv.org/abs/2505.12811", "authors": ["Wei-Chen Liao", "Ti-Rong Wu", "I-Chen Wu"], "title": "Dynamic Sight Range Selection in Multi-Agent Reinforcement Learning", "categories": ["cs.MA", "cs.AI", "cs.LG"], "comment": "Accepted at AAMAS 2025. The compiled PDF includes the appendix", "summary": "Multi-agent reinforcement Learning (MARL) is often challenged by the sight\nrange dilemma, where agents either receive insufficient or excessive\ninformation from their environment. In this paper, we propose a novel method,\ncalled Dynamic Sight Range Selection (DSR), to address this issue. DSR utilizes\nan Upper Confidence Bound (UCB) algorithm and dynamically adjusts the sight\nrange during training. Experiment results show several advantages of using DSR.\nFirst, we demonstrate using DSR achieves better performance in three common\nMARL environments, including Level-Based Foraging (LBF), Multi-Robot Warehouse\n(RWARE), and StarCraft Multi-Agent Challenge (SMAC). Second, our results show\nthat DSR consistently improves performance across multiple MARL algorithms,\nincluding QMIX and MAPPO. Third, DSR offers suitable sight ranges for different\ntraining steps, thereby accelerating the training process. Finally, DSR\nprovides additional interpretability by indicating the optimal sight range used\nduring training. Unlike existing methods that rely on global information or\ncommunication mechanisms, our approach operates solely based on the individual\nsight ranges of agents. This approach offers a practical and efficient solution\nto the sight range dilemma, making it broadly applicable to real-world complex\nenvironments."}
{"id": "2505.12814", "pdf": "https://arxiv.org/pdf/2505.12814", "abs": "https://arxiv.org/abs/2505.12814", "authors": ["Xilong Cheng", "Yunxiao Qin", "Yuting Tan", "Zhengnan Li", "Ye Wang", "Hongjiang Xiao", "Yuan Zhang"], "title": "PsyMem: Fine-grained psychological alignment and Explicit Memory Control for Advanced Role-Playing LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Existing LLM-based role-playing methods often rely on superficial textual\ndescriptions or simplistic metrics, inadequately modeling both intrinsic and\nextrinsic character dimensions. Additionally, they typically simulate character\nmemory with implicit model knowledge or basic retrieval augment generation\nwithout explicit memory alignment, compromising memory consistency. The two\nissues weaken reliability of role-playing LLMs in several applications, such as\ntrustworthy social simulation. To address these limitations, we propose PsyMem,\na novel framework integrating fine-grained psychological attributes and\nexplicit memory control for role-playing. PsyMem supplements textual\ndescriptions with 26 psychological indicators to detailed model character.\nAdditionally, PsyMem implements memory alignment training, explicitly trains\nthe model to align character's response with memory, thereby enabling dynamic\nmemory-controlled responding during inference. By training Qwen2.5-7B-Instruct\non our specially designed dataset (including 5,414 characters and 38,962\ndialogues extracted from novels), the resulting model, termed as PsyMem-Qwen,\noutperforms baseline models in role-playing, achieving the best performance in\nhuman-likeness and character fidelity."}
{"id": "2505.12815", "pdf": "https://arxiv.org/pdf/2505.12815", "abs": "https://arxiv.org/abs/2505.12815", "authors": ["Wenjiao Feng", "Rongxing Xiao", "Zonghang Li", "Hongfang Yu", "Gang Sun", "Long Luo", "Mohsen Guizani", "Qirong Ho"], "title": "Learning in Chaos: Efficient Autoscaling and Self-healing for Distributed Training at the Edge", "categories": ["cs.DC", "cs.AI", "68T99", "I.2.11"], "comment": "13 pages, 16 figures", "summary": "Frequent node and link changes in edge AI clusters disrupt distributed\ntraining, while traditional checkpoint-based recovery and cloud-centric\nautoscaling are too slow for scale-out and ill-suited to chaotic and\nself-governed edge. This paper proposes Chaos, a resilient and scalable edge\ndistributed training system with built-in self-healing and autoscaling. It\nspeeds up scale-out by using multi-neighbor replication with fast shard\nscheduling, allowing a new node to pull the latest training state from nearby\nneighbors in parallel while balancing the traffic load between them. It also\nuses a cluster monitor to track resource and topology changes to assist\nscheduler decisions, and handles scaling events through peer negotiation\nprotocols, enabling fully self-governed autoscaling without a central admin.\nExtensive experiments show that Chaos consistently achieves much lower\nscale-out delays than Pollux, EDL, and Autoscaling, and handles scale-in,\nconnect-link, and disconnect-link events within 1 millisecond, making it\nsmoother to handle node joins, exits, and failures. It also delivers the lowest\nidle time, showing superior resource use and scalability as the cluster grows."}
{"id": "2505.12821", "pdf": "https://arxiv.org/pdf/2505.12821", "abs": "https://arxiv.org/abs/2505.12821", "authors": ["Han Sun", "Zhen Sun", "Zongmin Zhang", "Linzhao Jia", "Wei Shao", "Min Zhang"], "title": "SynDec: A Synthesize-then-Decode Approach for Arbitrary Textual Style Transfer via Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are emerging as dominant forces for textual\nstyle transfer. However, for arbitrary style transfer, LLMs face two key\nchallenges: (1) considerable reliance on manually-constructed prompts and (2)\nrigid stylistic biases inherent in LLMs. In this paper, we propose a novel\nSynthesize-then-Decode (SynDec) approach, which automatically synthesizes\nhigh-quality prompts and amplifies their roles during decoding process.\nSpecifically, our approach synthesizes prompts by selecting representative\nfew-shot samples, conducting a four-dimensional style analysis, and reranking\nthe candidates. At LLM decoding stage, the TST effect is amplified by\nmaximizing the contrast in output probabilities between scenarios with and\nwithout the synthesized prompt, as well as between prompts and negative\nsamples. We conduct extensive experiments and the results show that SynDec\noutperforms existing state-of-the-art LLM-based methods on five out of six\nbenchmarks (e.g., achieving up to a 9\\% increase in accuracy for\nmodern-to-Elizabethan English transfer). Detailed ablation studies further\nvalidate the effectiveness of SynDec."}
{"id": "2505.12837", "pdf": "https://arxiv.org/pdf/2505.12837", "abs": "https://arxiv.org/abs/2505.12837", "authors": ["Christian Braun", "Alexander Lilienbeck", "Daniel Mentjukov"], "title": "The Hidden Structure -- Improving Legal Document Understanding Through Explicit Text Formatting", "categories": ["cs.CL", "cs.AI"], "comment": "20 pages, 3 figures", "summary": "Legal contracts possess an inherent, semantically vital structure (e.g.,\nsections, clauses) that is crucial for human comprehension but whose impact on\nLLM processing remains under-explored. This paper investigates the effects of\nexplicit input text structure and prompt engineering on the performance of\nGPT-4o and GPT-4.1 on a legal question-answering task using an excerpt of the\nCUAD. We compare model exact-match accuracy across various input formats:\nwell-structured plain-text (human-generated from CUAD), plain-text cleaned of\nline breaks, extracted plain-text from Azure OCR, plain-text extracted by\nGPT-4o Vision, and extracted (and interpreted) Markdown (MD) from GPT-4o\nVision. To give an indication of the impact of possible prompt engineering, we\nassess the impact of shifting task instructions to the system prompt and\nexplicitly informing the model about the structured nature of the input. Our\nfindings reveal that GPT-4o demonstrates considerable robustness to variations\nin input structure, but lacks in overall performance. Conversely, GPT-4.1's\nperformance is markedly sensitive; poorly structured inputs yield suboptimal\nresults (but identical with GPT-4o), while well-structured formats (original\nCUAD text, GPT-4o Vision text and GPT-4o MD) improve exact-match accuracy by\n~20 percentage points. Optimizing the system prompt to include task details and\nan advisory about structured input further elevates GPT-4.1's accuracy by an\nadditional ~10-13 percentage points, with Markdown ultimately achieving the\nhighest performance under these conditions (79 percentage points overall\nexact-match accuracy). This research empirically demonstrates that while newer\nmodels exhibit greater resilience, careful input structuring and strategic\nprompt design remain critical for optimizing the performance of LLMs, and can\nsignificantly affect outcomes in high-stakes legal applications."}
{"id": "2505.12843", "pdf": "https://arxiv.org/pdf/2505.12843", "abs": "https://arxiv.org/abs/2505.12843", "authors": ["Kangwen Zhao", "Jianfeng Cai", "Jinhua Zhu", "Ruopei Sun", "Dongyun Xue", "Wengang Zhou", "Li Li", "Houqiang Li"], "title": "Bias Fitting to Mitigate Length Bias of Reward Model in RLHF", "categories": ["cs.LG", "cs.AI"], "comment": "Due to the word limit for arXiv abstract, the abstract here has been\n  abridged compared to the one in the PDF", "summary": "Reinforcement Learning from Human Feedback relies on reward models to align\nlarge language models with human preferences. However, RLHF often suffers from\nreward hacking, wherein policy learning exploits flaws in the trained reward\nmodel to maximize reward scores without genuinely aligning with human\npreferences. A significant example of such reward hacking is length bias, where\nreward models usually favor longer responses irrespective of actual response\nquality. Previous works on length bias have notable limitations, these\napproaches either mitigate bias without characterizing the bias form, or simply\nassume a linear length-reward relation. To accurately model the intricate\nnature of length bias and facilitate more effective bias mitigation, we propose\nFiMi-RM (Bias Fitting to Mitigate Length Bias of Reward Model in RLHF), a\nframework that autonomously learns and corrects underlying bias patterns. Our\napproach consists of three stages: First, we train a standard reward model\nwhich inherently contains length bias. Next, we deploy a lightweight fitting\nmodel to explicitly capture the non-linear relation between length and reward.\nFinally, we incorporate this learned relation into the reward model to debias.\nExperimental results demonstrate that FiMi-RM achieves a more balanced\nlength-reward distribution. Furthermore, when applied to alignment algorithms,\nour debiased reward model improves length-controlled win rate and reduces\nverbosity without compromising its performance."}
{"id": "2505.12851", "pdf": "https://arxiv.org/pdf/2505.12851", "abs": "https://arxiv.org/abs/2505.12851", "authors": ["Yanhua Wen", "Lu Ai", "Gang Liu", "Chuang Li", "Jianhao Wei"], "title": "FLTG: Byzantine-Robust Federated Learning via Angle-Based Defense and Non-IID-Aware Weighting", "categories": ["cs.CR", "cs.AI"], "comment": "14 pages, 5 figures, BlockSys2025", "summary": "Byzantine attacks during model aggregation in Federated Learning (FL)\nthreaten training integrity by manipulating malicious clients' updates.\nExisting methods struggle with limited robustness under high malicious client\nratios and sensitivity to non-i.i.d. data, leading to degraded accuracy. To\naddress this, we propose FLTG, a novel aggregation algorithm integrating\nangle-based defense and dynamic reference selection. FLTG first filters clients\nvia ReLU-clipped cosine similarity, leveraging a server-side clean dataset to\nexclude misaligned updates. It then dynamically selects a reference client\nbased on the prior global model to mitigate non-i.i.d. bias, assigns\naggregation weights inversely proportional to angular deviations, and\nnormalizes update magnitudes to suppress malicious scaling. Evaluations across\ndatasets of varying complexity under five classic attacks demonstrate FLTG's\nsuperiority over state-of-the-art methods under extreme bias scenarios and\nsustains robustness with a higher proportion(over 50%) of malicious clients."}
{"id": "2505.12863", "pdf": "https://arxiv.org/pdf/2505.12863", "abs": "https://arxiv.org/abs/2505.12863", "authors": ["Jongmin Jung", "Dongmin Kim", "Sihun Lee", "Seola Cho", "Hyungjoon Soh", "Irmak Bukey", "Chris Donahue", "Dasaem Jeong"], "title": "Unified Cross-modal Translation of Score Images, Symbolic Music, and Performance Audio", "categories": ["cs.SD", "cs.AI", "cs.CV", "eess.AS"], "comment": "Submitted to IEEE Transactions on Audio, Speech and Language\n  Processing (TASLPRO)", "summary": "Music exists in various modalities, such as score images, symbolic scores,\nMIDI, and audio. Translations between each modality are established as core\ntasks of music information retrieval, such as automatic music transcription\n(audio-to-MIDI) and optical music recognition (score image to symbolic score).\nHowever, most past work on multimodal translation trains specialized models on\nindividual translation tasks. In this paper, we propose a unified approach,\nwhere we train a general-purpose model on many translation tasks\nsimultaneously. Two key factors make this unified approach viable: a new\nlarge-scale dataset and the tokenization of each modality. Firstly, we propose\na new dataset that consists of more than 1,300 hours of paired audio-score\nimage data collected from YouTube videos, which is an order of magnitude larger\nthan any existing music modal translation datasets. Secondly, our unified\ntokenization framework discretizes score images, audio, MIDI, and MusicXML into\na sequence of tokens, enabling a single encoder-decoder Transformer to tackle\nmultiple cross-modal translation as one coherent sequence-to-sequence task.\nExperimental results confirm that our unified multitask model improves upon\nsingle-task baselines in several key areas, notably reducing the symbol error\nrate for optical music recognition from 24.58% to a state-of-the-art 13.67%,\nwhile similarly substantial improvements are observed across the other\ntranslation tasks. Notably, our approach achieves the first successful\nscore-image-conditioned audio generation, marking a significant breakthrough in\ncross-modal music generation."}
{"id": "2505.12864", "pdf": "https://arxiv.org/pdf/2505.12864", "abs": "https://arxiv.org/abs/2505.12864", "authors": ["Yu Fan", "Jingwei Ni", "Jakob Merane", "Etienne Salimbeni", "Yang Tian", "Yoan Hermstrüwer", "Yinya Huang", "Mubashara Akhtar", "Florian Geering", "Oliver Dreyer", "Daniel Brunner", "Markus Leippold", "Mrinmaya Sachan", "Alexander Stremitzer", "Christoph Engel", "Elliott Ash", "Joel Niklaus"], "title": "LEXam: Benchmarking Legal Reasoning on 340 Law Exams", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2"], "comment": null, "summary": "Long-form legal reasoning remains a key challenge for large language models\n(LLMs) in spite of recent advances in test-time scaling. We introduce LEXam, a\nnovel benchmark derived from 340 law exams spanning 116 law school courses\nacross a range of subjects and degree levels. The dataset comprises 4,886 law\nexam questions in English and German, including 2,841 long-form, open-ended\nquestions and 2,045 multiple-choice questions. Besides reference answers, the\nopen questions are also accompanied by explicit guidance outlining the expected\nlegal reasoning approach such as issue spotting, rule recall, or rule\napplication. Our evaluation on both open-ended and multiple-choice questions\npresent significant challenges for current LLMs; in particular, they notably\nstruggle with open questions that require structured, multi-step legal\nreasoning. Moreover, our results underscore the effectiveness of the dataset in\ndifferentiating between models with varying capabilities. Adopting an\nLLM-as-a-Judge paradigm with rigorous human expert validation, we demonstrate\nhow model-generated reasoning steps can be evaluated consistently and\naccurately. Our evaluation setup provides a scalable method to assess legal\nreasoning quality beyond simple accuracy metrics. Project page:\nhttps://lexam-benchmark.github.io/"}
{"id": "2505.12869", "pdf": "https://arxiv.org/pdf/2505.12869", "abs": "https://arxiv.org/abs/2505.12869", "authors": ["Koki Wakiyama", "Tomohiro I", "Hiroshi Sakamoto"], "title": "Outsourced Privacy-Preserving Feature Selection Based on Fully Homomorphic Encryption", "categories": ["cs.CR", "cs.AI"], "comment": "14 pages", "summary": "Feature selection is a technique that extracts a meaningful subset from a set\nof features in training data. When the training data is large-scale,\nappropriate feature selection enables the removal of redundant features, which\ncan improve generalization performance, accelerate the training process, and\nenhance the interpretability of the model. This study proposes a\nprivacy-preserving computation model for feature selection. Generally, when the\ndata owner and analyst are the same, there is no need to conceal the private\ninformation. However, when they are different parties or when multiple owners\nexist, an appropriate privacy-preserving framework is required. Although\nvarious private feature selection algorithms, they all require two or more\ncomputing parties and do not guarantee security in environments where no\nexternal party can be fully trusted. To address this issue, we propose the\nfirst outsourcing algorithm for feature selection using fully homomorphic\nencryption. Compared to a prior two-party algorithm, our result improves the\ntime and space complexity O(kn^2) to O(kn log^3 n) and O(kn), where k and n\ndenote the number of features and data samples, respectively. We also\nimplemented the proposed algorithm and conducted comparative experiments with\nthe naive one. The experimental result shows the efficiency of our method even\nwith small datasets."}
{"id": "2505.12871", "pdf": "https://arxiv.org/pdf/2505.12871", "abs": "https://arxiv.org/abs/2505.12871", "authors": ["Zi Liang", "Haibo Hu", "Qingqing Ye", "Yaxin Xiao", "Ronghua Li"], "title": "Does Low Rank Adaptation Lead to Lower Robustness against Training-Time Attacks?", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "comment": "To appear at ICML 25", "summary": "Low rank adaptation (LoRA) has emerged as a prominent technique for\nfine-tuning large language models (LLMs) thanks to its superb efficiency gains\nover previous methods. While extensive studies have examined the performance\nand structural properties of LoRA, its behavior upon training-time attacks\nremain underexplored, posing significant security risks. In this paper, we\ntheoretically investigate the security implications of LoRA's low-rank\nstructure during fine-tuning, in the context of its robustness against data\npoisoning and backdoor attacks. We propose an analytical framework that models\nLoRA's training dynamics, employs the neural tangent kernel to simplify the\nanalysis of the training process, and applies information theory to establish\nconnections between LoRA's low rank structure and its vulnerability against\ntraining-time attacks. Our analysis indicates that LoRA exhibits better\nrobustness to backdoor attacks than full fine-tuning, while becomes more\nvulnerable to untargeted data poisoning due to its over-simplified information\ngeometry. Extensive experimental evaluations have corroborated our theoretical\nfindings."}
{"id": "2505.12880", "pdf": "https://arxiv.org/pdf/2505.12880", "abs": "https://arxiv.org/abs/2505.12880", "authors": ["Maksim Zhdanov", "Nabil Iqbal", "Erik Bekkers", "Patrick Forré"], "title": "AdS-GNN -- a Conformally Equivariant Graph Neural Network", "categories": ["cs.LG", "cs.AI", "hep-th"], "comment": null, "summary": "Conformal symmetries, i.e.\\ coordinate transformations that preserve angles,\nplay a key role in many fields, including physics, mathematics, computer vision\nand (geometric) machine learning. Here we build a neural network that is\nequivariant under general conformal transformations. To achieve this, we lift\ndata from flat Euclidean space to Anti de Sitter (AdS) space. This allows us to\nexploit a known correspondence between conformal transformations of flat space\nand isometric transformations on the AdS space. We then build upon the fact\nthat such isometric transformations have been extensively studied on general\ngeometries in the geometric deep learning literature. We employ message-passing\nlayers conditioned on the proper distance, yielding a computationally efficient\nframework. We validate our model on tasks from computer vision and statistical\nphysics, demonstrating strong performance, improved generalization capacities,\nand the ability to extract conformal data such as scaling dimensions from the\ntrained network."}
{"id": "2505.12882", "pdf": "https://arxiv.org/pdf/2505.12882", "abs": "https://arxiv.org/abs/2505.12882", "authors": ["Hao Wang", "Jindong Han", "Wei Fan", "Weijia Zhang", "Hao Liu"], "title": "PhyDA: Physics-Guided Diffusion Models for Data Assimilation in Atmospheric Systems", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Data Assimilation (DA) plays a critical role in atmospheric science by\nreconstructing spatially continous estimates of the system state, which serves\nas initial conditions for scientific analysis. While recent advances in\ndiffusion models have shown great potential for DA tasks, most existing\napproaches remain purely data-driven and often overlook the physical laws that\ngovern complex atmospheric dynamics. As a result, they may yield physically\ninconsistent reconstructions that impair downstream applications. To overcome\nthis limitation, we propose PhyDA, a physics-guided diffusion framework\ndesigned to ensure physical coherence in atmospheric data assimilation. PhyDA\nintroduces two key components: (1) a Physically Regularized Diffusion Objective\nthat integrates physical constraints into the training process by penalizing\ndeviations from known physical laws expressed as partial differential\nequations, and (2) a Virtual Reconstruction Encoder that bridges observational\nsparsity for structured latent representations, further enhancing the model's\nability to infer complete and physically coherent states. Experiments on the\nERA5 reanalysis dataset demonstrate that PhyDA achieves superior accuracy and\nbetter physical plausibility compared to state-of-the-art baselines. Our\nresults emphasize the importance of combining generative modeling with\ndomain-specific physical knowledge and show that PhyDA offers a promising\ndirection for improving real-world data assimilation systems."}
{"id": "2505.12884", "pdf": "https://arxiv.org/pdf/2505.12884", "abs": "https://arxiv.org/abs/2505.12884", "authors": ["Yuanze Hu", "Zhaoxin Fan", "Xinyu Wang", "Gen Li", "Ye Qiu", "Zhichao Yang", "Wenjun Wu", "Kejian Wu", "Yifan Sun", "Xiaotie Deng", "Jin Dong"], "title": "TinyAlign: Boosting Lightweight Vision-Language Models by Mitigating Modal Alignment Bottlenecks", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Lightweight Vision-Language Models (VLMs) are indispensable for\nresource-constrained applications. The prevailing approach to aligning vision\nand language models involves freezing both the vision encoder and the language\nmodel while training small connector modules. However, this strategy heavily\ndepends on the intrinsic capabilities of the language model, which can be\nsuboptimal for lightweight models with limited representational capacity. In\nthis work, we investigate this alignment bottleneck through the lens of mutual\ninformation, demonstrating that the constrained capacity of the language model\ninherently limits the Effective Mutual Information (EMI) between multimodal\ninputs and outputs, thereby compromising alignment quality. To address this\nchallenge, we propose TinyAlign, a novel framework inspired by\nRetrieval-Augmented Generation, which strategically retrieves relevant context\nfrom a memory bank to enrich multimodal inputs and enhance their alignment.\nExtensive empirical evaluations reveal that TinyAlign significantly reduces\ntraining loss, accelerates convergence, and enhances task performance.\nRemarkably, it allows models to achieve baseline-level performance with only\n40\\% of the fine-tuning data, highlighting exceptional data efficiency. Our\nwork thus offers a practical pathway for developing more capable lightweight\nVLMs while introducing a fresh theoretical lens to better understand and\naddress alignment bottlenecks in constrained multimodal systems."}
{"id": "2505.12894", "pdf": "https://arxiv.org/pdf/2505.12894", "abs": "https://arxiv.org/abs/2505.12894", "authors": ["Le Cheng", "Peican Zhu", "Yangming Guo", "Keke Tang", "Chao Gao", "Zhen Wang"], "title": "HyperDet: Source Detection in Hypergraphs via Interactive Relationship Construction and Feature-rich Attention Fusion", "categories": ["cs.SI", "cs.AI"], "comment": "Accepted by IJCAI25", "summary": "Hypergraphs offer superior modeling capabilities for social networks,\nparticularly in capturing group phenomena that extend beyond pairwise\ninteractions in rumor propagation. Existing approaches in rumor source\ndetection predominantly focus on dyadic interactions, which inadequately\naddress the complexity of more intricate relational structures. In this study,\nwe present a novel approach for Source Detection in Hypergraphs (HyperDet) via\nInteractive Relationship Construction and Feature-rich Attention Fusion.\nSpecifically, our methodology employs an Interactive Relationship Construction\nmodule to accurately model both the static topology and dynamic interactions\namong users, followed by the Feature-rich Attention Fusion module, which\nautonomously learns node features and discriminates between nodes using a\nself-attention mechanism, thereby effectively learning node representations\nunder the framework of accurately modeled higher-order relationships. Extensive\nexperimental validation confirms the efficacy of our HyperDet approach,\nshowcasing its superiority relative to current state-of-the-art methods."}
{"id": "2505.12900", "pdf": "https://arxiv.org/pdf/2505.12900", "abs": "https://arxiv.org/abs/2505.12900", "authors": ["Shuyang Hou", "Zhangxiao Shen", "Huayi Wu", "Jianyuan Liang", "Haoyue Jiao", "Yaxian Qing", "Xiaopu Zhang", "Xu Li", "Zhipeng Gui", "Xuefeng Guan", "Longgang Xiang"], "title": "AutoGEEval: A Multimodal and Automated Framework for Geospatial Code Generation on GEE with Large Language Models", "categories": ["cs.SE", "cs.AI", "cs.CG", "cs.CL", "cs.DB"], "comment": null, "summary": "Geospatial code generation is emerging as a key direction in the integration\nof artificial intelligence and geoscientific analysis. However, there remains a\nlack of standardized tools for automatic evaluation in this domain. To address\nthis gap, we propose AutoGEEval, the first multimodal, unit-level automated\nevaluation framework for geospatial code generation tasks on the Google Earth\nEngine (GEE) platform powered by large language models (LLMs). Built upon the\nGEE Python API, AutoGEEval establishes a benchmark suite (AutoGEEval-Bench)\ncomprising 1325 test cases that span 26 GEE data types. The framework\nintegrates both question generation and answer verification components to\nenable an end-to-end automated evaluation pipeline-from function invocation to\nexecution validation. AutoGEEval supports multidimensional quantitative\nanalysis of model outputs in terms of accuracy, resource consumption, execution\nefficiency, and error types. We evaluate 18 state-of-the-art LLMs-including\ngeneral-purpose, reasoning-augmented, code-centric, and geoscience-specialized\nmodels-revealing their performance characteristics and potential optimization\npathways in GEE code generation. This work provides a unified protocol and\nfoundational resource for the development and assessment of geospatial code\ngeneration models, advancing the frontier of automated natural language to\ndomain-specific code translation."}
{"id": "2505.12903", "pdf": "https://arxiv.org/pdf/2505.12903", "abs": "https://arxiv.org/abs/2505.12903", "authors": ["Shiao Wang", "Xiao Wang", "Liye Jin", "Bo Jiang", "Lin Zhu", "Lan Chen", "Yonghong Tian", "Bin Luo"], "title": "Towards Low-Latency Event Stream-based Visual Object Tracking: A Slow-Fast Approach", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Existing tracking algorithms typically rely on low-frame-rate RGB cameras\ncoupled with computationally intensive deep neural network architectures to\nachieve effective tracking. However, such frame-based methods inherently face\nchallenges in achieving low-latency performance and often fail in\nresource-constrained environments. Visual object tracking using bio-inspired\nevent cameras has emerged as a promising research direction in recent years,\noffering distinct advantages for low-latency applications. In this paper, we\npropose a novel Slow-Fast Tracking paradigm that flexibly adapts to different\noperational requirements, termed SFTrack. The proposed framework supports two\ncomplementary modes, i.e., a high-precision slow tracker for scenarios with\nsufficient computational resources, and an efficient fast tracker tailored for\nlatency-aware, resource-constrained environments. Specifically, our framework\nfirst performs graph-based representation learning from\nhigh-temporal-resolution event streams, and then integrates the learned\ngraph-structured information into two FlashAttention-based vision backbones,\nyielding the slow and fast trackers, respectively. The fast tracker achieves\nlow latency through a lightweight network design and by producing multiple\nbounding box outputs in a single forward pass. Finally, we seamlessly combine\nboth trackers via supervised fine-tuning and further enhance the fast tracker's\nperformance through a knowledge distillation strategy. Extensive experiments on\npublic benchmarks, including FE240, COESOT, and EventVOT, demonstrate the\neffectiveness and efficiency of our proposed method across different real-world\nscenarios. The source code has been released on\nhttps://github.com/Event-AHU/SlowFast_Event_Track."}
{"id": "2505.12904", "pdf": "https://arxiv.org/pdf/2505.12904", "abs": "https://arxiv.org/abs/2505.12904", "authors": ["Hilde I. Hummel", "Arwin Gansekoele", "Sandjai Bhulai", "Rob van der Mei"], "title": "The Computation of Generalized Embeddings for Underwater Acoustic Target Recognition using Contrastive Learning", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "The increasing level of sound pollution in marine environments poses an\nincreased threat to ocean health, making it crucial to monitor underwater\nnoise. By monitoring this noise, the sources responsible for this pollution can\nbe mapped. Monitoring is performed by passively listening to these sounds. This\ngenerates a large amount of data records, capturing a mix of sound sources such\nas ship activities and marine mammal vocalizations. Although machine learning\noffers a promising solution for automatic sound classification, current\nstate-of-the-art methods implement supervised learning. This requires a large\namount of high-quality labeled data that is not publicly available. In\ncontrast, a massive amount of lower-quality unlabeled data is publicly\navailable, offering the opportunity to explore unsupervised learning\ntechniques. This research explores this possibility by implementing an\nunsupervised Contrastive Learning approach. Here, a Conformer-based encoder is\noptimized by the so-called Variance-Invariance-Covariance Regularization loss\nfunction on these lower-quality unlabeled data and the translation to the\nlabeled data is made. Through classification tasks involving recognizing ship\ntypes and marine mammal vocalizations, our method demonstrates to produce\nrobust and generalized embeddings. This shows to potential of unsupervised\nmethods for various automatic underwater acoustic analysis tasks."}
{"id": "2505.12908", "pdf": "https://arxiv.org/pdf/2505.12908", "abs": "https://arxiv.org/abs/2505.12908", "authors": ["Xiao Wang", "Yu Jin", "Lan Chen", "Bo Jiang", "Lin Zhu", "Yonghong Tian", "Jin Tang", "Bin Luo"], "title": "Dynamic Graph Induced Contour-aware Heat Conduction Network for Event-based Object Detection", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Event-based Vision Sensors (EVS) have demonstrated significant advantages\nover traditional RGB frame-based cameras in low-light conditions, high-speed\nmotion capture, and low latency. Consequently, object detection based on EVS\nhas attracted increasing attention from researchers. Current event stream\nobject detection algorithms are typically built upon Convolutional Neural\nNetworks (CNNs) or Transformers, which either capture limited local features\nusing convolutional filters or incur high computational costs due to the\nutilization of self-attention. Recently proposed vision heat conduction\nbackbone networks have shown a good balance between efficiency and accuracy;\nhowever, these models are not specifically designed for event stream data. They\nexhibit weak capability in modeling object contour information and fail to\nexploit the benefits of multi-scale features. To address these issues, this\npaper proposes a novel dynamic graph induced contour-aware heat conduction\nnetwork for event stream based object detection, termed CvHeat-DET. The\nproposed model effectively leverages the clear contour information inherent in\nevent streams to predict the thermal diffusivity coefficients within the heat\nconduction model, and integrates hierarchical structural graph features to\nenhance feature learning across multiple scales. Extensive experiments on three\nbenchmark datasets for event stream-based object detection fully validated the\neffectiveness of the proposed model. The source code of this paper will be\nreleased on https://github.com/Event-AHU/OpenEvDET."}
{"id": "2505.12909", "pdf": "https://arxiv.org/pdf/2505.12909", "abs": "https://arxiv.org/abs/2505.12909", "authors": ["Alberto Fernández-Hernández", "Jose I. Mestre", "Manuel F. Dolz", "Jose Duato", "Enrique S. Quintana-Ortí"], "title": "Sinusoidal Initialization, Time for a New Start", "categories": ["cs.LG", "cs.AI", "I.2; G.3; I.2.6"], "comment": null, "summary": "Initialization plays a critical role in Deep Neural Network training,\ndirectly influencing convergence, stability, and generalization. Common\napproaches such as Glorot and He initializations rely on randomness, which can\nproduce uneven weight distributions across layer connections. In this paper, we\nintroduce the Sinusoidal initialization, a novel deterministic method that\nemploys sinusoidal functions to construct structured weight matrices expressly\nto improve the spread and balance of weights throughout the network while\nsimultaneously fostering a more uniform, well-conditioned distribution of\nneuron activation states from the very first forward pass. Because Sinusoidal\ninitialization begins with weights and activations that are already evenly and\nefficiently utilized, it delivers consistently faster convergence, greater\ntraining stability, and higher final accuracy across a wide range of models,\nincluding convolutional neural networks, vision transformers, and large\nlanguage models. On average, our experiments show an increase of 4.8 % in final\nvalidation accuracy and 20.9 % in convergence speed. By replacing randomness\nwith structure, this initialization provides a stronger and more reliable\nfoundation for Deep Learning systems."}
{"id": "2505.12910", "pdf": "https://arxiv.org/pdf/2505.12910", "abs": "https://arxiv.org/abs/2505.12910", "authors": ["Le Cheng", "Peican Zhu", "Yangming Guo", "Chao Gao", "Zhen Wang", "Keke Tang"], "title": "SourceDetMamba: A Graph-aware State Space Model for Source Detection in Sequential Hypergraphs", "categories": ["cs.SI", "cs.AI"], "comment": "Accepted by IJCAI25", "summary": "Source detection on graphs has demonstrated high efficacy in identifying\nrumor origins. Despite advances in machine learning-based methods, many fail to\ncapture intrinsic dynamics of rumor propagation. In this work, we present\nSourceDetMamba: A Graph-aware State Space Model for Source Detection in\nSequential Hypergraphs, which harnesses the recent success of the state space\nmodel Mamba, known for its superior global modeling capabilities and\ncomputational efficiency, to address this challenge. Specifically, we first\nemploy hypergraphs to model high-order interactions within social networks.\nSubsequently, temporal network snapshots generated during the propagation\nprocess are sequentially fed in reverse order into Mamba to infer underlying\npropagation dynamics. Finally, to empower the sequential model to effectively\ncapture propagation patterns while integrating structural information, we\npropose a novel graph-aware state update mechanism, wherein the state of each\nnode is propagated and refined by both temporal dependencies and topological\ncontext. Extensive evaluations on eight datasets demonstrate that\nSourceDetMamba consistently outperforms state-of-the-art approaches."}
{"id": "2505.12920", "pdf": "https://arxiv.org/pdf/2505.12920", "abs": "https://arxiv.org/abs/2505.12920", "authors": ["Paul Van Eecke", "Katrien Beuls"], "title": "PyFCG: Fluid Construction Grammar in Python", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": null, "summary": "We present PyFCG, an open source software library that ports Fluid\nConstruction Grammar (FCG) to the Python programming language. PyFCG enables\nits users to seamlessly integrate FCG functionality into Python programs, and\nto use FCG in combination with other libraries within Python's rich ecosystem.\nApart from a general description of the library, this paper provides three\nwalkthrough tutorials that demonstrate example usage of PyFCG in typical use\ncases of FCG: (i) formalising and testing construction grammar analyses, (ii)\nlearning usage-based construction grammars from corpora, and (iii) implementing\nagent-based experiments on emergent communication."}
{"id": "2505.12925", "pdf": "https://arxiv.org/pdf/2505.12925", "abs": "https://arxiv.org/abs/2505.12925", "authors": ["Han Deng", "Yuan Meng", "Shixiang Tang", "Wanli Ouyang", "Xinzhu Ma"], "title": "CPRet: A Dataset, Benchmark, and Model for Retrieval in Competitive Programming", "categories": ["cs.SE", "cs.AI", "cs.IR", "H.3.3"], "comment": "main 9 pages", "summary": "Competitive programming benchmarks are widely used in scenarios such as\nprogramming contests and large language model assessments. However, the growing\npresence of duplicate or highly similar problems raises concerns not only about\ncompetition fairness, but also about the validity of competitive programming as\na benchmark for model evaluation. In this paper, we propose a new problem --\nsimilar question retrieval -- to address this issue. Due to the lack of both\ndata and models, solving this problem is challenging. To this end, we introduce\nCPRet, a retrieval-oriented benchmark suite for competitive programming,\ncovering four retrieval tasks: two code-centric (i.e., Text-to-Code and\nCode-to-Code) and two newly proposed problem-centric tasks (i.e.,\nProblem-to-Duplicate and Simplified-to-Full), built from a combination of\nautomatically crawled problem-solution data and manually curated annotations.\nOur contribution includes both high-quality training data and temporally\nseparated test sets for reliable evaluation. In addition, we develop two\ntask-specialized retrievers based on this dataset: CPRetriever-Code, trained\nwith a novel Group-InfoNCE loss for problem-code alignment, and\nCPRetriever-Prob, fine-tuned for identifying problem-level similarity. Both\nmodels achieve strong results and are open-sourced for local use. Finally, we\nanalyze LiveCodeBench and find that high-similarity problems inflate model pass\nrates and reduce differentiation, underscoring the need for similarity-aware\nevaluation in future benchmarks.\n  Code and data are available at: https://github.com/coldchair/CPRet"}
{"id": "2505.12929", "pdf": "https://arxiv.org/pdf/2505.12929", "abs": "https://arxiv.org/abs/2505.12929", "authors": ["Zhihe Yang", "Xufang Luo", "Zilong Wang", "Dongqi Han", "Zhiyuan He", "Dongsheng Li", "Yunjian Xu"], "title": "Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "24 pages, 12 figures", "summary": "Reinforcement learning (RL) has become a cornerstone for enhancing the\nreasoning capabilities of large language models (LLMs), with recent innovations\nsuch as Group Relative Policy Optimization (GRPO) demonstrating exceptional\neffectiveness. In this study, we identify a critical yet underexplored issue in\nRL training: low-probability tokens disproportionately influence model updates\ndue to their large gradient magnitudes. This dominance hinders the effective\nlearning of high-probability tokens, whose gradients are essential for LLMs'\nperformance but are substantially suppressed. To mitigate this interference, we\npropose two novel methods: Advantage Reweighting and Low-Probability Token\nIsolation (Lopti), both of which effectively attenuate gradients from\nlow-probability tokens while emphasizing parameter updates driven by\nhigh-probability tokens. Our approaches promote balanced updates across tokens\nwith varying probabilities, thereby enhancing the efficiency of RL training.\nExperimental results demonstrate that they substantially improve the\nperformance of GRPO-trained LLMs, achieving up to a 46.2% improvement in K&K\nLogic Puzzle reasoning tasks. Our implementation is available at\nhttps://github.com/zhyang2226/AR-Lopti."}
{"id": "2505.12938", "pdf": "https://arxiv.org/pdf/2505.12938", "abs": "https://arxiv.org/abs/2505.12938", "authors": ["Uri Dalal", "Meirav Segal", "Zvika Ben-Haim", "Dan Lahav", "Omer Nevo"], "title": "Leveraging LLM Inconsistency to Boost Pass@k Performance", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) achieve impressive abilities in numerous\ndomains, but exhibit inconsistent performance in response to minor input\nchanges. Rather than view this as a drawback, in this paper we introduce a\nnovel method for leveraging models' inconsistency to boost Pass@k performance.\nSpecifically, we present a \"Variator\" agent that generates k variants of a\ngiven task and submits one candidate solution for each one. Our variant\ngeneration approach is applicable to a wide range of domains as it is task\nagnostic and compatible with free-form inputs. We demonstrate the efficacy of\nour agent theoretically using a probabilistic model of the inconsistency\neffect, and show empirically that it outperforms the baseline on the APPS\ndataset. Furthermore, we establish that inconsistency persists even in frontier\nreasoning models across coding and cybersecurity domains, suggesting our method\nis likely to remain relevant for future model generations."}
{"id": "2505.12942", "pdf": "https://arxiv.org/pdf/2505.12942", "abs": "https://arxiv.org/abs/2505.12942", "authors": ["Jeffrey T. H. Wong", "Cheng Zhang", "Xinye Cao", "Pedro Gimenes", "George A. Constantinides", "Wayne Luk", "Yiren Zhao"], "title": "A3 : an Analytical Low-Rank Approximation Framework for Attention", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance."}
{"id": "2505.12944", "pdf": "https://arxiv.org/pdf/2505.12944", "abs": "https://arxiv.org/abs/2505.12944", "authors": ["Jan Hagnberger", "Daniel Musekamp", "Mathias Niepert"], "title": "CALM-PDE: Continuous and Adaptive Convolutions for Latent Space Modeling of Time-dependent PDEs", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.NE", "physics.comp-ph"], "comment": null, "summary": "Solving time-dependent Partial Differential Equations (PDEs) using a densely\ndiscretized spatial domain is a fundamental problem in various scientific and\nengineering disciplines, including modeling climate phenomena and fluid\ndynamics. However, performing these computations directly in the physical space\noften incurs significant computational costs. To address this issue, several\nneural surrogate models have been developed that operate in a compressed latent\nspace to solve the PDE. While these approaches reduce computational complexity,\nthey often use Transformer-based attention mechanisms to handle irregularly\nsampled domains, resulting in increased memory consumption. In contrast,\nconvolutional neural networks allow memory-efficient encoding and decoding but\nare limited to regular discretizations. Motivated by these considerations, we\npropose CALM-PDE, a model class that efficiently solves arbitrarily discretized\nPDEs in a compressed latent space. We introduce a novel continuous\nconvolution-based encoder-decoder architecture that uses an\nepsilon-neighborhood-constrained kernel and learns to apply the convolution\noperator to adaptive and optimized query points. We demonstrate the\neffectiveness of CALM-PDE on a diverse set of PDEs with both regularly and\nirregularly sampled spatial domains. CALM-PDE is competitive with or\noutperforms existing baseline methods while offering significant improvements\nin memory and inference time efficiency compared to Transformer-based methods."}
{"id": "2505.12951", "pdf": "https://arxiv.org/pdf/2505.12951", "abs": "https://arxiv.org/abs/2505.12951", "authors": ["Xuerui Su", "Liya Guo", "Yue Wang", "Yi Zhu", "Zhiming Ma", "Zun Wang", "Yuting Liu"], "title": "DGRO: Enhancing LLM Reasoning via Exploration-Exploitation Control and Reward Variance Management", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Inference scaling further accelerates Large Language Models (LLMs) toward\nArtificial General Intelligence (AGI), with large-scale Reinforcement Learning\n(RL) to unleash long Chain-of-Thought reasoning. Most contemporary reasoning\napproaches usually rely on handcrafted rule-based reward functions. However,\nthe tarde-offs of exploration and exploitation in RL algorithms involves\nmultiple complex considerations, and the theoretical and empirical impacts of\nmanually designed reward functions remain insufficiently explored. In this\npaper, we propose Decoupled Group Reward Optimization (DGRO), a general RL\nalgorithm for LLM reasoning. On the one hand, DGRO decouples the traditional\nregularization coefficient into two independent hyperparameters: one scales the\npolicy gradient term, and the other regulates the distance from the sampling\npolicy. This decoupling not only enables precise control over balancing\nexploration and exploitation, but also can be seamlessly extended to Online\nPolicy Mirror Descent (OPMD) algorithms in Kimi k1.5 and Direct Reward\nOptimization. On the other hand, we observe that reward variance significantly\naffects both convergence speed and final model performance. We conduct both\ntheoretical analysis and extensive empirical validation to assess DGRO,\nincluding a detailed ablation study that investigates its performance and\noptimization dynamics. Experimental results show that DGRO achieves\nstate-of-the-art performance on the Logic dataset with an average accuracy of\n96.9\\%, and demonstrates strong generalization across mathematical benchmarks."}
{"id": "2505.12960", "pdf": "https://arxiv.org/pdf/2505.12960", "abs": "https://arxiv.org/abs/2505.12960", "authors": ["Chengping He", "Mingrui Jiang", "Keyi Shan", "Szu-Hao Yang", "Zefan Li", "Shengbo Wang", "Giacomo Pedretti", "Jim Ignowski", "Can Li"], "title": "Hardware-Adaptive and Superlinear-Capacity Memristor-based Associative Memory", "categories": ["cs.LG", "cs.AI", "cs.ET"], "comment": null, "summary": "Brain-inspired computing aims to mimic cognitive functions like associative\nmemory, the ability to recall complete patterns from partial cues. Memristor\ntechnology offers promising hardware for such neuromorphic systems due to its\npotential for efficient in-memory analog computing. Hopfield Neural Networks\n(HNNs) are a classic model for associative memory, but implementations on\nconventional hardware suffer from efficiency bottlenecks, while prior\nmemristor-based HNNs faced challenges with vulnerability to hardware defects\ndue to offline training, limited storage capacity, and difficulty processing\nanalog patterns. Here we introduce and experimentally demonstrate on integrated\nmemristor hardware a new hardware-adaptive learning algorithm for associative\nmemories that significantly improves defect tolerance and capacity, and\nnaturally extends to scalable multilayer architectures capable of handling both\nbinary and continuous patterns. Our approach achieves 3x effective capacity\nunder 50% device faults compared to state-of-the-art methods. Furthermore, its\nextension to multilayer architectures enables superlinear capacity scaling\n(\\(\\propto N^{1.49}\\ for binary patterns) and effective recalling of continuous\npatterns (\\propto N^{1.74}\\ scaling), as compared to linear capacity scaling\nfor previous HNNs. It also provides flexibility to adjust capacity by tuning\nhidden neurons for the same-sized patterns. By leveraging the massive\nparallelism of the hardware enabled by synchronous updates, it reduces energy\nby 8.8x and latency by 99.7% for 64-dimensional patterns over asynchronous\nschemes, with greater improvements at scale. This promises the development of\nmore reliable memristor-based associative memory systems and enables new\napplications research due to the significantly improved capacity, efficiency,\nand flexibility."}
{"id": "2505.12963", "pdf": "https://arxiv.org/pdf/2505.12963", "abs": "https://arxiv.org/abs/2505.12963", "authors": ["Maksim I. Ivanov", "Olga E. Mendybaeva", "Yuri E. Karyakin", "Igor N. Glukhikh", "Aleksey V. Lebedev"], "title": "Segmentation of temporomandibular joint structures on mri images using neural networks for diagnosis of pathologies", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "10 pages, 10 figures", "summary": "This article explores the use of artificial intelligence for the diagnosis of\npathologies of the temporomandibular joint (TMJ), in particular, for the\nsegmentation of the articular disc on MRI images. The relevance of the work is\ndue to the high prevalence of TMJ pathologies, as well as the need to improve\nthe accuracy and speed of diagnosis in medical institutions. During the study,\nthe existing solutions (Diagnocat, MandSeg) were analyzed, which, as a result,\nare not suitable for studying the articular disc due to the orientation towards\nbone structures. To solve the problem, an original dataset was collected from\n94 images with the classes \"temporomandibular joint\" and \"jaw\". To increase the\namount of data, augmentation methods were used. After that, the models of\nU-Net, YOLOv8n, YOLOv11n and Roboflow neural networks were trained and\ncompared. The evaluation was carried out according to the Dice Score,\nPrecision, Sensitivity, Specificity, and Mean Average Precision metrics. The\nresults confirm the potential of using the Roboflow model for segmentation of\nthe temporomandibular joint. In the future, it is planned to develop an\nalgorithm for measuring the distance between the jaws and determining the\nposition of the articular disc, which will improve the diagnosis of TMJ\npathologies."}
{"id": "2505.12966", "pdf": "https://arxiv.org/pdf/2505.12966", "abs": "https://arxiv.org/abs/2505.12966", "authors": ["Zihan Xiong", "Xiaohua Wu", "Lei Chen", "Fangqi Lou"], "title": "Multiscale Adaptive Conflict-Balancing Model For Multimedia Deepfake Detection", "categories": ["cs.CV", "cs.AI"], "comment": "9 pages,ICMR accepted", "summary": "Advances in computer vision and deep learning have blurred the line between\ndeepfakes and authentic media, undermining multimedia credibility through\naudio-visual forgery. Current multimodal detection methods remain limited by\nunbalanced learning between modalities. To tackle this issue, we propose an\nAudio-Visual Joint Learning Method (MACB-DF) to better mitigate modality\nconflicts and neglect by leveraging contrastive learning to assist in\nmulti-level and cross-modal fusion, thereby fully balancing and exploiting\ninformation from each modality. Additionally, we designed an\northogonalization-multimodal pareto module that preserves unimodal information\nwhile addressing gradient conflicts in audio-video encoders caused by differing\noptimization targets of the loss functions. Extensive experiments and ablation\nstudies conducted on mainstream deepfake datasets demonstrate consistent\nperformance gains of our model across key evaluation metrics, achieving an\naverage accuracy of 95.5% across multiple datasets. Notably, our method\nexhibits superior cross-dataset generalization capabilities, with absolute\nimprovements of 8.0% and 7.7% in ACC scores over the previous best-performing\napproach when trained on DFDC and tested on DefakeAVMiT and FakeAVCeleb\ndatasets."}
{"id": "2505.12981", "pdf": "https://arxiv.org/pdf/2505.12981", "abs": "https://arxiv.org/abs/2505.12981", "authors": ["Liangxuan Wu", "Chao Wang", "Tianming Liu", "Yanjie Zhao", "Haoyu Wang"], "title": "From Assistants to Adversaries: Exploring the Security Risks of Mobile LLM Agents", "categories": ["cs.CR", "cs.AI", "cs.HC"], "comment": null, "summary": "The growing adoption of large language models (LLMs) has led to a new\nparadigm in mobile computing--LLM-powered mobile AI agents--capable of\ndecomposing and automating complex tasks directly on smartphones. However, the\nsecurity implications of these agents remain largely unexplored. In this paper,\nwe present the first comprehensive security analysis of mobile LLM agents,\nencompassing three representative categories: System-level AI Agents developed\nby original equipment manufacturers (e.g., YOYO Assistant), Third-party\nUniversal Agents (e.g., Zhipu AI AutoGLM), and Emerging Agent Frameworks (e.g.,\nAlibaba Mobile Agent). We begin by analyzing the general workflow of mobile\nagents and identifying security threats across three core capability\ndimensions: language-based reasoning, GUI-based interaction, and system-level\nexecution. Our analysis reveals 11 distinct attack surfaces, all rooted in the\nunique capabilities and interaction patterns of mobile LLM agents, and spanning\ntheir entire operational lifecycle. To investigate these threats in practice,\nwe introduce AgentScan, a semi-automated security analysis framework that\nsystematically evaluates mobile LLM agents across all 11 attack scenarios.\nApplying AgentScan to nine widely deployed agents, we uncover a concerning\ntrend: every agent is vulnerable to targeted attacks. In the most severe cases,\nagents exhibit vulnerabilities across eight distinct attack vectors. These\nattacks can cause behavioral deviations, privacy leakage, or even full\nexecution hijacking. Based on these findings, we propose a set of defensive\ndesign principles and practical recommendations for building secure mobile LLM\nagents. Our disclosures have received positive feedback from two major device\nvendors. Overall, this work highlights the urgent need for standardized\nsecurity practices in the fast-evolving landscape of LLM-driven mobile\nautomation."}
{"id": "2505.12983", "pdf": "https://arxiv.org/pdf/2505.12983", "abs": "https://arxiv.org/abs/2505.12983", "authors": ["Jiaan Wang", "Fandong Meng", "Zengkui Sun", "Yunlong Liang", "Yuxuan Cao", "Jiarong Xu", "Haoxiang Shi", "Jie Zhou"], "title": "An Empirical Study of Many-to-Many Summarization with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 main conference", "summary": "Many-to-many summarization (M2MS) aims to process documents in any language\nand generate the corresponding summaries also in any language. Recently, large\nlanguage models (LLMs) have shown strong multi-lingual abilities, giving them\nthe potential to perform M2MS in real applications. This work presents a\nsystematic empirical study on LLMs' M2MS ability. Specifically, we first\nreorganize M2MS data based on eight previous domain-specific datasets. The\nreorganized data contains 47.8K samples spanning five domains and six\nlanguages, which could be used to train and evaluate LLMs. Then, we benchmark\n18 LLMs in a zero-shot manner and an instruction-tuning manner. Fine-tuned\ntraditional models (e.g., mBART) are also conducted for comparisons. Our\nexperiments reveal that, zero-shot LLMs achieve competitive results with\nfine-tuned traditional models. After instruct-tuning, open-source LLMs can\nsignificantly improve their M2MS ability, and outperform zero-shot LLMs\n(including GPT-4) in terms of automatic evaluations. In addition, we\ndemonstrate that this task-specific improvement does not sacrifice the LLMs'\ngeneral task-solving abilities. However, as revealed by our human evaluation,\nLLMs still face the factuality issue, and the instruction tuning might\nintensify the issue. Thus, how to control factual errors becomes the key when\nbuilding LLM summarizers in real applications, and is worth noting in future\nresearch."}
{"id": "2505.12992", "pdf": "https://arxiv.org/pdf/2505.12992", "abs": "https://arxiv.org/abs/2505.12992", "authors": ["Baohao Liao", "Hanze Dong", "Yuhui Xu", "Doyen Sahoo", "Christof Monz", "Junnan Li", "Caiming Xiong"], "title": "Fractured Chain-of-Thought Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Inference-time scaling techniques have significantly bolstered the reasoning\ncapabilities of large language models (LLMs) by harnessing additional\ncomputational effort at inference without retraining. Similarly,\nChain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy\nby generating rich intermediate reasoning trajectories, but these approaches\nincur substantial token costs that impede their deployment in latency-sensitive\nsettings. In this work, we first show that truncated CoT, which stops reasoning\nbefore completion and directly generates the final answer, often matches full\nCoT sampling while using dramatically fewer tokens. Building on this insight,\nwe introduce Fractured Sampling, a unified inference-time strategy that\ninterpolates between full CoT and solution-only sampling along three orthogonal\naxes: (1) the number of reasoning trajectories, (2) the number of final\nsolutions per trajectory, and (3) the depth at which reasoning traces are\ntruncated. Through extensive experiments on five diverse reasoning benchmarks\nand several model scales, we demonstrate that Fractured Sampling consistently\nachieves superior accuracy-cost trade-offs, yielding steep log-linear scaling\ngains in Pass@k versus token budget. Our analysis reveals how to allocate\ncomputation across these dimensions to maximize performance, paving the way for\nmore efficient and scalable LLM reasoning."}
{"id": "2505.12996", "pdf": "https://arxiv.org/pdf/2505.12996", "abs": "https://arxiv.org/abs/2505.12996", "authors": ["Jiaan Wang", "Fandong Meng", "Jie Zhou"], "title": "ExTrans: Multilingual Deep Reasoning Translation via Exemplar-Enhanced Reinforcement Learning", "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 2 figures", "summary": "In recent years, the emergence of large reasoning models (LRMs), such as\nOpenAI-o1 and DeepSeek-R1, has shown impressive capabilities in complex\nproblems, e.g., mathematics and coding. Some pioneering studies attempt to\nbring the success of LRMs in neural machine translation (MT). They try to build\nLRMs with deep reasoning MT ability via reinforcement learning (RL). Despite\nsome progress that has been made, these attempts generally focus on several\nhigh-resource languages, e.g., English and Chinese, leaving the performance on\nother languages unclear. Besides, the reward modeling methods in previous work\ndo not fully unleash the potential of reinforcement learning in MT. In this\nwork, we first design a new reward modeling method that compares the\ntranslation results of the policy MT model with a strong LRM (i.e.,\nDeepSeek-R1-671B), and quantifies the comparisons to provide rewards.\nExperimental results demonstrate the superiority of the reward modeling method.\nUsing Qwen2.5-7B-Instruct as the backbone, the trained model achieves the new\nstate-of-the-art performance in literary translation, and outperforms strong\nLRMs including OpenAI-o1 and DeepSeeK-R1. Furthermore, we extend our method to\nthe multilingual settings with 11 languages. With a carefully designed\nlightweight reward modeling in RL, we can simply transfer the strong MT ability\nfrom a single direction into multiple (i.e., 90) translation directions and\nachieve impressive multilingual MT performance."}
{"id": "2505.13010", "pdf": "https://arxiv.org/pdf/2505.13010", "abs": "https://arxiv.org/abs/2505.13010", "authors": ["Himel Ghosh", "Ahmed Mosharafa", "Georg Groh"], "title": "To Bias or Not to Bias: Detecting bias in News with bias-detector", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "7 pages, 5 figures, 2 tables", "summary": "Media bias detection is a critical task in ensuring fair and balanced\ninformation dissemination, yet it remains challenging due to the subjectivity\nof bias and the scarcity of high-quality annotated data. In this work, we\nperform sentence-level bias classification by fine-tuning a RoBERTa-based model\non the expert-annotated BABE dataset. Using McNemar's test and the 5x2\ncross-validation paired t-test, we show statistically significant improvements\nin performance when comparing our model to a domain-adaptively pre-trained\nDA-RoBERTa baseline. Furthermore, attention-based analysis shows that our model\navoids common pitfalls like oversensitivity to politically charged terms and\ninstead attends more meaningfully to contextually relevant tokens. For a\ncomprehensive examination of media bias, we present a pipeline that combines\nour model with an already-existing bias-type classifier. Our method exhibits\ngood generalization and interpretability, despite being constrained by\nsentence-level analysis and dataset size because of a lack of larger and more\nadvanced bias corpora. We talk about context-aware modeling, bias\nneutralization, and advanced bias type classification as potential future\ndirections. Our findings contribute to building more robust, explainable, and\nsocially responsible NLP systems for media bias detection."}
{"id": "2505.13023", "pdf": "https://arxiv.org/pdf/2505.13023", "abs": "https://arxiv.org/abs/2505.13023", "authors": ["Yimao Guo", "Zuomin Qu", "Wei Lu", "Xiangyang Luo"], "title": "Anti-Inpainting: A Proactive Defense against Malicious Diffusion-based Inpainters under Unknown Conditions", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": null, "summary": "As diffusion-based malicious image manipulation becomes increasingly\nprevalent, multiple proactive defense methods are developed to safeguard images\nagainst unauthorized tampering. However, most proactive defense methods only\ncan safeguard images against manipulation under known conditions, and fail to\nprotect images from manipulations guided by tampering conditions crafted by\nmalicious users. To tackle this issue, we propose Anti-Inpainting, a proactive\ndefense method that achieves adequate protection under unknown conditions\nthrough a triple mechanism to address this challenge. Specifically, a\nmulti-level deep feature extractor is presented to obtain intricate features\nduring the diffusion denoising process to improve protective effectiveness. We\ndesign multi-scale semantic-preserving data augmentation to enhance the\ntransferability of adversarial perturbations across unknown conditions by\nmulti-scale transformations while preserving semantic integrity. In addition,\nwe propose a selection-based distribution deviation optimization strategy to\nimprove the protection of adversarial perturbation against manipulation under\ndiverse random seeds. Extensive experiments indicate the proactive defensive\nperformance of Anti-Inpainting against diffusion-based inpainters guided by\nunknown conditions in InpaintGuardBench and CelebA-HQ. At the same time, we\nalso demonstrate the proposed approach's robustness under various image\npurification methods and its transferability across different versions of\ndiffusion models."}
{"id": "2505.13025", "pdf": "https://arxiv.org/pdf/2505.13025", "abs": "https://arxiv.org/abs/2505.13025", "authors": ["Jiyuan Pei", "Yi Mei", "Jialin Liu", "Mengjie Zhang"], "title": "LiBOG: Lifelong Learning for Black-Box Optimizer Generation", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at IJCAI 2025. To appear", "summary": "Meta-Black-Box Optimization (MetaBBO) garners attention due to its success in\nautomating the configuration and generation of black-box optimizers,\nsignificantly reducing the human effort required for optimizer design and\ndiscovering optimizers with higher performance than classic human-designed\noptimizers. However, existing MetaBBO methods conduct one-off training under\nthe assumption that a stationary problem distribution with extensive and\nrepresentative training problem samples is pre-available. This assumption is\noften impractical in real-world scenarios, where diverse problems following\nshifting distribution continually arise. Consequently, there is a pressing need\nfor methods that can continuously learn from new problems encountered\non-the-fly and progressively enhance their capabilities. In this work, we\nexplore a novel paradigm of lifelong learning in MetaBBO and introduce LiBOG, a\nnovel approach designed to learn from sequentially encountered problems and\ngenerate high-performance optimizers for Black-Box Optimization (BBO). LiBOG\nconsolidates knowledge both across tasks and within tasks to mitigate\ncatastrophic forgetting. Extensive experiments demonstrate LiBOG's\neffectiveness in learning to generate high-performance optimizers in a lifelong\nlearning manner, addressing catastrophic forgetting while maintaining\nplasticity to learn new tasks."}
{"id": "2505.13026", "pdf": "https://arxiv.org/pdf/2505.13026", "abs": "https://arxiv.org/abs/2505.13026", "authors": ["Jack Chen", "Fazhong Liu", "Naruto Liu", "Yuhan Luo", "Erqu Qin", "Harry Zheng", "Tian Dong", "Haojin Zhu", "Yan Meng", "Xiao Wang"], "title": "Step-wise Adaptive Integration of Supervised Fine-tuning and Reinforcement Learning for Task-Specific LLMs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) excel at mathematical reasoning and logical\nproblem-solving. The current popular training paradigms primarily use\nsupervised fine-tuning (SFT) and reinforcement learning (RL) to enhance the\nmodels' reasoning abilities. However, when using SFT or RL alone, there are\nrespective challenges: SFT may suffer from overfitting, while RL is prone to\nmode collapse. The state-of-the-art methods have proposed hybrid training\nschemes. However, static switching faces challenges such as poor generalization\nacross different tasks and high dependence on data quality. In response to\nthese challenges, inspired by the curriculum learning-quiz mechanism in human\nreasoning cultivation, We propose SASR, a step-wise adaptive hybrid training\nframework that theoretically unifies SFT and RL and dynamically balances the\ntwo throughout optimization. SASR uses SFT for initial warm-up to establish\nbasic reasoning skills, and then uses an adaptive dynamic adjustment algorithm\nbased on gradient norm and divergence relative to the original distribution to\nseamlessly integrate SFT with the online RL method GRPO. By monitoring the\ntraining status of LLMs and adjusting the training process in sequence, SASR\nensures a smooth transition between training schemes, maintaining core\nreasoning abilities while exploring different paths. Experimental results\ndemonstrate that SASR outperforms SFT, RL, and static hybrid training methods."}
{"id": "2505.13028", "pdf": "https://arxiv.org/pdf/2505.13028", "abs": "https://arxiv.org/abs/2505.13028", "authors": ["Sayon Palit", "Daniel Woods"], "title": "Evaluatiing the efficacy of LLM Safety Solutions : The Palit Benchmark Dataset", "categories": ["cs.CR", "cs.AI", "cs.CL", "F.2.2, I.2.7; F.2.2, I.2.7; F.2.2, I.2.7"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly integrated into critical\nsystems in industries like healthcare and finance. Users can often submit\nqueries to LLM-enabled chatbots, some of which can enrich responses with\ninformation retrieved from internal databases storing sensitive data. This\ngives rise to a range of attacks in which a user submits a malicious query and\nthe LLM-system outputs a response that creates harm to the owner, such as\nleaking internal data or creating legal liability by harming a third-party.\nWhile security tools are being developed to counter these threats, there is\nlittle formal evaluation of their effectiveness and usability. This study\naddresses this gap by conducting a thorough comparative analysis of LLM\nsecurity tools. We identified 13 solutions (9 closed-source, 4 open-source),\nbut only 7 were evaluated due to a lack of participation by proprietary model\nowners.To evaluate, we built a benchmark dataset of malicious prompts, and\nevaluate these tools performance against a baseline LLM model\n(ChatGPT-3.5-Turbo). Our results show that the baseline model has too many\nfalse positives to be used for this task. Lakera Guard and ProtectAI LLM Guard\nemerged as the best overall tools showcasing the tradeoff between usability and\nperformance. The study concluded with recommendations for greater transparency\namong closed source providers, improved context-aware detections, enhanced\nopen-source engagement, increased user awareness, and the adoption of more\nrepresentative performance metrics."}
{"id": "2505.13033", "pdf": "https://arxiv.org/pdf/2505.13033", "abs": "https://arxiv.org/abs/2505.13033", "authors": ["Vijay Ekambaram", "Subodh Kumar", "Arindam Jati", "Sumanta Mukherjee", "Tomoya Sakai", "Pankaj Dayama", "Wesley M. Gifford", "Jayant Kalagnanam"], "title": "TSPulse: Dual Space Tiny Pre-Trained Models for Rapid Time-Series Analysis", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The rise of time-series pre-trained models has advanced temporal\nrepresentation learning, but current state-of-the-art models are often\nlarge-scale, requiring substantial compute. We introduce TSPulse, ultra-compact\ntime-series pre-trained models with only 1M parameters, specialized to perform\nstrongly across classification, anomaly detection, imputation, and retrieval\ntasks. TSPulse introduces innovations at both the architecture and task levels.\nAt the architecture level, it employs a dual-space masked reconstruction,\nlearning from both time and frequency domains to capture complementary signals.\nThis is further enhanced by a dual-embedding disentanglement, generating both\ndetailed embeddings for fine-grained analysis and high-level semantic\nembeddings for broader task understanding. Notably, TSPulse's semantic\nembeddings are robust to shifts in time, magnitude, and noise, which is\nimportant for robust retrieval. At the task level, TSPulse incorporates TSLens,\na fine-tuning component enabling task-specific feature attention. It also\nintroduces a multi-head triangulation technique that correlates deviations from\nmultiple prediction heads, enhancing anomaly detection by fusing complementary\nmodel outputs. Additionally, a hybrid mask pretraining is proposed to improves\nzero-shot imputation by reducing pre-training bias. These architecture and task\ninnovations collectively contribute to TSPulse's significant performance gains:\n5-16% on the UEA classification benchmarks, +20% on the TSB-AD anomaly\ndetection leaderboard, +50% in zero-shot imputation, and +25% in time-series\nretrieval. Remarkably, these results are achieved with just 1M parameters,\nmaking TSPulse 10-100X smaller than existing pre-trained models. Its efficiency\nenables GPU-free inference and rapid pre-training, setting a new standard for\nefficient time-series pre-trained models. Models will be open-sourced soon."}
{"id": "2505.13036", "pdf": "https://arxiv.org/pdf/2505.13036", "abs": "https://arxiv.org/abs/2505.13036", "authors": ["Sai Koneru", "Maike Züfle", "Thai-Binh Nguyen", "Seymanur Akti", "Jan Niehues", "Alexander Waibel"], "title": "KIT's Offline Speech Translation and Instruction Following Submission for IWSLT 2025", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The scope of the International Workshop on Spoken Language Translation\n(IWSLT) has recently broadened beyond traditional Speech Translation (ST) to\nencompass a wider array of tasks, including Speech Question Answering and\nSummarization. This shift is partly driven by the growing capabilities of\nmodern systems, particularly with the success of Large Language Models (LLMs).\nIn this paper, we present the Karlsruhe Institute of Technology's submissions\nfor the Offline ST and Instruction Following (IF) tracks, where we leverage\nLLMs to enhance performance across all tasks. For the Offline ST track, we\npropose a pipeline that employs multiple automatic speech recognition systems,\nwhose outputs are fused using an LLM with document-level context. This is\nfollowed by a two-step translation process, incorporating additional refinement\nstep to improve translation quality. For the IF track, we develop an end-to-end\nmodel that integrates a speech encoder with an LLM to perform a wide range of\ninstruction-following tasks. We complement it with a final document-level\nrefinement stage to further enhance output quality by using contextual\ninformation."}
{"id": "2505.13043", "pdf": "https://arxiv.org/pdf/2505.13043", "abs": "https://arxiv.org/abs/2505.13043", "authors": ["Hao-Ran Yang", "Xiaohui Chen", "Chuan-Xian Ren"], "title": "A Generalized Label Shift Perspective for Cross-Domain Gaze Estimation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Aiming to generalize the well-trained gaze estimation model to new target\ndomains, Cross-domain Gaze Estimation (CDGE) is developed for real-world\napplication scenarios. Existing CDGE methods typically extract the\ndomain-invariant features to mitigate domain shift in feature space, which is\nproved insufficient by Generalized Label Shift (GLS) theory. In this paper, we\nintroduce a novel GLS perspective to CDGE and modelize the cross-domain problem\nby label and conditional shift problem. A GLS correction framework is presented\nand a feasible realization is proposed, in which a importance reweighting\nstrategy based on truncated Gaussian distribution is introduced to overcome the\ncontinuity challenges in label shift correction. To embed the reweighted source\ndistribution to conditional invariant learning, we further derive a\nprobability-aware estimation of conditional operator discrepancy. Extensive\nexperiments on standard CDGE tasks with different backbone models validate the\nsuperior generalization capability across domain and applicability on various\nmodels of proposed method."}
{"id": "2505.13053", "pdf": "https://arxiv.org/pdf/2505.13053", "abs": "https://arxiv.org/abs/2505.13053", "authors": ["Amelie S. Robrecht", "Christoph R. Kowalski", "Stefan Kopp"], "title": "SNAPE-PM: Building and Utilizing Dynamic Partner Models for Adaptive Explanation Generation", "categories": ["cs.CL", "cs.AI"], "comment": "currently under review at Frontiers in Communication", "summary": "Adapting to the addressee is crucial for successful explanations, yet poses\nsignificant challenges for dialogsystems. We adopt the approach of treating\nexplanation generation as a non-stationary decision process, where the optimal\nstrategy varies according to changing beliefs about the explainee and the\ninteraction context. In this paper we address the questions of (1) how to track\nthe interaction context and the relevant listener features in a formally\ndefined computational partner model, and (2) how to utilize this model in the\ndynamically adjusted, rational decision process that determines the currently\nbest explanation strategy. We propose a Bayesian inference-based approach to\ncontinuously update the partner model based on user feedback, and a\nnon-stationary Markov Decision Process to adjust decision-making based on the\npartner model values. We evaluate an implementation of this framework with five\nsimulated interlocutors, demonstrating its effectiveness in adapting to\ndifferent partners with constant and even changing feedback behavior. The\nresults show high adaptivity with distinct explanation strategies emerging for\ndifferent partners, highlighting the potential of our approach to improve\nexplainable AI systems and dialogsystems in general."}
{"id": "2505.13073", "pdf": "https://arxiv.org/pdf/2505.13073", "abs": "https://arxiv.org/abs/2505.13073", "authors": ["Dengfeng Liu", "Jucai Zhai", "Xiaoguang Jiang", "Ziqun Li", "Qianjin Yu", "Feng Liu", "Rui Ye", "Huang Liu", "Zhiguo Yang", "Yongsheng Du", "Fang Tan"], "title": "Structure-Aware Corpus Construction and User-Perception-Aligned Metrics for Large-Language-Model Code Completion", "categories": ["cs.SE", "cs.AI"], "comment": "14 pages,8 figures", "summary": "Code completion technology based on large language model has significantly\nimproved the development efficiency of programmers. However, in practical\napplications, there remains a gap between current commonly used code completion\nevaluation metrics and users' actual perception. To address this issue, we\npropose two evaluation metrics for code completion tasks--LCP and ROUGE-LCP,\nfrom the perspective of probabilistic modeling. Furthermore, to tackle the lack\nof effective structural semantic modeling and cross-module dependency\ninformation in LLMs for repository-level code completion scenarios, we propose\na data processing method based on a Structure-Preserving and\nSemantically-Reordered Code Graph (SPSR-Graph). Through theoretical analysis\nand experimental validation, we demonstrate the superiority of the proposed\nevaluation metrics in terms of user perception consistency, as well as the\neffectiveness of the data processing method in enhancing model performance."}
{"id": "2505.13076", "pdf": "https://arxiv.org/pdf/2505.13076", "abs": "https://arxiv.org/abs/2505.13076", "authors": ["Mykyta Mudryi", "Markiyan Chaklosh", "Grzegorz Wójcik"], "title": "The Hidden Dangers of Browsing AI Agents", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Autonomous browsing agents powered by large language models (LLMs) are\nincreasingly used to automate web-based tasks. However, their reliance on\ndynamic content, tool execution, and user-provided data exposes them to a broad\nattack surface. This paper presents a comprehensive security evaluation of such\nagents, focusing on systemic vulnerabilities across multiple architectural\nlayers. Our work outlines the first end-to-end threat model for browsing agents\nand provides actionable guidance for securing their deployment in real-world\nenvironments. To address discovered threats, we propose a defense in depth\nstrategy incorporating input sanitization, planner executor isolation, formal\nanalyzers, and session safeguards. These measures protect against both initial\naccess and post exploitation attack vectors. Through a white box analysis of a\npopular open source project, Browser Use, we demonstrate how untrusted web\ncontent can hijack agent behavior and lead to critical security breaches. Our\nfindings include prompt injection, domain validation bypass, and credential\nexfiltration, evidenced by a disclosed CVE and a working proof of concept\nexploit."}
{"id": "2505.13077", "pdf": "https://arxiv.org/pdf/2505.13077", "abs": "https://arxiv.org/abs/2505.13077", "authors": ["Xiang Fei", "Jinghui Lu", "Qi Sun", "Hao Feng", "Yanjie Wang", "Wei Shi", "An-Lan Wang", "Jingqun Tang", "Can Huang"], "title": "Advancing Sequential Numerical Prediction in Autoregressive Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Autoregressive models have become the de facto choice for sequence generation\ntasks, but standard approaches treat digits as independent tokens and apply\ncross-entropy loss, overlooking the coherent structure of numerical sequences.\nThis paper introduces Numerical Token Integrity Loss (NTIL) to address this\ngap. NTIL operates at two levels: (1) token-level, where it extends the Earth\nMover's Distance (EMD) to preserve ordinal relationships between numerical\nvalues, and (2) sequence-level, where it penalizes the overall discrepancy\nbetween the predicted and actual sequences. This dual approach improves\nnumerical prediction and integrates effectively with LLMs/MLLMs. Extensive\nexperiments show significant performance improvements with NTIL."}
{"id": "2505.13079", "pdf": "https://arxiv.org/pdf/2505.13079", "abs": "https://arxiv.org/abs/2505.13079", "authors": ["Xugang Lu", "Peng Shen", "Yu Tsao", "Hisashi Kawai"], "title": "Cross-modal Knowledge Transfer Learning as Graph Matching Based on Optimal Transport for ASR", "categories": ["eess.AS", "cs.AI"], "comment": "To appear in Interspeech 2025", "summary": "Transferring linguistic knowledge from a pretrained language model (PLM) to\nacoustic feature learning has proven effective in enhancing end-to-end\nautomatic speech recognition (E2E-ASR). However, aligning representations\nbetween linguistic and acoustic modalities remains a challenge due to inherent\nmodality gaps. Optimal transport (OT) has shown promise in mitigating these\ngaps by minimizing the Wasserstein distance (WD) between linguistic and\nacoustic feature distributions. However, previous OT-based methods overlook\nstructural relationships, treating feature vectors as unordered sets. To\naddress this, we propose Graph Matching Optimal Transport (GM-OT), which models\nlinguistic and acoustic sequences as structured graphs. Nodes represent feature\nembeddings, while edges capture temporal and sequential relationships. GM-OT\nminimizes both WD (between nodes) and Gromov-Wasserstein distance (GWD)\n(between edges), leading to a fused Gromov-Wasserstein distance (FGWD)\nformulation. This enables structured alignment and more efficient knowledge\ntransfer compared to existing OT-based approaches. Theoretical analysis further\nshows that prior OT-based methods in linguistic knowledge transfer can be\nviewed as a special case within our GM-OT framework. We evaluate GM-OT on\nMandarin ASR using a CTC-based E2E-ASR system with a PLM for knowledge\ntransfer. Experimental results demonstrate significant performance gains over\nstate-of-the-art models, validating the effectiveness of our approach."}
{"id": "2505.13082", "pdf": "https://arxiv.org/pdf/2505.13082", "abs": "https://arxiv.org/abs/2505.13082", "authors": ["Kyeongman Park", "Seongho Joo", "Kyomin Jung"], "title": "MultiActor-Audiobook: Zero-Shot Audiobook Generation with Faces and Voices of Multiple Speakers", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "We introduce MultiActor-Audiobook, a zero-shot approach for generating\naudiobooks that automatically produces consistent, expressive, and\nspeaker-appropriate prosody, including intonation and emotion. Previous\naudiobook systems have several limitations: they require users to manually\nconfigure the speaker's prosody, read each sentence with a monotonic tone\ncompared to voice actors, or rely on costly training. However, our\nMultiActor-Audiobook addresses these issues by introducing two novel processes:\n(1) MSP (**Multimodal Speaker Persona Generation**) and (2) LSI (**LLM-based\nScript Instruction Generation**). With these two processes,\nMultiActor-Audiobook can generate more emotionally expressive audiobooks with a\nconsistent speaker prosody without additional training. We compare our system\nwith commercial products, through human and MLLM evaluations, achieving\ncompetitive results. Furthermore, we demonstrate the effectiveness of MSP and\nLSI through ablation studies."}
{"id": "2505.13087", "pdf": "https://arxiv.org/pdf/2505.13087", "abs": "https://arxiv.org/abs/2505.13087", "authors": ["Adrien Lagesse", "Marc Lelarge"], "title": "Graph Alignment for Benchmarking Graph Neural Networks and Learning Positional Encodings", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We propose a novel benchmarking methodology for graph neural networks (GNNs)\nbased on the graph alignment problem, a combinatorial optimization task that\ngeneralizes graph isomorphism by aligning two unlabeled graphs to maximize\noverlapping edges. We frame this problem as a self-supervised learning task and\npresent several methods to generate graph alignment datasets using synthetic\nrandom graphs and real-world graph datasets from multiple domains. For a given\ngraph dataset, we generate a family of graph alignment datasets with increasing\ndifficulty, allowing us to rank the performance of various architectures. Our\nexperiments indicate that anisotropic graph neural networks outperform standard\nconvolutional architectures. To further demonstrate the utility of the graph\nalignment task, we show its effectiveness for unsupervised GNN pre-training,\nwhere the learned node embeddings outperform other positional encodings on\nthree molecular regression tasks and achieve state-of-the-art results on the\nPCQM4Mv2 dataset with significantly fewer parameters. To support\nreproducibility and further research, we provide an open-source Python package\nto generate graph alignment datasets and benchmark new GNN architectures."}
{"id": "2505.13094", "pdf": "https://arxiv.org/pdf/2505.13094", "abs": "https://arxiv.org/abs/2505.13094", "authors": ["Guo Chen", "Kai Li", "Runxuan Yang", "Xiaolin Hu"], "title": "Time-Frequency-Based Attention Cache Memory Model for Real-Time Speech Separation", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "Existing causal speech separation models often underperform compared to\nnon-causal models due to difficulties in retaining historical information. To\naddress this, we propose the Time-Frequency Attention Cache Memory (TFACM)\nmodel, which effectively captures spatio-temporal relationships through an\nattention mechanism and cache memory (CM) for historical information storage.\nIn TFACM, an LSTM layer captures frequency-relative positions, while causal\nmodeling is applied to the time dimension using local and global\nrepresentations. The CM module stores past information, and the causal\nattention refinement (CAR) module further enhances time-based feature\nrepresentations for finer granularity. Experimental results showed that TFACM\nachieveed comparable performance to the SOTA TF-GridNet-Causal model, with\nsignificantly lower complexity and fewer trainable parameters. For more\ndetails, visit the project page: https://cslikai.cn/TFACM/."}
{"id": "2505.13101", "pdf": "https://arxiv.org/pdf/2505.13101", "abs": "https://arxiv.org/abs/2505.13101", "authors": ["Shaowu Wu", "Liting Zeng", "Wei Lu", "Xiangyang Luo"], "title": "ARIW-Framework: Adaptive Robust Iterative Watermarking Framework", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 4 figures", "summary": "With the rapid rise of large models, copyright protection for generated image\ncontent has become a critical security challenge. Although deep learning\nwatermarking techniques offer an effective solution for digital image copyright\nprotection, they still face limitations in terms of visual quality, robustness\nand generalization. To address these issues, this paper proposes an adaptive\nrobust iterative watermarking framework (ARIW-Framework) that achieves\nhigh-quality watermarked images while maintaining exceptional robustness and\ngeneralization performance. Specifically, we introduce an iterative approach to\noptimize the encoder for generating robust residuals. The encoder incorporates\nnoise layers and a decoder to compute robustness weights for residuals under\nvarious noise attacks. By employing a parallel optimization strategy, the\nframework enhances robustness against multiple types of noise attacks.\nFurthermore, we leverage image gradients to determine the embedding strength at\neach pixel location, significantly improving the visual quality of the\nwatermarked images. Extensive experiments demonstrate that the proposed method\nachieves superior visual quality while exhibiting remarkable robustness and\ngeneralization against noise attacks."}
{"id": "2505.13102", "pdf": "https://arxiv.org/pdf/2505.13102", "abs": "https://arxiv.org/abs/2505.13102", "authors": ["Ji Qi", "Tam Thuc Do", "Mingxiao Liu", "Zhuoshi Pan", "Yuzhe Li", "Gene Cheung", "H. Vicky Zhao"], "title": "Lightweight Transformer via Unrolling of Mixed Graph Algorithms for Traffic Forecast", "categories": ["cs.LG", "cs.AI", "eess.SP"], "comment": "19 pages, 5 figures, 8 tables", "summary": "To forecast traffic with both spatial and temporal dimensions, we unroll a\nmixed-graph-based optimization algorithm into a lightweight and interpretable\ntransformer-like neural net. Specifically, we construct two graphs: an\nundirected graph $\\mathcal{G}^u$ capturing spatial correlations across\ngeography, and a directed graph $\\mathcal{G}^d$ capturing sequential\nrelationships over time. We formulate a prediction problem for the future\nsamples of signal $\\mathbf{x}$, assuming it is \"smooth\" with respect to both\n$\\mathcal{G}^u$ and $\\mathcal{G}^d$, where we design new $\\ell_2$ and\n$\\ell_1$-norm variational terms to quantify and promote signal smoothness\n(low-frequency reconstruction) on a directed graph. We construct an iterative\nalgorithm based on alternating direction method of multipliers (ADMM), and\nunroll it into a feed-forward network for data-driven parameter learning. We\ninsert graph learning modules for $\\mathcal{G}^u$ and $\\mathcal{G}^d$, which\nare akin to the self-attention mechanism in classical transformers. Experiments\nshow that our unrolled networks achieve competitive traffic forecast\nperformance as state-of-the-art prediction schemes, while reducing parameter\ncounts drastically. Our code is available in\nhttps://github.com/SingularityUndefined/Unrolling-GSP-STForecast."}
{"id": "2505.13109", "pdf": "https://arxiv.org/pdf/2505.13109", "abs": "https://arxiv.org/abs/2505.13109", "authors": ["Guangda Liu", "Chengwei Li", "Zhenyu Ning", "Jing Lin", "Yiwu Yao", "Danning Ke", "Minyi Guo", "Jieru Zhao"], "title": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods."}
{"id": "2505.13115", "pdf": "https://arxiv.org/pdf/2505.13115", "abs": "https://arxiv.org/abs/2505.13115", "authors": ["Debarpan Bhattacharya", "Apoorva Kulkarni", "Sriram Ganapathy"], "title": "Benchmarking and Confidence Evaluation of LALMs For Temporal Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": "Accepted in INTERSPEECH, 2025, Rotterdam, The Netherlands", "summary": "The popular success of text-based large language models (LLM) has streamlined\nthe attention of the multimodal community to combine other modalities like\nvision and audio along with text to achieve similar multimodal capabilities. In\nthis quest, large audio language models (LALMs) have to be evaluated on\nreasoning related tasks which are different from traditional classification or\ngeneration tasks. Towards this goal, we propose a novel dataset called temporal\nreasoning evaluation of audio (TREA).\n  We benchmark open-source LALMs and observe that they are consistently behind\nhuman capabilities on the tasks in the TREA dataset. While evaluating LALMs, we\nalso propose an uncertainty metric, which computes the invariance of the model\nto semantically identical perturbations of the input. Our analysis shows that\nthe accuracy and uncertainty metrics are not necessarily correlated and thus,\npoints to a need for wholesome evaluation of LALMs for high-stakes\napplications."}
{"id": "2505.13116", "pdf": "https://arxiv.org/pdf/2505.13116", "abs": "https://arxiv.org/abs/2505.13116", "authors": ["Kathrin Lammers", "Valerie Vaquet", "Barbara Hammer"], "title": "Continuous Fair SMOTE -- Fairness-Aware Stream Learning from Imbalanced Data", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "As machine learning is increasingly applied in an online fashion to deal with\nevolving data streams, the fairness of these algorithms is a matter of growing\nethical and legal concern. In many use cases, class imbalance in the data also\nneeds to be dealt with to ensure predictive performance. Current fairness-aware\nstream learners typically attempt to solve these issues through in- or\npost-processing by focusing on optimizing one specific discrimination metric,\naddressing class imbalance in a separate processing step. While C-SMOTE is a\nhighly effective model-agnostic pre-processing approach to mitigate class\nimbalance, as a side effect of this method, algorithmic bias is often\nintroduced.\n  Therefore, we propose CFSMOTE - a fairness-aware, continuous SMOTE variant -\nas a pre-processing approach to simultaneously address the class imbalance and\nfairness concerns by employing situation testing and balancing\nfairness-relevant groups during oversampling. Unlike other fairness-aware\nstream learners, CFSMOTE is not optimizing for only one specific fairness\nmetric, therefore avoiding potentially problematic trade-offs. Our experiments\nshow significant improvement on several common group fairness metrics in\ncomparison to vanilla C-SMOTE while maintaining competitive performance, also\nin comparison to other fairness-aware algorithms."}
{"id": "2505.13122", "pdf": "https://arxiv.org/pdf/2505.13122", "abs": "https://arxiv.org/abs/2505.13122", "authors": ["François Bachoc", "Jérôme Bolte", "Ryan Boustany", "Jean-Michel Loubes"], "title": "When majority rules, minority loses: bias amplification of gradient descent", "categories": ["cs.LG", "cs.AI", "math.OC"], "comment": null, "summary": "Despite growing empirical evidence of bias amplification in machine learning,\nits theoretical foundations remain poorly understood. We develop a formal\nframework for majority-minority learning tasks, showing how standard training\ncan favor majority groups and produce stereotypical predictors that neglect\nminority-specific features. Assuming population and variance imbalance, our\nanalysis reveals three key findings: (i) the close proximity between\n``full-data'' and stereotypical predictors, (ii) the dominance of a region\nwhere training the entire model tends to merely learn the majority traits, and\n(iii) a lower bound on the additional training required. Our results are\nillustrated through experiments in deep learning for tabular and image\nclassification tasks."}
{"id": "2505.13123", "pdf": "https://arxiv.org/pdf/2505.13123", "abs": "https://arxiv.org/abs/2505.13123", "authors": ["Snehashis Majhi", "Giacomo D'Amicantonio", "Antitza Dantcheva", "Quan Kong", "Lorenzo Garattoni", "Gianpiero Francesca", "Egor Bondarev", "Francois Bremond"], "title": "Just Dance with $π$! A Poly-modal Inductor for Weakly-supervised Video Anomaly Detection", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Weakly-supervised methods for video anomaly detection (VAD) are\nconventionally based merely on RGB spatio-temporal features, which continues to\nlimit their reliability in real-world scenarios. This is due to the fact that\nRGB-features are not sufficiently distinctive in setting apart categories such\nas shoplifting from visually similar events. Therefore, towards robust complex\nreal-world VAD, it is essential to augment RGB spatio-temporal features by\nadditional modalities. Motivated by this, we introduce the Poly-modal Induced\nframework for VAD: \"PI-VAD\", a novel approach that augments RGB representations\nby five additional modalities. Specifically, the modalities include sensitivity\nto fine-grained motion (Pose), three dimensional scene and entity\nrepresentation (Depth), surrounding objects (Panoptic masks), global motion\n(optical flow), as well as language cues (VLM). Each modality represents an\naxis of a polygon, streamlined to add salient cues to RGB. PI-VAD includes two\nplug-in modules, namely Pseudo-modality Generation module and Cross Modal\nInduction module, which generate modality-specific prototypical representation\nand, thereby, induce multi-modal information into RGB cues. These modules\noperate by performing anomaly-aware auxiliary tasks and necessitate five\nmodality backbones -- only during training. Notably, PI-VAD achieves\nstate-of-the-art accuracy on three prominent VAD datasets encompassing\nreal-world scenarios, without requiring the computational overhead of five\nmodality backbones at inference."}
{"id": "2505.13124", "pdf": "https://arxiv.org/pdf/2505.13124", "abs": "https://arxiv.org/abs/2505.13124", "authors": ["Francesco Innocenti", "El Mehdi Achour", "Christopher L. Buckley"], "title": "$μ$PC: Scaling Predictive Coding to 100+ Layer Networks", "categories": ["cs.LG", "cs.AI", "cs.NE", "I.2.6"], "comment": "34 pages, 41 figures", "summary": "The biological implausibility of backpropagation (BP) has motivated many\nalternative, brain-inspired algorithms that attempt to rely only on local\ninformation, such as predictive coding (PC) and equilibrium propagation.\nHowever, these algorithms have notoriously struggled to train very deep\nnetworks, preventing them from competing with BP in large-scale settings.\nIndeed, scaling PC networks (PCNs) has recently been posed as a challenge for\nthe community (Pinchetti et al., 2024). Here, we show that 100+ layer PCNs can\nbe trained reliably using a Depth-$\\mu$P parameterisation (Yang et al., 2023;\nBordelon et al., 2023) which we call \"$\\mu$PC\". Through an extensive analysis\nof the scaling behaviour of PCNs, we reveal several pathologies that make\nstandard PCNs difficult to train at large depths. We then show that, despite\naddressing only some of these instabilities, $\\mu$PC allows stable training of\nvery deep (up to 128-layer) residual networks on simple classification tasks\nwith competitive performance and little tuning compared to current benchmarks.\nMoreover, $\\mu$PC enables zero-shot transfer of both weight and activity\nlearning rates across widths and depths. Our results have implications for\nother local algorithms and could be extended to convolutional and transformer\narchitectures. Code for $\\mu$PC is made available as part of a JAX library for\nPCNs at https://github.com/thebuckleylab/jpc (Innocenti et al., 2024)."}
{"id": "2505.13130", "pdf": "https://arxiv.org/pdf/2505.13130", "abs": "https://arxiv.org/abs/2505.13130", "authors": ["Muhammad Awais Amin", "Adama Ilboudo", "Abdul Samad bin Shahid", "Amjad Ali", "Waqas Haider Khan Bangyal"], "title": "Adaptive Image Restoration for Video Surveillance: A Real-Time Approach", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "One of the major challenges in the field of computer vision especially for\ndetection, segmentation, recognition, monitoring, and automated solutions, is\nthe quality of images. Image degradation, often caused by factors such as rain,\nfog, lighting, etc., has a negative impact on automated\ndecision-making.Furthermore, several image restoration solutions exist,\nincluding restoration models for single degradation and restoration models for\nmultiple degradations. However, these solutions are not suitable for real-time\nprocessing. In this study, the aim was to develop a real-time image restoration\nsolution for video surveillance. To achieve this, using transfer learning with\nResNet_50, we developed a model for automatically identifying the types of\ndegradation present in an image to reference the necessary treatment(s) for\nimage restoration. Our solution has the advantage of being flexible and\nscalable."}
{"id": "2505.13136", "pdf": "https://arxiv.org/pdf/2505.13136", "abs": "https://arxiv.org/abs/2505.13136", "authors": ["Anton Ehrmanntraut", "Julia Wunderle", "Jan Pfister", "Fotis Jannidis", "Andreas Hotho"], "title": "ModernGBERT: German-only 1B Encoder Model Trained from Scratch", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "under review @ARR", "summary": "Despite the prominence of decoder-only language models, encoders remain\ncrucial for resource-constrained applications. We introduce ModernGBERT (134M,\n1B), a fully transparent family of German encoder models trained from scratch,\nincorporating architectural innovations from ModernBERT. To evaluate the\npractical trade-offs of training encoders from scratch, we also present\nLL\\\"aMmlein2Vec (120M, 1B, 7B), a family of encoders derived from German\ndecoder-only models via LLM2Vec. We benchmark all models on natural language\nunderstanding, text embedding, and long-context reasoning tasks, enabling a\ncontrolled comparison between dedicated encoders and converted decoders. Our\nresults show that ModernGBERT 1B outperforms prior state-of-the-art German\nencoders as well as encoders adapted via LLM2Vec, with regard to performance\nand parameter-efficiency. All models, training data, checkpoints and code are\npublicly available, advancing the German NLP ecosystem with transparent,\nhigh-performance encoder models."}
{"id": "2505.13144", "pdf": "https://arxiv.org/pdf/2505.13144", "abs": "https://arxiv.org/abs/2505.13144", "authors": ["Dongsu Lee", "Minhae Kwon"], "title": "Temporal Distance-aware Transition Augmentation for Offline Model-based Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "2025 ICML", "summary": "The goal of offline reinforcement learning (RL) is to extract a\nhigh-performance policy from the fixed datasets, minimizing performance\ndegradation due to out-of-distribution (OOD) samples. Offline model-based RL\n(MBRL) is a promising approach that ameliorates OOD issues by enriching\nstate-action transitions with augmentations synthesized via a learned dynamics\nmodel. Unfortunately, seminal offline MBRL methods often struggle in\nsparse-reward, long-horizon tasks. In this work, we introduce a novel MBRL\nframework, dubbed Temporal Distance-Aware Transition Augmentation (TempDATA),\nthat generates augmented transitions in a temporally structured latent space\nrather than in raw state space. To model long-horizon behavior, TempDATA learns\na latent abstraction that captures a temporal distance from both trajectory and\ntransition levels of state space. Our experiments confirm that TempDATA\noutperforms previous offline MBRL methods and achieves matching or surpassing\nthe performance of diffusion-based trajectory augmentation and goal-conditioned\nRL on the D4RL AntMaze, FrankaKitchen, CALVIN, and pixel-based FrankaKitchen."}
{"id": "2505.13156", "pdf": "https://arxiv.org/pdf/2505.13156", "abs": "https://arxiv.org/abs/2505.13156", "authors": ["Zhi Liu", "Tao Yang", "Jing Wang", "Yexin Chen", "Zhan Gao", "Jiaxi Yang", "Kui Chen", "Bingji Lu", "Xiaochen Li", "Changyong Luo", "Yan Li", "Xiaohong Gu", "Peng Cao"], "title": "Tianyi: A Traditional Chinese Medicine all-rounder language model and its Real-World Clinical Practice", "categories": ["cs.CL", "cs.AI"], "comment": "23 pages, 4 figures, and 1 tables", "summary": "Natural medicines, particularly Traditional Chinese Medicine (TCM), are\ngaining global recognition for their therapeutic potential in addressing human\nsymptoms and diseases. TCM, with its systematic theories and extensive\npractical experience, provides abundant resources for healthcare. However, the\neffective application of TCM requires precise syndrome diagnosis, determination\nof treatment principles, and prescription formulation, which demand decades of\nclinical expertise. Despite advancements in TCM-based decision systems, machine\nlearning, and deep learning research, limitations in data and single-objective\nconstraints hinder their practical application. In recent years, large language\nmodels (LLMs) have demonstrated potential in complex tasks, but lack\nspecialization in TCM and face significant challenges, such as too big model\nscale to deploy and issues with hallucination. To address these challenges, we\nintroduce Tianyi with 7.6-billion-parameter LLM, a model scale proper and\nspecifically designed for TCM, pre-trained and fine-tuned on diverse TCM\ncorpora, including classical texts, expert treatises, clinical records, and\nknowledge graphs. Tianyi is designed to assimilate interconnected and\nsystematic TCM knowledge through a progressive learning manner. Additionally,\nwe establish TCMEval, a comprehensive evaluation benchmark, to assess LLMs in\nTCM examinations, clinical tasks, domain-specific question-answering, and\nreal-world trials. The extensive evaluations demonstrate the significant\npotential of Tianyi as an AI assistant in TCM clinical practice and research,\nbridging the gap between TCM knowledge and practical application."}
{"id": "2505.13157", "pdf": "https://arxiv.org/pdf/2505.13157", "abs": "https://arxiv.org/abs/2505.13157", "authors": ["Yassine El Boudouri", "Walter Nuninger", "Julian Alvarez", "Yvan Peter"], "title": "Role-Playing Evaluation for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) demonstrate a notable capacity for adopting\npersonas and engaging in role-playing. However, evaluating this ability\npresents significant challenges, as human assessments are resource-intensive\nand automated evaluations can be biased. To address this, we introduce\nRole-Playing Eval (RPEval), a novel benchmark designed to assess LLM\nrole-playing capabilities across four key dimensions: emotional understanding,\ndecision-making, moral alignment, and in-character consistency. This article\ndetails the construction of RPEval and presents baseline evaluations. Our code\nand dataset are available at https://github.com/yelboudouri/RPEval"}
{"id": "2505.13176", "pdf": "https://arxiv.org/pdf/2505.13176", "abs": "https://arxiv.org/abs/2505.13176", "authors": ["Zihao Cheng", "Hongru Wang", "Zeming Liu", "Yuhang Guo", "Yuanfang Guo", "Yunhong Wang", "Haifeng Wang"], "title": "ToolSpectrum : Towards Personalized Tool Utilization for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025 Findings", "summary": "While integrating external tools into large language models (LLMs) enhances\ntheir ability to access real-time information and domain-specific services,\nexisting approaches focus narrowly on functional tool selection following user\ninstructions, overlooking the context-aware personalization in tool selection.\nThis oversight leads to suboptimal user satisfaction and inefficient tool\nutilization, particularly when overlapping toolsets require nuanced selection\nbased on contextual factors. To bridge this gap, we introduce ToolSpectrum, a\nbenchmark designed to evaluate LLMs' capabilities in personalized tool\nutilization. Specifically, we formalize two key dimensions of personalization,\nuser profile and environmental factors, and analyze their individual and\nsynergistic impacts on tool utilization. Through extensive experiments on\nToolSpectrum, we demonstrate that personalized tool utilization significantly\nimproves user experience across diverse scenarios. However, even\nstate-of-the-art LLMs exhibit the limited ability to reason jointly about user\nprofiles and environmental factors, often prioritizing one dimension at the\nexpense of the other. Our findings underscore the necessity of context-aware\npersonalization in tool-augmented LLMs and reveal critical limitations for\ncurrent models. Our data and code are available at\nhttps://github.com/Chengziha0/ToolSpectrum."}
{"id": "2505.13182", "pdf": "https://arxiv.org/pdf/2505.13182", "abs": "https://arxiv.org/abs/2505.13182", "authors": ["Jianfeng Xu"], "title": "Information Science Principles of Machine Learning: A Causal Chain Meta-Framework Based on Formalized Information Mapping", "categories": ["cs.LO", "cs.AI"], "comment": null, "summary": "[Objective] This study focuses on addressing the current lack of a unified\nformal theoretical framework in machine learning, as well as the deficiencies\nin interpretability and ethical safety assurance. [Methods] A formal\ninformation model is first constructed, utilizing sets of well-formed formulas\nto explicitly define the ontological states and carrier mappings of typical\ncomponents in machine learning. Learnable and processable predicates, along\nwith learning and processing functions, are introduced to analyze the logical\ndeduction and constraint rules of the causal chains within models. [Results] A\nmeta-framework for machine learning theory (MLT-MF) is established. Based on\nthis framework, universal definitions for model interpretability and ethical\nsafety are proposed. Furthermore, three key theorems are proved: the\nequivalence of model interpretability and information recoverability, the\nassurance of ethical safety, and the estimation of generalization error.\n[Limitations] The current framework assumes ideal conditions with noiseless\ninformation-enabling mappings and primarily targets model learning and\nprocessing logic in static scenarios. It does not yet address information\nfusion and conflict resolution across ontological spaces in multimodal or\nmulti-agent systems. [Conclusions] This work overcomes the limitations of\nfragmented research and provides a unified theoretical foundation for\nsystematically addressing the critical challenges currently faced in machine\nlearning."}
{"id": "2505.13188", "pdf": "https://arxiv.org/pdf/2505.13188", "abs": "https://arxiv.org/abs/2505.13188", "authors": ["Juntian Zhu", "Miguel de Carvalho", "Zhouwang Yang", "Fengxiang He"], "title": "When a Reinforcement Learning Agent Encounters Unknown Unknowns", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "An AI agent might surprisingly find she has reached an unknown state which\nshe has never been aware of -- an unknown unknown. We mathematically ground\nthis scenario in reinforcement learning: an agent, after taking an action\ncalculated from value functions $Q$ and $V$ defined on the {\\it {aware\ndomain}}, reaches a state out of the domain. To enable the agent to handle this\nscenario, we propose an {\\it episodic Markov decision {process} with growing\nawareness} (EMDP-GA) model, taking a new {\\it noninformative value expansion}\n(NIVE) approach to expand value functions to newly aware areas: when an agent\narrives at an unknown unknown, value functions $Q$ and $V$ whereon are\ninitialised by noninformative beliefs -- the averaged values on the aware\ndomain. This design is out of respect for the complete absence of knowledge in\nthe newly discovered state. The upper confidence bound momentum Q-learning is\nthen adapted to the growing awareness for training the EMDP-GA model. We prove\nthat (1) the regret of our approach is asymptotically consistent with the state\nof the art (SOTA) without exposure to unknown unknowns in an extremely\nuncertain environment, and (2) our computational complexity and space\ncomplexity are comparable with the SOTA -- these collectively suggest that\nthough an unknown unknown is surprising, it will be asymptotically properly\ndiscovered with decent speed and an affordable cost."}
{"id": "2505.13191", "pdf": "https://arxiv.org/pdf/2505.13191", "abs": "https://arxiv.org/abs/2505.13191", "authors": ["Pengcheng Pan", "Yonekura Shogo", "Yasuo Kuniyoshi"], "title": "Emergence of Fixational and Saccadic Movements in a Multi-Level Recurrent Attention Model for Vision", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Inspired by foveal vision, hard attention models promise interpretability and\nparameter economy. However, existing models like the Recurrent Model of Visual\nAttention (RAM) and Deep Recurrent Attention Model (DRAM) failed to model the\nhierarchy of human vision system, that compromise on the visual exploration\ndynamics. As a result, they tend to produce attention that are either overly\nfixational or excessively saccadic, diverging from human eye movement behavior.\nIn this paper, we propose a Multi-Level Recurrent Attention Model (MRAM), a\nnovel hard attention framework that explicitly models the neural hierarchy of\nhuman visual processing. By decoupling the function of glimpse location\ngeneration and task execution in two recurrent layers, MRAM emergent a balanced\nbehavior between fixation and saccadic movement. Our results show that MRAM not\nonly achieves more human-like attention dynamics, but also consistently\noutperforms CNN, RAM and DRAM baselines on standard image classification\nbenchmarks."}
{"id": "2505.13192", "pdf": "https://arxiv.org/pdf/2505.13192", "abs": "https://arxiv.org/abs/2505.13192", "authors": ["Christoph Jürgen Hemmer", "Daniel Durstewitz"], "title": "True Zero-Shot Inference of Dynamical Systems Preserving Long-Term Statistics", "categories": ["cs.LG", "cs.AI", "math.DS", "nlin.CD"], "comment": null, "summary": "Complex, temporally evolving phenomena, from climate to brain activity, are\ngoverned by dynamical systems (DS). DS reconstruction (DSR) seeks to infer\ngenerative surrogate models of these from observed data, reproducing their\nlong-term behavior. Existing DSR approaches require purpose-training for any\nnew system observed, lacking the zero-shot and in-context inference\ncapabilities known from LLMs. Here we introduce DynaMix, a novel multivariate\nALRNN-based mixture-of-experts architecture pre-trained for DSR, the first DSR\nmodel able to generalize zero-shot to out-of-domain DS. Just from a provided\ncontext signal, without any re-training, DynaMix faithfully forecasts the\nlong-term evolution of novel DS where existing time series (TS) foundation\nmodels, like Chronos, fail -- at a fraction of the number of parameters and\norders of magnitude faster inference times. DynaMix outperforms TS foundation\nmodels in terms of long-term statistics, and often also short-term forecasts,\neven on real-world time series, like traffic or weather data, typically used\nfor training and evaluating TS models, but not at all part of DynaMix' training\ncorpus. We illustrate some of the failure modes of TS models for DSR problems,\nand conclude that models built on DS principles may bear a huge potential also\nfor advancing the TS prediction field."}
{"id": "2505.13196", "pdf": "https://arxiv.org/pdf/2505.13196", "abs": "https://arxiv.org/abs/2505.13196", "authors": ["Pranav Vaidhyanathan", "Lucas Schorling", "Natalia Ares", "Michael A. Osborne"], "title": "A Physics-Inspired Optimizer: Velocity Regularized Adam", "categories": ["cs.LG", "cs.AI", "quant-ph"], "comment": "L. Schorling and P. Vaidhyanathan contributed equally to this work.\n  20 pages, 13 figures", "summary": "We introduce Velocity-Regularized Adam (VRAdam), a physics-inspired optimizer\nfor training deep neural networks that draws on ideas from quartic terms for\nkinetic energy with its stabilizing effects on various system dynamics.\nPrevious algorithms, including the ubiquitous Adam, operate at the so called\nadaptive edge of stability regime during training leading to rapid oscillations\nand slowed convergence of loss. However, VRAdam adds a higher order penalty on\nthe learning rate based on the velocity such that the algorithm automatically\nslows down whenever weight updates become large. In practice, we observe that\nthe effective dynamic learning rate shrinks in high-velocity regimes, damping\noscillations and allowing for a more aggressive base step size when necessary\nwithout divergence. By combining this velocity-based regularizer for global\ndamping with per-parameter scaling of Adam to create a hybrid optimizer, we\ndemonstrate that VRAdam consistently exceeds the performance against standard\noptimizers including AdamW. We benchmark various tasks such as image\nclassification, language modeling, image generation and generative modeling\nusing diverse architectures and training methodologies including Convolutional\nNeural Networks (CNNs), Transformers, and GFlowNets."}
{"id": "2505.13201", "pdf": "https://arxiv.org/pdf/2505.13201", "abs": "https://arxiv.org/abs/2505.13201", "authors": ["Yuzhen Chen", "Hojun Son", "Arpan Kusari"], "title": "MatPredict: a dataset and benchmark for learning material properties of diverse indoor objects", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Determining material properties from camera images can expand the ability to\nidentify complex objects in indoor environments, which is valuable for consumer\nrobotics applications. To support this, we introduce MatPredict, a dataset that\ncombines the high-quality synthetic objects from Replica dataset with MatSynth\ndataset's material properties classes - to create objects with diverse material\nproperties. We select 3D meshes of specific foreground objects and render them\nwith different material properties. In total, we generate \\textbf{18} commonly\noccurring objects with \\textbf{14} different materials. We showcase how we\nprovide variability in terms of lighting and camera placement for these\nobjects. Next, we provide a benchmark for inferring material properties from\nvisual images using these perturbed models in the scene, discussing the\nspecific neural network models involved and their performance based on\ndifferent image comparison metrics. By accurately simulating light interactions\nwith different materials, we can enhance realism, which is crucial for training\nmodels effectively through large-scale simulations. This research aims to\nrevolutionize perception in consumer robotics. The dataset is provided\n\\href{https://huggingface.co/datasets/UMTRI/MatPredict}{here} and the code is\nprovided \\href{https://github.com/arpan-kusari/MatPredict}{here}."}
{"id": "2505.13208", "pdf": "https://arxiv.org/pdf/2505.13208", "abs": "https://arxiv.org/abs/2505.13208", "authors": ["Colin Krawchuk", "Nikhil Khatri", "Neil John Ortega", "Dimitri Kartsaklis"], "title": "Efficient Generation of Parameterised Quantum Circuits from Large Texts", "categories": ["quant-ph", "cs.AI", "cs.CL"], "comment": null, "summary": "Quantum approaches to natural language processing (NLP) are redefining how\nlinguistic information is represented and processed. While traditional hybrid\nquantum-classical models rely heavily on classical neural networks, recent\nadvancements propose a novel framework, DisCoCirc, capable of directly encoding\nentire documents as parameterised quantum circuits (PQCs), besides enjoying\nsome additional interpretability and compositionality benefits. Following these\nideas, this paper introduces an efficient methodology for converting\nlarge-scale texts into quantum circuits using tree-like representations of\npregroup diagrams. Exploiting the compositional parallels between language and\nquantum mechanics, grounded in symmetric monoidal categories, our approach\nenables faithful and efficient encoding of syntactic and discourse\nrelationships in long and complex texts (up to 6410 words in our experiments)\nto quantum circuits. The developed system is provided to the community as part\nof the augmented open-source quantum NLP package lambeq Gen II."}
{"id": "2505.13210", "pdf": "https://arxiv.org/pdf/2505.13210", "abs": "https://arxiv.org/abs/2505.13210", "authors": ["Xiaocong Du", "Haoyu Pei", "Haipeng Zhang"], "title": "Picturized and Recited with Dialects: A Multimodal Chinese Representation Framework for Sentiment Analysis of Classical Chinese Poetry", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Classical Chinese poetry is a vital and enduring part of Chinese literature,\nconveying profound emotional resonance. Existing studies analyze sentiment\nbased on textual meanings, overlooking the unique rhythmic and visual features\ninherent in poetry,especially since it is often recited and accompanied by\nChinese paintings. In this work, we propose a dialect-enhanced multimodal\nframework for classical Chinese poetry sentiment analysis. We extract\nsentence-level audio features from the poetry and incorporate audio from\nmultiple dialects,which may retain regional ancient Chinese phonetic features,\nenriching the phonetic representation. Additionally, we generate sentence-level\nvisual features, and the multimodal features are fused with textual features\nenhanced by LLM translation through multimodal contrastive representation\nlearning. Our framework outperforms state-of-the-art methods on two public\ndatasets, achieving at least 2.51% improvement in accuracy and 1.63% in macro\nF1. We open-source the code to facilitate research in this area and provide\ninsights for general multimodal Chinese representation."}
{"id": "2505.13211", "pdf": "https://arxiv.org/pdf/2505.13211", "abs": "https://arxiv.org/abs/2505.13211", "authors": ["Sand. ai", "Hansi Teng", "Hongyu Jia", "Lei Sun", "Lingzhi Li", "Maolin Li", "Mingqiu Tang", "Shuai Han", "Tianning Zhang", "W. Q. Zhang", "Weifeng Luo", "Xiaoyang Kang", "Yuchen Sun", "Yue Cao", "Yunpeng Huang", "Yutong Lin", "Yuxin Fang", "Zewei Tao", "Zheng Zhang", "Zhongshu Wang", "Zixun Liu", "Dai Shi", "Guoli Su", "Hanwen Sun", "Hong Pan", "Jie Wang", "Jiexin Sheng", "Min Cui", "Min Hu", "Ming Yan", "Shucheng Yin", "Siran Zhang", "Tingting Liu", "Xianping Yin", "Xiaoyu Yang", "Xin Song", "Xuan Hu", "Yankai Zhang", "Yuqiao Li"], "title": "MAGI-1: Autoregressive Video Generation at Scale", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We present MAGI-1, a world model that generates videos by autoregressively\npredicting a sequence of video chunks, defined as fixed-length segments of\nconsecutive frames. Trained to denoise per-chunk noise that increases\nmonotonically over time, MAGI-1 enables causal temporal modeling and naturally\nsupports streaming generation. It achieves strong performance on image-to-video\n(I2V) tasks conditioned on text instructions, providing high temporal\nconsistency and scalability, which are made possible by several algorithmic\ninnovations and a dedicated infrastructure stack. MAGI-1 facilitates\ncontrollable generation via chunk-wise prompting and supports real-time,\nmemory-efficient deployment by maintaining constant peak inference cost,\nregardless of video length. The largest variant of MAGI-1 comprises 24 billion\nparameters and supports context lengths of up to 4 million tokens,\ndemonstrating the scalability and robustness of our approach. The code and\nmodels are available at https://github.com/SandAI-org/MAGI-1 and\nhttps://github.com/SandAI-org/MagiAttention. The product can be accessed at\nhttps://sand.ai."}
{"id": "2505.13253", "pdf": "https://arxiv.org/pdf/2505.13253", "abs": "https://arxiv.org/abs/2505.13253", "authors": ["Lennart Röstel", "Dominik Winkelbauer", "Johannes Pitz", "Leon Sievers", "Berthold Bäuml"], "title": "Composing Dextrous Grasping and In-hand Manipulation via Scoring with a Reinforcement Learning Critic", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "In-hand manipulation and grasping are fundamental yet often separately\naddressed tasks in robotics. For deriving in-hand manipulation policies,\nreinforcement learning has recently shown great success. However, the derived\ncontrollers are not yet useful in real-world scenarios because they often\nrequire a human operator to place the objects in suitable initial (grasping)\nstates. Finding stable grasps that also promote the desired in-hand\nmanipulation goal is an open problem. In this work, we propose a method for\nbridging this gap by leveraging the critic network of a reinforcement learning\nagent trained for in-hand manipulation to score and select initial grasps. Our\nexperiments show that this method significantly increases the success rate of\nin-hand manipulation without requiring additional training. We also present an\nimplementation of a full grasp manipulation pipeline on a real-world system,\nenabling autonomous grasping and reorientation even of unwieldy objects."}
{"id": "2505.13257", "pdf": "https://arxiv.org/pdf/2505.13257", "abs": "https://arxiv.org/abs/2505.13257", "authors": ["Zilu Tang", "Afra Feyza Akyürek", "Ekin Akyürek", "Derry Wijaya"], "title": "WikiPersonas: What Can We Learn From Personalized Alignment to Famous People?", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "9 pages, preprint", "summary": "Preference alignment has become a standard pipeline in finetuning models to\nfollow \\emph{generic} human preferences. Majority of work seeks to optimize\nmodel to produce responses that would be preferable \\emph{on average},\nsimplifying the diverse and often \\emph{contradicting} space of human\npreferences. While research has increasingly focused on personalized alignment:\nadapting models to individual user preferences, there is a lack of personalized\npreference dataset which focus on nuanced individual-level preferences. To\naddress this, we introduce WikiPersona: the first fine-grained personalization\nusing well-documented, famous individuals. Our dataset challenges models to\nalign with these personas through an interpretable process: generating\nverifiable textual descriptions of a persona's background and preferences in\naddition to alignment. We systematically evaluate different personalization\napproaches and find that as few-shot prompting with preferences and fine-tuning\nfail to simultaneously ensure effectiveness and efficiency, using\n\\textit{inferred personal preferences} as prefixes enables effective\npersonalization, especially in topics where preferences clash while leading to\nmore equitable generalization across unseen personas."}
{"id": "2505.13264", "pdf": "https://arxiv.org/pdf/2505.13264", "abs": "https://arxiv.org/abs/2505.13264", "authors": ["Carlos Rodriguez-Pardo", "Louis Daumas", "Leonardo Chiani", "Massimo Tavoni"], "title": "Net-Zero: A Comparative Study on Neural Network Design for Climate-Economic PDEs Under Uncertainty", "categories": ["cs.LG", "cs.AI", "cs.NE", "cs.PF", "math.AP", "68T07 (Primary) 35Q91, 91B76 (Secondary)", "I.2.1; I.5.1; J.4"], "comment": "Under review", "summary": "Climate-economic modeling under uncertainty presents significant\ncomputational challenges that may limit policymakers' ability to address\nclimate change effectively. This paper explores neural network-based approaches\nfor solving high-dimensional optimal control problems arising from models that\nincorporate ambiguity aversion in climate mitigation decisions. We develop a\ncontinuous-time endogenous-growth economic model that accounts for multiple\nmitigation pathways, including emission-free capital and carbon intensity\nreductions. Given the inherent complexity and high dimensionality of these\nmodels, traditional numerical methods become computationally intractable. We\nbenchmark several neural network architectures against finite-difference\ngenerated solutions, evaluating their ability to capture the dynamic\ninteractions between uncertainty, technology transitions, and optimal climate\npolicy. Our findings demonstrate that appropriate neural architecture selection\nsignificantly impacts both solution accuracy and computational efficiency when\nmodeling climate-economic systems under uncertainty. These methodological\nadvances enable more sophisticated modeling of climate policy decisions,\nallowing for better representation of technology transitions and\nuncertainty-critical elements for developing effective mitigation strategies in\nthe face of climate change."}
{"id": "2505.13268", "pdf": "https://arxiv.org/pdf/2505.13268", "abs": "https://arxiv.org/abs/2505.13268", "authors": ["Livia Qian", "Carol Figueroa", "Gabriel Skantze"], "title": "Representation of perceived prosodic similarity of conversational feedback", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Interspeech 2025", "summary": "Vocal feedback (e.g., `mhm', `yeah', `okay') is an important component of\nspoken dialogue and is crucial to ensuring common ground in conversational\nsystems. The exact meaning of such feedback is conveyed through both lexical\nand prosodic form. In this work, we investigate the perceived prosodic\nsimilarity of vocal feedback with the same lexical form, and to what extent\nexisting speech representations reflect such similarities. A triadic comparison\ntask with recruited participants is used to measure perceived similarity of\nfeedback responses taken from two different datasets. We find that spectral and\nself-supervised speech representations encode prosody better than extracted\npitch features, especially in the case of feedback from the same speaker. We\nalso find that it is possible to further condense and align the representations\nto human perception through contrastive learning."}
{"id": "2505.13280", "pdf": "https://arxiv.org/pdf/2505.13280", "abs": "https://arxiv.org/abs/2505.13280", "authors": ["Elias Collaert", "Abel Rodríguez", "Sander Joos", "Lieven Desmet", "Vera Rimmer"], "title": "FlowPure: Continuous Normalizing Flows for Adversarial Purification", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Despite significant advancements in the area, adversarial robustness remains\na critical challenge in systems employing machine learning models. The removal\nof adversarial perturbations at inference time, known as adversarial\npurification, has emerged as a promising defense strategy. To achieve this,\nstate-of-the-art methods leverage diffusion models that inject Gaussian noise\nduring a forward process to dilute adversarial perturbations, followed by a\ndenoising step to restore clean samples before classification. In this work, we\npropose FlowPure, a novel purification method based on Continuous Normalizing\nFlows (CNFs) trained with Conditional Flow Matching (CFM) to learn mappings\nfrom adversarial examples to their clean counterparts. Unlike prior\ndiffusion-based approaches that rely on fixed noise processes, FlowPure can\nleverage specific attack knowledge to improve robustness under known threats,\nwhile also supporting a more general stochastic variant trained on Gaussian\nperturbations for settings where such knowledge is unavailable. Experiments on\nCIFAR-10 and CIFAR-100 demonstrate that our method outperforms state-of-the-art\npurification-based defenses in preprocessor-blind and white-box scenarios, and\ncan do so while fully preserving benign accuracy in the former. Moreover, our\nresults show that not only is FlowPure a highly effective purifier but it also\nholds a strong potential for adversarial detection, identifying\npreprocessor-blind PGD samples with near-perfect accuracy."}
{"id": "2505.13291", "pdf": "https://arxiv.org/pdf/2505.13291", "abs": "https://arxiv.org/abs/2505.13291", "authors": ["Yifu Cai", "Xinyu Li", "Mononito Goswami", "Michał Wiliński", "Gus Welter", "Artur Dubrawski"], "title": "TimeSeriesGym: A Scalable Benchmark for (Time Series) Machine Learning Engineering Agents", "categories": ["cs.LG", "cs.AI"], "comment": "Open source code available at\n  https://github.com/moment-timeseries-foundation-model/TimeSeriesGym. YC, XL,\n  MG and MW contributed equally, and should be considered joint first authors", "summary": "We introduce TimeSeriesGym, a scalable benchmarking framework for evaluating\nArtificial Intelligence (AI) agents on time series machine learning engineering\nchallenges. Existing benchmarks lack scalability, focus narrowly on model\nbuilding in well-defined settings, and evaluate only a limited set of research\nartifacts (e.g., CSV submission files). To make AI agent benchmarking more\nrelevant to the practice of machine learning engineering, our framework scales\nalong two critical dimensions. First, recognizing that effective ML engineering\nrequires a range of diverse skills, TimeSeriesGym incorporates challenges from\ndiverse sources spanning multiple domains and tasks. We design challenges to\nevaluate both isolated capabilities (including data handling, understanding\nresearch repositories, and code translation) and their combinations, and rather\nthan addressing each challenge independently, we develop tools that support\ndesigning multiple challenges at scale. Second, we implement evaluation\nmechanisms for multiple research artifacts, including submission files, code,\nand models, using both precise numeric measures and more flexible LLM-based\nevaluation approaches. This dual strategy balances objective assessment with\ncontextual judgment. Although our initial focus is on time series applications,\nour framework can be readily extended to other data modalities, broadly\nenhancing the comprehensiveness and practical utility of agentic AI evaluation.\nWe open-source our benchmarking framework to facilitate future research on the\nML engineering capabilities of AI agents."}
{"id": "2505.13292", "pdf": "https://arxiv.org/pdf/2505.13292", "abs": "https://arxiv.org/abs/2505.13292", "authors": ["Huaiying Luo", "Cheng Ji"], "title": "Cross-Cloud Data Privacy Protection: Optimizing Collaborative Mechanisms of AI Systems by Integrating Federated Learning and LLMs", "categories": ["cs.CR", "cs.AI"], "comment": "Accepted by 2025 IEEE 7th International Conference on Communications,\n  Information System and Computer Engineering", "summary": "In the age of cloud computing, data privacy protection has become a major\nchallenge, especially when sharing sensitive data across cloud environments.\nHowever, how to optimize collaboration across cloud environments remains an\nunresolved problem. In this paper, we combine federated learning with\nlarge-scale language models to optimize the collaborative mechanism of AI\nsystems. Based on the existing federated learning framework, we introduce a\ncross-cloud architecture in which federated learning works by aggregating model\nupdates from decentralized nodes without exposing the original data. At the\nsame time, combined with large-scale language models, its powerful context and\nsemantic understanding capabilities are used to improve model training\nefficiency and decision-making ability. We've further innovated by introducing\na secure communication layer to ensure the privacy and integrity of model\nupdates and training data. The model enables continuous model adaptation and\nfine-tuning across different cloud environments while protecting sensitive\ndata. Experimental results show that the proposed method is significantly\nbetter than the traditional federated learning model in terms of accuracy,\nconvergence speed and data privacy protection."}
{"id": "2505.13307", "pdf": "https://arxiv.org/pdf/2505.13307", "abs": "https://arxiv.org/abs/2505.13307", "authors": ["Qiguang Chen", "Libo Qin", "Jinhao Liu", "Yue Liao", "Jiaqi Wang", "Jingxuan Zhou", "Wanxiang Che"], "title": "RBF++: Quantifying and Optimizing Reasoning Boundaries across Measurable and Unmeasurable Capabilities for Chain-of-Thought Reasoning", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Manuscript", "summary": "Chain-of-Thought (CoT) reasoning has proven effective in enhancing large\nlanguage models (LLMs) on complex tasks, spurring research into its underlying\nmechanisms. However, two primary challenges remain for real-world applications:\n(1) the lack of quantitative metrics and actionable guidelines for evaluating\nand optimizing measurable boundaries of CoT capability, and (2) the absence of\nmethods to assess boundaries of unmeasurable CoT capability, such as multimodal\nperception. To address these gaps, we introduce the Reasoning Boundary\nFramework++ (RBF++). To tackle the first challenge, we define the reasoning\nboundary (RB) as the maximum limit of CoT performance. We also propose a\ncombination law for RBs, enabling quantitative analysis and offering actionable\nguidance across various CoT tasks. For the second challenge, particularly in\nmultimodal scenarios, we introduce a constant assumption, which replaces\nunmeasurable RBs with scenario-specific constants. Additionally, we propose the\nreasoning boundary division mechanism, which divides unmeasurable RBs into two\nsub-boundaries, facilitating the quantification and optimization of both\nunmeasurable domain knowledge and multimodal perception capabilities. Extensive\nexperiments involving 38 models across 13 tasks validate the feasibility of our\nframework in cross-modal settings. Additionally, we evaluate 10 CoT strategies,\noffer insights into optimization and decay from two complementary perspectives,\nand expand evaluation benchmarks for measuring RBs in LLM reasoning. We hope\nthis work advances the understanding of RBs and optimization strategies in\nLLMs. Code and data are available at\nhttps://github.com/LightChen233/reasoning-boundary."}
{"id": "2505.13308", "pdf": "https://arxiv.org/pdf/2505.13308", "abs": "https://arxiv.org/abs/2505.13308", "authors": ["Hengli Li", "Chenxi Li", "Tong Wu", "Xuekai Zhu", "Yuxuan Wang", "Zhaoxin Yu", "Eric Hanchen Jiang", "Song-Chun Zhu", "Zixia Jia", "Ying Nian Wu", "Zilong Zheng"], "title": "Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient in Latent Space", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reasoning ability, a core component of human intelligence, continues to pose\na significant challenge for Large Language Models (LLMs) in the pursuit of AGI.\nAlthough model performance has improved under the training scaling law,\nsignificant challenges remain, particularly with respect to training\nalgorithms, such as catastrophic forgetting, and the limited availability of\nnovel training data. As an alternative, test-time scaling enhances reasoning\nperformance by increasing test-time computation without parameter updating.\nUnlike prior methods in this paradigm focused on token space, we propose\nleveraging latent space for more effective reasoning and better adherence to\nthe test-time scaling law. We introduce LatentSeek, a novel framework that\nenhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA)\nwithin the model's latent space. Specifically, LatentSeek leverages policy\ngradient to iteratively update latent representations, guided by self-generated\nreward signals. LatentSeek is evaluated on a range of reasoning benchmarks,\nincluding GSM8K, MATH-500, and AIME2024, across multiple LLM architectures.\nResults show that LatentSeek consistently outperforms strong baselines, such as\nChain-of-Thought prompting and fine-tuning-based methods. Furthermore, our\nanalysis demonstrates that LatentSeek is highly efficient, typically converging\nwithin a few iterations for problems of average complexity, while also\nbenefiting from additional iterations, thereby highlighting the potential of\ntest-time scaling in the latent space. These findings position LatentSeek as a\nlightweight, scalable, and effective solution for enhancing the reasoning\ncapabilities of LLMs."}
{"id": "2505.13315", "pdf": "https://arxiv.org/pdf/2505.13315", "abs": "https://arxiv.org/abs/2505.13315", "authors": ["Reza T. Batley", "Sourav Saha"], "title": "KHRONOS: a Kernel-Based Neural Architecture for Rapid, Resource-Efficient Scientific Computation", "categories": ["cs.LG", "cs.AI", "cs.MS"], "comment": null, "summary": "Contemporary models of high dimensional physical systems are constrained by\nthe curse of dimensionality and a reliance on dense data. We introduce KHRONOS\n(Kernel Expansion Hierarchy for Reduced Order, Neural Optimized Surrogates), an\nAI framework for model based, model free and model inversion tasks. KHRONOS\nconstructs continuously differentiable target fields with a hierarchical\ncomposition of per-dimension kernel expansions, which are tensorized into modes\nand then superposed. We evaluate KHRONOS on a canonical 2D, Poisson equation\nbenchmark: across 16 to 512 degrees of freedom (DoFs), it obtained L2 square\nerrors of 5e-4 down to 6e-10. This represents a 100 time gain over Kolmogorov\nArnold Networks (which itself reports a 100 times improvement on MLPs/PINNs\nwith 100 times fewer parameters) when controlling for the number of parameters.\nThis also represents a 1e4 times improvement in L2 square error compared to\nstandard linear FEM at comparable DoFs. Inference complexity is dominated by\ninner products, yielding sub-millisecond full-field predictions that scale to\nan arbitrary resolution. For inverse problems, KHRONOS facilitates rapid,\niterative level set recovery in only a few forward evaluations, with\nsub-microsecond per sample latency. KHRONOS scalability, expressivity, and\ninterpretability open new avenues in constrained edge computing, online\ncontrol, computer vision, and beyond."}
{"id": "2505.13316", "pdf": "https://arxiv.org/pdf/2505.13316", "abs": "https://arxiv.org/abs/2505.13316", "authors": ["Gabriele Spadaro", "Alberto Presta", "Jhony H. Giraldo", "Marco Grangetto", "Wei Hu", "Giuseppe Valenzise", "Attilio Fiandrotti", "Enzo Tartaglione"], "title": "Denoising Diffusion Probabilistic Model for Point Cloud Compression at Low Bit-Rates", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "6 pages, 5 figures, accepted at ICME 2025", "summary": "Efficient compression of low-bit-rate point clouds is critical for\nbandwidth-constrained applications. However, existing techniques mainly focus\non high-fidelity reconstruction, requiring many bits for compression. This\npaper proposes a \"Denoising Diffusion Probabilistic Model\" (DDPM) architecture\nfor point cloud compression (DDPM-PCC) at low bit-rates. A PointNet encoder\nproduces the condition vector for the generation, which is then quantized via a\nlearnable vector quantizer. This configuration allows to achieve a low bitrates\nwhile preserving quality. Experiments on ShapeNet and ModelNet40 show improved\nrate-distortion at low rates compared to standardized and state-of-the-art\napproaches. We publicly released the code at\nhttps://github.com/EIDOSLAB/DDPM-PCC."}
{"id": "2505.13324", "pdf": "https://arxiv.org/pdf/2505.13324", "abs": "https://arxiv.org/abs/2505.13324", "authors": ["Galit Shmueli", "David Martens", "Jaewon Yoo", "Travis Greene"], "title": "From What Ifs to Insights: Counterfactuals in Causal Inference vs. Explainable AI", "categories": ["stat.ML", "cs.AI", "cs.LG", "econ.EM", "stat.ME"], "comment": null, "summary": "Counterfactuals play a pivotal role in the two distinct data science fields\nof causal inference (CI) and explainable artificial intelligence (XAI). While\nthe core idea behind counterfactuals remains the same in both fields--the\nexamination of what would have happened under different circumstances--there\nare key differences in how they are used and interpreted. We introduce a formal\ndefinition that encompasses the multi-faceted concept of the counterfactual in\nCI and XAI. We then discuss how counterfactuals are used, evaluated, generated,\nand operationalized in CI vs. XAI, highlighting conceptual and practical\ndifferences. By comparing and contrasting the two, we hope to identify\nopportunities for cross-fertilization across CI and XAI."}
{"id": "2505.13329", "pdf": "https://arxiv.org/pdf/2505.13329", "abs": "https://arxiv.org/abs/2505.13329", "authors": ["Frédéric Berdoz", "Dustin Brunner", "Yann Vonlanthen", "Roger Wattenhofer"], "title": "Recommender Systems for Democracy: Toward Adversarial Robustness in Voting Advice Applications", "categories": ["cs.CY", "cs.AI", "cs.CR"], "comment": "This is the extended version of the paper, accepted at IJCAI 2025", "summary": "Voting advice applications (VAAs) help millions of voters understand which\npolitical parties or candidates best align with their views. This paper\nexplores the potential risks these applications pose to the democratic process\nwhen targeted by adversarial entities. In particular, we expose 11 manipulation\nstrategies and measure their impact using data from Switzerland's primary VAA,\nSmartvote, collected during the last two national elections. We find that\naltering application parameters, such as the matching method, can shift a\nparty's recommendation frequency by up to 105%. Cherry-picking questionnaire\nitems can increase party recommendation frequency by over 261%, while subtle\nchanges to parties' or candidates' responses can lead to a 248% increase. To\naddress these vulnerabilities, we propose adversarial robustness properties\nVAAs should satisfy, introduce empirical metrics for assessing the resilience\nof various matching methods, and suggest possible avenues for research toward\nmitigating the effect of manipulation. Our framework is key to ensuring secure\nand reliable AI-based VAAs poised to emerge in the near future."}
{"id": "2505.13338", "pdf": "https://arxiv.org/pdf/2505.13338", "abs": "https://arxiv.org/abs/2505.13338", "authors": ["Qiongqiong Wang", "Hardik B. Sailor", "Tianchi Liu", "Ai Ti Aw"], "title": "Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data Condensation and Spoken QA Generation", "categories": ["cs.CL", "cs.AI", "eess.AS"], "comment": "Accepted at Interspeech 2025", "summary": "Current speech-LLMs exhibit limited capability in contextual reasoning\nalongside paralinguistic understanding, primarily due to the lack of\nQuestion-Answer (QA) datasets that cover both aspects. We propose a novel\nframework for dataset generation from in-the-wild speech data, that integrates\ncontextual reasoning with paralinguistic information. It consists of a pseudo\nparalinguistic label-based data condensation of in-the-wild speech and\nLLM-based Contextual Paralinguistic QA (CPQA) generation. The effectiveness is\nvalidated by a strong correlation in evaluations of the Qwen2-Audio-7B-Instruct\nmodel on a dataset created by our framework and human-generated CPQA dataset.\nThe results also reveal the speech-LLM's limitations in handling empathetic\nreasoning tasks, highlighting the need for such datasets and more robust\nmodels. The proposed framework is first of its kind and has potential in\ntraining more robust speech-LLMs with paralinguistic reasoning capabilities."}
{"id": "2505.13339", "pdf": "https://arxiv.org/pdf/2505.13339", "abs": "https://arxiv.org/abs/2505.13339", "authors": ["Jia-Hui Pan", "Yeok Tatt Cheah", "Zhengzhe Liu", "Ka-Hei Hui", "Xiaojie Gao", "Pheng-Ann Heng", "Yun-Hui Liu", "Chi-Wing Fu"], "title": "OPA-Pack: Object-Property-Aware Robotic Bin Packing", "categories": ["cs.RO", "cs.AI"], "comment": "Submitted to IEEE Transactions on Robotics (TRO) on Feb. 10, 2025", "summary": "Robotic bin packing aids in a wide range of real-world scenarios such as\ne-commerce and warehouses. Yet, existing works focus mainly on considering the\nshape of objects to optimize packing compactness and neglect object properties\nsuch as fragility, edibility, and chemistry that humans typically consider when\npacking objects. This paper presents OPA-Pack (Object-Property-Aware Packing\nframework), the first framework that equips the robot with object property\nconsiderations in planning the object packing. Technical-wise, we develop a\nnovel object property recognition scheme with retrieval-augmented generation\nand chain-of-thought reasoning, and build a dataset with object property\nannotations for 1,032 everyday objects. Also, we formulate OPA-Net, aiming to\njointly separate incompatible object pairs and reduce pressure on fragile\nobjects, while compacting the packing. Further, OPA-Net consists of a property\nembedding layer to encode the property of candidate objects to be packed,\ntogether with a fragility heightmap and an avoidance heightmap to keep track of\nthe packed objects. Then, we design a reward function and adopt a deep\nQ-learning scheme to train OPA-Net. Experimental results manifest that OPA-Pack\ngreatly improves the accuracy of separating incompatible object pairs (from 52%\nto 95%) and largely reduces pressure on fragile objects (by 29.4%), while\nmaintaining good packing compactness. Besides, we demonstrate the effectiveness\nof OPA-Pack on a real packing platform, showcasing its practicality in\nreal-world scenarios."}
{"id": "2505.13344", "pdf": "https://arxiv.org/pdf/2505.13344", "abs": "https://arxiv.org/abs/2505.13344", "authors": ["Ahmet Berke Gokmen", "Yigit Ekin", "Bahri Batuhan Bilecen", "Aysegul Dundar"], "title": "RoPECraft: Training-Free Motion Transfer with Trajectory-Guided RoPE Optimization on Diffusion Transformers", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "https://berkegokmen1.github.io/RoPECraft/", "summary": "We propose RoPECraft, a training-free video motion transfer method for\ndiffusion transformers that operates solely by modifying their rotary\npositional embeddings (RoPE). We first extract dense optical flow from a\nreference video, and utilize the resulting motion offsets to warp the\ncomplex-exponential tensors of RoPE, effectively encoding motion into the\ngeneration process. These embeddings are then further optimized during\ndenoising time steps via trajectory alignment between the predicted and target\nvelocities using a flow-matching objective. To keep the output faithful to the\ntext prompt and prevent duplicate generations, we incorporate a regularization\nterm based on the phase components of the reference video's Fourier transform,\nprojecting the phase angles onto a smooth manifold to suppress high-frequency\nartifacts. Experiments on benchmarks reveal that RoPECraft outperforms all\nrecently published methods, both qualitatively and quantitatively."}
{"id": "2505.13346", "pdf": "https://arxiv.org/pdf/2505.13346", "abs": "https://arxiv.org/abs/2505.13346", "authors": ["Austin Xu", "Yilun Zhou", "Xuan-Phi Nguyen", "Caiming Xiong", "Shafiq Joty"], "title": "J4R: Learning to Judge with Equivalent Initial State Group Relative Preference Optimization", "categories": ["cs.CL", "cs.AI"], "comment": "25 pages, 4 figures, 6 tables. To be updated with links for\n  code/benchmark", "summary": "To keep pace with the increasing pace of large language models (LLM)\ndevelopment, model output evaluation has transitioned away from time-consuming\nhuman evaluation to automatic evaluation, where LLMs themselves are tasked with\nassessing and critiquing other model outputs. LLM-as-judge models are a class\nof generative evaluators that excel in evaluating relatively simple domains,\nlike chat quality, but struggle in reasoning intensive domains where model\nresponses contain more substantive and challenging content. To remedy existing\njudge shortcomings, we explore training judges with reinforcement learning\n(RL). We make three key contributions: (1) We propose the Equivalent Initial\nState Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us\nto train our judge to be robust to positional biases that arise in more complex\nevaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that\nevaluates judges in diverse reasoning settings not covered by prior work. (3)\nWe train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that\noutperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or\nexceeding the performance of larger GRPO-trained judges on both JudgeBench and\nReasoningJudgeBench."}
{"id": "2505.13358", "pdf": "https://arxiv.org/pdf/2505.13358", "abs": "https://arxiv.org/abs/2505.13358", "authors": ["Nimrod Berman", "Ilan Naiman", "Moshe Eliasof", "Hedi Zisling", "Omri Azencot"], "title": "One-Step Offline Distillation of Diffusion-based Models via Koopman Modeling", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Diffusion-based generative models have demonstrated exceptional performance,\nyet their iterative sampling procedures remain computationally expensive. A\nprominent strategy to mitigate this cost is distillation, with offline\ndistillation offering particular advantages in terms of efficiency, modularity,\nand flexibility. In this work, we identify two key observations that motivate a\nprincipled distillation framework: (1) while diffusion models have been viewed\nthrough the lens of dynamical systems theory, powerful and underexplored tools\ncan be further leveraged; and (2) diffusion models inherently impose\nstructured, semantically coherent trajectories in latent space. Building on\nthese observations, we introduce the Koopman Distillation Model KDM, a novel\noffline distillation approach grounded in Koopman theory-a classical framework\nfor representing nonlinear dynamics linearly in a transformed space. KDM\nencodes noisy inputs into an embedded space where a learned linear operator\npropagates them forward, followed by a decoder that reconstructs clean samples.\nThis enables single-step generation while preserving semantic fidelity. We\nprovide theoretical justification for our approach: (1) under mild assumptions,\nthe learned diffusion dynamics admit a finite-dimensional Koopman\nrepresentation; and (2) proximity in the Koopman latent space correlates with\nsemantic similarity in the generated outputs, allowing for effective trajectory\nalignment. Empirically, KDM achieves state-of-the-art performance across\nstandard offline distillation benchmarks, improving FID scores by up to 40% in\na single generation step. All implementation details and code for the\nexperimental setups are provided in our GitHub -\nhttps://github.com/azencot-group/KDM, or in our project page -\nhttps://sites.google.com/view/koopman-distillation-model."}
{"id": "2505.13379", "pdf": "https://arxiv.org/pdf/2505.13379", "abs": "https://arxiv.org/abs/2505.13379", "authors": ["Gongfan Fang", "Xinyin Ma", "Xinchao Wang"], "title": "Thinkless: LLM Learns When to Think", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reasoning Language Models, capable of extended chain-of-thought reasoning,\nhave demonstrated remarkable performance on tasks requiring complex logical\ninference. However, applying elaborate reasoning for all queries often results\nin substantial computational inefficiencies, particularly when many problems\nadmit straightforward solutions. This motivates an open question: Can LLMs\nlearn when to think? To answer this, we propose Thinkless, a learnable\nframework that empowers an LLM to adaptively select between short-form and\nlong-form reasoning, based on both task complexity and the model's ability.\nThinkless is trained under a reinforcement learning paradigm and employs two\ncontrol tokens, <short> for concise responses and <think> for detailed\nreasoning. At the core of our method is a Decoupled Group Relative Policy\nOptimization (DeGRPO) algorithm, which decomposes the learning objective of\nhybrid reasoning into two components: (1) a control token loss that governs the\nselection of the reasoning mode, and (2) a response loss that improves the\naccuracy of the generated answers. This decoupled formulation enables\nfine-grained control over the contributions of each objective, stabilizing\ntraining and effectively preventing collapse observed in vanilla GRPO.\nEmpirically, on several benchmarks such as Minerva Algebra, MATH-500, and\nGSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% -\n90%, significantly improving the efficiency of Reasoning Language Models. The\ncode is available at https://github.com/VainF/Thinkless"}
{"id": "2505.13381", "pdf": "https://arxiv.org/pdf/2505.13381", "abs": "https://arxiv.org/abs/2505.13381", "authors": ["Mak Ahmad", "Prerna Ravi", "David Karger", "Marc Facciotti"], "title": "How Adding Metacognitive Requirements in Support of AI Feedback in Practice Exams Transforms Student Learning Behaviors", "categories": ["cs.HC", "cs.AI", "K.3.1; I.2.7; H.5.2"], "comment": "10 pages, 3 figures, to appear in Proceedings of the Twelfth ACM\n  Conference on Learning @ Scale (L@S 2025), July 2025, Palermo, Italy", "summary": "Providing personalized, detailed feedback at scale in large undergraduate\nSTEM courses remains a persistent challenge. We present an empirically\nevaluated practice exam system that integrates AI generated feedback with\ntargeted textbook references, deployed in a large introductory biology course.\nOur system encourages metacognitive behavior by asking students to explain\ntheir answers and declare their confidence. It uses OpenAI's GPT-4o to generate\npersonalized feedback based on this information, while directing them to\nrelevant textbook sections. Through interaction logs from consenting\nparticipants across three midterms (541, 342, and 413 students respectively),\ntotaling 28,313 question-student interactions across 146 learning objectives,\nalong with 279 surveys and 23 interviews, we examined the system's impact on\nlearning outcomes and engagement. Across all midterms, feedback types showed no\nstatistically significant performance differences, though some trends suggested\npotential benefits. The most substantial impact came from the required\nconfidence ratings and explanations, which students reported transferring to\ntheir actual exam strategies. About 40 percent of students engaged with\ntextbook references when prompted by feedback -- far higher than traditional\nreading rates. Survey data revealed high satisfaction (mean rating 4.1 of 5),\nwith 82.1 percent reporting increased confidence on practiced midterm topics,\nand 73.4 percent indicating they could recall and apply specific concepts. Our\nfindings suggest that embedding structured reflection requirements may be more\nimpactful than sophisticated feedback mechanisms."}
{"id": "2505.13388", "pdf": "https://arxiv.org/pdf/2505.13388", "abs": "https://arxiv.org/abs/2505.13388", "authors": ["David Anugraha", "Zilu Tang", "Lester James V. Miranda", "Hanyang Zhao", "Mohammad Rifqi Farhansyah", "Garry Kuwanto", "Derry Wijaya", "Genta Indra Winata"], "title": "R3: Robust Rubric-Agnostic Reward Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Preprint", "summary": "Reward models are essential for aligning language model outputs with human\npreferences, yet existing approaches often lack both controllability and\ninterpretability. These models are typically optimized for narrow objectives,\nlimiting their generalizability to broader downstream tasks. Moreover, their\nscalar outputs are difficult to interpret without contextual reasoning. To\naddress these limitations, we introduce R3, a novel reward modeling framework\nthat is rubric-agnostic, generalizable across evaluation dimensions, and\nprovides interpretable, reasoned score assignments. R3 enables more transparent\nand flexible evaluation of language models, supporting robust alignment with\ndiverse human values and use cases. Our models, data, and code are available as\nopen source at https://github.com/rubricreward/r3"}
{"id": "2505.13393", "pdf": "https://arxiv.org/pdf/2505.13393", "abs": "https://arxiv.org/abs/2505.13393", "authors": ["Christopher K. Frantz"], "title": "IG Parser: A Software Package for the Encoding of Institutional Statements using the Institutional Grammar", "categories": ["cs.MA", "cs.AI", "cs.CL", "68T30, 68T50", "E.2; H.1.0; I.7.2; I.6.5; K.4.1"], "comment": "24 pages", "summary": "This article provides an overview of IG Parser, a software that facilitates\nqualitative content analysis of formal (e.g., legal) rules or informal (e.g.,\nsocio-normative) norms, and strategies (such as conventions) -- referred to as\n\\emph{institutions} -- that govern social systems and operate configurally to\ndescribe \\emph{institutional systems}. To this end, the IG Parser employs a\ndistinctive syntax that ensures rigorous encoding of natural language, while\nautomating the transformation into various formats that support the downstream\nanalysis using diverse analytical techniques. The conceptual core of the IG\nParser is an associated syntax, IG Script, that operationalizes the conceptual\nfoundations of the Institutional Grammar, and more specifically Institutional\nGrammar 2.0, an analytical paradigm for institutional analysis. This article\npresents the IG Parser, including its conceptual foundations, syntactic\nspecification of IG Script, alongside architectural principles. This\nintroduction is augmented with selective illustrative examples that highlight\nthe use and benefit associated with the tool."}
{"id": "2505.13417", "pdf": "https://arxiv.org/pdf/2505.13417", "abs": "https://arxiv.org/abs/2505.13417", "authors": ["Jiajie Zhang", "Nianyi Lin", "Lei Hou", "Ling Feng", "Juanzi Li"], "title": "AdaptThink: Reasoning Models Can Learn When to Think", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recently, large reasoning models have achieved impressive performance on\nvarious tasks by employing human-like deep thinking. However, the lengthy\nthinking process substantially increases inference overhead, making efficiency\na critical bottleneck. In this work, we first demonstrate that NoThinking,\nwhich prompts the reasoning model to skip thinking and directly generate the\nfinal solution, is a better choice for relatively simple tasks in terms of both\nperformance and efficiency. Motivated by this, we propose AdaptThink, a novel\nRL algorithm to teach reasoning models to choose the optimal thinking mode\nadaptively based on problem difficulty. Specifically, AdaptThink features two\ncore components: (1) a constrained optimization objective that encourages the\nmodel to choose NoThinking while maintaining the overall performance; (2) an\nimportance sampling strategy that balances Thinking and NoThinking samples\nduring on-policy training, thereby enabling cold start and allowing the model\nto explore and exploit both thinking modes throughout the training process. Our\nexperiments indicate that AdaptThink significantly reduces the inference costs\nwhile further enhancing performance. Notably, on three math datasets,\nAdaptThink reduces the average response length of DeepSeek-R1-Distill-Qwen-1.5B\nby 53% and improves its accuracy by 2.4%, highlighting the promise of adaptive\nthinking-mode selection for optimizing the balance between reasoning quality\nand efficiency. Our codes and models are available at\nhttps://github.com/THU-KEG/AdaptThink."}
{"id": "2505.13425", "pdf": "https://arxiv.org/pdf/2505.13425", "abs": "https://arxiv.org/abs/2505.13425", "authors": ["Zhi-Hao Tan", "Zi-Chen Zhao", "Hao-Yu Shi", "Xin-Yu Zhang", "Peng Tan", "Yang Yu", "Zhi-Hua Zhou"], "title": "Learnware of Language Models: Specialized Small Language Models Can Do Big", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The learnware paradigm offers a novel approach to machine learning by\nenabling users to reuse a set of well-trained models for tasks beyond the\nmodels' original purposes. It eliminates the need to build models from scratch,\ninstead relying on specifications (representations of a model's capabilities)\nto identify and leverage the most suitable models for new tasks. While\nlearnware has proven effective in many scenarios, its application to language\nmodels has remained largely unexplored. At the same time, large language models\n(LLMs) have demonstrated remarkable universal question-answering abilities, yet\nthey face challenges in specialized scenarios due to data scarcity, privacy\nconcerns, and high computational costs, thus more and more specialized small\nlanguage models (SLMs) are being trained for specific domains. To address these\nlimitations systematically, the learnware paradigm provides a promising\nsolution by enabling maximum utilization of specialized SLMs, and allowing\nusers to identify and reuse them in a collaborative and privacy-preserving\nmanner.\n  This paper presents a preliminary attempt to apply the learnware paradigm to\nlanguage models. We simulated a learnware system comprising approximately 100\nlearnwares of specialized SLMs with 8B parameters, fine-tuned across finance,\nhealthcare, and mathematics domains. Each learnware contains an SLM and a\nspecification, which enables users to identify the most relevant models without\nexposing their own data. Experimental results demonstrate promising\nperformance: by selecting one suitable learnware for each task-specific\ninference, the system outperforms the base SLMs on all benchmarks. Compared to\nLLMs, the system outperforms Qwen1.5-110B, Qwen2.5-72B, and\nLlama3.1-70B-Instruct by at least 14% in finance domain tasks, and surpasses\nFlan-PaLM-540B (ranked 7th on the Open Medical LLM Leaderboard) in medical\ndomain tasks."}
{"id": "2505.13437", "pdf": "https://arxiv.org/pdf/2505.13437", "abs": "https://arxiv.org/abs/2505.13437", "authors": ["Dian Shao", "Mingfei Shi", "Shengda Xu", "Haodong Chen", "Yongle Huang", "Binglu Wang"], "title": "FinePhys: Fine-grained Human Action Generation by Explicitly Incorporating Physical Laws for Effective Skeletal Guidance", "categories": ["cs.CV", "cs.AI"], "comment": "CVPR 2025", "summary": "Despite significant advances in video generation, synthesizing physically\nplausible human actions remains a persistent challenge, particularly in\nmodeling fine-grained semantics and complex temporal dynamics. For instance,\ngenerating gymnastics routines such as \"switch leap with 0.5 turn\" poses\nsubstantial difficulties for current methods, often yielding unsatisfactory\nresults. To bridge this gap, we propose FinePhys, a Fine-grained human action\ngeneration framework that incorporates Physics to obtain effective skeletal\nguidance. Specifically, FinePhys first estimates 2D poses in an online manner\nand then performs 2D-to-3D dimension lifting via in-context learning. To\nmitigate the instability and limited interpretability of purely data-driven 3D\nposes, we further introduce a physics-based motion re-estimation module\ngoverned by Euler-Lagrange equations, calculating joint accelerations via\nbidirectional temporal updating. The physically predicted 3D poses are then\nfused with data-driven ones, offering multi-scale 2D heatmap guidance for the\ndiffusion process. Evaluated on three fine-grained action subsets from FineGym\n(FX-JUMP, FX-TURN, and FX-SALTO), FinePhys significantly outperforms\ncompetitive baselines. Comprehensive qualitative results further demonstrate\nFinePhys's ability to generate more natural and plausible fine-grained human\nactions."}
{"id": "2505.13438", "pdf": "https://arxiv.org/pdf/2505.13438", "abs": "https://arxiv.org/abs/2505.13438", "authors": ["Penghui Qi", "Zichen Liu", "Tianyu Pang", "Chao Du", "Wee Sun Lee", "Min Lin"], "title": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Scaling test-time compute is crucial for enhancing the reasoning capabilities\nof large language models (LLMs). Existing approaches typically employ\nreinforcement learning (RL) to maximize a verifiable reward obtained at the end\nof reasoning traces. However, such methods optimize only the final performance\nunder a large and fixed token budget, which hinders efficiency in both training\nand deployment. In this work, we present a novel framework, AnytimeReasoner, to\noptimize anytime reasoning performance, which aims to improve token efficiency\nand the flexibility of reasoning under varying token budget constraints. To\nachieve this, we truncate the complete thinking process to fit within sampled\ntoken budgets from a prior distribution, compelling the model to summarize the\noptimal answer for each truncated thinking for verification. This introduces\nverifiable dense rewards into the reasoning process, facilitating more\neffective credit assignment in RL optimization. We then optimize the thinking\nand summary policies in a decoupled manner to maximize the cumulative reward.\nAdditionally, we introduce a novel variance reduction technique, Budget\nRelative Policy Optimization (BRPO), to enhance the robustness and efficiency\nof the learning process when reinforcing the thinking policy. Empirical results\nin mathematical reasoning tasks demonstrate that our method consistently\noutperforms GRPO across all thinking budgets under various prior distributions,\nenhancing both training and token efficiency."}
{"id": "2505.13439", "pdf": "https://arxiv.org/pdf/2505.13439", "abs": "https://arxiv.org/abs/2505.13439", "authors": ["Huawei Lin", "Tong Geng", "Zhaozhuo Xu", "Weijie Zhao"], "title": "VTBench: Evaluating Visual Tokenizers for Autoregressive Image Generation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "24 pages, 13 figures, 3 tables", "summary": "Autoregressive (AR) models have recently shown strong performance in image\ngeneration, where a critical component is the visual tokenizer (VT) that maps\ncontinuous pixel inputs to discrete token sequences. The quality of the VT\nlargely defines the upper bound of AR model performance. However, current\ndiscrete VTs fall significantly behind continuous variational autoencoders\n(VAEs), leading to degraded image reconstructions and poor preservation of\ndetails and text. Existing benchmarks focus on end-to-end generation quality,\nwithout isolating VT performance. To address this gap, we introduce VTBench, a\ncomprehensive benchmark that systematically evaluates VTs across three core\ntasks: Image Reconstruction, Detail Preservation, and Text Preservation, and\ncovers a diverse range of evaluation scenarios. We systematically assess\nstate-of-the-art VTs using a set of metrics to evaluate the quality of\nreconstructed images. Our findings reveal that continuous VAEs produce superior\nvisual representations compared to discrete VTs, particularly in retaining\nspatial structure and semantic detail. In contrast, the degraded\nrepresentations produced by discrete VTs often lead to distorted\nreconstructions, loss of fine-grained textures, and failures in preserving text\nand object integrity. Furthermore, we conduct experiments on GPT-4o image\ngeneration and discuss its potential AR nature, offering new insights into the\nrole of visual tokenization. We release our benchmark and codebase publicly to\nsupport further research and call on the community to develop strong,\ngeneral-purpose open-source VTs."}
{"id": "2505.13448", "pdf": "https://arxiv.org/pdf/2505.13448", "abs": "https://arxiv.org/abs/2505.13448", "authors": ["Vinay Samuel", "Harshita Diddee", "Yiming Zhang", "Daphne Ippolito"], "title": "CIE: Controlling Language Model Text Generations Using Continuous Signals", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, 3 figures", "summary": "Aligning language models with user intent is becoming increasingly relevant\nto enhance user experience. This calls for designing methods that can allow\nusers to control the properties of the language that LMs generate. For example,\ncontrolling the length of the generation, the complexity of the language that\ngets chosen, the sentiment, tone, etc. Most existing work attempts to integrate\nusers' control by conditioning LM generations on natural language prompts or\ndiscrete control signals, which are often brittle and hard to scale. In this\nwork, we are interested in \\textit{continuous} control signals, ones that exist\nalong a spectrum that can't easily be captured in a natural language prompt or\nvia existing techniques in conditional generation. Through a case study in\ncontrolling the precise response-length of generations produced by LMs, we\ndemonstrate how after fine-tuning, behaviors of language models can be\ncontrolled via continuous signals -- as vectors that are interpolated between a\n\"low\" and a \"high\" token embedding. Our method more reliably exerts\nresponse-length control than in-context learning methods or fine-tuning methods\nthat represent the control signal as a discrete signal. Our full open-sourced\ncode and datasets are available at https://github.com/vsamuel2003/CIE."}
