{"id": "2505.05541", "pdf": "https://arxiv.org/pdf/2505.05541", "abs": "https://arxiv.org/abs/2505.05541", "authors": ["Markov Grey", "Charbel-RaphaÃ«l Segerie"], "title": "Safety by Measurement: A Systematic Literature Review of AI Safety Evaluation Methods", "categories": ["cs.AI"], "comment": null, "summary": "As frontier AI systems advance toward transformative capabilities, we need a\nparallel transformation in how we measure and evaluate these systems to ensure\nsafety and inform governance. While benchmarks have been the primary method for\nestimating model capabilities, they often fail to establish true upper bounds\nor predict deployment behavior. This literature review consolidates the rapidly\nevolving field of AI safety evaluations, proposing a systematic taxonomy around\nthree dimensions: what properties we measure, how we measure them, and how\nthese measurements integrate into frameworks. We show how evaluations go beyond\nbenchmarks by measuring what models can do when pushed to the limit\n(capabilities), the behavioral tendencies exhibited by default (propensities),\nand whether our safety measures remain effective even when faced with\nsubversive adversarial AI (control). These properties are measured through\nbehavioral techniques like scaffolding, red teaming and supervised fine-tuning,\nalongside internal techniques such as representation analysis and mechanistic\ninterpretability. We provide deeper explanations of some safety-critical\ncapabilities like cybersecurity exploitation, deception, autonomous\nreplication, and situational awareness, alongside concerning propensities like\npower-seeking and scheming. The review explores how these evaluation methods\nintegrate into governance frameworks to translate results into concrete\ndevelopment decisions. We also highlight challenges to safety evaluations -\nproving absence of capabilities, potential model sandbagging, and incentives\nfor \"safetywashing\" - while identifying promising research directions. By\nsynthesizing scattered resources, this literature review aims to provide a\ncentral reference point for understanding AI safety evaluations."}
{"id": "2505.05602", "pdf": "https://arxiv.org/pdf/2505.05602", "abs": "https://arxiv.org/abs/2505.05602", "authors": ["Lennart Luettgau", "Harry Coppock", "Magda Dubois", "Christopher Summerfield", "Cozmin Ududec"], "title": "HiBayES: A Hierarchical Bayesian Modeling Framework for AI Evaluation Statistics", "categories": ["cs.AI", "stat.AP"], "comment": "23 pages, 9 figures", "summary": "As Large Language Models (LLMs) and other AI systems evolve, robustly\nestimating their capabilities from inherently stochastic outputs while\nsystematically quantifying uncertainty in these estimates becomes increasingly\nimportant. Further, advanced AI evaluations often have a nested hierarchical\nstructure, exhibit high levels of complexity, and come with high costs in\ntesting the most advanced AI systems. To address these challenges, we introduce\nHiBayES, a generalizable Hierarchical Bayesian modeling framework for AI\nEvaluation Statistics. HiBayES supports robust inferences in classical\nquestion-answer benchmarks and advanced agentic evaluations, particularly in\nlow-data scenarios (e.g., < 20 data points per evaluation). Built on\nGeneralized Linear Models (GLMs), Bayesian data analysis, and formal model\ncomparison, HiBayES provides principled uncertainty quantification and robust\nparameter estimation. This paper offers a comprehensive introduction to\nHiBayES, including illustrative examples, comparisons to conventional\nstatistical methods, and practical guidance for implementing multilevel\nBayesian GLMs. Additionally, we provide a HiBayES software package [4] (Beta\nversion) for out-of-the-box implementation."}
{"id": "2505.05612", "pdf": "https://arxiv.org/pdf/2505.05612", "abs": "https://arxiv.org/abs/2505.05612", "authors": ["Qing Wang", "Yining Pan", "Minghao Zhou", "Zijia Tang", "Yanfei Wang", "Guangyu Wang", "Qianqian Song"], "title": "scDrugMap: Benchmarking Large Foundation Models for Drug Response Prediction", "categories": ["cs.AI", "cs.LG", "q-bio.QM"], "comment": "14 pages, 7 figures", "summary": "Drug resistance presents a major challenge in cancer therapy. Single cell\nprofiling offers insights into cellular heterogeneity, yet the application of\nlarge-scale foundation models for predicting drug response in single cell data\nremains underexplored. To address this, we developed scDrugMap, an integrated\nframework featuring both a Python command-line interface and a web server for\ndrug response prediction. scDrugMap evaluates a wide range of foundation\nmodels, including eight single-cell models and two large language models, using\na curated dataset of over 326,000 cells in the primary collection and 18,800\ncells in the validation set, spanning 36 datasets and diverse tissue and cancer\ntypes. We benchmarked model performance under pooled-data and cross-data\nevaluation settings, employing both layer freezing and Low-Rank Adaptation\n(LoRA) fine-tuning strategies. In the pooled-data scenario, scFoundation\nachieved the best performance, with mean F1 scores of 0.971 (layer freezing)\nand 0.947 (fine-tuning), outperforming the lowest-performing model by over 50%.\nIn the cross-data setting, UCE excelled post fine-tuning (mean F1: 0.774),\nwhile scGPT led in zero-shot learning (mean F1: 0.858). Overall, scDrugMap\nprovides the first large-scale benchmark of foundation models for drug response\nprediction in single-cell data and serves as a user-friendly, flexible platform\nfor advancing drug discovery and translational research."}
{"id": "2505.05616", "pdf": "https://arxiv.org/pdf/2505.05616", "abs": "https://arxiv.org/abs/2505.05616", "authors": ["Lorenzo Di Fruscia", "Jana Marie Weber"], "title": "Leveraging Large Language Models for enzymatic reaction prediction and characterization", "categories": ["cs.AI", "cs.LG", "q-bio.BM"], "comment": null, "summary": "Predicting enzymatic reactions is crucial for applications in biocatalysis,\nmetabolic engineering, and drug discovery, yet it remains a complex and\nresource-intensive task. Large Language Models (LLMs) have recently\ndemonstrated remarkable success in various scientific domains, e.g., through\ntheir ability to generalize knowledge, reason over complex structures, and\nleverage in-context learning strategies. In this study, we systematically\nevaluate the capability of LLMs, particularly the Llama-3.1 family (8B and\n70B), across three core biochemical tasks: Enzyme Commission number prediction,\nforward synthesis, and retrosynthesis. We compare single-task and multitask\nlearning strategies, employing parameter-efficient fine-tuning via LoRA\nadapters. Additionally, we assess performance across different data regimes to\nexplore their adaptability in low-data settings. Our results demonstrate that\nfine-tuned LLMs capture biochemical knowledge, with multitask learning\nenhancing forward- and retrosynthesis predictions by leveraging shared\nenzymatic information. We also identify key limitations, for example challenges\nin hierarchical EC classification schemes, highlighting areas for further\nimprovement in LLM-driven biochemical modeling."}
{"id": "2505.05583", "pdf": "https://arxiv.org/pdf/2505.05583", "abs": "https://arxiv.org/abs/2505.05583", "authors": ["Qianbo Zang", "Christophe Zgrzendek", "Igor Tchappi", "Afshin Khadangi", "Johannes Sedlmeir"], "title": "KG-HTC: Integrating Knowledge Graphs into LLMs for Effective Zero-shot Hierarchical Text Classification", "categories": ["cs.CL"], "comment": null, "summary": "Hierarchical Text Classification (HTC) involves assigning documents to labels\norganized within a taxonomy. Most previous research on HTC has focused on\nsupervised methods. However, in real-world scenarios, employing supervised HTC\ncan be challenging due to a lack of annotated data. Moreover, HTC often faces\nissues with large label spaces and long-tail distributions. In this work, we\npresent Knowledge Graphs for zero-shot Hierarchical Text Classification\n(KG-HTC), which aims to address these challenges of HTC in applications by\nintegrating knowledge graphs with Large Language Models (LLMs) to provide\nstructured semantic context during classification. Our method retrieves\nrelevant subgraphs from knowledge graphs related to the input text using a\nRetrieval-Augmented Generation (RAG) approach. Our KG-HTC can enhance LLMs to\nunderstand label semantics at various hierarchy levels. We evaluate KG-HTC on\nthree open-source HTC datasets: WoS, DBpedia, and Amazon. Our experimental\nresults show that KG-HTC significantly outperforms three baselines in the\nstrict zero-shot setting, particularly achieving substantial improvements at\ndeeper levels of the hierarchy. This evaluation demonstrates the effectiveness\nof incorporating structured knowledge into LLMs to address HTC's challenges in\nlarge label spaces and long-tailed label distributions. Our code is available\nat: https://github.com/QianboZang/KG-HTC."}
{"id": "2505.05487", "pdf": "https://arxiv.org/pdf/2505.05487", "abs": "https://arxiv.org/abs/2505.05487", "authors": ["Shrinivas Pundlik", "Seonggyu Choe", "Patrick Baker", "Chen-Yuan Lee", "Naser Al-Madi", "Alex R. Bowers", "Gang Luo"], "title": "Data extraction and processing methods to aid the study of driving behaviors at intersections in naturalistic driving", "categories": ["cs.CV", "cs.RO"], "comment": "19 pages, 11 figures", "summary": "Naturalistic driving studies use devices in participants' own vehicles to\nrecord daily driving over many months. Due to diverse and extensive amounts of\ndata recorded, automated processing is necessary. This report describes methods\nto extract and characterize driver head scans at intersections from data\ncollected from an in-car recording system that logged vehicle speed, GPS\nlocation, scene videos, and cabin videos. Custom tools were developed to mark\nthe intersections, synchronize location and video data, and clip the cabin and\nscene videos for +/-100 meters from the intersection location. A\ncustom-developed head pose detection AI model for wide angle head turns was run\non the cabin videos to estimate the driver head pose, from which head scans >20\ndeg were computed in the horizontal direction. The scene videos were processed\nusing a YOLO object detection model to detect traffic lights, stop signs,\npedestrians, and other vehicles on the road. Turning maneuvers were\nindependently detected using vehicle self-motion patterns. Stop lines on the\nroad surface were detected using changing intensity patterns over time as the\nvehicle moved. The information obtained from processing the scene videos, along\nwith the speed data was used in a rule-based algorithm to infer the\nintersection type, maneuver, and bounds. We processed 190 intersections from 3\nvehicles driven in cities and suburban areas from Massachusetts and California.\nThe automated video processing algorithm correctly detected intersection\nsignage and maneuvers in 100% and 94% of instances, respectively. The median\n[IQR] error in detecting vehicle entry into the intersection was 1.1[0.4-4.9]\nmeters and 0.2[0.1-0.54] seconds. The median overlap between ground truth and\nestimated intersection bounds was 0.88[0.82-0.93]."}
{"id": "2505.05684", "pdf": "https://arxiv.org/pdf/2505.05684", "abs": "https://arxiv.org/abs/2505.05684", "authors": ["Han Wu", "Jie Yin"], "title": "Prompted Meta-Learning for Few-shot Knowledge Graph Completion", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Few-shot knowledge graph completion (KGC) has obtained significant attention\ndue to its practical applications in real-world scenarios, where new knowledge\noften emerges with limited available data. While most existing methods for\nfew-shot KGC have predominantly focused on leveraging relational information,\nrich semantics inherent in KGs have been largely overlooked. To address this\ngap, we propose a novel prompted meta-learning (PromptMeta) framework that\nseamlessly integrates meta-semantics with relational information for few-shot\nKGC. PrompMeta has two key innovations: (1) a meta-semantic prompt pool that\ncaptures and consolidates high-level meta-semantics, enabling effective\nknowledge transfer and adaptation to rare and newly emerging relations. (2) a\nlearnable fusion prompt that dynamically combines meta-semantic information\nwith task-specific relational information tailored to different few-shot tasks.\nBoth components are optimized together with model parameters within a\nmeta-learning framework. Extensive experiments on two benchmark datasets\ndemonstrate the effectiveness of our approach."}
{"id": "2505.05648", "pdf": "https://arxiv.org/pdf/2505.05648", "abs": "https://arxiv.org/abs/2505.05648", "authors": ["Abdelrahman Abouelenin", "Mohamed Abdelrehim", "Raffy Fahim", "Amr Hendy", "Mohamed Afify"], "title": "Privacy-Preserving Transformers: SwiftKey's Differential Privacy Implementation", "categories": ["cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "In this paper we train a transformer using differential privacy (DP) for\nlanguage modeling in SwiftKey. We run multiple experiments to balance the\ntrade-off between the model size, run-time speed and accuracy. We show that we\nget small and consistent gains in the next-word-prediction and accuracy with\ngraceful increase in memory and speed compared to the production GRU. This is\nobtained by scaling down a GPT2 architecture to fit the required size and a two\nstage training process that builds a seed model on general data and DP\nfinetunes it on typing data. The transformer is integrated using ONNX offering\nboth flexibility and efficiency."}
{"id": "2505.05488", "pdf": "https://arxiv.org/pdf/2505.05488", "abs": "https://arxiv.org/abs/2505.05488", "authors": ["Yunfan Lu", "Xiaogang Xu", "Pengteng Li", "Yusheng Wang", "Yi Cui", "Huizai Yao", "Hui Xiong"], "title": "From Events to Enhancement: A Survey on Event-Based Imaging Technologies", "categories": ["cs.CV"], "comment": null, "summary": "Event cameras offering high dynamic range and low latency have emerged as\ndisruptive technologies in imaging. Despite growing research on leveraging\nthese benefits for different imaging tasks, a comprehensive study of recently\nadvances and challenges are still lacking. This limits the broader\nunderstanding of how to utilize events in universal imaging applications. In\nthis survey, we first introduce a physical model and the characteristics of\ndifferent event sensors as the foundation. Following this, we highlight the\nadvancement and interaction of image/video enhancement tasks with events.\nAdditionally, we explore advanced tasks, which capture richer light information\nwith events, \\eg~light field estimation, multi-view generation, and\nphotometric. Finally, we discuss new challenges and open questions offering a\nperspective for this rapidly evolving field. More continuously updated\nresources are at this link: https://github.com/yunfanLu/Awesome-Event-Imaging"}
{"id": "2505.05701", "pdf": "https://arxiv.org/pdf/2505.05701", "abs": "https://arxiv.org/abs/2505.05701", "authors": ["Jongchan Park", "Mingyu Park", "Donghwan Lee"], "title": "Pretraining a Shared Q-Network for Data-Efficient Offline Reinforcement Learning", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Offline reinforcement learning (RL) aims to learn a policy from a static\ndataset without further interactions with the environment. Collecting\nsufficiently large datasets for offline RL is exhausting since this data\ncollection requires colossus interactions with environments and becomes tricky\nwhen the interaction with the environment is restricted. Hence, how an agent\nlearns the best policy with a minimal static dataset is a crucial issue in\noffline RL, similar to the sample efficiency problem in online RL. In this\npaper, we propose a simple yet effective plug-and-play pretraining method to\ninitialize a feature of a $Q$-network to enhance data efficiency in offline RL.\nSpecifically, we introduce a shared $Q$-network structure that outputs\npredictions of the next state and $Q$-value. We pretrain the shared $Q$-network\nthrough a supervised regression task that predicts a next state and trains the\nshared $Q$-network using diverse offline RL methods. Through extensive\nexperiments, we empirically demonstrate that our method enhances the\nperformance of existing popular offline RL methods on the D4RL, Robomimic and\nV-D4RL benchmarks. Furthermore, we show that our method significantly boosts\ndata-efficient offline RL across various data qualities and data distributions\ntrough D4RL and ExoRL benchmarks. Notably, our method adapted with only 10% of\nthe dataset outperforms standard algorithms even with full datasets."}
{"id": "2505.05687", "pdf": "https://arxiv.org/pdf/2505.05687", "abs": "https://arxiv.org/abs/2505.05687", "authors": ["Cindy Kim", "Daniela Puchall", "Jiangyi Liang", "Jiwon Kim"], "title": "Exploration of COVID-19 Discourse on Twitter: American Politician Edition", "categories": ["cs.CL"], "comment": null, "summary": "The advent of the COVID-19 pandemic has undoubtedly affected the political\nscene worldwide and the introduction of new terminology and public opinions\nregarding the virus has further polarized partisan stances. Using a collection\nof tweets gathered from leading American political figures online (Republican\nand Democratic), we explored the partisan differences in approach, response,\nand attitude towards handling the international crisis. Implementation of the\nbag-of-words, bigram, and TF-IDF models was used to identify and analyze\nkeywords, topics, and overall sentiments from each party. Results suggest that\nDemocrats are more concerned with the casualties of the pandemic, and give more\nmedical precautions and recommendations to the public whereas Republicans are\nmore invested in political responsibilities such as keeping the public updated\nthrough media and carefully watching the progress of the virus. We propose a\nsystematic approach to predict and distinguish a tweet's political stance (left\nor right leaning) based on its COVID-19 related terms using different\nclassification algorithms on different language models."}
{"id": "2505.05491", "pdf": "https://arxiv.org/pdf/2505.05491", "abs": "https://arxiv.org/abs/2505.05491", "authors": ["TianYi Yu"], "title": "MDDFNet: Mamba-based Dynamic Dual Fusion Network for Traffic Sign Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The Detection of small objects, especially traffic signs, is a critical\nsub-task in object detection and autonomous driving. Despite signficant\nprogress in previous research, two main challenges remain. First, the issue of\nfeature extraction being too singular. Second, the detection process struggles\nto efectively handle objects of varying sizes or scales. These problems are\nalso prevalent in general object detection tasks. To address these challenges,\nwe propose a novel object detection network, Mamba-based Dynamic Dual Fusion\nNetwork (MDDFNet), for traffic sign detection. The network integrates a dynamic\ndual fusion module and a Mamba-based backbone to simultaneously tackle the\naforementioned issues. Specifically, the dynamic dual fusion module utilizes\nmultiple branches to consolidate various spatial and semantic information, thus\nenhancing feature diversity. The Mamba-based backbone leverages global feature\nfusion and local feature interaction, combining features in an adaptive manner\nto generate unique classification characteristics. Extensive experiments\nconducted on the TT100K (Tsinghua-Tencent 100K) datasets demonstrate that\nMDDFNet outperforms other state-of-the-art detectors, maintaining real-time\nprocessing capabilities of single-stage models while achieving superior\nperformance. This confirms the efectiveness of MDDFNet in detecting small\ntraffic signs."}
{"id": "2505.05758", "pdf": "https://arxiv.org/pdf/2505.05758", "abs": "https://arxiv.org/abs/2505.05758", "authors": ["Azim Ospanov", "Roozbeh Yousefzadeh"], "title": "APOLLO: Automated LLM and Lean Collaboration for Advanced Formal Reasoning", "categories": ["cs.AI", "cs.LO"], "comment": null, "summary": "Formal reasoning and automated theorem proving constitute a challenging\nsubfield of machine learning, in which machines are tasked with proving\nmathematical theorems using formal languages like Lean. A formal verification\nsystem can check whether a formal proof is correct or not almost\ninstantaneously, but generating a completely correct formal proof with large\nlanguage models (LLMs) remains a formidable task. The usual approach in the\nliterature is to prompt the LLM many times (up to several thousands) until one\nof the generated proofs passes the verification system. In this work, we\npresent APOLLO (Automated PrOof repair via LLM and Lean cOllaboration), a\nmodular, model-agnostic pipeline that combines the strengths of the Lean\ncompiler with an LLM's reasoning abilities to achieve better proof-generation\nresults at a low sampling budget. Apollo directs a fully automated process in\nwhich the LLM generates proofs for theorems, a set of agents analyze the\nproofs, fix the syntax errors, identify the mistakes in the proofs using Lean,\nisolate failing sub-lemmas, utilize automated solvers, and invoke an LLM on\neach remaining goal with a low top-K budget. The repaired sub-proofs are\nrecombined and reverified, iterating up to a user-controlled maximum number of\nattempts. On the miniF2F benchmark, we establish a new state-of-the-art\naccuracy of 75.0% among 7B-parameter models while keeping the sampling budget\nbelow one thousand. Moreover, Apollo raises the state-of-the-art accuracy for\nGoedel-Prover-SFT to 65.6% while cutting sample complexity from 25,600 to a few\nhundred. General-purpose models (o3-mini, o4-mini) jump from 3-7% to over 40%\naccuracy. Our results demonstrate that targeted, compiler-guided repair of LLM\noutputs yields dramatic gains in both efficiency and correctness, suggesting a\ngeneral paradigm for scalable automated theorem proving."}
{"id": "2505.05704", "pdf": "https://arxiv.org/pdf/2505.05704", "abs": "https://arxiv.org/abs/2505.05704", "authors": ["Julia Shuieh", "Prasann Singhal", "Apaar Shanker", "John Heyer", "George Pu", "Samuel Denton"], "title": "Assessing Robustness to Spurious Correlations in Post-Training Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "ICLR '25 Workshop on Spurious Correlation and Shortcut Learning", "summary": "Supervised and preference-based fine-tuning techniques have become popular\nfor aligning large language models (LLMs) with user intent and correctness\ncriteria. However, real-world training data often exhibits spurious\ncorrelations -- arising from biases, dataset artifacts, or other \"shortcut\"\nfeatures -- that can compromise a model's performance or generalization. In\nthis paper, we systematically evaluate three post-training algorithms --\nSupervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and KTO\n(Kahneman-Tversky Optimization) -- across a diverse set of synthetic tasks and\nspuriousness conditions. Our tasks span mathematical reasoning, constrained\ninstruction-following, and document-grounded question answering. We vary the\ndegree of spurious correlation (10% vs. 90%) and investigate two forms of\nartifacts: \"Feature Ambiguity\" and \"Distributional Narrowness.\" Our results\nshow that the models often but not always degrade under higher spuriousness.\nThe preference-based methods (DPO/KTO) can demonstrate relative robustness in\nmathematical reasoning tasks. By contrast, SFT maintains stronger performance\nin complex, context-intensive tasks. These findings highlight that no single\npost-training strategy universally outperforms in all scenarios; the best\nchoice depends on the type of target task and the nature of spurious\ncorrelations."}
{"id": "2505.05492", "pdf": "https://arxiv.org/pdf/2505.05492", "abs": "https://arxiv.org/abs/2505.05492", "authors": ["Ignacy StÄpka", "Lukasz Sztukiewicz", "MichaÅ WiliÅski", "Jerzy Stefanowski"], "title": "DetoxAI: a Python Toolkit for Debiasing Deep Learning Models in Computer Vision", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "While machine learning fairness has made significant progress in recent\nyears, most existing solutions focus on tabular data and are poorly suited for\nvision-based classification tasks, which rely heavily on deep learning. To\nbridge this gap, we introduce DetoxAI, an open-source Python library for\nimproving fairness in deep learning vision classifiers through post-hoc\ndebiasing. DetoxAI implements state-of-the-art debiasing algorithms, fairness\nmetrics, and visualization tools. It supports debiasing via interventions in\ninternal representations and includes attribution-based visualization tools and\nquantitative algorithmic fairness metrics to show how bias is mitigated. This\npaper presents the motivation, design, and use cases of DetoxAI, demonstrating\nits tangible value to engineers and researchers."}
{"id": "2505.05880", "pdf": "https://arxiv.org/pdf/2505.05880", "abs": "https://arxiv.org/abs/2505.05880", "authors": ["Bettina Fazzinga", "Sergio Flesca", "Filippo Furfaro", "Luigi Pontieri", "Francesco Scala"], "title": "Combining Abstract Argumentation and Machine Learning for Efficiently Analyzing Low-Level Process Event Streams", "categories": ["cs.AI"], "comment": null, "summary": "Monitoring and analyzing process traces is a critical task for modern\ncompanies and organizations. In scenarios where there is a gap between trace\nevents and reference business activities, this entails an interpretation\nproblem, amounting to translating each event of any ongoing trace into the\ncorresponding step of the activity instance. Building on a recent approach that\nframes the interpretation problem as an acceptance problem within an Abstract\nArgumentation Framework (AAF), one can elegantly analyze plausible event\ninterpretations (possibly in an aggregated form), as well as offer explanations\nfor those that conflict with prior process knowledge. Since, in settings where\nevent-to-activity mapping is highly uncertain (or simply under-specified) this\nreasoning-based approach may yield lowly-informative results and heavy\ncomputation, one can think of discovering a sequencetagging model, trained to\nsuggest highly-probable candidate event interpretations in a context-aware way.\nHowever, training such a model optimally may require using a large amount of\nmanually-annotated example traces. Considering the urgent need of developing\nGreen AI solutions enabling environmental and societal sustainability (with\nreduced labor/computational costs and carbon footprint), we propose a\ndata/computation-efficient neuro-symbolic approach to the problem, where the\ncandidate interpretations returned by the example-driven sequence tagger is\nrefined by the AAF-based reasoner. This allows us to also leverage prior\nknowledge to compensate for the scarcity of example data, as confirmed by\nexperimental results; clearly, this property is particularly useful in settings\nwhere data annotation and model optimization costs are subject to stringent\nconstraints."}
{"id": "2505.05714", "pdf": "https://arxiv.org/pdf/2505.05714", "abs": "https://arxiv.org/abs/2505.05714", "authors": ["Jinze Lv", "Jian Chen", "Zi Long", "Xianghua Fu", "Yin Chen"], "title": "TopicVD: A Topic-Based Dataset of Video-Guided Multimodal Machine Translation for Documentaries", "categories": ["cs.CL"], "comment": "NLDB 2025", "summary": "Most existing multimodal machine translation (MMT) datasets are predominantly\ncomposed of static images or short video clips, lacking extensive video data\nacross diverse domains and topics. As a result, they fail to meet the demands\nof real-world MMT tasks, such as documentary translation. In this study, we\ndeveloped TopicVD, a topic-based dataset for video-supported multimodal machine\ntranslation of documentaries, aiming to advance research in this field. We\ncollected video-subtitle pairs from documentaries and categorized them into\neight topics, such as economy and nature, to facilitate research on domain\nadaptation in video-guided MMT. Additionally, we preserved their contextual\ninformation to support research on leveraging the global context of\ndocumentaries in video-guided MMT. To better capture the shared semantics\nbetween text and video, we propose an MMT model based on a cross-modal\nbidirectional attention module. Extensive experiments on the TopicVD dataset\ndemonstrate that visual information consistently improves the performance of\nthe NMT model in documentary translation. However, the MMT model's performance\nsignificantly declines in out-of-domain scenarios, highlighting the need for\neffective domain adaptation methods. Additionally, experiments demonstrate that\nglobal context can effectively improve translation performance. % Dataset and\nour implementations are available at https://github.com/JinzeLv/TopicVD"}
{"id": "2505.05495", "pdf": "https://arxiv.org/pdf/2505.05495", "abs": "https://arxiv.org/abs/2505.05495", "authors": ["Siyuan Zhou", "Yilun Du", "Yuncong Yang", "Lei Han", "Peihao Chen", "Dit-Yan Yeung", "Chuang Gan"], "title": "Learning 3D Persistent Embodied World Models", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "The ability to simulate the effects of future actions on the world is a\ncrucial ability of intelligent embodied agents, enabling agents to anticipate\nthe effects of their actions and make plans accordingly. While a large body of\nexisting work has explored how to construct such world models using video\nmodels, they are often myopic in nature, without any memory of a scene not\ncaptured by currently observed images, preventing agents from making consistent\nlong-horizon plans in complex environments where many parts of the scene are\npartially observed. We introduce a new persistent embodied world model with an\nexplicit memory of previously generated content, enabling much more consistent\nlong-horizon simulation. During generation time, our video diffusion model\npredicts RGB-D video of the future observations of the agent. This generation\nis then aggregated into a persistent 3D map of the environment. By conditioning\nthe video model on this 3D spatial map, we illustrate how this enables video\nworld models to faithfully simulate both seen and unseen parts of the world.\nFinally, we illustrate the efficacy of such a world model in downstream\nembodied applications, enabling effective planning and policy learning."}
{"id": "2505.05976", "pdf": "https://arxiv.org/pdf/2505.05976", "abs": "https://arxiv.org/abs/2505.05976", "authors": ["Chico Sundermann", "Stefan Vill", "Elias Kuiter", "Sebastian Krieter", "Thomas ThÃ¼m", "Matthias Tichy"], "title": "Pseudo-Boolean d-DNNF Compilation for Expressive Feature Modeling Constructs", "categories": ["cs.AI", "cs.LO", "cs.SE"], "comment": null, "summary": "Configurable systems typically consist of reusable assets that have\ndependencies between each other. To specify such dependencies, feature models\nare commonly used. As feature models in practice are often complex, automated\nreasoning is typically employed to analyze the dependencies. Here, the de facto\nstandard is translating the feature model to conjunctive normal form (CNF) to\nenable employing off-the-shelf tools, such as SAT or #SAT solvers. However,\nmodern feature-modeling dialects often contain constructs, such as cardinality\nconstraints, that are ill-suited for conversion to CNF. This mismatch between\nthe input of reasoning engines and the available feature-modeling dialects\nlimits the applicability of the more expressive constructs. In this work, we\nshorten this gap between expressive constructs and scalable automated\nreasoning. Our contribution is twofold: First, we provide a pseudo-Boolean\nencoding for feature models, which facilitates smaller representations of\ncommonly employed constructs compared to Boolean encoding. Second, we propose a\nnovel method to compile pseudo-Boolean formulas to Boolean d-DNNF. With the\ncompiled d-DNNFs, we can resort to a plethora of efficient analyses already\nused in feature modeling. Our empirical evaluation shows that our proposal\nsubstantially outperforms the state-of-the-art based on CNF inputs for\nexpressive constructs. For every considered dataset representing different\nfeature models and feature-modeling constructs, the feature models can be\nsignificantly faster translated to pseudo-Boolean than to CNF. Overall,\nderiving d-DNNFs from a feature model with the targeted expressive constraints\ncan be substantially accelerated using our pseudo-Boolean approach.\nFurthermore, our approach is competitive on feature models with only basic\nconstructs."}
{"id": "2505.05755", "pdf": "https://arxiv.org/pdf/2505.05755", "abs": "https://arxiv.org/abs/2505.05755", "authors": ["Dhruvesh Patel", "Aishwarya Sahoo", "Avinash Amballa", "Tahira Naseem", "Tim G. J. Rudner", "Andrew McCallum"], "title": "Insertion Language Models: Sequence Generation with Arbitrary-Position Insertions", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Autoregressive models (ARMs), which predict subsequent tokens one-by-one\n``from left to right,'' have achieved significant success across a wide range\nof sequence generation tasks. However, they struggle to accurately represent\nsequences that require satisfying sophisticated constraints or whose sequential\ndependencies are better addressed by out-of-order generation. Masked Diffusion\nModels (MDMs) address some of these limitations, but the process of unmasking\nmultiple tokens simultaneously in MDMs can introduce incoherences, and MDMs\ncannot handle arbitrary infilling constraints when the number of tokens to be\nfilled in is not known in advance. In this work, we introduce Insertion\nLanguage Models (ILMs), which learn to insert tokens at arbitrary positions in\na sequence -- that is, they select jointly both the position and the vocabulary\nelement to be inserted. By inserting tokens one at a time, ILMs can represent\nstrong dependencies between tokens, and their ability to generate sequences in\narbitrary order allows them to accurately model sequences where token\ndependencies do not follow a left-to-right sequential structure. To train ILMs,\nwe propose a tailored network parameterization and use a simple denoising\nobjective. Our empirical evaluation demonstrates that ILMs outperform both ARMs\nand MDMs on common planning tasks. Furthermore, we show that ILMs outperform\nMDMs and perform on par with ARMs in an unconditional text generation task\nwhile offering greater flexibility than MDMs in arbitrary-length text\ninfilling."}
{"id": "2505.05501", "pdf": "https://arxiv.org/pdf/2505.05501", "abs": "https://arxiv.org/abs/2505.05501", "authors": ["Pu Cao", "Feng Zhou", "Junyi Ji", "Qingye Kong", "Zhixiang Lv", "Mingjian Zhang", "Xuekun Zhao", "Siqi Wu", "Yinghui Lin", "Qing Song", "Lu Yang"], "title": "Preliminary Explorations with GPT-4o(mni) Native Image Generation", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": null, "summary": "Recently, the visual generation ability by GPT-4o(mni) has been unlocked by\nOpenAI. It demonstrates a very remarkable generation capability with excellent\nmultimodal condition understanding and varied task instructions. In this paper,\nwe aim to explore the capabilities of GPT-4o across various tasks. Inspired by\nprevious study, we constructed a task taxonomy along with a carefully curated\nset of test samples to conduct a comprehensive qualitative test. Benefiting\nfrom GPT-4o's powerful multimodal comprehension, its image-generation process\ndemonstrates abilities surpassing those of traditional image-generation tasks.\nThus, regarding the dimensions of model capabilities, we evaluate its\nperformance across six task categories: traditional image generation tasks,\ndiscriminative tasks, knowledge-based generation, commonsense-based generation,\nspatially-aware image generation, and temporally-aware image generation. These\ntasks not only assess the quality and conditional alignment of the model's\noutputs but also probe deeper into GPT-4o's understanding of real-world\nconcepts. Our results reveal that GPT-4o performs impressively well in\ngeneral-purpose synthesis tasks, showing strong capabilities in text-to-image\ngeneration, visual stylization, and low-level image processing. However,\nsignificant limitations remain in its ability to perform precise spatial\nreasoning, instruction-grounded generation, and consistent temporal prediction.\nFurthermore, when faced with knowledge-intensive or domain-specific scenarios,\nsuch as scientific illustrations or mathematical plots, the model often\nexhibits hallucinations, factual errors, or structural inconsistencies. These\nfindings suggest that while GPT-4o marks a substantial advancement in unified\nmultimodal generation, there is still a long way to go before it can be\nreliably applied to professional or safety-critical domains."}
{"id": "2505.06020", "pdf": "https://arxiv.org/pdf/2505.06020", "abs": "https://arxiv.org/abs/2505.06020", "authors": ["Shuai Wang", "Ivona Najdenkoska", "Hongyi Zhu", "Stevan Rudinac", "Monika Kackovic", "Nachoem Wijnberg", "Marcel Worring"], "title": "ArtRAG: Retrieval-Augmented Generation with Structured Context for Visual Art Understanding", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Understanding visual art requires reasoning across multiple perspectives --\ncultural, historical, and stylistic -- beyond mere object recognition. While\nrecent multimodal large language models (MLLMs) perform well on general image\ncaptioning, they often fail to capture the nuanced interpretations that fine\nart demands. We propose ArtRAG, a novel, training-free framework that combines\nstructured knowledge with retrieval-augmented generation (RAG) for\nmulti-perspective artwork explanation. ArtRAG automatically constructs an Art\nContext Knowledge Graph (ACKG) from domain-specific textual sources, organizing\nentities such as artists, movements, themes, and historical events into a rich,\ninterpretable graph. At inference time, a multi-granular structured retriever\nselects semantically and topologically relevant subgraphs to guide generation.\nThis enables MLLMs to produce contextually grounded, culturally informed art\ndescriptions. Experiments on the SemArt and Artpedia datasets show that ArtRAG\noutperforms several heavily trained baselines. Human evaluations further\nconfirm that ArtRAG generates coherent, insightful, and culturally enriched\ninterpretations."}
{"id": "2505.05772", "pdf": "https://arxiv.org/pdf/2505.05772", "abs": "https://arxiv.org/abs/2505.05772", "authors": ["Zehao Fan", "Garrett Gagnon", "Zhenyu Liu", "Liu Liu"], "title": "Sparse Attention Remapping with Clustering for Efficient LLM Decoding on PIM", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Transformer-based models are the foundation of modern machine learning, but\ntheir execution, particularly during autoregressive decoding in large language\nmodels (LLMs), places significant pressure on memory systems due to frequent\nmemory accesses and growing key-value (KV) caches. This creates a bottleneck in\nmemory bandwidth, especially as context lengths increase. Processing-in-memory\n(PIM) architectures are a promising solution, offering high internal bandwidth\nand compute parallelism near memory. However, current PIM designs are primarily\noptimized for dense attention and struggle with the dynamic, irregular access\npatterns introduced by modern KV cache sparsity techniques. Consequently, they\nsuffer from workload imbalance, reducing throughput and resource utilization.\nIn this work, we propose STARC, a novel sparsity-optimized data mapping scheme\ntailored specifically for efficient LLM decoding on PIM architectures. STARC\nclusters KV pairs by semantic similarity and maps them to contiguous memory\nregions aligned with PIM bank structures. During decoding, queries retrieve\nrelevant tokens at cluster granularity by matching against precomputed\ncentroids, enabling selective attention and parallel processing without\nfrequent reclustering or data movement overhead. Experiments on the HBM-PIM\nsystem show that, compared to common token-wise sparsity methods, STARC reduces\nattention-layer latency by 19%--31% and energy consumption by 19%--27%. Under a\nKV cache budget of 1024, it achieves up to 54%--74% latency reduction and\n45%--67% energy reduction compared to full KV cache retrieval. Meanwhile, STARC\nmaintains model accuracy comparable to state-of-the-art sparse attention\nmethods, demonstrating its effectiveness in enabling efficient and\nhardware-friendly long-context LLM inference on PIM architectures."}
{"id": "2505.05505", "pdf": "https://arxiv.org/pdf/2505.05505", "abs": "https://arxiv.org/abs/2505.05505", "authors": ["Yiming Qin", "Zhu Xu", "Yang Liu"], "title": "Apply Hierarchical-Chain-of-Generation to Complex Attributes Text-to-3D Generation", "categories": ["cs.CV", "eess.IV"], "comment": "Project page here:\n  https://hierarchical-chain-of-generation.github.io/", "summary": "Recent text-to-3D models can render high-quality assets, yet they still\nstumble on objects with complex attributes. The key obstacles are: (1) existing\ntext-to-3D approaches typically lift text-to-image models to extract semantics\nvia text encoders, while the text encoder exhibits limited comprehension\nability for long descriptions, leading to deviated cross-attention focus,\nsubsequently wrong attribute binding in generated results. (2) Occluded object\nparts demand a disciplined generation order and explicit part disentanglement.\nThough some works introduce manual efforts to alleviate the above issues, their\nquality is unstable and highly reliant on manual information. To tackle above\nproblems, we propose a automated method Hierarchical-Chain-of-Generation\n(HCoG). It leverages a large language model to decompose the long description\ninto blocks representing different object parts, and orders them from inside\nout according to occlusions, forming a hierarchical chain. Within each block we\nfirst coarsely create components, then precisely bind attributes via\ntarget-region localization and corresponding 3D Gaussian kernel optimization.\nBetween blocks, we introduce Gaussian Extension and Label Elimination to\nseamlessly generate new parts by extending new Gaussian kernels, re-assigning\nsemantic labels, and eliminating unnecessary kernels, ensuring that only\nrelevant parts are added without disrupting previously optimized parts.\nExperiments confirm that HCoG yields structurally coherent, attribute-faithful\n3D objects with complex attributes. The code is available at\nhttps://github.com/Wakals/GASCOL ."}
{"id": "2505.06030", "pdf": "https://arxiv.org/pdf/2505.06030", "abs": "https://arxiv.org/abs/2505.06030", "authors": ["Tobias Preintner", "Weixuan Yuan", "Qi Huang", "Adrian KÃ¶nig", "Thomas BÃ¤ck", "Elena Raponi", "Niki van Stein"], "title": "Why Are You Wrong? Counterfactual Explanations for Language Grounding with 3D Objects", "categories": ["cs.AI", "cs.CV", "cs.LG"], "comment": "Accepted at IJCNN 2025", "summary": "Combining natural language and geometric shapes is an emerging research area\nwith multiple applications in robotics and language-assisted design. A crucial\ntask in this domain is object referent identification, which involves selecting\na 3D object given a textual description of the target. Variability in language\ndescriptions and spatial relationships of 3D objects makes this a complex task,\nincreasing the need to better understand the behavior of neural network models\nin this domain. However, limited research has been conducted in this area.\nSpecifically, when a model makes an incorrect prediction despite being provided\nwith a seemingly correct object description, practitioners are left wondering:\n\"Why is the model wrong?\". In this work, we present a method answering this\nquestion by generating counterfactual examples. Our method takes a\nmisclassified sample, which includes two objects and a text description, and\ngenerates an alternative yet similar formulation that would have resulted in a\ncorrect prediction by the model. We have evaluated our approach with data from\nthe ShapeTalk dataset along with three distinct models. Our counterfactual\nexamples maintain the structure of the original description, are semantically\nsimilar and meaningful. They reveal weaknesses in the description, model bias\nand enhance the understanding of the models behavior. Theses insights help\npractitioners to better interact with systems as well as engineers to improve\nmodels."}
{"id": "2505.05815", "pdf": "https://arxiv.org/pdf/2505.05815", "abs": "https://arxiv.org/abs/2505.05815", "authors": ["Machi Shimmei", "Masaki Uto", "Yuichiroh Matsubayashi", "Kentaro Inui", "Aditi Mallavarapu", "Noboru Matsuda"], "title": "Tell Me Who Your Students Are: GPT Can Generate Valid Multiple-Choice Questions When Students' (Mis)Understanding Is Hinted", "categories": ["cs.CL"], "comment": "This is a pre-print version of a paper to appear in AIED2025", "summary": "The primary goal of this study is to develop and evaluate an innovative\nprompting technique, AnaQuest, for generating multiple-choice questions (MCQs)\nusing a pre-trained large language model. In AnaQuest, the choice items are\nsentence-level assertions about complex concepts. The technique integrates\nformative and summative assessments. In the formative phase, students answer\nopen-ended questions for target concepts in free text. For summative\nassessment, AnaQuest analyzes these responses to generate both correct and\nincorrect assertions. To evaluate the validity of the generated MCQs, Item\nResponse Theory (IRT) was applied to compare item characteristics between MCQs\ngenerated by AnaQuest, a baseline ChatGPT prompt, and human-crafted items. An\nempirical study found that expert instructors rated MCQs generated by both AI\nmodels to be as valid as those created by human instructors. However, IRT-based\nanalysis revealed that AnaQuest-generated questions - particularly those with\nincorrect assertions (foils) - more closely resembled human-crafted items in\nterms of difficulty and discrimination than those produced by ChatGPT."}
{"id": "2505.05512", "pdf": "https://arxiv.org/pdf/2505.05512", "abs": "https://arxiv.org/abs/2505.05512", "authors": ["Zhang Zhang", "Qiang Zhang", "Wei Cui", "Shuai Shi", "Yijie Guo", "Gang Han", "Wen Zhao", "Jingkai Sun", "Jiahang Cao", "Jiaxu Wang", "Hao Cheng", "Xiaozhu Ju", "Zhengping Che", "Renjing Xu", "Jian Tang"], "title": "Occupancy World Model for Robots", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Understanding and forecasting the scene evolutions deeply affect the\nexploration and decision of embodied agents. While traditional methods simulate\nscene evolutions through trajectory prediction of potential instances, current\nworks use the occupancy world model as a generative framework for describing\nfine-grained overall scene dynamics. However, existing methods cluster on the\noutdoor structured road scenes, while ignoring the exploration of forecasting\n3D occupancy scene evolutions for robots in indoor scenes. In this work, we\nexplore a new framework for learning the scene evolutions of observed\nfine-grained occupancy and propose an occupancy world model based on the\ncombined spatio-temporal receptive field and guided autoregressive transformer\nto forecast the scene evolutions, called RoboOccWorld. We propose the\nConditional Causal State Attention (CCSA), which utilizes camera poses of next\nstate as conditions to guide the autoregressive transformer to adapt and\nunderstand the indoor robotics scenarios. In order to effectively exploit the\nspatio-temporal cues from historical observations, Hybrid Spatio-Temporal\nAggregation (HSTA) is proposed to obtain the combined spatio-temporal receptive\nfield based on multi-scale spatio-temporal windows. In addition, we restructure\nthe OccWorld-ScanNet benchmark based on local annotations to facilitate the\nevaluation of the indoor 3D occupancy scene evolution prediction task.\nExperimental results demonstrate that our RoboOccWorld outperforms\nstate-of-the-art methods in indoor 3D occupancy scene evolution prediction\ntask. The code will be released soon."}
{"id": "2505.06049", "pdf": "https://arxiv.org/pdf/2505.06049", "abs": "https://arxiv.org/abs/2505.06049", "authors": ["Aleena Siji", "Joscha CÃ¼ppers", "Osman Ali Mian", "Jilles Vreeken"], "title": "Seqret: Mining Rule Sets from Event Sequences", "categories": ["cs.AI"], "comment": null, "summary": "Summarizing event sequences is a key aspect of data mining. Most existing\nmethods neglect conditional dependencies and focus on discovering sequential\npatterns only. In this paper, we study the problem of discovering both\nconditional and unconditional dependencies from event sequence data. We do so\nby discovering rules of the form $X \\rightarrow Y$ where $X$ and $Y$ are\nsequential patterns. Rules like these are simple to understand and provide a\nclear description of the relation between the antecedent and the consequent. To\ndiscover succinct and non-redundant sets of rules we formalize the problem in\nterms of the Minimum Description Length principle. As the search space is\nenormous and does not exhibit helpful structure, we propose the Seqret method\nto discover high-quality rule sets in practice. Through extensive empirical\nevaluation we show that unlike the state of the art, Seqret ably recovers the\nground truth on synthetic datasets and finds useful rules from real datasets."}
{"id": "2505.05864", "pdf": "https://arxiv.org/pdf/2505.05864", "abs": "https://arxiv.org/abs/2505.05864", "authors": ["Junhyeong Lee", "Jong Min Yuk", "Chan-Woo Lee"], "title": "Symbol-based entity marker highlighting for enhanced text mining in materials science with generative AI", "categories": ["cs.CL"], "comment": "29 pages", "summary": "The construction of experimental datasets is essential for expanding the\nscope of data-driven scientific discovery. Recent advances in natural language\nprocessing (NLP) have facilitated automatic extraction of structured data from\nunstructured scientific literature. While existing approaches-multi-step and\ndirect methods-offer valuable capabilities, they also come with limitations\nwhen applied independently. Here, we propose a novel hybrid text-mining\nframework that integrates the advantages of both methods to convert\nunstructured scientific text into structured data. Our approach first\ntransforms raw text into entity-recognized text, and subsequently into\nstructured form. Furthermore, beyond the overall data structuring framework, we\nalso enhance entity recognition performance by introducing an entity marker-a\nsimple yet effective technique that uses symbolic annotations to highlight\ntarget entities. Specifically, our entity marker-based hybrid approach not only\nconsistently outperforms previous entity recognition approaches across three\nbenchmark datasets (MatScholar, SOFC, and SOFC slot NER) but also improve the\nquality of final structured data-yielding up to a 58% improvement in\nentity-level F1 score and up to 83% improvement in relation-level F1 score\ncompared to direct approach."}
{"id": "2505.05513", "pdf": "https://arxiv.org/pdf/2505.05513", "abs": "https://arxiv.org/abs/2505.05513", "authors": ["Muhammad Junaid Asif", "Hamza Khan", "Rabia Tehseen", "Syed Tahir Hussain Rizvi", "Mujtaba Asad", "Shazia Saqib", "Rana Fayyaz Ahmad"], "title": "Exploring Convolutional Neural Networks for Rice Grain Classification: An Explainable AI Approach", "categories": ["cs.CV"], "comment": null, "summary": "Rice is an essential staple food worldwide that is important in promoting\ninternational trade, economic growth, and nutrition. Asian countries such as\nChina, India, Pakistan, Thailand, Vietnam, and Indonesia are notable for their\nsignificant contribution to the cultivation and utilization of rice. These\nnations are also known for cultivating different rice grains, including short\nand long grains. These sizes are further classified as basmati, jasmine, kainat\nsaila, ipsala, arborio, etc., catering to diverse culinary preferences and\ncultural traditions. For both local and international trade, inspecting and\nmaintaining the quality of rice grains to satisfy customers and preserve a\ncountry's reputation is necessary. Manual quality check and classification is\nquite a laborious and time-consuming process. It is also highly prone to\nmistakes. Therefore, an automatic solution must be proposed for the effective\nand efficient classification of different varieties of rice grains. This\nresearch paper presents an automatic framework based on a convolutional neural\nnetwork (CNN) for classifying different varieties of rice grains. We evaluated\nthe proposed model based on performance metrics such as accuracy, recall,\nprecision, and F1-Score. The CNN model underwent rigorous training and\nvalidation, achieving a remarkable accuracy rate and a perfect area under each\nclass's Receiver Operating Characteristic (ROC) curve. The confusion matrix\nanalysis confirmed the model's effectiveness in distinguishing between the\ndifferent rice varieties, indicating minimal misclassifications. Additionally,\nthe integration of explainability techniques such as LIME (Local Interpretable\nModel-agnostic Explanations) and SHAP (SHapley Additive exPlanations) provided\nvaluable insights into the model's decision-making process, revealing how\nspecific features of the rice grains influenced classification outcomes."}
{"id": "2505.06096", "pdf": "https://arxiv.org/pdf/2505.06096", "abs": "https://arxiv.org/abs/2505.06096", "authors": ["Sam Bush", "Matthew DeLorenzo", "Phat Tieu", "Jeyavijayan Rajendran"], "title": "Free and Fair Hardware: A Pathway to Copyright Infringement-Free Verilog Generation using LLMs", "categories": ["cs.AI"], "comment": "Accepted at DAC 2025", "summary": "Limitations in Large Language Model (LLM) capabilities for hardware design\ntasks, such as generating functional Verilog codes, have motivated various\nfine-tuning optimizations utilizing curated hardware datasets from open-source\nrepositories. However, these datasets remain limited in size and contain\nminimal checks on licensing for reuse, resulting in potential copyright\nviolations by fine-tuned LLMs. Therefore, we propose an evaluation benchmark to\nestimate the risk of Verilog-trained LLMs to generate copyright-protected\ncodes. To minimize this risk, we present an open-source Verilog dataset,\nFreeSet, containing over 220k files, along with the automated dataset curation\nframework utilized to provide additional guarantees of fair-use Verilog data.\nWe then execute an LLM fine-tuning framework consisting of continual\npre-training, resulting in a fine-tuned Llama model for Verilog, FreeV. Our\nresults indicate that FreeV demonstrates the smallest risk of\ncopyright-infringement among prior works, with only a 3% violation rate.\nFurthermore, experimental results demonstrate improvements in Verilog\ngeneration functionality over its baseline model, improving VerilogEval pass@10\nrates by over 10%."}
{"id": "2505.05946", "pdf": "https://arxiv.org/pdf/2505.05946", "abs": "https://arxiv.org/abs/2505.05946", "authors": ["Vytenis Å liogeris", "Povilas DaniuÅ¡is", "ArtÅ«ras Nakvosas"], "title": "Elastic Weight Consolidation for Full-Parameter Continual Pre-Training of Gemma2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "8 pages, 4 figures", "summary": "This technical report describes an experiment on autoregressive pre-training\nof Gemma2 2 billion parameter large language model (LLM) with 10\\% on the\nLithuanian language component of CulturaX from the point of view of continual\nlearning. We apply elastic weight consolidation (EWC) to the full set of the\nmodel's parameters and investigate language understanding benchmarks,\nconsisting of Arc, Belebele, Gsm8K, Hellaswag, MMLU, TruthfulQA, and Winogrande\nsets (both in English and Lithuanian versions), and perplexity benchmarks. We\nempirically demonstrate that EWC regularisation allows us not only to mitigate\ncatastrophic forgetting effects but also that it is potentially beneficial for\nlearning of the new task with LLMs."}
{"id": "2505.05517", "pdf": "https://arxiv.org/pdf/2505.05517", "abs": "https://arxiv.org/abs/2505.05517", "authors": ["Hongyi Chen", "Yunchao Yao", "Yufei Ye", "Zhixuan Xu", "Homanga Bharadhwaj", "Jiashun Wang", "Shubham Tulsiani", "Zackory Erickson", "Jeffrey Ichnowski"], "title": "Web2Grasp: Learning Functional Grasps from Web Images of Hand-Object Interactions", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": null, "summary": "Functional grasp is essential for enabling dexterous multi-finger robot hands\nto manipulate objects effectively. However, most prior work either focuses on\npower grasping, which simply involves holding an object still, or relies on\ncostly teleoperated robot demonstrations to teach robots how to grasp each\nobject functionally. Instead, we propose extracting human grasp information\nfrom web images since they depict natural and functional object interactions,\nthereby bypassing the need for curated demonstrations. We reconstruct human\nhand-object interaction (HOI) 3D meshes from RGB images, retarget the human\nhand to multi-finger robot hands, and align the noisy object mesh with its\naccurate 3D shape. We show that these relatively low-quality HOI data from\ninexpensive web sources can effectively train a functional grasping model. To\nfurther expand the grasp dataset for seen and unseen objects, we use the\ninitially-trained grasping policy with web data in the IsaacGym simulator to\ngenerate physically feasible grasps while preserving functionality. We train\nthe grasping model on 10 object categories and evaluate it on 9 unseen objects,\nincluding challenging items such as syringes, pens, spray bottles, and tongs,\nwhich are underrepresented in existing datasets. The model trained on the web\nHOI dataset, achieving a 75.8% success rate on seen objects and 61.8% across\nall objects in simulation, with a 6.7% improvement in success rate and a 1.8x\nincrease in functionality ratings over baselines. Simulator-augmented data\nfurther boosts performance from 61.8% to 83.4%. The sim-to-real transfer to the\nLEAP Hand achieves a 85% success rate. Project website is at:\nhttps://webgrasp.github.io/."}
{"id": "2505.06191", "pdf": "https://arxiv.org/pdf/2505.06191", "abs": "https://arxiv.org/abs/2505.06191", "authors": ["Jiayuan Mao", "Joshua B. Tenenbaum", "Jiajun Wu"], "title": "Neuro-Symbolic Concepts", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.RO"], "comment": "To appear in Communications of the ACM", "summary": "This article presents a concept-centric paradigm for building agents that can\nlearn continually and reason flexibly. The concept-centric agent utilizes a\nvocabulary of neuro-symbolic concepts. These concepts, such as object,\nrelation, and action concepts, are grounded on sensory inputs and actuation\noutputs. They are also compositional, allowing for the creation of novel\nconcepts through their structural combination. To facilitate learning and\nreasoning, the concepts are typed and represented using a combination of\nsymbolic programs and neural network representations. Leveraging such\nneuro-symbolic concepts, the agent can efficiently learn and recombine them to\nsolve various tasks across different domains, ranging from 2D images, videos,\n3D scenes, and robotic manipulation tasks. This concept-centric framework\noffers several advantages, including data efficiency, compositional\ngeneralization, continual learning, and zero-shot transfer."}
{"id": "2505.05947", "pdf": "https://arxiv.org/pdf/2505.05947", "abs": "https://arxiv.org/abs/2505.05947", "authors": ["Bianca Steffes", "Nils Torben Wiedemann", "Alexander Gratz", "Pamela Hochreither", "Jana Elina Meyer", "Katharina Luise Schilke"], "title": "Summarisation of German Judgments in conjunction with a Class-based Evaluation", "categories": ["cs.CL"], "comment": null, "summary": "The automated summarisation of long legal documents can be a great aid for\nlegal experts in their daily work. We automatically create summaries (guiding\nprinciples) of German judgments by fine-tuning a decoder-based large language\nmodel. We enrich the judgments with information about legal entities before the\ntraining. For the evaluation of the created summaries, we define a set of\nevaluation classes which allows us to measure their language, pertinence,\ncompleteness and correctness. Our results show that employing legal entities\nhelps the generative model to find the relevant content, but the quality of the\ncreated summaries is not yet sufficient for a use in practice."}
{"id": "2505.05519", "pdf": "https://arxiv.org/pdf/2505.05519", "abs": "https://arxiv.org/abs/2505.05519", "authors": ["Minkyu Choi", "Yunhao Yang", "Neel P. Bhatt", "Kushagra Gupta", "Sahil Shah", "Aditya Rai", "David Fridovich-Keil", "Ufuk Topcu", "Sandeep P. Chinchali"], "title": "Real-Time Privacy Preservation for Robot Visual Perception", "categories": ["cs.CV"], "comment": null, "summary": "Many robots (e.g., iRobot's Roomba) operate based on visual observations from\nlive video streams, and such observations may inadvertently include\nprivacy-sensitive objects, such as personal identifiers. Existing approaches\nfor preserving privacy rely on deep learning models, differential privacy, or\ncryptography. They lack guarantees for the complete concealment of all\nsensitive objects. Guaranteeing concealment requires post-processing techniques\nand thus is inadequate for real-time video streams. We develop a method for\nprivacy-constrained video streaming, PCVS, that conceals sensitive objects\nwithin real-time video streams. PCVS takes a logical specification constraining\nthe existence of privacy-sensitive objects, e.g., never show faces when a\nperson exists. It uses a detection model to evaluate the existence of these\nobjects in each incoming frame. Then, it blurs out a subset of objects such\nthat the existence of the remaining objects satisfies the specification. We\nthen propose a conformal prediction approach to (i) establish a theoretical\nlower bound on the probability of the existence of these objects in a sequence\nof frames satisfying the specification and (ii) update the bound with the\narrival of each subsequent frame. Quantitative evaluations show that PCVS\nachieves over 95 percent specification satisfaction rate in multiple datasets,\nsignificantly outperforming other methods. The satisfaction rate is\nconsistently above the theoretical bounds across all datasets, indicating that\nthe established bounds hold. Additionally, we deploy PCVS on robots in\nreal-time operation and show that the robots operate normally without being\ncompromised when PCVS conceals objects."}
{"id": "2505.04999", "pdf": "https://arxiv.org/pdf/2505.04999", "abs": "https://arxiv.org/abs/2505.04999", "authors": ["Anthony Liang", "Pavel Czempin", "Matthew Hong", "Yutai Zhou", "Erdem Biyik", "Stephen Tu"], "title": "CLAM: Continuous Latent Action Models for Robot Learning from Unlabeled Demonstrations", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Latent Action Models, Self-supervised Pretraining, Learning from\n  Videos", "summary": "Learning robot policies using imitation learning requires collecting large\namounts of costly action-labeled expert demonstrations, which fundamentally\nlimits the scale of training data. A promising approach to address this\nbottleneck is to harness the abundance of unlabeled observations-e.g., from\nvideo demonstrations-to learn latent action labels in an unsupervised way.\nHowever, we find that existing methods struggle when applied to complex robot\ntasks requiring fine-grained motions. We design continuous latent action models\n(CLAM) which incorporate two key ingredients we find necessary for learning to\nsolve complex continuous control tasks from unlabeled observation data: (a)\nusing continuous latent action labels instead of discrete representations, and\n(b) jointly training an action decoder to ensure that the latent action space\ncan be easily grounded to real actions with relatively few labeled examples.\nImportantly, the labeled examples can be collected from non-optimal play data,\nenabling CLAM to learn performant policies without access to any action-labeled\nexpert data. We demonstrate on continuous control benchmarks in DMControl\n(locomotion) and MetaWorld (manipulation), as well as on a real WidowX robot\narm that CLAM significantly outperforms prior state-of-the-art methods,\nremarkably with a 2-3x improvement in task success rate compared to the best\nbaseline. Videos and code can be found at clamrobot.github.io."}
{"id": "2505.05949", "pdf": "https://arxiv.org/pdf/2505.05949", "abs": "https://arxiv.org/abs/2505.05949", "authors": ["Max Glockner", "Xiang Jiang", "Leonardo F. R. Ribeiro", "Iryna Gurevych", "Markus Dreyer"], "title": "NeoQA: Evidence-based Question Answering with Generated News Events", "categories": ["cs.CL"], "comment": null, "summary": "Evaluating Retrieval-Augmented Generation (RAG) in large language models\n(LLMs) is challenging because benchmarks can quickly become stale. Questions\ninitially requiring retrieval may become answerable from pretraining knowledge\nas newer models incorporate more recent information during pretraining, making\nit difficult to distinguish evidence-based reasoning from recall. We introduce\nNeoQA (News Events for Out-of-training Question Answering), a benchmark\ndesigned to address this issue. To construct NeoQA, we generated timelines and\nknowledge bases of fictional news events and entities along with news articles\nand Q\\&A pairs to prevent LLMs from leveraging pretraining knowledge, ensuring\nthat no prior evidence exists in their training data. We propose our dataset as\na new platform for evaluating evidence-based question answering, as it requires\nLLMs to generate responses exclusively from retrieved evidence and only when\nsufficient evidence is available. NeoQA enables controlled evaluation across\nvarious evidence scenarios, including cases with missing or misleading details.\nOur findings indicate that LLMs struggle to distinguish subtle mismatches\nbetween questions and evidence, and suffer from short-cut reasoning when key\ninformation required to answer a question is missing from the evidence,\nunderscoring key limitations in evidence-based reasoning."}
{"id": "2505.05520", "pdf": "https://arxiv.org/pdf/2505.05520", "abs": "https://arxiv.org/abs/2505.05520", "authors": ["Chengwei Ye", "Huanzhen Zhang", "Yufei Lin", "Kangsheng Wang", "Linuo Xu", "Shuyan Liu"], "title": "GaMNet: A Hybrid Network with Gabor Fusion and NMamba for Efficient 3D Glioma Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Gliomas are aggressive brain tumors that pose serious health risks. Deep\nlearning aids in lesion segmentation, but CNN and Transformer-based models\noften lack context modeling or demand heavy computation, limiting real-time use\non mobile medical devices. We propose GaMNet, integrating the NMamba module for\nglobal modeling and a multi-scale CNN for efficient local feature extraction.\nTo improve interpretability and mimic the human visual system, we apply Gabor\nfilters at multiple scales. Our method achieves high segmentation accuracy with\nfewer parameters and faster computation. Extensive experiments show GaMNet\noutperforms existing methods, notably reducing false positives and negatives,\nwhich enhances the reliability of clinical diagnosis."}
{"id": "2505.05481", "pdf": "https://arxiv.org/pdf/2505.05481", "abs": "https://arxiv.org/abs/2505.05481", "authors": ["Ryan Williams"], "title": "Structure & Quality: Conceptual and Formal Foundations for the Mind-Body Problem", "categories": ["q-bio.NC", "cs.AI"], "comment": null, "summary": "This paper explores the hard problem of consciousness from a different\nperspective. Instead of drawing distinctions between the physical and the\nmental, an exploration of a more foundational relationship is examined: the\nrelationship between structure and quality.\n  Information-theoretic measures are developed to quantify the mutual\ndeterminability between structure and quality, including a novel Q-S space for\nanalyzing fidelity between the two domains. This novel space naturally points\ntoward a five-fold categorization of possible relationships between structural\nand qualitative properties, illustrating each through conceptual and formal\nmodels.\n  The ontological implications of each category are examined, shedding light on\ndebates around functionalism, emergentism, idealism, panpsychism, and neutral\nmonism.\n  This new line of inquiry has established a framework for deriving theoretical\nconstraints on qualitative systems undergoing evolution that is explored in my\ncompanion paper, Qualia & Natural Selection."}
{"id": "2505.05970", "pdf": "https://arxiv.org/pdf/2505.05970", "abs": "https://arxiv.org/abs/2505.05970", "authors": ["Lennart StÃ¶pler", "Rufat Asadli", "Mitja Nikolaus", "Ryan Cotterell", "Alex Warstadt"], "title": "Towards Developmentally Plausible Rewards: Communicative Success as a Learning Signal for Interactive Language Models", "categories": ["cs.CL"], "comment": null, "summary": "We propose a method for training language models in an interactive setting\ninspired by child language acquisition. In our setting, a speaker attempts to\ncommunicate some information to a listener in a single-turn dialogue and\nreceives a reward if communicative success is achieved. Unlike earlier related\nwork using image--caption data for interactive reference games, we\noperationalize communicative success in a more abstract language-only\nquestion--answering setting. First, we present a feasibility study\ndemonstrating that our reward provides an indirect signal about grammaticality.\nSecond, we conduct experiments using reinforcement learning to fine-tune\nlanguage models. We observe that cognitively plausible constraints on the\ncommunication channel lead to interpretable changes in speaker behavior.\nHowever, we do not yet see improvements on linguistic evaluations from our\ntraining regime. We outline potential modifications to the task design and\ntraining configuration that could better position future work to use our\nmethodology to observe the benefits of interaction on language learning in\ncomputational cognitive models."}
{"id": "2505.05528", "pdf": "https://arxiv.org/pdf/2505.05528", "abs": "https://arxiv.org/abs/2505.05528", "authors": ["Hanxun Huang", "Sarah Erfani", "Yige Li", "Xingjun Ma", "James Bailey"], "title": "X-Transfer Attacks: Towards Super Transferable Adversarial Attacks on CLIP", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "ICML 2025", "summary": "As Contrastive Language-Image Pre-training (CLIP) models are increasingly\nadopted for diverse downstream tasks and integrated into large vision-language\nmodels (VLMs), their susceptibility to adversarial perturbations has emerged as\na critical concern. In this work, we introduce \\textbf{X-Transfer}, a novel\nattack method that exposes a universal adversarial vulnerability in CLIP.\nX-Transfer generates a Universal Adversarial Perturbation (UAP) capable of\ndeceiving various CLIP encoders and downstream VLMs across different samples,\ntasks, and domains. We refer to this property as \\textbf{super\ntransferability}--a single perturbation achieving cross-data, cross-domain,\ncross-model, and cross-task adversarial transferability simultaneously. This is\nachieved through \\textbf{surrogate scaling}, a key innovation of our approach.\nUnlike existing methods that rely on fixed surrogate models, which are\ncomputationally intensive to scale, X-Transfer employs an efficient surrogate\nscaling strategy that dynamically selects a small subset of suitable surrogates\nfrom a large search space. Extensive evaluations demonstrate that X-Transfer\nsignificantly outperforms previous state-of-the-art UAP methods, establishing a\nnew benchmark for adversarial transferability across CLIP models. The code is\npublicly available in our\n\\href{https://github.com/HanxunH/XTransferBench}{GitHub repository}."}
{"id": "2505.05486", "pdf": "https://arxiv.org/pdf/2505.05486", "abs": "https://arxiv.org/abs/2505.05486", "authors": ["Anthony Kiggundu", "Dennis Krummacker", "Hans D. Schotten"], "title": "FedAvgen: Metadata for Model Aggregation In Communication Systems", "categories": ["cs.NE", "cs.AI", "cs.LG"], "comment": "Accepted in IEEE NetSoft 2025", "summary": "To improve business efficiency and minimize costs, Artificial Intelligence\n(AI) practitioners have adopted a shift from formulating models from scratch\ntowards sharing pretrained models. The pretrained models are then aggregated\ninto a global model with higher generalization capabilities, which is\nafterwards distributed to the client devices. This approach is known as\nfederated learning and inherently utilizes different techniques to select the\ncandidate client models averaged to obtain the global model. This approach, in\nthe case of communication systems, faces challenges arising from the\nexistential diversity in device profiles. The multiplicity in profiles\nmotivates our conceptual assessment of a metaheuristic algorithm (FedAvgen),\nwhich relates each pretrained model with its weight space as metadata, to a\nphenotype and genotype, respectively. This parent-child genetic evolution\ncharacterizes the global averaging step in federated learning. We then compare\nthe results of our approach to two widely adopted baseline federated learning\nalgorithms like Federated Averaging (FedAvg) and Federated Stochastic Gradient\nDescent (FedSGD)."}
{"id": "2505.05973", "pdf": "https://arxiv.org/pdf/2505.05973", "abs": "https://arxiv.org/abs/2505.05973", "authors": ["M. Maziyah Mohamed", "R. H. Baayen"], "title": "An Exploratory Analysis on the Explanatory Potential of Embedding-Based Measures of Semantic Transparency for Malay Word Recognition", "categories": ["cs.CL"], "comment": "24 pages, 5 figures, and 9 tables. Submitted to the Journal of\n  Morphology", "summary": "Studies of morphological processing have shown that semantic transparency is\ncrucial for word recognition. Its computational operationalization is still\nunder discussion. Our primary objectives are to explore embedding-based\nmeasures of semantic transparency, and assess their impact on reading. First,\nwe explored the geometry of complex words in semantic space. To do so, we\nconducted a t-distributed Stochastic Neighbor Embedding clustering analysis on\n4,226 Malay prefixed words. Several clusters were observed for complex words\nvaried by their prefix class. Then, we derived five simple measures, and\ninvestigated whether they were significant predictors of lexical decision\nlatencies. Two sets of Linear Discriminant Analyses were run in which the\nprefix of a word is predicted from either word embeddings or shift vectors\n(i.e., a vector subtraction of the base word from the derived word). The\naccuracy with which the model predicts the prefix of a word indicates the\ndegree of transparency of the prefix. Three further measures were obtained by\ncomparing embeddings between each word and all other words containing the same\nprefix (i.e., centroid), between each word and the shift from their base word,\nand between each word and the predicted word of the Functional Representations\nof Affixes in Compositional Semantic Space model. In a series of Generalized\nAdditive Mixed Models, all measures predicted decision latencies after\naccounting for word frequency, word length, and morphological family size. The\nmodel that included the correlation between each word and their centroid as a\npredictor provided the best fit to the data."}
{"id": "2505.05531", "pdf": "https://arxiv.org/pdf/2505.05531", "abs": "https://arxiv.org/abs/2505.05531", "authors": ["Hanie Moghaddasi", "Christina Chambers", "Sarah N. Mattson", "Jeffrey R. Wozniak", "Claire D. Coles", "Raja Mukherjee", "Michael Suttie"], "title": "OXSeg: Multidimensional attention UNet-based lip segmentation using semi-supervised lip contours", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Lip segmentation plays a crucial role in various domains, such as lip\nsynchronization, lipreading, and diagnostics. However, the effectiveness of\nsupervised lip segmentation is constrained by the availability of lip contour\nin the training phase. A further challenge with lip segmentation is its\nreliance on image quality , lighting, and skin tone, leading to inaccuracies in\nthe detected boundaries. To address these challenges, we propose a sequential\nlip segmentation method that integrates attention UNet and multidimensional\ninput. We unravel the micro-patterns in facial images using local binary\npatterns to build multidimensional inputs. Subsequently, the multidimensional\ninputs are fed into sequential attention UNets, where the lip contour is\nreconstructed. We introduce a mask generation method that uses a few anatomical\nlandmarks and estimates the complete lip contour to improve segmentation\naccuracy. This mask has been utilized in the training phase for lip\nsegmentation. To evaluate the proposed method, we use facial images to segment\nthe upper lips and subsequently assess lip-related facial anomalies in subjects\nwith fetal alcohol syndrome (FAS). Using the proposed lip segmentation method,\nwe achieved a mean dice score of 84.75%, and a mean pixel accuracy of 99.77% in\nupper lip segmentation. To further evaluate the method, we implemented\nclassifiers to identify those with FAS. Using a generative adversarial network\n(GAN), we reached an accuracy of 98.55% in identifying FAS in one of the study\npopulations. This method could be used to improve lip segmentation accuracy,\nespecially around Cupid's bow, and shed light on distinct lip-related\ncharacteristics of FAS."}
{"id": "2505.05491", "pdf": "https://arxiv.org/pdf/2505.05491", "abs": "https://arxiv.org/abs/2505.05491", "authors": ["TianYi Yu"], "title": "MDDFNet: Mamba-based Dynamic Dual Fusion Network for Traffic Sign Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The Detection of small objects, especially traffic signs, is a critical\nsub-task in object detection and autonomous driving. Despite signficant\nprogress in previous research, two main challenges remain. First, the issue of\nfeature extraction being too singular. Second, the detection process struggles\nto efectively handle objects of varying sizes or scales. These problems are\nalso prevalent in general object detection tasks. To address these challenges,\nwe propose a novel object detection network, Mamba-based Dynamic Dual Fusion\nNetwork (MDDFNet), for traffic sign detection. The network integrates a dynamic\ndual fusion module and a Mamba-based backbone to simultaneously tackle the\naforementioned issues. Specifically, the dynamic dual fusion module utilizes\nmultiple branches to consolidate various spatial and semantic information, thus\nenhancing feature diversity. The Mamba-based backbone leverages global feature\nfusion and local feature interaction, combining features in an adaptive manner\nto generate unique classification characteristics. Extensive experiments\nconducted on the TT100K (Tsinghua-Tencent 100K) datasets demonstrate that\nMDDFNet outperforms other state-of-the-art detectors, maintaining real-time\nprocessing capabilities of single-stage models while achieving superior\nperformance. This confirms the efectiveness of MDDFNet in detecting small\ntraffic signs."}
{"id": "2505.06004", "pdf": "https://arxiv.org/pdf/2505.06004", "abs": "https://arxiv.org/abs/2505.06004", "authors": ["Dawid Wisniewski", "Antoni Solarski", "Artur Nowakowski"], "title": "Exploring the Feasibility of Multilingual Grammatical Error Correction with a Single LLM up to 9B parameters: A Comparative Study of 17 Models", "categories": ["cs.CL"], "comment": "Accepted at MTSummit 2025 (The 20th Machine Translation Summit)", "summary": "Recent language models can successfully solve various language-related tasks,\nand many understand inputs stated in different languages. In this paper, we\nexplore the performance of 17 popular models used to correct grammatical issues\nin texts stated in English, German, Italian, and Swedish when using a single\nmodel to correct texts in all those languages. We analyze the outputs generated\nby these models, focusing on decreasing the number of grammatical errors while\nkeeping the changes small. The conclusions drawn help us understand what\nproblems occur among those models and which models can be recommended for\nmultilingual grammatical error correction tasks. We list six models that\nimprove grammatical correctness in all four languages and show that Gemma 9B is\ncurrently the best performing one for the languages considered."}
{"id": "2505.05540", "pdf": "https://arxiv.org/pdf/2505.05540", "abs": "https://arxiv.org/abs/2505.05540", "authors": ["Pranav Guruprasad", "Yangyue Wang", "Sudipta Chowdhury", "Harshvardhan Sikka"], "title": "Benchmarking Vision, Language, & Action Models in Procedurally Generated, Open Ended Action Environments", "categories": ["cs.CV", "cs.LG"], "comment": "16 pages, 26 figures", "summary": "Vision-language-action (VLA) models represent an important step toward\ngeneral-purpose robotic systems by integrating visual perception, language\nunderstanding, and action execution. However, systematic evaluation of these\nmodels, particularly their zero-shot generalization capabilities in\nout-of-distribution (OOD) environments, remains limited. In this paper, we\nintroduce MultiNet v0.2, a comprehensive benchmark designed to evaluate and\nanalyze the generalization performance of state-of-the-art VLM and VLA\nmodels-including GPT-4o, GPT-4.1, OpenVLA,Pi0 Base, and Pi0 FAST-on diverse\nprocedural tasks from the Procgen benchmark. Our analysis reveals several\ncritical insights: (1) all evaluated models exhibit significant limitations in\nzero-shot generalization to OOD tasks, with performance heavily influenced by\nfactors such as action representation and task complexit; (2) VLAs generally\noutperform other models due to their robust architectural design; and (3) VLM\nvariants demonstrate substantial improvements when constrained appropriately,\nhighlighting the sensitivity of model performance to precise prompt\nengineering."}
{"id": "2505.05492", "pdf": "https://arxiv.org/pdf/2505.05492", "abs": "https://arxiv.org/abs/2505.05492", "authors": ["Ignacy StÄpka", "Lukasz Sztukiewicz", "MichaÅ WiliÅski", "Jerzy Stefanowski"], "title": "DetoxAI: a Python Toolkit for Debiasing Deep Learning Models in Computer Vision", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "While machine learning fairness has made significant progress in recent\nyears, most existing solutions focus on tabular data and are poorly suited for\nvision-based classification tasks, which rely heavily on deep learning. To\nbridge this gap, we introduce DetoxAI, an open-source Python library for\nimproving fairness in deep learning vision classifiers through post-hoc\ndebiasing. DetoxAI implements state-of-the-art debiasing algorithms, fairness\nmetrics, and visualization tools. It supports debiasing via interventions in\ninternal representations and includes attribution-based visualization tools and\nquantitative algorithmic fairness metrics to show how bias is mitigated. This\npaper presents the motivation, design, and use cases of DetoxAI, demonstrating\nits tangible value to engineers and researchers."}
{"id": "2505.06010", "pdf": "https://arxiv.org/pdf/2505.06010", "abs": "https://arxiv.org/abs/2505.06010", "authors": ["Dawid Wisniewski", "Mikolaj Pokrywka", "Zofia Rostek"], "title": "Do Not Change Me: On Transferring Entities Without Modification in Neural Machine Translation -- a Multilingual Perspective", "categories": ["cs.CL"], "comment": "Accepted at MTSummit 2025 (The 20th Machine Translation Summit)", "summary": "Current machine translation models provide us with high-quality outputs in\nmost scenarios. However, they still face some specific problems, such as\ndetecting which entities should not be changed during translation. In this\npaper, we explore the abilities of popular NMT models, including models from\nthe OPUS project, Google Translate, MADLAD, and EuroLLM, to preserve entities\nsuch as URL addresses, IBAN numbers, or emails when producing translations\nbetween four languages: English, German, Polish, and Ukrainian. We investigate\nthe quality of popular NMT models in terms of accuracy, discuss errors made by\nthe models, and examine the reasons for errors. Our analysis highlights\nspecific categories, such as emojis, that pose significant challenges for many\nmodels considered. In addition to the analysis, we propose a new multilingual\nsynthetic dataset of 36,000 sentences that can help assess the quality of\nentity transfer across nine categories and four aforementioned languages."}
{"id": "2505.05573", "pdf": "https://arxiv.org/pdf/2505.05573", "abs": "https://arxiv.org/abs/2505.05573", "authors": ["Mikhail Chaichuk", "Sushant Gautam", "Steven Hicks", "Elena Tutubalina"], "title": "Prompt to Polyp: Clinically-Aware Medical Image Synthesis with Diffusion Models", "categories": ["cs.CV", "cs.AI", "68T07, 68U10, 92C55", "I.2.10; I.4.8; J.3"], "comment": "code available at\n  https://github.com/THunderCondOR/ImageCLEFmed-MEDVQA-GI-2024-MMCP-Team", "summary": "The generation of realistic medical images from text descriptions has\nsignificant potential to address data scarcity challenges in healthcare AI\nwhile preserving patient privacy. This paper presents a comprehensive study of\ntext-to-image synthesis in the medical domain, comparing two distinct\napproaches: (1) fine-tuning large pre-trained latent diffusion models and (2)\ntraining small, domain-specific models. We introduce a novel model named MSDM,\nan optimized architecture based on Stable Diffusion that integrates a clinical\ntext encoder, variational autoencoder, and cross-attention mechanisms to better\nalign medical text prompts with generated images. Our study compares two\napproaches: fine-tuning large pre-trained models (FLUX, Kandinsky) versus\ntraining compact domain-specific models (MSDM). Evaluation across colonoscopy\n(MedVQA-GI) and radiology (ROCOv2) datasets reveals that while large models\nachieve higher fidelity, our optimized MSDM delivers comparable quality with\nlower computational costs. Quantitative metrics and qualitative evaluations by\nmedical experts reveal strengths and limitations of each approach."}
{"id": "2505.05494", "pdf": "https://arxiv.org/pdf/2505.05494", "abs": "https://arxiv.org/abs/2505.05494", "authors": ["Avanija Menon", "Ovidiu Serban"], "title": "An Automated LLM-based Pipeline for Asset-Level Database Creation to Assess Deforestation Impact", "categories": ["cs.DB", "cs.AI", "cs.IR", "cs.LG"], "comment": "Accepted to ACL ClimateNLP 2025", "summary": "The European Union Deforestation Regulation (EUDR) requires companies to\nprove their products do not contribute to deforestation, creating a critical\ndemand for precise, asset-level environmental impact data. Current databases\nlack the necessary detail, relying heavily on broad financial metrics and\nmanual data collection, which limits regulatory compliance and accurate\nenvironmental modeling. This study presents an automated, end-to-end data\nextraction pipeline that uses LLMs to create, clean, and validate structured\ndatabases, specifically targeting sectors with a high risk of deforestation.\nThe pipeline introduces Instructional, Role-Based, Zero-Shot Chain-of-Thought\n(IRZ-CoT) prompting to enhance data extraction accuracy and a\nRetrieval-Augmented Validation (RAV) process that integrates real-time web\nsearches for improved data reliability. Applied to SEC EDGAR filings in the\nMining, Oil & Gas, and Utilities sectors, the pipeline demonstrates significant\nimprovements over traditional zero-shot prompting approaches, particularly in\nextraction accuracy and validation coverage. This work advances NLP-driven\nautomation for regulatory compliance, CSR (Corporate Social Responsibility),\nand ESG, with broad sectoral applicability."}
{"id": "2505.06027", "pdf": "https://arxiv.org/pdf/2505.06027", "abs": "https://arxiv.org/abs/2505.06027", "authors": ["Stefan Vasilev", "Christian Herold", "Baohao Liao", "Seyyed Hadi Hashemi", "Shahram Khadivi", "Christof Monz"], "title": "Unilogit: Robust Machine Unlearning for LLMs Using Uniform-Target Self-Distillation", "categories": ["cs.CL", "cs.LG", "68T50", "I.2.7"], "comment": "16 pages, 6 figures, 5 tables, under review at ACL", "summary": "This paper introduces Unilogit, a novel self-distillation method for machine\nunlearning in Large Language Models. Unilogit addresses the challenge of\nselectively forgetting specific information while maintaining overall model\nutility, a critical task in compliance with data privacy regulations like GDPR.\nUnlike prior methods that rely on static hyperparameters or starting model\noutputs, Unilogit dynamically adjusts target logits to achieve a uniform\nprobability for the target token, leveraging the current model's outputs for\nmore accurate self-distillation targets. This approach not only eliminates the\nneed for additional hyperparameters but also enhances the model's ability to\napproximate the golden targets. Extensive experiments on public benchmarks and\nan in-house e-commerce dataset demonstrate Unilogit's superior performance in\nbalancing forget and retain objectives, outperforming state-of-the-art methods\nsuch as NPO and UnDIAL. Our analysis further reveals Unilogit's robustness\nacross various scenarios, highlighting its practical applicability and\neffectiveness in achieving efficacious machine unlearning."}
{"id": "2505.05587", "pdf": "https://arxiv.org/pdf/2505.05587", "abs": "https://arxiv.org/abs/2505.05587", "authors": ["Peihao Wang", "Yuehao Wang", "Dilin Wang", "Sreyas Mohan", "Zhiwen Fan", "Lemeng Wu", "Ruisi Cai", "Yu-Ying Yeh", "Zhangyang Wang", "Qiang Liu", "Rakesh Ranjan"], "title": "Steepest Descent Density Control for Compact 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": "CVPR 2025, Project page: https://vita-group.github.io/SteepGS/", "summary": "3D Gaussian Splatting (3DGS) has emerged as a powerful technique for\nreal-time, high-resolution novel view synthesis. By representing scenes as a\nmixture of Gaussian primitives, 3DGS leverages GPU rasterization pipelines for\nefficient rendering and reconstruction. To optimize scene coverage and capture\nfine details, 3DGS employs a densification algorithm to generate additional\npoints. However, this process often leads to redundant point clouds, resulting\nin excessive memory usage, slower performance, and substantial storage demands\n- posing significant challenges for deployment on resource-constrained devices.\nTo address this limitation, we propose a theoretical framework that demystifies\nand improves density control in 3DGS. Our analysis reveals that splitting is\ncrucial for escaping saddle points. Through an optimization-theoretic approach,\nwe establish the necessary conditions for densification, determine the minimal\nnumber of offspring Gaussians, identify the optimal parameter update direction,\nand provide an analytical solution for normalizing off-spring opacity. Building\non these insights, we introduce SteepGS, incorporating steepest density\ncontrol, a principled strategy that minimizes loss while maintaining a compact\npoint cloud. SteepGS achieves a ~50% reduction in Gaussian points without\ncompromising rendering quality, significantly enhancing both efficiency and\nscalability."}
{"id": "2505.05498", "pdf": "https://arxiv.org/pdf/2505.05498", "abs": "https://arxiv.org/abs/2505.05498", "authors": ["Noor ul Misbah Khanum", "Hayssam Dahrouj", "Ramesh C. Bansal", "Hissam Mouayad Tawfik"], "title": "An Overview of the Prospects and Challenges of Using Artificial Intelligence for Energy Management Systems in Microgrids", "categories": ["eess.SY", "cs.AI", "cs.SY"], "comment": "70 pages, 7 figures", "summary": "Microgrids have emerged as a pivotal solution in the quest for a sustainable\nand energy-efficient future. While microgrids offer numerous advantages, they\nare also prone to issues related to reliably forecasting renewable energy\ndemand and production, protecting against cyberattacks, controlling operational\ncosts, optimizing power flow, and regulating the performance of energy\nmanagement systems (EMS). Tackling these energy management challenges is\nessential to facilitate microgrid applications and seamlessly incorporate\nrenewable energy resources. Artificial intelligence (AI) has recently\ndemonstrated immense potential for optimizing energy management in microgrids,\nproviding efficient and reliable solutions. This paper highlights the combined\nbenefits of enabling AI-based methodologies in the energy management systems of\nmicrogrids by examining the applicability and efficiency of AI-based EMS in\nachieving specific technical and economic objectives. The paper also points out\nseveral future research directions that promise to spearhead AI-driven EMS,\nnamely the development of self-healing microgrids, integration with blockchain\ntechnology, use of Internet of things (IoT), and addressing interpretability,\ndata privacy, scalability, and the prospects to generative AI in the context of\nfuture AI-based EMS."}
{"id": "2505.06046", "pdf": "https://arxiv.org/pdf/2505.06046", "abs": "https://arxiv.org/abs/2505.06046", "authors": ["Joshua Harris", "Fan Grayson", "Felix Feldman", "Timothy Laurence", "Toby Nonnenmacher", "Oliver Higgins", "Leo Loman", "Selina Patel", "Thomas Finnie", "Samuel Collins", "Michael Borowitz"], "title": "Healthy LLMs? Benchmarking LLM Knowledge of UK Government Public Health Information", "categories": ["cs.CL", "cs.LG", "68T50"], "comment": "24 pages, 10 pages main text", "summary": "As Large Language Models (LLMs) become widely accessible, a detailed\nunderstanding of their knowledge within specific domains becomes necessary for\nsuccessful real world use. This is particularly critical in public health,\nwhere failure to retrieve relevant, accurate, and current information could\nsignificantly impact UK residents. However, currently little is known about LLM\nknowledge of UK Government public health information. To address this issue,\nthis paper introduces a new benchmark, PubHealthBench, with over 8000 questions\nfor evaluating LLMs' Multiple Choice Question Answering (MCQA) and free form\nresponses to public health queries, created via an automated pipeline. We also\nrelease a new dataset of the extracted UK Government public health guidance\ndocuments used as source text for PubHealthBench. Assessing 24 LLMs on\nPubHealthBench we find the latest private LLMs (GPT-4.5, GPT-4.1 and o1) have a\nhigh degree of knowledge, achieving >90% in the MCQA setup, and outperform\nhumans with cursory search engine use. However, in the free form setup we see\nlower performance with no model scoring >75%. Therefore, whilst there are\npromising signs that state of the art (SOTA) LLMs are an increasingly accurate\nsource of public health information, additional safeguards or tools may still\nbe needed when providing free form responses on public health topics."}
{"id": "2505.05589", "pdf": "https://arxiv.org/pdf/2505.05589", "abs": "https://arxiv.org/abs/2505.05589", "authors": ["Jingzhong Lin", "Yuanyuan Qi", "Xinru Li", "Wenxuan Huang", "Xiangfeng Xu", "Bangyan Li", "Xuejiao Wang", "Gaoqi He"], "title": "ReactDance: Progressive-Granular Representation for Long-Term Coherent Reactive Dance Generation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Reactive dance generation (RDG) produces follower movements conditioned on\nguiding dancer and music while ensuring spatial coordination and temporal\ncoherence. However, existing methods overemphasize global constraints and\noptimization, overlooking local information, such as fine-grained spatial\ninteractions and localized temporal context. Therefore, we present ReactDance,\na novel diffusion-based framework for high-fidelity RDG with long-term\ncoherence and multi-scale controllability. Unlike existing methods that\nstruggle with interaction fidelity, synchronization, and temporal consistency\nin duet synthesis, our approach introduces two key innovations: 1)Group\nResidual Finite Scalar Quantization (GRFSQ), a multi-scale disentangled motion\nrepresentation that captures interaction semantics from coarse body rhythms to\nfine-grained joint dynamics, and 2)Blockwise Local Context (BLC), a sampling\nstrategy eliminating error accumulation in long sequence generation via local\nblock causal masking and periodic positional encoding. Built on the decoupled\nmulti-scale GRFSQ representation, we implement a diffusion model\nwithLayer-Decoupled Classifier-free Guidance (LDCFG), allowing granular control\nover motion semantics across scales. Extensive experiments on standard\nbenchmarks demonstrate that ReactDance surpasses existing methods, achieving\nstate-of-the-art performance."}
{"id": "2505.05501", "pdf": "https://arxiv.org/pdf/2505.05501", "abs": "https://arxiv.org/abs/2505.05501", "authors": ["Pu Cao", "Feng Zhou", "Junyi Ji", "Qingye Kong", "Zhixiang Lv", "Mingjian Zhang", "Xuekun Zhao", "Siqi Wu", "Yinghui Lin", "Qing Song", "Lu Yang"], "title": "Preliminary Explorations with GPT-4o(mni) Native Image Generation", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": null, "summary": "Recently, the visual generation ability by GPT-4o(mni) has been unlocked by\nOpenAI. It demonstrates a very remarkable generation capability with excellent\nmultimodal condition understanding and varied task instructions. In this paper,\nwe aim to explore the capabilities of GPT-4o across various tasks. Inspired by\nprevious study, we constructed a task taxonomy along with a carefully curated\nset of test samples to conduct a comprehensive qualitative test. Benefiting\nfrom GPT-4o's powerful multimodal comprehension, its image-generation process\ndemonstrates abilities surpassing those of traditional image-generation tasks.\nThus, regarding the dimensions of model capabilities, we evaluate its\nperformance across six task categories: traditional image generation tasks,\ndiscriminative tasks, knowledge-based generation, commonsense-based generation,\nspatially-aware image generation, and temporally-aware image generation. These\ntasks not only assess the quality and conditional alignment of the model's\noutputs but also probe deeper into GPT-4o's understanding of real-world\nconcepts. Our results reveal that GPT-4o performs impressively well in\ngeneral-purpose synthesis tasks, showing strong capabilities in text-to-image\ngeneration, visual stylization, and low-level image processing. However,\nsignificant limitations remain in its ability to perform precise spatial\nreasoning, instruction-grounded generation, and consistent temporal prediction.\nFurthermore, when faced with knowledge-intensive or domain-specific scenarios,\nsuch as scientific illustrations or mathematical plots, the model often\nexhibits hallucinations, factual errors, or structural inconsistencies. These\nfindings suggest that while GPT-4o marks a substantial advancement in unified\nmultimodal generation, there is still a long way to go before it can be\nreliably applied to professional or safety-critical domains."}
{"id": "2505.06062", "pdf": "https://arxiv.org/pdf/2505.06062", "abs": "https://arxiv.org/abs/2505.06062", "authors": ["Iuliia Zaitova", "Vitalii Hirak", "Badr M. Abdullah", "Dietrich Klakow", "Bernd MÃ¶bius", "Tania Avgustinova"], "title": "Attention on Multiword Expressions: A Multilingual Study of BERT-based Models with Regard to Idiomaticity and Microsyntax", "categories": ["cs.CL"], "comment": "10 pages, 3 figures. Findings 2025", "summary": "This study analyzes the attention patterns of fine-tuned encoder-only models\nbased on the BERT architecture (BERT-based models) towards two distinct types\nof Multiword Expressions (MWEs): idioms and microsyntactic units (MSUs). Idioms\npresent challenges in semantic non-compositionality, whereas MSUs demonstrate\nunconventional syntactic behavior that does not conform to standard grammatical\ncategorizations. We aim to understand whether fine-tuning BERT-based models on\nspecific tasks influences their attention to MWEs, and how this attention\ndiffers between semantic and syntactic tasks. We examine attention scores to\nMWEs in both pre-trained and fine-tuned BERT-based models. We utilize\nmonolingual models and datasets in six Indo-European languages - English,\nGerman, Dutch, Polish, Russian, and Ukrainian. Our results show that\nfine-tuning significantly influences how models allocate attention to MWEs.\nSpecifically, models fine-tuned on semantic tasks tend to distribute attention\nto idiomatic expressions more evenly across layers. Models fine-tuned on\nsyntactic tasks show an increase in attention to MSUs in the lower layers,\ncorresponding with syntactic processing requirements."}
{"id": "2505.05591", "pdf": "https://arxiv.org/pdf/2505.05591", "abs": "https://arxiv.org/abs/2505.05591", "authors": ["Yueh-Cheng Liu", "Lukas HÃ¶llein", "Matthias NieÃner", "Angela Dai"], "title": "QuickSplat: Fast 3D Surface Reconstruction via Learned Gaussian Initialization", "categories": ["cs.CV"], "comment": "Project page: https://liu115.github.io/quicksplat, Video:\n  https://youtu.be/2IA_gnFvFG8", "summary": "Surface reconstruction is fundamental to computer vision and graphics,\nenabling applications in 3D modeling, mixed reality, robotics, and more.\nExisting approaches based on volumetric rendering obtain promising results, but\noptimize on a per-scene basis, resulting in a slow optimization that can\nstruggle to model under-observed or textureless regions. We introduce\nQuickSplat, which learns data-driven priors to generate dense initializations\nfor 2D gaussian splatting optimization of large-scale indoor scenes. This\nprovides a strong starting point for the reconstruction, which accelerates the\nconvergence of the optimization and improves the geometry of flat wall\nstructures. We further learn to jointly estimate the densification and update\nof the scene parameters during each iteration; our proposed densifier network\npredicts new Gaussians based on the rendering gradients of existing ones,\nremoving the needs of heuristics for densification. Extensive experiments on\nlarge-scale indoor scene reconstruction demonstrate the superiority of our\ndata-driven optimization. Concretely, we accelerate runtime by 8x, while\ndecreasing depth errors by up to 48% in comparison to state of the art methods."}
{"id": "2505.05516", "pdf": "https://arxiv.org/pdf/2505.05516", "abs": "https://arxiv.org/abs/2505.05516", "authors": ["Yue Wu", "Yibo Guo", "Yulong Yan", "Jiancheng Yang", "Xin Zhou", "Ching-Yu Cheng", "Danli Shi", "Mingguang He"], "title": "AI-powered virtual eye: perspective, challenges and opportunities", "categories": ["q-bio.TO", "cs.AI", "cs.HC"], "comment": "30 Pages, 3 figures, 1 table", "summary": "We envision the \"virtual eye\" as a next-generation, AI-powered platform that\nuses interconnected foundation models to simulate the eye's intricate structure\nand biological function across all scales. Advances in AI, imaging, and\nmultiomics provide a fertile ground for constructing a universal, high-fidelity\ndigital replica of the human eye. This perspective traces the evolution from\nearly mechanistic and rule-based models to contemporary AI-driven approaches,\nintegrating in a unified model with multimodal, multiscale, dynamic predictive\ncapabilities and embedded feedback mechanisms. We propose a development roadmap\nemphasizing the roles of large-scale multimodal datasets, generative AI,\nfoundation models, agent-based architectures, and interactive interfaces.\nDespite challenges in interpretability, ethics, data processing and evaluation,\nthe virtual eye holds the potential to revolutionize personalized ophthalmic\ncare and accelerate research into ocular health and disease."}
{"id": "2505.06110", "pdf": "https://arxiv.org/pdf/2505.06110", "abs": "https://arxiv.org/abs/2505.06110", "authors": ["Jugal Gajjar", "Kaustik Ranaware"], "title": "Multimodal Sentiment Analysis on CMU-MOSEI Dataset using Transformer-based Models", "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 2 figures, 5 tables, and 19 references", "summary": "This project performs multimodal sentiment analysis using the CMU-MOSEI\ndataset, using transformer-based models with early fusion to integrate text,\naudio, and visual modalities. We employ BERT-based encoders for each modality,\nextracting embeddings that are concatenated before classification. The model\nachieves strong performance, with 97.87\\% 7-class accuracy and a 0.9682\nF1-score on the test set, demonstrating the effectiveness of early fusion in\ncapturing cross-modal interactions. The training utilized Adam optimization\n(lr=1e-4), dropout (0.3), and early stopping to ensure generalization and\nrobustness. Results highlight the superiority of transformer architectures in\nmodeling multimodal sentiment, with a low MAE (0.1060) indicating precise\nsentiment intensity prediction. Future work may compare fusion strategies or\nenhance interpretability. This approach utilizes multimodal learning by\neffectively combining linguistic, acoustic, and visual cues for sentiment\nanalysis."}
{"id": "2505.05599", "pdf": "https://arxiv.org/pdf/2505.05599", "abs": "https://arxiv.org/abs/2505.05599", "authors": ["Seraj Al Mahmud Mostafa", "Chenxi Wang", "Jia Yue", "Yuta Hozumi", "Jianwu Wang"], "title": "Enhancing Satellite Object Localization with Dilated Convolutions and Attention-aided Spatial Pooling", "categories": ["cs.CV", "cs.AI"], "comment": "This paper has been accepted to International conference on Advanced\n  Machine Learning and Data Science (AMLDS) 2025", "summary": "Object localization in satellite imagery is particularly challenging due to\nthe high variability of objects, low spatial resolution, and interference from\nnoise and dominant features such as clouds and city lights. In this research,\nwe focus on three satellite datasets: upper atmospheric Gravity Waves (GW),\nmesospheric Bores (Bore), and Ocean Eddies (OE), each presenting its own unique\nchallenges. These challenges include the variability in the scale and\nappearance of the main object patterns, where the size, shape, and feature\nextent of objects of interest can differ significantly. To address these\nchallenges, we introduce YOLO-DCAP, a novel enhanced version of YOLOv5 designed\nto improve object localization in these complex scenarios. YOLO-DCAP\nincorporates a Multi-scale Dilated Residual Convolution (MDRC) block to capture\nmulti-scale features at scale with varying dilation rates, and an\nAttention-aided Spatial Pooling (AaSP) module to focus on the global relevant\nspatial regions, enhancing feature selection. These structural improvements\nhelp to better localize objects in satellite imagery. Experimental results\ndemonstrate that YOLO-DCAP significantly outperforms both the YOLO base model\nand state-of-the-art approaches, achieving an average improvement of 20.95% in\nmAP50 and 32.23% in IoU over the base model, and 7.35% and 9.84% respectively\nover state-of-the-art alternatives, consistently across all three satellite\ndatasets. These consistent gains across all three satellite datasets highlight\nthe robustness and generalizability of the proposed approach. Our code is open\nsourced at\nhttps://github.com/AI-4-atmosphere-remote-sensing/satellite-object-localization."}
{"id": "2505.05520", "pdf": "https://arxiv.org/pdf/2505.05520", "abs": "https://arxiv.org/abs/2505.05520", "authors": ["Chengwei Ye", "Huanzhen Zhang", "Yufei Lin", "Kangsheng Wang", "Linuo Xu", "Shuyan Liu"], "title": "GaMNet: A Hybrid Network with Gabor Fusion and NMamba for Efficient 3D Glioma Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Gliomas are aggressive brain tumors that pose serious health risks. Deep\nlearning aids in lesion segmentation, but CNN and Transformer-based models\noften lack context modeling or demand heavy computation, limiting real-time use\non mobile medical devices. We propose GaMNet, integrating the NMamba module for\nglobal modeling and a multi-scale CNN for efficient local feature extraction.\nTo improve interpretability and mimic the human visual system, we apply Gabor\nfilters at multiple scales. Our method achieves high segmentation accuracy with\nfewer parameters and faster computation. Extensive experiments show GaMNet\noutperforms existing methods, notably reducing false positives and negatives,\nwhich enhances the reliability of clinical diagnosis."}
{"id": "2505.06120", "pdf": "https://arxiv.org/pdf/2505.06120", "abs": "https://arxiv.org/abs/2505.06120", "authors": ["Philippe Laban", "Hiroaki Hayashi", "Yingbo Zhou", "Jennifer Neville"], "title": "LLMs Get Lost In Multi-Turn Conversation", "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Large Language Models (LLMs) are conversational interfaces. As such, LLMs\nhave the potential to assist their users not only when they can fully specify\nthe task at hand, but also to help them define, explore, and refine what they\nneed through multi-turn conversational exchange. Although analysis of LLM\nconversation logs has confirmed that underspecification occurs frequently in\nuser instructions, LLM evaluation has predominantly focused on the single-turn,\nfully-specified instruction setting. In this work, we perform large-scale\nsimulation experiments to compare LLM performance in single- and multi-turn\nsettings. Our experiments confirm that all the top open- and closed-weight LLMs\nwe test exhibit significantly lower performance in multi-turn conversations\nthan single-turn, with an average drop of 39% across six generation tasks.\nAnalysis of 200,000+ simulated conversations decomposes the performance\ndegradation into two components: a minor loss in aptitude and a significant\nincrease in unreliability. We find that LLMs often make assumptions in early\nturns and prematurely attempt to generate final solutions, on which they overly\nrely. In simpler terms, we discover that *when LLMs take a wrong turn in a\nconversation, they get lost and do not recover*."}
{"id": "2505.05621", "pdf": "https://arxiv.org/pdf/2505.05621", "abs": "https://arxiv.org/abs/2505.05621", "authors": ["Hao Yang", "Yan Yang", "Ruikun Zhang", "Liyuan Pan"], "title": "A Preliminary Study for GPT-4o on Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "OpenAI's GPT-4o model, integrating multi-modal inputs and outputs within an\nautoregressive architecture, has demonstrated unprecedented performance in\nimage generation. In this work, we investigate its potential impact on the\nimage restoration community. We present the first systematic evaluation of\nGPT-4o across diverse restoration tasks. Our experiments reveal that, although\nrestoration outputs from GPT-4o are visually appealing, they often suffer from\npixel-level structural fidelity when compared to ground-truth images. Common\nissues are variations in image proportions, shifts in object positions and\nquantities, and changes in viewpoint.To address it, taking image dehazing,\nderainning, and low-light enhancement as representative case studies, we show\nthat GPT-4o's outputs can serve as powerful visual priors, substantially\nenhancing the performance of existing dehazing networks. It offers practical\nguidelines and a baseline framework to facilitate the integration of GPT-4o\ninto future image restoration pipelines. We hope the study on GPT-4o image\nrestoration will accelerate innovation in the broader field of image generation\nareas. To support further research, we will release GPT-4o-restored images from\nover 10 widely used image restoration datasets."}
{"id": "2505.05522", "pdf": "https://arxiv.org/pdf/2505.05522", "abs": "https://arxiv.org/abs/2505.05522", "authors": ["Luke Darlow", "Ciaran Regan", "Sebastian Risi", "Jeffrey Seely", "Llion Jones"], "title": "Continuous Thought Machines", "categories": ["cs.LG", "cs.AI"], "comment": "Technical report accompanied by online project page", "summary": "Biological brains demonstrate complex neural activity, where the timing and\ninterplay between neurons is critical to how brains process information. Most\ndeep learning architectures simplify neural activity by abstracting away\ntemporal dynamics. In this paper we challenge that paradigm. By incorporating\nneuron-level processing and synchronization, we can effectively reintroduce\nneural timing as a foundational element. We present the Continuous Thought\nMachine (CTM), a model designed to leverage neural dynamics as its core\nrepresentation. The CTM has two core innovations: (1) neuron-level temporal\nprocessing, where each neuron uses unique weight parameters to process a\nhistory of incoming signals; and (2) neural synchronization employed as a\nlatent representation. The CTM aims to strike a balance between oversimplified\nneuron abstractions that improve computational efficiency, and biological\nrealism. It operates at a level of abstraction that effectively captures\nessential temporal dynamics while remaining computationally tractable for deep\nlearning. We demonstrate the CTM's strong performance and versatility across a\nrange of challenging tasks, including ImageNet-1K classification, solving 2D\nmazes, sorting, parity computation, question-answering, and RL tasks. Beyond\ndisplaying rich internal representations and offering a natural avenue for\ninterpretation owing to its internal process, the CTM is able to perform tasks\nthat require complex sequential reasoning. The CTM can also leverage adaptive\ncompute, where it can stop earlier for simpler tasks, or keep computing when\nfaced with more challenging instances. The goal of this work is to share the\nCTM and its associated innovations, rather than pushing for new\nstate-of-the-art results. To that end, we believe the CTM represents a\nsignificant step toward developing more biologically plausible and powerful\nartificial intelligence systems."}
{"id": "2505.06145", "pdf": "https://arxiv.org/pdf/2505.06145", "abs": "https://arxiv.org/abs/2505.06145", "authors": ["Xu Han", "Yumeng Sun", "Weiqiang Huang", "Hongye Zheng", "Junliang Du"], "title": "Towards Robust Few-Shot Text Classification Using Transformer Architectures and Dual Loss Strategies", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Few-shot text classification has important application value in low-resource\nenvironments. This paper proposes a strategy that combines adaptive\nfine-tuning, contrastive learning, and regularization optimization to improve\nthe classification performance of Transformer-based models. Experiments on the\nFewRel 2.0 dataset show that T5-small, DeBERTa-v3, and RoBERTa-base perform\nwell in few-shot tasks, especially in the 5-shot setting, which can more\neffectively capture text features and improve classification accuracy. The\nexperiment also found that there are significant differences in the\nclassification difficulty of different relationship categories. Some categories\nhave fuzzy semantic boundaries or complex feature distributions, making it\ndifficult for the standard cross entropy loss to learn the discriminative\ninformation required to distinguish categories. By introducing contrastive loss\nand regularization loss, the generalization ability of the model is enhanced,\neffectively alleviating the overfitting problem in few-shot environments. In\naddition, the research results show that the use of Transformer models or\ngenerative architectures with stronger self-attention mechanisms can help\nimprove the stability and accuracy of few-shot classification."}
{"id": "2505.05626", "pdf": "https://arxiv.org/pdf/2505.05626", "abs": "https://arxiv.org/abs/2505.05626", "authors": ["Aarti Ghatkesar", "Uddeshya Upadhyay", "Ganesh Venkatesh"], "title": "Looking Beyond Language Priors: Enhancing Visual Comprehension and Attention in Multimodal Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Achieving deep alignment between vision and language remains a central\nchallenge for Multimodal Large Language Models (MLLMs). These models often fail\nto fully leverage visual input, defaulting to strong language priors. Our\napproach first provides insights into how MLLMs internally build visual\nunderstanding of image regions and then introduces techniques to amplify this\ncapability. Specifically, we explore techniques designed both to deepen the\nmodel's understanding of visual content and to ensure that these visual\ninsights actively guide language generation. We demonstrate the superior\nmultimodal understanding of our resultant model through a detailed upstream\nanalysis quantifying its ability to predict visually-dependent tokens as well\nas 10 pt boost on visually challenging tasks."}
{"id": "2505.05523", "pdf": "https://arxiv.org/pdf/2505.05523", "abs": "https://arxiv.org/abs/2505.05523", "authors": ["Anna Kusetogullari", "Huseyin Kusetogullari", "Martin Andersson", "Tony Gorschek"], "title": "GenAI in Entrepreneurship: a systematic review of generative artificial intelligence in entrepreneurship research: current issues and future directions", "categories": ["econ.GN", "cs.AI", "q-fin.EC"], "comment": null, "summary": "Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs)\nare recognized to have significant effects on industry and business dynamics,\nnot least because of their impact on the preconditions for entrepreneurship.\nThere is still a lack of knowledge of GenAI as a theme in entrepreneurship\nresearch. This paper presents a systematic literature review aimed at\nidentifying and analyzing the evolving landscape of research on the effects of\nGenAI on entrepreneurship. We analyze 83 peer-reviewed articles obtained from\nleading academic databases: Web of Science and Scopus. Using natural language\nprocessing and unsupervised machine learning techniques with TF-IDF\nvectorization, Principal Component Analysis (PCA), and hierarchical clustering,\nfive major thematic clusters are identified: (1) Digital Transformation and\nBehavioral Models, (2) GenAI-Enhanced Education and Learning Systems, (3)\nSustainable Innovation and Strategic AI Impact, (4) Business Models and Market\nTrends, and (5) Data-Driven Technological Trends in Entrepreneurship. Based on\nthe review, we discuss future research directions, gaps in the current\nliterature, as well as ethical concerns raised in the literature. We highlight\nthe need for more macro-level research on GenAI and LLMs as external enablers\nfor entrepreneurship and for research on effective regulatory frameworks that\nfacilitate business experimentation, innovation, and further technology\ndevelopment."}
{"id": "2505.06149", "pdf": "https://arxiv.org/pdf/2505.06149", "abs": "https://arxiv.org/abs/2505.06149", "authors": ["Faeze Ghorbanpour", "Daryna Dementieva", "Alexander Fraser"], "title": "Can Prompting LLMs Unlock Hate Speech Detection across Languages? A Zero-shot and Few-shot Study", "categories": ["cs.CL", "cs.CY", "cs.MM"], "comment": null, "summary": "Despite growing interest in automated hate speech detection, most existing\napproaches overlook the linguistic diversity of online content. Multilingual\ninstruction-tuned large language models such as LLaMA, Aya, Qwen, and BloomZ\noffer promising capabilities across languages, but their effectiveness in\nidentifying hate speech through zero-shot and few-shot prompting remains\nunderexplored. This work evaluates LLM prompting-based detection across eight\nnon-English languages, utilizing several prompting techniques and comparing\nthem to fine-tuned encoder models. We show that while zero-shot and few-shot\nprompting lag behind fine-tuned encoder models on most of the real-world\nevaluation sets, they achieve better generalization on functional tests for\nhate speech detection. Our study also reveals that prompt design plays a\ncritical role, with each language often requiring customized prompting\ntechniques to maximize performance."}
{"id": "2505.05635", "pdf": "https://arxiv.org/pdf/2505.05635", "abs": "https://arxiv.org/abs/2505.05635", "authors": ["Faizan Farooq Khan", "Jun Chen", "Youssef Mohamed", "Chun-Mei Feng", "Mohamed Elhoseiny"], "title": "VR-RAG: Open-vocabulary Species Recognition with RAG-Assisted Large Multi-Modal Models", "categories": ["cs.CV"], "comment": "7 figures", "summary": "Open-vocabulary recognition remains a challenging problem in computer vision,\nas it requires identifying objects from an unbounded set of categories. This is\nparticularly relevant in nature, where new species are discovered every year.\nIn this work, we focus on open-vocabulary bird species recognition, where the\ngoal is to classify species based on their descriptions without being\nconstrained to a predefined set of taxonomic categories. Traditional benchmarks\nlike CUB-200-2011 and Birdsnap have been evaluated in a closed-vocabulary\nparadigm, limiting their applicability to real-world scenarios where novel\nspecies continually emerge. We show that the performance of current systems\nwhen evaluated under settings closely aligned with open-vocabulary drops by a\nhuge margin. To address this gap, we propose a scalable framework integrating\nstructured textual knowledge from Wikipedia articles of 11,202 bird species\ndistilled via GPT-4o into concise, discriminative summaries. We propose Visual\nRe-ranking Retrieval-Augmented Generation(VR-RAG), a novel, retrieval-augmented\ngeneration framework that uses visual similarities to rerank the top m\ncandidates retrieved by a set of multimodal vision language encoders. This\nallows for the recognition of unseen taxa. Extensive experiments across five\nestablished classification benchmarks show that our approach is highly\neffective. By integrating VR-RAG, we improve the average performance of\nstate-of-the-art Large Multi-Modal Model QWEN2.5-VL by 15.4% across five\nbenchmarks. Our approach outperforms conventional VLM-based approaches, which\nstruggle with unseen species. By bridging the gap between encyclopedic\nknowledge and visual recognition, our work advances open-vocabulary\nrecognition, offering a flexible, scalable solution for biodiversity monitoring\nand ecological research."}
{"id": "2505.05527", "pdf": "https://arxiv.org/pdf/2505.05527", "abs": "https://arxiv.org/abs/2505.05527", "authors": ["Giovanni Perin", "Cesare Bidini", "Riccardo Mazzieri", "Michele Rossi"], "title": "ADMM-Based Training for Spiking Neural Networks", "categories": ["cs.LG", "cs.AI", "cs.NE", "eess.SP", "math.OC"], "comment": "6 pages, 4 figures. Preprint submitted to IEEE MLSP 2025", "summary": "In recent years, spiking neural networks (SNNs) have gained momentum due to\ntheir high potential in time-series processing combined with minimal energy\nconsumption. However, they still lack a dedicated and efficient training\nalgorithm. The popular backpropagation with surrogate gradients, adapted from\nstochastic gradient descent (SGD)-derived algorithms, has several drawbacks\nwhen used as an optimizer for SNNs. Specifically, it suffers from low\nscalability and numerical imprecision. In this paper, we propose a novel SNN\ntraining method based on the alternating direction method of multipliers\n(ADMM). Our ADMM-based training aims to solve the problem of the SNN step\nfunction's non-differentiability. We formulate the problem, derive closed-form\nupdates, and empirically show the optimizer's convergence properties, great\npotential, and possible new research directions to improve the method in a\nsimulated proof-of-concept."}
{"id": "2505.06150", "pdf": "https://arxiv.org/pdf/2505.06150", "abs": "https://arxiv.org/abs/2505.06150", "authors": ["Ryan Lagasse", "Aidan Kiernans", "Avijit Ghosh", "Shiri Dori-Hacohen"], "title": "A Scaling Law for Token Efficiency in LLM Fine-Tuning Under Fixed Compute Budgets", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce a scaling law for fine-tuning large language models (LLMs) under\nfixed compute budgets that explicitly accounts for data composition.\nConventional approaches measure training data solely by total tokens, yet the\nnumber of examples and their average token length -- what we term \\emph{dataset\nvolume} -- play a decisive role in model performance. Our formulation is tuned\nfollowing established procedures. Experiments on the BRICC dataset\n\\cite{salavati2024reducing} and subsets of the MMLU dataset\n\\cite{hendrycks2021measuringmassivemultitasklanguage}, evaluated under multiple\nsubsampling strategies, reveal that data composition significantly affects\ntoken efficiency. These results motivate refined scaling laws for practical LLM\nfine-tuning in resource-constrained settings."}
{"id": "2505.05640", "pdf": "https://arxiv.org/pdf/2505.05640", "abs": "https://arxiv.org/abs/2505.05640", "authors": ["Anadil Hussein", "Anna Zamansky", "George Martvel"], "title": "Semantic Style Transfer for Enhancing Animal Facial Landmark Detection", "categories": ["cs.CV"], "comment": null, "summary": "Neural Style Transfer (NST) is a technique for applying the visual\ncharacteristics of one image onto another while preserving structural content.\nTraditionally used for artistic transformations, NST has recently been adapted,\ne.g., for domain adaptation and data augmentation. This study investigates the\nuse of this technique for enhancing animal facial landmark detectors training.\nAs a case study, we use a recently introduced Ensemble Landmark Detector for 48\nanatomical cat facial landmarks and the CatFLW dataset it was trained on,\nmaking three main contributions. First, we demonstrate that applying style\ntransfer to cropped facial images rather than full-body images enhances\nstructural consistency, improving the quality of generated images. Secondly,\nreplacing training images with style-transferred versions raised challenges of\nannotation misalignment, but Supervised Style Transfer (SST) - which selects\nstyle sources based on landmark accuracy - retained up to 98% of baseline\naccuracy. Finally, augmenting the dataset with style-transferred images further\nimproved robustness, outperforming traditional augmentation methods. These\nfindings establish semantic style transfer as an effective augmentation\nstrategy for enhancing the performance of facial landmark detection models for\nanimals and beyond. While this study focuses on cat facial landmarks, the\nproposed method can be generalized to other species and landmark detection\nmodels."}
{"id": "2505.05528", "pdf": "https://arxiv.org/pdf/2505.05528", "abs": "https://arxiv.org/abs/2505.05528", "authors": ["Hanxun Huang", "Sarah Erfani", "Yige Li", "Xingjun Ma", "James Bailey"], "title": "X-Transfer Attacks: Towards Super Transferable Adversarial Attacks on CLIP", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "ICML 2025", "summary": "As Contrastive Language-Image Pre-training (CLIP) models are increasingly\nadopted for diverse downstream tasks and integrated into large vision-language\nmodels (VLMs), their susceptibility to adversarial perturbations has emerged as\na critical concern. In this work, we introduce \\textbf{X-Transfer}, a novel\nattack method that exposes a universal adversarial vulnerability in CLIP.\nX-Transfer generates a Universal Adversarial Perturbation (UAP) capable of\ndeceiving various CLIP encoders and downstream VLMs across different samples,\ntasks, and domains. We refer to this property as \\textbf{super\ntransferability}--a single perturbation achieving cross-data, cross-domain,\ncross-model, and cross-task adversarial transferability simultaneously. This is\nachieved through \\textbf{surrogate scaling}, a key innovation of our approach.\nUnlike existing methods that rely on fixed surrogate models, which are\ncomputationally intensive to scale, X-Transfer employs an efficient surrogate\nscaling strategy that dynamically selects a small subset of suitable surrogates\nfrom a large search space. Extensive evaluations demonstrate that X-Transfer\nsignificantly outperforms previous state-of-the-art UAP methods, establishing a\nnew benchmark for adversarial transferability across CLIP models. The code is\npublicly available in our\n\\href{https://github.com/HanxunH/XTransferBench}{GitHub repository}."}
{"id": "2505.06151", "pdf": "https://arxiv.org/pdf/2505.06151", "abs": "https://arxiv.org/abs/2505.06151", "authors": ["Alice Rueda", "Argyrios Perivolaris", "Niloy Roy", "Dylan Weston", "Sarmed Shaya", "Zachary Cote", "Martin Ivanov", "Bazen G. Teferra", "Yuqi Wu", "Sirisha Rambhatla", "Divya Sharma", "Andrew Greenshaw", "Rakesh Jetly", "Yanbo Zhang", "Bo Cao", "Reza Samavi", "Sridhar Krishnan", "Venkat Bhat"], "title": "Estimating Quality in Therapeutic Conversations: A Multi-Dimensional Natural Language Processing Framework", "categories": ["cs.CL"], "comment": "12 pages, 4 figures, 7 tables", "summary": "Engagement between client and therapist is a critical determinant of\ntherapeutic success. We propose a multi-dimensional natural language processing\n(NLP) framework that objectively classifies engagement quality in counseling\nsessions based on textual transcripts. Using 253 motivational interviewing\ntranscripts (150 high-quality, 103 low-quality), we extracted 42 features\nacross four domains: conversational dynamics, semantic similarity as topic\nalignment, sentiment classification, and question detection. Classifiers,\nincluding Random Forest (RF), Cat-Boost, and Support Vector Machines (SVM),\nwere hyperparameter tuned and trained using a stratified 5-fold\ncross-validation and evaluated on a holdout test set. On balanced\n(non-augmented) data, RF achieved the highest classification accuracy (76.7%),\nand SVM achieved the highest AUC (85.4%). After SMOTE-Tomek augmentation,\nperformance improved significantly: RF achieved up to 88.9% accuracy, 90.0%\nF1-score, and 94.6% AUC, while SVM reached 81.1% accuracy, 83.1% F1-score, and\n93.6% AUC. The augmented data results reflect the potential of the framework in\nfuture larger-scale applications. Feature contribution revealed conversational\ndynamics and semantic similarity between clients and therapists were among the\ntop contributors, led by words uttered by the client (mean and standard\ndeviation). The framework was robust across the original and augmented datasets\nand demonstrated consistent improvements in F1 scores and recall. While\ncurrently text-based, the framework supports future multimodal extensions\n(e.g., vocal tone, facial affect) for more holistic assessments. This work\nintroduces a scalable, data-driven method for evaluating engagement quality of\nthe therapy session, offering clinicians real-time feedback to enhance the\nquality of both virtual and in-person therapeutic interactions."}
{"id": "2505.05644", "pdf": "https://arxiv.org/pdf/2505.05644", "abs": "https://arxiv.org/abs/2505.05644", "authors": ["Tom Sander", "Moritz Tenthoff", "Kay Wohlfarth", "Christian WÃ¶hler"], "title": "The Moon's Many Faces: A Single Unified Transformer for Multimodal Lunar Reconstruction", "categories": ["cs.CV", "eess.IV"], "comment": "14pages", "summary": "Multimodal learning is an emerging research topic across multiple disciplines\nbut has rarely been applied to planetary science. In this contribution, we\nidentify that reflectance parameter estimation and image-based 3D\nreconstruction of lunar images can be formulated as a multimodal learning\nproblem. We propose a single, unified transformer architecture trained to learn\nshared representations between multiple sources like grayscale images, digital\nelevation models, surface normals, and albedo maps. The architecture supports\nflexible translation from any input modality to any target modality. Predicting\nDEMs and albedo maps from grayscale images simultaneously solves the task of 3D\nreconstruction of planetary surfaces and disentangles photometric parameters\nand height information. Our results demonstrate that our foundation model\nlearns physically plausible relations across these four modalities. Adding more\ninput modalities in the future will enable tasks such as photometric\nnormalization and co-registration."}
{"id": "2505.05530", "pdf": "https://arxiv.org/pdf/2505.05530", "abs": "https://arxiv.org/abs/2505.05530", "authors": ["Kai Liu", "Qian Zheng", "Kaiwen Tao", "Zhiteng Li", "Haotong Qin", "Wenbo Li", "Yong Guo", "Xianglong Liu", "Linghe Kong", "Guihai Chen", "Yulun Zhang", "Xiaokang Yang"], "title": "Low-bit Model Quantization for Deep Neural Networks: A Survey", "categories": ["cs.LG", "cs.AI"], "comment": "We have systematically collected and reviewed the state-of-the-art\n  quantization methods from the past five years, categorizing them into eight\n  distinct groups. A curated list of model quantization is provided at\n  https://github.com/Kai-Liu001/Awesome-Model-Quantization", "summary": "With unprecedented rapid development, deep neural networks (DNNs) have deeply\ninfluenced almost all fields. However, their heavy computation costs and model\nsizes are usually unacceptable in real-world deployment. Model quantization, an\neffective weight-lighting technique, has become an indispensable procedure in\nthe whole deployment pipeline. The essence of quantization acceleration is the\nconversion from continuous floating-point numbers to discrete integer ones,\nwhich significantly speeds up the memory I/O and calculation, i.e., addition\nand multiplication. However, performance degradation also comes with the\nconversion because of the loss of precision. Therefore, it has become\nincreasingly popular and critical to investigate how to perform the conversion\nand how to compensate for the information loss. This article surveys the recent\nfive-year progress towards low-bit quantization on DNNs. We discuss and compare\nthe state-of-the-art quantization methods and classify them into 8 main\ncategories and 24 sub-categories according to their core techniques.\nFurthermore, we shed light on the potential research opportunities in the field\nof model quantization. A curated list of model quantization is provided at\nhttps://github.com/Kai-Liu001/Awesome-Model-Quantization."}
{"id": "2505.06186", "pdf": "https://arxiv.org/pdf/2505.06186", "abs": "https://arxiv.org/abs/2505.06186", "authors": ["Massimiliano Pronesti", "Joao Bettencourt-Silva", "Paul Flanagan", "Alessandra Pascale", "Oisin Redmond", "Anya Belz", "Yufang Hou"], "title": "Query-driven Document-level Scientific Evidence Extraction from Biomedical Studies", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Extracting scientific evidence from biomedical studies for clinical research\nquestions (e.g., Does stem cell transplantation improve quality of life in\npatients with medically refractory Crohn's disease compared to placebo?) is a\ncrucial step in synthesising biomedical evidence. In this paper, we focus on\nthe task of document-level scientific evidence extraction for clinical\nquestions with conflicting evidence. To support this task, we create a dataset\ncalled CochraneForest, leveraging forest plots from Cochrane systematic\nreviews. It comprises 202 annotated forest plots, associated clinical research\nquestions, full texts of studies, and study-specific conclusions. Building on\nCochraneForest, we propose URCA (Uniform Retrieval Clustered Augmentation), a\nretrieval-augmented generation framework designed to tackle the unique\nchallenges of evidence extraction. Our experiments show that URCA outperforms\nthe best existing methods by up to 10.3% in F1 score on this task. However, the\nresults also underscore the complexity of CochraneForest, establishing it as a\nchallenging testbed for advancing automated evidence synthesis systems."}
{"id": "2505.05666", "pdf": "https://arxiv.org/pdf/2505.05666", "abs": "https://arxiv.org/abs/2505.05666", "authors": ["Alexander Most", "Joseph Winjum", "Ayan Biswas", "Shawn Jones", "Nishath Rajiv Ranasinghe", "Dan O'Malley", "Manish Bhattarai"], "title": "Lost in OCR Translation? Vision-Based Approaches to Robust Document Retrieval", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has become a popular technique for\nenhancing the reliability and utility of Large Language Models (LLMs) by\ngrounding responses in external documents. Traditional RAG systems rely on\nOptical Character Recognition (OCR) to first process scanned documents into\ntext. However, even state-of-the-art OCRs can introduce errors, especially in\ndegraded or complex documents. Recent vision-language approaches, such as\nColPali, propose direct visual embedding of documents, eliminating the need for\nOCR. This study presents a systematic comparison between a vision-based RAG\nsystem (ColPali) and more traditional OCR-based pipelines utilizing Llama 3.2\n(90B) and Nougat OCR across varying document qualities. Beyond conventional\nretrieval accuracy metrics, we introduce a semantic answer evaluation benchmark\nto assess end-to-end question-answering performance. Our findings indicate that\nwhile vision-based RAG performs well on documents it has been fine-tuned on,\nOCR-based RAG is better able to generalize to unseen documents of varying\nquality. We highlight the key trade-offs between computational efficiency and\nsemantic accuracy, offering practical guidance for RAG practitioners in\nselecting between OCR-dependent and vision-based document retrieval systems in\nproduction environments."}
{"id": "2505.05533", "pdf": "https://arxiv.org/pdf/2505.05533", "abs": "https://arxiv.org/abs/2505.05533", "authors": ["Zhiyuan Ning", "Pengfei Wang", "Ziyue Qiao", "Pengyang Wang", "Yuanchun Zhou"], "title": "Rethinking Graph Contrastive Learning through Relative Similarity Preservation", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by IJCAI2025; full version including appendix", "summary": "Graph contrastive learning (GCL) has achieved remarkable success by following\nthe computer vision paradigm of preserving absolute similarity between\naugmented views. However, this approach faces fundamental challenges in graphs\ndue to their discrete, non-Euclidean nature -- view generation often breaks\nsemantic validity and similarity verification becomes unreliable. Through\nanalyzing 11 real-world graphs, we discover a universal pattern transcending\nthe homophily-heterophily dichotomy: label consistency systematically\ndiminishes as structural distance increases, manifesting as smooth decay in\nhomophily graphs and oscillatory decay in heterophily graphs. We establish\ntheoretical guarantees for this pattern through random walk theory, proving\nlabel distribution convergence and characterizing the mechanisms behind\ndifferent decay behaviors. This discovery reveals that graphs naturally encode\nrelative similarity patterns, where structurally closer nodes exhibit\ncollectively stronger semantic relationships. Leveraging this insight, we\npropose RELGCL, a novel GCL framework with complementary pairwise and listwise\nimplementations that preserve these inherent patterns through collective\nsimilarity objectives. Extensive experiments demonstrate that our method\nconsistently outperforms 20 existing approaches across both homophily and\nheterophily graphs, validating the effectiveness of leveraging natural relative\nsimilarity over artificial absolute similarity."}
{"id": "2505.05528", "pdf": "https://arxiv.org/pdf/2505.05528", "abs": "https://arxiv.org/abs/2505.05528", "authors": ["Hanxun Huang", "Sarah Erfani", "Yige Li", "Xingjun Ma", "James Bailey"], "title": "X-Transfer Attacks: Towards Super Transferable Adversarial Attacks on CLIP", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "ICML 2025", "summary": "As Contrastive Language-Image Pre-training (CLIP) models are increasingly\nadopted for diverse downstream tasks and integrated into large vision-language\nmodels (VLMs), their susceptibility to adversarial perturbations has emerged as\na critical concern. In this work, we introduce \\textbf{X-Transfer}, a novel\nattack method that exposes a universal adversarial vulnerability in CLIP.\nX-Transfer generates a Universal Adversarial Perturbation (UAP) capable of\ndeceiving various CLIP encoders and downstream VLMs across different samples,\ntasks, and domains. We refer to this property as \\textbf{super\ntransferability}--a single perturbation achieving cross-data, cross-domain,\ncross-model, and cross-task adversarial transferability simultaneously. This is\nachieved through \\textbf{surrogate scaling}, a key innovation of our approach.\nUnlike existing methods that rely on fixed surrogate models, which are\ncomputationally intensive to scale, X-Transfer employs an efficient surrogate\nscaling strategy that dynamically selects a small subset of suitable surrogates\nfrom a large search space. Extensive evaluations demonstrate that X-Transfer\nsignificantly outperforms previous state-of-the-art UAP methods, establishing a\nnew benchmark for adversarial transferability across CLIP models. The code is\npublicly available in our\n\\href{https://github.com/HanxunH/XTransferBench}{GitHub repository}."}
{"id": "2505.05672", "pdf": "https://arxiv.org/pdf/2505.05672", "abs": "https://arxiv.org/abs/2505.05672", "authors": ["Gengyan Li", "Paulo Gotardo", "Timo Bolkart", "Stephan Garbin", "Kripasindhu Sarkar", "Abhimitra Meka", "Alexandros Lattas", "Thabo Beeler"], "title": "TeGA: Texture Space Gaussian Avatars for High-Resolution Dynamic Head Modeling", "categories": ["cs.CV", "I.3.7; I.3.5"], "comment": "10 pages, 9 figures, supplementary results found at:\n  https://syntec-research.github.io/UVGA/, to be published in SIGGRAPH 2025", "summary": "Sparse volumetric reconstruction and rendering via 3D Gaussian splatting have\nrecently enabled animatable 3D head avatars that are rendered under arbitrary\nviewpoints with impressive photorealism. Today, such photoreal avatars are seen\nas a key component in emerging applications in telepresence, extended reality,\nand entertainment. Building a photoreal avatar requires estimating the complex\nnon-rigid motion of different facial components as seen in input video images;\ndue to inaccurate motion estimation, animatable models typically present a loss\nof fidelity and detail when compared to their non-animatable counterparts,\nbuilt from an individual facial expression. Also, recent state-of-the-art\nmodels are often affected by memory limitations that reduce the number of 3D\nGaussians used for modeling, leading to lower detail and quality. To address\nthese problems, we present a new high-detail 3D head avatar model that improves\nupon the state of the art, largely increasing the number of 3D Gaussians and\nmodeling quality for rendering at 4K resolution. Our high-quality model is\nreconstructed from multiview input video and builds on top of a mesh-based 3D\nmorphable model, which provides a coarse deformation layer for the head.\nPhotoreal appearance is modelled by 3D Gaussians embedded within the continuous\nUVD tangent space of this mesh, allowing for more effective densification where\nmost needed. Additionally, these Gaussians are warped by a novel UVD\ndeformation field to capture subtle, localized motion. Our key contribution is\nthe novel deformable Gaussian encoding and overall fitting procedure that\nallows our head model to preserve appearance detail, while capturing facial\nmotion and other transient high-frequency features such as skin wrinkling."}
{"id": "2505.05538", "pdf": "https://arxiv.org/pdf/2505.05538", "abs": "https://arxiv.org/abs/2505.05538", "authors": ["Md Kamrujjaman Mobin", "Md Saiful Islam", "Sadik Al Barid", "Md Masum"], "title": "Cardioformer: Advancing AI in ECG Analysis with Multi-Granularity Patching and ResNet", "categories": ["cs.LG", "cs.AI", "eess.SP"], "comment": null, "summary": "Electrocardiogram (ECG) classification is crucial for automated cardiac\ndisease diagnosis, yet existing methods often struggle to capture local\nmorphological details and long-range temporal dependencies simultaneously. To\naddress these challenges, we propose Cardioformer, a novel multi-granularity\nhybrid model that integrates cross-channel patching, hierarchical residual\nlearning, and a two-stage self-attention mechanism. Cardioformer first encodes\nmulti-scale token embeddings to capture fine-grained local features and global\ncontextual information and then selectively fuses these representations through\nintra- and inter-granularity self-attention. Extensive evaluations on three\nbenchmark ECG datasets under subject-independent settings demonstrate that\nmodel consistently outperforms four state-of-the-art baselines. Our\nCardioformer model achieves the AUROC of 96.34$\\pm$0.11, 89.99$\\pm$0.12, and\n95.59$\\pm$1.66 in MIMIC-IV, PTB-XL and PTB dataset respectively outperforming\nPatchTST, Reformer, Transformer, and Medformer models. It also demonstrates\nstrong cross-dataset generalization, achieving 49.18% AUROC on PTB and 68.41%\non PTB-XL when trained on MIMIC-IV. These findings underscore the potential of\nCardioformer to advance automated ECG analysis, paving the way for more\naccurate and robust cardiovascular disease diagnosis. We release the source\ncode at https://github.com/KMobin555/Cardioformer."}
{"id": "2505.05665", "pdf": "https://arxiv.org/pdf/2505.05665", "abs": "https://arxiv.org/abs/2505.05665", "authors": ["Neeloy Chakraborty", "John Pohovey", "Melkior Ornik", "Katherine Driggs-Campbell"], "title": "Adaptive Stress Testing Black-Box LLM Planners", "categories": ["cs.RO", "cs.AI", "cs.CL"], "comment": "26 pages, 16 figures, 4 tables", "summary": "Large language models (LLMs) have recently demonstrated success in\ngeneralizing across decision-making tasks including planning, control and\nprediction, but their tendency to hallucinate unsafe and undesired outputs\nposes risks. We argue that detecting such failures is necessary, especially in\nsafety-critical scenarios. Existing black-box methods often detect\nhallucinations by identifying inconsistencies across multiple samples. Many of\nthese approaches typically introduce prompt perturbations like randomizing\ndetail order or generating adversarial inputs, with the intuition that a\nconfident model should produce stable outputs. We first perform a manual case\nstudy showing that other forms of perturbations (e.g., adding noise, removing\nsensor details) cause LLMs to hallucinate in a driving environment. We then\npropose a novel method for efficiently searching the space of prompt\nperturbations using Adaptive Stress Testing (AST) with Monte-Carlo Tree Search\n(MCTS). Our AST formulation enables discovery of scenarios and prompts that\ncause language models to act with high uncertainty. By generating MCTS prompt\nperturbation trees across diverse scenarios, we show that offline analyses can\nbe used at runtime to automatically generate prompts that influence model\nuncertainty, and to inform real-time trust assessments of an LLM."}
{"id": "2505.05678", "pdf": "https://arxiv.org/pdf/2505.05678", "abs": "https://arxiv.org/abs/2505.05678", "authors": ["Etai Sella", "Yanir Kleiman", "Hadar Averbuch-Elor"], "title": "InstanceGen: Image Generation with Instance-level Instructions", "categories": ["cs.CV"], "comment": "Project page: https://tau-vailab.github.io/InstanceGen/", "summary": "Despite rapid advancements in the capabilities of generative models,\npretrained text-to-image models still struggle in capturing the semantics\nconveyed by complex prompts that compound multiple objects and instance-level\nattributes. Consequently, we are witnessing growing interests in integrating\nadditional structural constraints, %leveraging additional structural inputs\ntypically in the form of coarse bounding boxes, to better guide the generation\nprocess in such challenging cases. In this work, we take the idea of structural\nguidance a step further by making the observation that contemporary image\ngeneration models can directly provide a plausible \\emph{fine-grained}\nstructural initialization. We propose a technique that couples this image-based\nstructural guidance with LLM-based instance-level instructions, yielding output\nimages that adhere to all parts of the text prompt, including object counts,\ninstance-level attributes, and spatial relations between instances."}
{"id": "2505.05543", "pdf": "https://arxiv.org/pdf/2505.05543", "abs": "https://arxiv.org/abs/2505.05543", "authors": ["Ahdiyeh Alipour", "Tilo Hartmann", "Maryam Alimardani"], "title": "Would You Rely on an Eerie Agent? A Systematic Review of the Impact of the Uncanny Valley Effect on Trust in Human-Agent Interaction", "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": "75 pages, Figure 11, Table 5", "summary": "Trust is a fundamental component of human-agent interaction. With the\nincreasing presence of artificial agents in daily life, it is essential to\nunderstand how people perceive and trust these agents. One of the key\nchallenges affecting this perception is the Uncanny Valley Effect (UVE), where\nincreasingly human-like artificial beings can be perceived as eerie or\nrepelling. Despite growing interest in trust and the UVE, existing research\nvaries widely in terms of how these concepts are defined and operationalized.\nThis inconsistency raises important questions about how and under what\nconditions the UVE influences trust in agents. A systematic understanding of\ntheir relationship is currently lacking. This review aims to examine the impact\nof the UVE on human trust in agents and to identify methodological patterns,\nlimitations, and gaps in the existing empirical literature. Following PRISMA\nguidelines, a systematic search identified 53 empirical studies that\ninvestigated both UVE-related constructs and trust or trust-related outcomes.\nStudies were analyzed based on a structured set of categories, including types\nof agents and interactions, methodological and measurement approaches, and key\nfindings. The results of our systematic review reveal that most studies rely on\nstatic images or hypothetical scenarios with limited real-time interaction, and\nthe majority use subjective trust measures. This review offers a novel\nframework for classifying trust measurement approaches with regard to the\nbest-practice criteria for empirically investigating the UVE. As the first\nsystematic attempt to map the intersection of UVE and trust, this review\ncontributes to a deeper understanding of their interplay and offers a\nfoundation for future research. Keywords: the uncanny valley effect, trust,\nhuman-likeness, affinity response, human-agent interaction"}
{"id": "2505.05684", "pdf": "https://arxiv.org/pdf/2505.05684", "abs": "https://arxiv.org/abs/2505.05684", "authors": ["Han Wu", "Jie Yin"], "title": "Prompted Meta-Learning for Few-shot Knowledge Graph Completion", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Few-shot knowledge graph completion (KGC) has obtained significant attention\ndue to its practical applications in real-world scenarios, where new knowledge\noften emerges with limited available data. While most existing methods for\nfew-shot KGC have predominantly focused on leveraging relational information,\nrich semantics inherent in KGs have been largely overlooked. To address this\ngap, we propose a novel prompted meta-learning (PromptMeta) framework that\nseamlessly integrates meta-semantics with relational information for few-shot\nKGC. PrompMeta has two key innovations: (1) a meta-semantic prompt pool that\ncaptures and consolidates high-level meta-semantics, enabling effective\nknowledge transfer and adaptation to rare and newly emerging relations. (2) a\nlearnable fusion prompt that dynamically combines meta-semantic information\nwith task-specific relational information tailored to different few-shot tasks.\nBoth components are optimized together with model parameters within a\nmeta-learning framework. Extensive experiments on two benchmark datasets\ndemonstrate the effectiveness of our approach."}
{"id": "2505.05681", "pdf": "https://arxiv.org/pdf/2505.05681", "abs": "https://arxiv.org/abs/2505.05681", "authors": ["Giulio Cesare Mastrocinque Santo", "PatrÃ­cia Izar", "Irene Delval", "Victor de Napole Gregolin", "Nina S. T. Hirata"], "title": "Fine-Tuning Video-Text Contrastive Model for Primate Behavior Retrieval from Unlabeled Raw Videos", "categories": ["cs.CV"], "comment": null, "summary": "Video recordings of nonhuman primates in their natural habitat are a common\nsource for studying their behavior in the wild. We fine-tune pre-trained\nvideo-text foundational models for the specific domain of capuchin monkeys,\nwith the goal of developing useful computational models to help researchers to\nretrieve useful clips from videos. We focus on the challenging problem of\ntraining a model based solely on raw, unlabeled video footage, using weak audio\ndescriptions sometimes provided by field collaborators. We leverage recent\nadvances in Multimodal Large Language Models (MLLMs) and Vision-Language Models\n(VLMs) to address the extremely noisy nature of both video and audio content.\nSpecifically, we propose a two-folded approach: an agentic data treatment\npipeline and a fine-tuning process. The data processing pipeline automatically\nextracts clean and semantically aligned video-text pairs from the raw videos,\nwhich are subsequently used to fine-tune a pre-trained Microsoft's X-CLIP model\nthrough Low-Rank Adaptation (LoRA). We obtained an uplift in $Hits@5$ of\n$167\\%$ for the 16 frames model and an uplift of $114\\%$ for the 8 frame model\non our domain data. Moreover, based on $NDCG@K$ results, our model is able to\nrank well most of the considered behaviors, while the tested raw pre-trained\nmodels are not able to rank them at all. The code will be made available upon\nacceptance."}
{"id": "2505.05568", "pdf": "https://arxiv.org/pdf/2505.05568", "abs": "https://arxiv.org/abs/2505.05568", "authors": ["Yanbo Wang", "Xiyuan Wang", "Quan Gan", "Minjie Wang", "Qibin Yang", "David Wipf", "Muhan Zhang"], "title": "Griffin: Towards a Graph-Centric Relational Database Foundation Model", "categories": ["cs.LG", "cs.AI", "cs.DB"], "comment": null, "summary": "We introduce Griffin, the first foundation model attemptation designed\nspecifically for Relational Databases (RDBs). Unlike previous smaller models\nfocused on single RDB tasks, Griffin unifies the data encoder and task decoder\nto handle diverse tasks. Additionally, we enhance the architecture by\nincorporating a cross-attention module and a novel aggregator. Griffin utilizes\npretraining on both single-table and RDB datasets, employing advanced encoders\nfor categorical, numerical, and metadata features, along with innovative\ncomponents such as cross-attention modules and enhanced message-passing neural\nnetworks (MPNNs) to capture the complexities of relational data. Evaluated on\nlarge-scale, heterogeneous, and temporal graphs extracted from RDBs across\nvarious domains (spanning over 150 million nodes), Griffin demonstrates\nsuperior or comparable performance to individually trained models, excels in\nlow-data scenarios, and shows strong transferability with similarity and\ndiversity in pretraining across new datasets and tasks, highlighting its\npotential as a universally applicable foundation model for RDBs. Code available\nat https://github.com/yanxwb/Griffin."}
{"id": "2505.05736", "pdf": "https://arxiv.org/pdf/2505.05736", "abs": "https://arxiv.org/abs/2505.05736", "authors": ["Da Wu", "Zhanliang Wang", "Quan Nguyen", "Zhuoran Xu", "Kai Wang"], "title": "Multimodal Integrated Knowledge Transfer to Large Language Models through Preference Optimization with Biomedical Applications", "categories": ["q-bio.QM", "cs.CL", "cs.CV", "cs.LG"], "comment": "First Draft", "summary": "The scarcity of high-quality multimodal biomedical data limits the ability to\neffectively fine-tune pretrained Large Language Models (LLMs) for specialized\nbiomedical tasks. To address this challenge, we introduce MINT (Multimodal\nIntegrated kNowledge Transfer), a framework that aligns unimodal large decoder\nmodels with domain-specific decision patterns from multimodal biomedical data\nthrough preference optimization. While MINT supports different optimization\ntechniques, we primarily implement it with the Odds Ratio Preference\nOptimization (ORPO) framework as its backbone. This strategy enables the\naligned LLMs to perform predictive tasks using text-only or image-only inputs\nwhile retaining knowledge learnt from multimodal data. MINT leverages an\nupstream multimodal machine learning (MML) model trained on high-quality\nmultimodal data to transfer domain-specific insights to downstream text-only or\nimage-only LLMs. We demonstrate its effectiveness through two key applications:\n(1) Rare genetic disease prediction from texts, where MINT uses a multimodal\nencoder model, trained on facial photos and clinical notes, to generate a\npreference dataset for aligning a lightweight Llama 3.2-3B-Instruct. Despite\nrelying on text input only, the MINT-derived model outperforms models trained\nwith SFT, RAG, or DPO, and even outperforms Llama 3.1-405B-Instruct. (2) Tissue\ntype classification using cell nucleus images, where MINT uses a\nvision-language foundation model as the preference generator, containing\nknowledge learnt from both text and histopathological images to align\ndownstream image-only models. The resulting MINT-derived model significantly\nimproves the performance of Llama 3.2-Vision-11B-Instruct on tissue type\nclassification. In summary, MINT provides an effective strategy to align\nunimodal LLMs with high-quality multimodal expertise through preference\noptimization."}
{"id": "2505.05710", "pdf": "https://arxiv.org/pdf/2505.05710", "abs": "https://arxiv.org/abs/2505.05710", "authors": ["Wooyoung Jeong", "Hyun Jae Park", "Seonghun Jeong", "Jong Wook Jang", "Tae Hoon Lim", "Dae Seoung Kim"], "title": "HyperspectralMAE: The Hyperspectral Imagery Classification Model using Fourier-Encoded Dual-Branch Masked Autoencoder", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": null, "summary": "Hyperspectral imagery provides rich spectral detail but poses unique\nchallenges because of its high dimensionality in both spatial and spectral\ndomains. We propose \\textit{HyperspectralMAE}, a Transformer-based foundation\nmodel for hyperspectral data that employs a \\textit{dual masking} strategy:\nduring pre-training we randomly occlude 50\\% of spatial patches and 50\\% of\nspectral bands. This forces the model to learn representations capable of\nreconstructing missing information across both dimensions. To encode spectral\norder, we introduce learnable harmonic Fourier positional embeddings based on\nwavelength. The reconstruction objective combines mean-squared error (MSE) with\nthe spectral angle mapper (SAM) to balance pixel-level accuracy and\nspectral-shape fidelity.\n  The resulting model contains about $1.8\\times10^{8}$ parameters and produces\n768-dimensional embeddings, giving it sufficient capacity for transfer\nlearning. We pre-trained HyperspectralMAE on two large hyperspectral corpora --\nNASA EO-1 Hyperion ($\\sim$1\\,600 scenes, $\\sim$$3\\times10^{11}$ pixel spectra)\nand DLR EnMAP Level-0 ($\\sim$1\\,300 scenes, $\\sim$$3\\times10^{11}$ pixel\nspectra) -- and fine-tuned it for land-cover classification on the Indian Pines\nbenchmark. HyperspectralMAE achieves state-of-the-art transfer-learning\naccuracy on Indian Pines, confirming that masked dual-dimensional pre-training\nyields robust spectral-spatial representations. These results demonstrate that\ndual masking and wavelength-aware embeddings advance hyperspectral image\nreconstruction and downstream analysis."}
{"id": "2505.05573", "pdf": "https://arxiv.org/pdf/2505.05573", "abs": "https://arxiv.org/abs/2505.05573", "authors": ["Mikhail Chaichuk", "Sushant Gautam", "Steven Hicks", "Elena Tutubalina"], "title": "Prompt to Polyp: Clinically-Aware Medical Image Synthesis with Diffusion Models", "categories": ["cs.CV", "cs.AI", "68T07, 68U10, 92C55", "I.2.10; I.4.8; J.3"], "comment": "code available at\n  https://github.com/THunderCondOR/ImageCLEFmed-MEDVQA-GI-2024-MMCP-Team", "summary": "The generation of realistic medical images from text descriptions has\nsignificant potential to address data scarcity challenges in healthcare AI\nwhile preserving patient privacy. This paper presents a comprehensive study of\ntext-to-image synthesis in the medical domain, comparing two distinct\napproaches: (1) fine-tuning large pre-trained latent diffusion models and (2)\ntraining small, domain-specific models. We introduce a novel model named MSDM,\nan optimized architecture based on Stable Diffusion that integrates a clinical\ntext encoder, variational autoencoder, and cross-attention mechanisms to better\nalign medical text prompts with generated images. Our study compares two\napproaches: fine-tuning large pre-trained models (FLUX, Kandinsky) versus\ntraining compact domain-specific models (MSDM). Evaluation across colonoscopy\n(MedVQA-GI) and radiology (ROCOv2) datasets reveals that while large models\nachieve higher fidelity, our optimized MSDM delivers comparable quality with\nlower computational costs. Quantitative metrics and qualitative evaluations by\nmedical experts reveal strengths and limitations of each approach."}
{"id": "2505.05744", "pdf": "https://arxiv.org/pdf/2505.05744", "abs": "https://arxiv.org/abs/2505.05744", "authors": ["Ruxue Shi", "Hengrui Gu", "Xu Shen", "Xin Wang"], "title": "Harnessing LLMs Explanations to Boost Surrogate Models in Tabular Data Classification", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable ability in solving complex\ntasks, making them a promising tool for enhancing tabular learning. However,\nexisting LLM-based methods suffer from high resource requirements, suboptimal\ndemonstration selection, and limited interpretability, which largely hinder\ntheir prediction performance and application in the real world. To overcome\nthese problems, we propose a novel in-context learning framework for tabular\nprediction. The core idea is to leverage the explanations generated by LLMs to\nguide a smaller, locally deployable Surrogate Language Model (SLM) to make\ninterpretable tabular predictions. Specifically, our framework mainly involves\nthree stages: (i) Post Hoc Explanation Generation, where LLMs are utilized to\ngenerate explanations for question-answer pairs in candidate demonstrations,\nproviding insights into the reasoning behind the answer. (ii) Post Hoc\nExplanation-Guided Demonstrations Selection, which utilizes explanations\ngenerated by LLMs to guide the process of demonstration selection from\ncandidate demonstrations. (iii) Post Hoc Explanation-Guided Interpretable SLM\nPrediction, which utilizes the demonstrations obtained in step (ii) as\nin-context and merges corresponding explanations as rationales to improve the\nperformance of SLM and guide the model to generate interpretable outputs.\nExperimental results highlight the framework's effectiveness, with an average\naccuracy improvement of 5.31% across various tabular datasets in diverse\ndomains."}
{"id": "2505.05711", "pdf": "https://arxiv.org/pdf/2505.05711", "abs": "https://arxiv.org/abs/2505.05711", "authors": ["Ho-Joong Kim", "Yearang Lee", "Jung-Ho Hong", "Seong-Whan Lee"], "title": "DiGIT: Multi-Dilated Gated Encoder and Central-Adjacent Region Integrated Decoder for Temporal Action Detection Transformer", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "In this paper, we examine a key limitation in query-based detectors for\ntemporal action detection (TAD), which arises from their direct adaptation of\noriginally designed architectures for object detection. Despite the\neffectiveness of the existing models, they struggle to fully address the unique\nchallenges of TAD, such as the redundancy in multi-scale features and the\nlimited ability to capture sufficient temporal context. To address these\nissues, we propose a multi-dilated gated encoder and central-adjacent region\nintegrated decoder for temporal action detection transformer (DiGIT). Our\napproach replaces the existing encoder that consists of multi-scale deformable\nattention and feedforward network with our multi-dilated gated encoder. Our\nproposed encoder reduces the redundant information caused by multi-level\nfeatures while maintaining the ability to capture fine-grained and long-range\ntemporal information. Furthermore, we introduce a central-adjacent region\nintegrated decoder that leverages a more comprehensive sampling strategy for\ndeformable cross-attention to capture the essential information. Extensive\nexperiments demonstrate that DiGIT achieves state-of-the-art performance on\nTHUMOS14, ActivityNet v1.3, and HACS-Segment. Code is available at:\nhttps://github.com/Dotori-HJ/DiGIT"}
{"id": "2505.05577", "pdf": "https://arxiv.org/pdf/2505.05577", "abs": "https://arxiv.org/abs/2505.05577", "authors": ["Alejandro Velez-Arce", "Marinka Zitnik"], "title": "PyTDC: A multimodal machine learning training, evaluation, and inference platform for biomedical foundation models", "categories": ["cs.LG", "cs.AI", "68-04, 92-04", "D.2.11; I.2.5; J.3"], "comment": "Proceedings of the 42nd International Conference on Machine Learning,\n  Vancouver, Canada. PMLR 267, 2025", "summary": "Existing biomedical benchmarks do not provide end-to-end infrastructure for\ntraining, evaluation, and inference of models that integrate multimodal\nbiological data and a broad range of machine learning tasks in therapeutics. We\npresent PyTDC, an open-source machine-learning platform providing streamlined\ntraining, evaluation, and inference software for multimodal biological AI\nmodels. PyTDC unifies distributed, heterogeneous, continuously updated data\nsources and model weights and standardizes benchmarking and inference\nendpoints. This paper discusses the components of PyTDC's architecture and, to\nour knowledge, the first-of-its-kind case study on the introduced single-cell\ndrug-target nomination ML task. We find state-of-the-art methods in graph\nrepresentation learning and domain-specific methods from graph theory perform\npoorly on this task. Though we find a context-aware geometric deep learning\nmethod that outperforms the evaluated SoTA and domain-specific baseline\nmethods, the model is unable to generalize to unseen cell types or incorporate\nadditional modalities, highlighting PyTDC's capacity to facilitate an exciting\navenue of research developing multimodal, context-aware, foundation models for\nopen problems in biomedical AI."}
{"id": "2505.05763", "pdf": "https://arxiv.org/pdf/2505.05763", "abs": "https://arxiv.org/abs/2505.05763", "authors": ["Yize Zhou", "Jie Zhang", "Meijie Wang", "Lun Yu"], "title": "BMMDetect: A Multimodal Deep Learning Framework for Comprehensive Biomedical Misconduct Detection", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Academic misconduct detection in biomedical research remains challenging due\nto algorithmic narrowness in existing methods and fragmented analytical\npipelines. We present BMMDetect, a multimodal deep learning framework that\nintegrates journal metadata (SJR, institutional data), semantic embeddings\n(PubMedBERT), and GPT-4o-mined textual attributes (methodological statistics,\ndata anomalies) for holistic manuscript evaluation. Key innovations include:\n(1) multimodal fusion of domain-specific features to reduce detection bias; (2)\nquantitative evaluation of feature importance, identifying journal authority\nmetrics (e.g., SJR-index) and textual anomalies (e.g., statistical outliers) as\ndominant predictors; and (3) the BioMCD dataset, a large-scale benchmark with\n13,160 retracted articles and 53,411 controls. BMMDetect achieves 74.33% AUC,\noutperforming single-modality baselines by 8.6%, and demonstrates\ntransferability across biomedical subfields. This work advances scalable,\ninterpretable tools for safeguarding research integrity."}
{"id": "2505.05721", "pdf": "https://arxiv.org/pdf/2505.05721", "abs": "https://arxiv.org/abs/2505.05721", "authors": ["Zixuan Li", "Lei Meng", "Guoqing Chao", "Wei Wu", "Xiaoshuo Yan", "Yimeng Yang", "Zhuang Qi", "Xiangxu Meng"], "title": "Semantic-Space-Intervened Diffusive Alignment for Visual Classification", "categories": ["cs.CV"], "comment": null, "summary": "Cross-modal alignment is an effective approach to improving visual\nclassification. Existing studies typically enforce a one-step mapping that uses\ndeep neural networks to project the visual features to mimic the distribution\nof textual features. However, they typically face difficulties in finding such\na projection due to the two modalities in both the distribution of class-wise\nsamples and the range of their feature values. To address this issue, this\npaper proposes a novel Semantic-Space-Intervened Diffusive Alignment method,\ntermed SeDA, models a semantic space as a bridge in the visual-to-textual\nprojection, considering both types of features share the same class-level\ninformation in classification. More importantly, a bi-stage diffusion framework\nis developed to enable the progressive alignment between the two modalities.\nSpecifically, SeDA first employs a Diffusion-Controlled Semantic Learner to\nmodel the semantic features space of visual features by constraining the\ninteractive features of the diffusion model and the category centers of visual\nfeatures. In the later stage of SeDA, the Diffusion-Controlled Semantic\nTranslator focuses on learning the distribution of textual features from the\nsemantic space. Meanwhile, the Progressive Feature Interaction Network\nintroduces stepwise feature interactions at each alignment step, progressively\nintegrating textual information into mapped features. Experimental results show\nthat SeDA achieves stronger cross-modal feature alignment, leading to superior\nperformance over existing methods across multiple scenarios."}
{"id": "2505.05588", "pdf": "https://arxiv.org/pdf/2505.05588", "abs": "https://arxiv.org/abs/2505.05588", "authors": ["Somrita Banerjee", "Abhishek Cauligi", "Marco Pavone"], "title": "Flight Validation of Learning-Based Trajectory Optimization for the Astrobee Free-Flyer", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Submitted to RSS 2025 Workshop on Space Robotics", "summary": "Although widely used in commercial and industrial robotics, trajectory\noptimization has seen limited use in space applications due to its high\ncomputational demands. In this work, we present flight results from experiments\nwith the Astrobee free-flying robot on board the International Space Station\n(ISS), that demonstrate how machine learning can accelerate on-board trajectory\noptimization while preserving theoretical solver guarantees. To the best of the\nauthors' knowledge, this is the first-ever demonstration of learning-based\ncontrol on the ISS. Our approach leverages the GuSTO sequential convex\nprogramming framework and uses a neural network, trained offline, to map\nproblem parameters to effective initial ``warm-start'' trajectories, paving the\nway for faster real-time optimization on resource-constrained space platforms."}
{"id": "2505.05828", "pdf": "https://arxiv.org/pdf/2505.05828", "abs": "https://arxiv.org/abs/2505.05828", "authors": ["Alba MarÃ­a MÃ¡rmol-Romero", "Manuel GarcÃ­a-Vega", "Miguel Ãngel GarcÃ­a-Cumbreras", "Arturo Montejo-RÃ¡ez"], "title": "An empathic GPT-based chatbot to talk about mental disorders with Spanish teenagers", "categories": ["cs.HC", "cs.CL"], "comment": "This is an Accepted Manuscript version of the following article,\n  accepted for publication in International Journal of Human-Computer\n  Interaction. It is deposited under the terms of the Creative Commons\n  Attribution-NonCommercial-NoDerivatives License", "summary": "This paper presents a chatbot-based system to engage young Spanish people in\nthe awareness of certain mental disorders through a self-disclosure technique.\nThe study was carried out in a population of teenagers aged between 12 and 18\nyears. The dialogue engine mixes closed and open conversations, so certain\ncontrolled messages are sent to focus the chat on a specific disorder, which\nwill change over time. Once a set of trial questions is answered, the system\ncan initiate the conversation on the disorder under the focus according to the\nuser's sensibility to that disorder, in an attempt to establish a more\nempathetic communication. Then, an open conversation based on the GPT-3\nlanguage model is initiated, allowing the user to express themselves with more\nfreedom. The results show that these systems are of interest to young people\nand could help them become aware of certain mental disorders."}
{"id": "2505.05722", "pdf": "https://arxiv.org/pdf/2505.05722", "abs": "https://arxiv.org/abs/2505.05722", "authors": ["Valay Bundele", "Mehran Hosseinzadeh", "Hendrik Lensch"], "title": "You Are Your Best Teacher: Semi-Supervised Surgical Point Tracking with Cycle-Consistent Self-Distillation", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025 SynData4CV Workshop", "summary": "Synthetic datasets have enabled significant progress in point tracking by\nproviding large-scale, densely annotated supervision. However, deploying these\nmodels in real-world domains remains challenging due to domain shift and lack\nof labeled data-issues that are especially severe in surgical videos, where\nscenes exhibit complex tissue deformation, occlusion, and lighting variation.\nWhile recent approaches adapt synthetic-trained trackers to natural videos\nusing teacher ensembles or augmentation-heavy pseudo-labeling pipelines, their\neffectiveness in high-shift domains like surgery remains unexplored. This work\npresents SurgTracker, a semi-supervised framework for adapting\nsynthetic-trained point trackers to surgical video using filtered\nself-distillation. Pseudo-labels are generated online by a fixed\nteacher-identical in architecture and initialization to the student-and are\nfiltered using a cycle consistency constraint to discard temporally\ninconsistent trajectories. This simple yet effective design enforces geometric\nconsistency and provides stable supervision throughout training, without the\ncomputational overhead of maintaining multiple teachers. Experiments on the\nSTIR benchmark show that SurgTracker improves tracking performance using only\n80 unlabeled videos, demonstrating its potential for robust adaptation in\nhigh-shift, data-scarce domains."}
{"id": "2505.05589", "pdf": "https://arxiv.org/pdf/2505.05589", "abs": "https://arxiv.org/abs/2505.05589", "authors": ["Jingzhong Lin", "Yuanyuan Qi", "Xinru Li", "Wenxuan Huang", "Xiangfeng Xu", "Bangyan Li", "Xuejiao Wang", "Gaoqi He"], "title": "ReactDance: Progressive-Granular Representation for Long-Term Coherent Reactive Dance Generation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Reactive dance generation (RDG) produces follower movements conditioned on\nguiding dancer and music while ensuring spatial coordination and temporal\ncoherence. However, existing methods overemphasize global constraints and\noptimization, overlooking local information, such as fine-grained spatial\ninteractions and localized temporal context. Therefore, we present ReactDance,\na novel diffusion-based framework for high-fidelity RDG with long-term\ncoherence and multi-scale controllability. Unlike existing methods that\nstruggle with interaction fidelity, synchronization, and temporal consistency\nin duet synthesis, our approach introduces two key innovations: 1)Group\nResidual Finite Scalar Quantization (GRFSQ), a multi-scale disentangled motion\nrepresentation that captures interaction semantics from coarse body rhythms to\nfine-grained joint dynamics, and 2)Blockwise Local Context (BLC), a sampling\nstrategy eliminating error accumulation in long sequence generation via local\nblock causal masking and periodic positional encoding. Built on the decoupled\nmulti-scale GRFSQ representation, we implement a diffusion model\nwithLayer-Decoupled Classifier-free Guidance (LDCFG), allowing granular control\nover motion semantics across scales. Extensive experiments on standard\nbenchmarks demonstrate that ReactDance surpasses existing methods, achieving\nstate-of-the-art performance."}
{"id": "2505.05863", "pdf": "https://arxiv.org/pdf/2505.05863", "abs": "https://arxiv.org/abs/2505.05863", "authors": ["Reiji Suzuki", "Takaya Arita"], "title": "Evolutionary ecology of words", "categories": ["q-bio.PE", "cs.AI", "cs.CL", "92B20"], "comment": "8 pages, 5 figures. Preprint of the paper published in Proceedings of\n  2025 IEEE Symposium on Computational Intelligence in Artificial Life and\n  Cooperative Intelligent Systems (ALIFE-CIS)", "summary": "We propose a model for the evolutionary ecology of words as one attempt to\nextend evolutionary game theory and agent-based models by utilizing the rich\nlinguistic expressions of Large Language Models (LLMs). Our model enables the\nemergence and evolution of diverse and infinite options for interactions among\nagents. Within the population, each agent possesses a short word (or phrase)\ngenerated by an LLM and moves within a spatial environment. When agents become\nadjacent, the outcome of their interaction is determined by the LLM based on\nthe relationship between their words, with the loser's word being replaced by\nthe winner's. Word mutations, also based on LLM outputs, may occur. We\nconducted preliminary experiments assuming that ``strong animal species\" would\nsurvive. The results showed that from an initial population consisting of\nwell-known species, many species emerged both gradually and in a punctuated\nequilibrium manner. Each trial demonstrated the unique evolution of diverse\npopulations, with one type of large species becoming dominant, such as\nterrestrial animals, marine life, or extinct species, which were ecologically\nspecialized and adapted ones across diverse extreme habitats. We also conducted\na long-term experiment with a large population, demonstrating the emergence and\ncoexistence of diverse species."}
{"id": "2505.05741", "pdf": "https://arxiv.org/pdf/2505.05741", "abs": "https://arxiv.org/abs/2505.05741", "authors": ["Zhangchi Hu", "Peixi Wu", "Jie Chen", "Huyue Zhu", "Yijun Wang", "Yansong Peng", "Hebei Li", "Xiaoyan Sun"], "title": "Dome-DETR: DETR with Density-Oriented Feature-Query Manipulation for Efficient Tiny Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Tiny object detection plays a vital role in drone surveillance, remote\nsensing, and autonomous systems, enabling the identification of small targets\nacross vast landscapes. However, existing methods suffer from inefficient\nfeature leverage and high computational costs due to redundant feature\nprocessing and rigid query allocation. To address these challenges, we propose\nDome-DETR, a novel framework with Density-Oriented Feature-Query Manipulation\nfor Efficient Tiny Object Detection. To reduce feature redundancies, we\nintroduce a lightweight Density-Focal Extractor (DeFE) to produce clustered\ncompact foreground masks. Leveraging these masks, we incorporate Masked Window\nAttention Sparsification (MWAS) to focus computational resources on the most\ninformative regions via sparse attention. Besides, we propose Progressive\nAdaptive Query Initialization (PAQI), which adaptively modulates query density\nacross spatial areas for better query allocation. Extensive experiments\ndemonstrate that Dome-DETR achieves state-of-the-art performance (+3.3 AP on\nAI-TOD-V2 and +2.5 AP on VisDrone) while maintaining low computational\ncomplexity and a compact model size. Code will be released upon acceptance."}
{"id": "2505.05595", "pdf": "https://arxiv.org/pdf/2505.05595", "abs": "https://arxiv.org/abs/2505.05595", "authors": ["Wenhao Guo", "Yuda Wang", "Zeqiao Huang", "Changjiang Zhang", "Shumin ma"], "title": "Trading Under Uncertainty: A Distribution-Based Strategy for Futures Markets Using FutureQuant Transformer", "categories": ["q-fin.TR", "cs.AI", "cs.CE", "cs.LG"], "comment": "16 pages, 12 figures", "summary": "In the complex landscape of traditional futures trading, where vast data and\nvariables like real-time Limit Order Books (LOB) complicate price predictions,\nwe introduce the FutureQuant Transformer model, leveraging attention mechanisms\nto navigate these challenges. Unlike conventional models focused on point\npredictions, the FutureQuant model excels in forecasting the range and\nvolatility of future prices, thus offering richer insights for trading\nstrategies. Its ability to parse and learn from intricate market patterns\nallows for enhanced decision-making, significantly improving risk management\nand achieving a notable average gain of 0.1193% per 30-minute trade over\nstate-of-the-art models with a simple algorithm using factors such as RSI, ATR,\nand Bollinger Bands. This innovation marks a substantial leap forward in\npredictive analytics within the volatile domain of futures trading."}
{"id": "2505.06032", "pdf": "https://arxiv.org/pdf/2505.06032", "abs": "https://arxiv.org/abs/2505.06032", "authors": ["Leon Eshuijs", "Shihan Wang", "Antske Fokkens"], "title": "Short-circuiting Shortcuts: Mechanistic Investigation of Shortcuts in Text Classification", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Reliance on spurious correlations (shortcuts) has been shown to underlie many\nof the successes of language models. Previous work focused on identifying the\ninput elements that impact prediction. We investigate how shortcuts are\nactually processed within the model's decision-making mechanism. We use actor\nnames in movie reviews as controllable shortcuts with known impact on the\noutcome. We use mechanistic interpretability methods and identify specific\nattention heads that focus on shortcuts. These heads gear the model towards a\nlabel before processing the complete input, effectively making premature\ndecisions that bypass contextual analysis. Based on these findings, we\nintroduce Head-based Token Attribution (HTA), which traces intermediate\ndecisions back to input tokens. We show that HTA is effective in detecting\nshortcuts in LLMs and enables targeted mitigation by selectively deactivating\nshortcut-related attention heads."}
{"id": "2505.05748", "pdf": "https://arxiv.org/pdf/2505.05748", "abs": "https://arxiv.org/abs/2505.05748", "authors": ["Huan Yan", "Junjie Hu"], "title": "kFuse: A novel density based agglomerative clustering", "categories": ["cs.CV"], "comment": "13 pages, 11 figures", "summary": "Agglomerative clustering has emerged as a vital tool in data analysis due to\nits intuitive and flexible characteristics. However, existing agglomerative\nclustering methods often involve additional parameters for sub-cluster\npartitioning and inter-cluster similarity assessment. This necessitates\ndifferent parameter settings across various datasets, which is undoubtedly\nchallenging in the absence of prior knowledge. Moreover, existing agglomerative\nclustering techniques are constrained by the calculation method of connection\ndistance, leading to unstable clustering results. To address these issues, this\npaper introduces a novel density-based agglomerative clustering method, termed\nkFuse. kFuse comprises four key components: (1) sub-cluster partitioning based\non natural neighbors; (2) determination of boundary connectivity between\nsub-clusters through the computation of adjacent samples and shortest\ndistances; (3) assessment of density similarity between sub-clusters via the\ncalculation of mean density and variance; and (4) establishment of merging\nrules between sub-clusters based on boundary connectivity and density\nsimilarity. kFuse requires the specification of the number of clusters only at\nthe final merging stage. Additionally, by comprehensively considering adjacent\nsamples, distances, and densities among different sub-clusters, kFuse\nsignificantly enhances accuracy during the merging phase, thereby greatly\nimproving its identification capability. Experimental results on both synthetic\nand real-world datasets validate the effectiveness of kFuse."}
{"id": "2505.05599", "pdf": "https://arxiv.org/pdf/2505.05599", "abs": "https://arxiv.org/abs/2505.05599", "authors": ["Seraj Al Mahmud Mostafa", "Chenxi Wang", "Jia Yue", "Yuta Hozumi", "Jianwu Wang"], "title": "Enhancing Satellite Object Localization with Dilated Convolutions and Attention-aided Spatial Pooling", "categories": ["cs.CV", "cs.AI"], "comment": "This paper has been accepted to International conference on Advanced\n  Machine Learning and Data Science (AMLDS) 2025", "summary": "Object localization in satellite imagery is particularly challenging due to\nthe high variability of objects, low spatial resolution, and interference from\nnoise and dominant features such as clouds and city lights. In this research,\nwe focus on three satellite datasets: upper atmospheric Gravity Waves (GW),\nmesospheric Bores (Bore), and Ocean Eddies (OE), each presenting its own unique\nchallenges. These challenges include the variability in the scale and\nappearance of the main object patterns, where the size, shape, and feature\nextent of objects of interest can differ significantly. To address these\nchallenges, we introduce YOLO-DCAP, a novel enhanced version of YOLOv5 designed\nto improve object localization in these complex scenarios. YOLO-DCAP\nincorporates a Multi-scale Dilated Residual Convolution (MDRC) block to capture\nmulti-scale features at scale with varying dilation rates, and an\nAttention-aided Spatial Pooling (AaSP) module to focus on the global relevant\nspatial regions, enhancing feature selection. These structural improvements\nhelp to better localize objects in satellite imagery. Experimental results\ndemonstrate that YOLO-DCAP significantly outperforms both the YOLO base model\nand state-of-the-art approaches, achieving an average improvement of 20.95% in\nmAP50 and 32.23% in IoU over the base model, and 7.35% and 9.84% respectively\nover state-of-the-art alternatives, consistently across all three satellite\ndatasets. These consistent gains across all three satellite datasets highlight\nthe robustness and generalizability of the proposed approach. Our code is open\nsourced at\nhttps://github.com/AI-4-atmosphere-remote-sensing/satellite-object-localization."}
{"id": "2505.06107", "pdf": "https://arxiv.org/pdf/2505.06107", "abs": "https://arxiv.org/abs/2505.06107", "authors": ["Faeze Ghorbanpour", "Thiago Zordan Malaguth", "Aliakbar Akbaritabar"], "title": "Differentiating Emigration from Return Migration of Scholars Using Name-Based Nationality Detection Models", "categories": ["cs.DL", "cs.CL", "cs.MM"], "comment": "Accepted to appear @ ICWSM 2025. The link to the camera-ready paper\n  will be added soon", "summary": "Most web and digital trace data do not include information about an\nindividual's nationality due to privacy concerns. The lack of data on\nnationality can create challenges for migration research. It can lead to a\nleft-censoring issue since we are uncertain about the migrant's country of\norigin. Once we observe an emigration event, if we know the nationality, we can\ndifferentiate it from return migration. We propose methods to detect the\nnationality with the least available data, i.e., full names. We use the\ndetected nationality in comparison with the country of academic origin, which\nis a common approach in studying the migration of researchers. We gathered 2.6\nmillion unique name-nationality pairs from Wikipedia and categorized them into\nfamilies of nationalities with three granularity levels to use as our training\ndata. Using a character-based machine learning model, we achieved a weighted F1\nscore of 84% for the broadest and 67% for the most granular, country-level\ncategorization. In our empirical study, we used the trained and tested model to\nassign nationality to 8+ million scholars' full names in Scopus data. Our\nresults show that using the country of first publication as a proxy for\nnationality underestimates the size of return flows, especially for countries\nwith a more diverse academic workforce, such as the USA, Australia, and Canada.\nWe found that around 48% of emigration from the USA was return migration once\nwe used the country of name origin, in contrast to 33% based on academic\norigin. In the most recent period, 79% of scholars whose affiliation has\nconsistently changed from the USA to China, and are considered emigrants, have\nChinese names in contrast to 41% with a Chinese academic origin. Our proposed\nmethods for addressing left-censoring issues are beneficial for other research\nthat uses digital trace data to study migration."}
{"id": "2505.05752", "pdf": "https://arxiv.org/pdf/2505.05752", "abs": "https://arxiv.org/abs/2505.05752", "authors": ["Amin Ghafourian", "Andrew Lee", "Dechen Gao", "Tyler Beer", "Kin Yen", "Iman Soltani"], "title": "Automating Infrastructure Surveying: A Framework for Geometric Measurements and Compliance Assessment Using Point Cloud Data", "categories": ["cs.CV", "cs.CY", "cs.LG", "cs.RO", "eess.IV"], "comment": "19 pages, 15 figures, 4 tables", "summary": "Automation can play a prominent role in improving efficiency, accuracy, and\nscalability in infrastructure surveying and assessing construction and\ncompliance standards. This paper presents a framework for automation of\ngeometric measurements and compliance assessment using point cloud data. The\nproposed approach integrates deep learning-based detection and segmentation, in\nconjunction with geometric and signal processing techniques, to automate\nsurveying tasks. As a proof of concept, we apply this framework to\nautomatically evaluate the compliance of curb ramps with the Americans with\nDisabilities Act (ADA), demonstrating the utility of point cloud data in survey\nautomation. The method leverages a newly collected, large annotated dataset of\ncurb ramps, made publicly available as part of this work, to facilitate robust\nmodel training and evaluation. Experimental results, including comparison with\nmanual field measurements of several ramps, validate the accuracy and\nreliability of the proposed method, highlighting its potential to significantly\nreduce manual effort and improve consistency in infrastructure assessment.\nBeyond ADA compliance, the proposed framework lays the groundwork for broader\napplications in infrastructure surveying and automated construction evaluation,\npromoting wider adoption of point cloud data in these domains. The annotated\ndatabase, manual ramp survey data, and developed algorithms are publicly\navailable on the project's GitHub page:\nhttps://github.com/Soltanilara/SurveyAutomation."}
{"id": "2505.05622", "pdf": "https://arxiv.org/pdf/2505.05622", "abs": "https://arxiv.org/abs/2505.05622", "authors": ["Weichen Zhang", "Chen Gao", "Shiquan Yu", "Ruiying Peng", "Baining Zhao", "Qian Zhang", "Jinqiang Cui", "Xinlei Chen", "Yong Li"], "title": "CityNavAgent: Aerial Vision-and-Language Navigation with Hierarchical Semantic Planning and Global Memory", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Aerial vision-and-language navigation (VLN), requiring drones to interpret\nnatural language instructions and navigate complex urban environments, emerges\nas a critical embodied AI challenge that bridges human-robot interaction, 3D\nspatial reasoning, and real-world deployment. Although existing ground VLN\nagents achieved notable results in indoor and outdoor settings, they struggle\nin aerial VLN due to the absence of predefined navigation graphs and the\nexponentially expanding action space in long-horizon exploration. In this work,\nwe propose \\textbf{CityNavAgent}, a large language model (LLM)-empowered agent\nthat significantly reduces the navigation complexity for urban aerial VLN.\nSpecifically, we design a hierarchical semantic planning module (HSPM) that\ndecomposes the long-horizon task into sub-goals with different semantic levels.\nThe agent reaches the target progressively by achieving sub-goals with\ndifferent capacities of the LLM. Additionally, a global memory module storing\nhistorical trajectories into a topological graph is developed to simplify\nnavigation for visited targets. Extensive benchmark experiments show that our\nmethod achieves state-of-the-art performance with significant improvement.\nFurther experiments demonstrate the effectiveness of different modules of\nCityNavAgent for aerial VLN in continuous city environments. The code is\navailable at \\href{https://github.com/VinceOuti/CityNavAgent}{link}."}
{"id": "2505.06184", "pdf": "https://arxiv.org/pdf/2505.06184", "abs": "https://arxiv.org/abs/2505.06184", "authors": ["Vahid Rahimzadeh", "Ali Hamzehpour", "Azadeh Shakery", "Masoud Asadpour"], "title": "From Millions of Tweets to Actionable Insights: Leveraging LLMs for User Profiling", "categories": ["cs.SI", "cs.CL", "cs.IR", "I.2.7"], "comment": "Accepted at MisD @ AAAI ICWSM 2025", "summary": "Social media user profiling through content analysis is crucial for tasks\nlike misinformation detection, engagement prediction, hate speech monitoring,\nand user behavior modeling. However, existing profiling techniques, including\ntweet summarization, attribute-based profiling, and latent representation\nlearning, face significant limitations: they often lack transferability,\nproduce non-interpretable features, require large labeled datasets, or rely on\nrigid predefined categories that limit adaptability. We introduce a novel large\nlanguage model (LLM)-based approach that leverages domain-defining statements,\nwhich serve as key characteristics outlining the important pillars of a domain\nas foundations for profiling. Our two-stage method first employs\nsemi-supervised filtering with a domain-specific knowledge base, then generates\nboth abstractive (synthesized descriptions) and extractive (representative\ntweet selections) user profiles. By harnessing LLMs' inherent knowledge with\nminimal human validation, our approach is adaptable across domains while\nreducing the need for large labeled datasets. Our method generates\ninterpretable natural language user profiles, condensing extensive user data\ninto a scale that unlocks LLMs' reasoning and knowledge capabilities for\ndownstream social network tasks. We contribute a Persian political Twitter (X)\ndataset and an LLM-based evaluation framework with human validation.\nExperimental results show our method significantly outperforms state-of-the-art\nLLM-based and traditional methods by 9.8%, demonstrating its effectiveness in\ncreating flexible, adaptable, and interpretable user profiles."}
{"id": "2505.05759", "pdf": "https://arxiv.org/pdf/2505.05759", "abs": "https://arxiv.org/abs/2505.05759", "authors": ["Fangxue Liu", "Lei Fan"], "title": "A review of advancements in low-light image enhancement using deep learning", "categories": ["cs.CV"], "comment": null, "summary": "In low-light environments, the performance of computer vision algorithms\noften deteriorates significantly, adversely affecting key vision tasks such as\nsegmentation, detection, and classification. With the rapid advancement of deep\nlearning, its application to low-light image processing has attracted\nwidespread attention and seen significant progress in recent years. However,\nthere remains a lack of comprehensive surveys that systematically examine how\nrecent deep-learning-based low-light image enhancement methods function and\nevaluate their effectiveness in enhancing downstream vison tasks. To address\nthis gap, this review provides a detailed elaboration on how various recent\napproaches (from 2020) operate and their enhancement mechanisms, supplemented\nwith clear illustrations. It also investigates the impact of different\nenhancement techniques on subsequent vision tasks, critically analyzing their\nstrengths and limitations. Additionally, it proposes future research\ndirections. This review serves as a useful reference for determining low-light\nimage enhancement techniques and optimizing vision task performance in\nlow-light conditions."}
{"id": "2505.05625", "pdf": "https://arxiv.org/pdf/2505.05625", "abs": "https://arxiv.org/abs/2505.05625", "authors": ["Wenqing Peng", "Zhi-Song Liu", "Michael Boy"], "title": "SPIN-ODE: Stiff Physics-Informed Neural ODE for Chemical Reaction Rate Estimation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Estimating rate constants from complex chemical reactions is essential for\nadvancing detailed chemistry. However, the stiffness inherent in real-world\natmospheric chemistry systems poses severe challenges, leading to training\ninstability and poor convergence that hinder effective rate constant estimation\nusing learning-based approaches. To address this, we propose a Stiff\nPhysics-Informed Neural ODE framework (SPIN-ODE) for chemical reaction\nmodelling. Our method introduces a three-stage optimisation process: first, a\nlatent neural ODE learns the continuous and differentiable trajectory between\nchemical concentrations and their time derivatives; second, an explicit\nChemical Reaction Neural Network (CRNN) extracts the underlying rate\ncoefficients based on the learned dynamics; and third, fine-tune CRNN using a\nneural ODE solver to further improve rate coefficient estimation. Extensive\nexperiments on both synthetic and newly proposed real-world datasets validate\nthe effectiveness and robustness of our approach. As the first work on stiff\nNeural ODEs for chemical rate coefficient discovery, our study opens promising\ndirections for integrating neural networks with detailed chemistry."}
{"id": "2505.06191", "pdf": "https://arxiv.org/pdf/2505.06191", "abs": "https://arxiv.org/abs/2505.06191", "authors": ["Jiayuan Mao", "Joshua B. Tenenbaum", "Jiajun Wu"], "title": "Neuro-Symbolic Concepts", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.RO"], "comment": "To appear in Communications of the ACM", "summary": "This article presents a concept-centric paradigm for building agents that can\nlearn continually and reason flexibly. The concept-centric agent utilizes a\nvocabulary of neuro-symbolic concepts. These concepts, such as object,\nrelation, and action concepts, are grounded on sensory inputs and actuation\noutputs. They are also compositional, allowing for the creation of novel\nconcepts through their structural combination. To facilitate learning and\nreasoning, the concepts are typed and represented using a combination of\nsymbolic programs and neural network representations. Leveraging such\nneuro-symbolic concepts, the agent can efficiently learn and recombine them to\nsolve various tasks across different domains, ranging from 2D images, videos,\n3D scenes, and robotic manipulation tasks. This concept-centric framework\noffers several advantages, including data efficiency, compositional\ngeneralization, continual learning, and zero-shot transfer."}
{"id": "2505.05804", "pdf": "https://arxiv.org/pdf/2505.05804", "abs": "https://arxiv.org/abs/2505.05804", "authors": ["Xi Xiao", "Yunbei Zhang", "Thanh-Huy Nguyen", "Ba-Thinh Lam", "Janet Wang", "Jihun Hamm", "Tianyang Wang", "Xingjian Li", "Xiao Wang", "Hao Xu", "Tianming Liu", "Min Xu"], "title": "Describe Anything in Medical Images", "categories": ["cs.CV"], "comment": null, "summary": "Localized image captioning has made significant progress with models like the\nDescribe Anything Model (DAM), which can generate detailed region-specific\ndescriptions without explicit region-text supervision. However, such\ncapabilities have yet to be widely applied to specialized domains like medical\nimaging, where diagnostic interpretation relies on subtle regional findings\nrather than global understanding. To mitigate this gap, we propose MedDAM, the\nfirst comprehensive framework leveraging large vision-language models for\nregion-specific captioning in medical images. MedDAM employs medical\nexpert-designed prompts tailored to specific imaging modalities and establishes\na robust evaluation benchmark comprising a customized assessment protocol, data\npre-processing pipeline, and specialized QA template library. This benchmark\nevaluates both MedDAM and other adaptable large vision-language models,\nfocusing on clinical factuality through attribute-level verification tasks,\nthereby circumventing the absence of ground-truth region-caption pairs in\nmedical datasets. Extensive experiments on the VinDr-CXR, LIDC-IDRI, and\nSkinCon datasets demonstrate MedDAM's superiority over leading peers (including\nGPT-4o, Claude 3.7 Sonnet, LLaMA-3.2 Vision, Qwen2.5-VL, GPT-4Rol, and\nOMG-LLaVA) in the task, revealing the importance of region-level semantic\nalignment in medical image understanding and establishing MedDAM as a promising\nfoundation for clinical vision-language integration."}
{"id": "2505.05626", "pdf": "https://arxiv.org/pdf/2505.05626", "abs": "https://arxiv.org/abs/2505.05626", "authors": ["Aarti Ghatkesar", "Uddeshya Upadhyay", "Ganesh Venkatesh"], "title": "Looking Beyond Language Priors: Enhancing Visual Comprehension and Attention in Multimodal Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Achieving deep alignment between vision and language remains a central\nchallenge for Multimodal Large Language Models (MLLMs). These models often fail\nto fully leverage visual input, defaulting to strong language priors. Our\napproach first provides insights into how MLLMs internally build visual\nunderstanding of image regions and then introduces techniques to amplify this\ncapability. Specifically, we explore techniques designed both to deepen the\nmodel's understanding of visual content and to ensure that these visual\ninsights actively guide language generation. We demonstrate the superior\nmultimodal understanding of our resultant model through a detailed upstream\nanalysis quantifying its ability to predict visually-dependent tokens as well\nas 10 pt boost on visually challenging tasks."}
{"id": "2505.05806", "pdf": "https://arxiv.org/pdf/2505.05806", "abs": "https://arxiv.org/abs/2505.05806", "authors": ["Kaili Qi", "Wenli Yang", "Ye Li", "Zhongyi Huang"], "title": "Image Segmentation via Variational Model Based Tailored UNet: A Deep Variational Framework", "categories": ["cs.CV"], "comment": null, "summary": "Traditional image segmentation methods, such as variational models based on\npartial differential equations (PDEs), offer strong mathematical\ninterpretability and precise boundary modeling, but often suffer from\nsensitivity to parameter settings and high computational costs. In contrast,\ndeep learning models such as UNet, which are relatively lightweight in\nparameters, excel in automatic feature extraction but lack theoretical\ninterpretability and require extensive labeled data. To harness the\ncomplementary strengths of both paradigms, we propose Variational Model Based\nTailored UNet (VM_TUNet), a novel hybrid framework that integrates the\nfourth-order modified Cahn-Hilliard equation with the deep learning backbone of\nUNet, which combines the interpretability and edge-preserving properties of\nvariational methods with the adaptive feature learning of neural networks.\nSpecifically, a data-driven operator is introduced to replace manual parameter\ntuning, and we incorporate the tailored finite point method (TFPM) to enforce\nhigh-precision boundary preservation. Experimental results on benchmark\ndatasets demonstrate that VM_TUNet achieves superior segmentation performance\ncompared to existing approaches, especially for fine boundary delineation."}
{"id": "2505.05638", "pdf": "https://arxiv.org/pdf/2505.05638", "abs": "https://arxiv.org/abs/2505.05638", "authors": ["Mohamed-Khalil Bouzidi", "Christian Schlauch", "Nicole Scheuerer", "Yue Yao", "Nadja Klein", "Daniel GÃ¶hring", "JÃ¶rg Reichardt"], "title": "Closing the Loop: Motion Prediction Models beyond Open-Loop Benchmarks", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "Fueled by motion prediction competitions and benchmarks, recent years have\nseen the emergence of increasingly large learning based prediction models, many\nwith millions of parameters, focused on improving open-loop prediction accuracy\nby mere centimeters. However, these benchmarks fail to assess whether such\nimprovements translate to better performance when integrated into an autonomous\ndriving stack. In this work, we systematically evaluate the interplay between\nstate-of-the-art motion predictors and motion planners. Our results show that\nhigher open-loop accuracy does not always correlate with better closed-loop\ndriving behavior and that other factors, such as temporal consistency of\npredictions and planner compatibility, also play a critical role. Furthermore,\nwe investigate downsized variants of these models, and, surprisingly, find that\nin some cases models with up to 86% fewer parameters yield comparable or even\nsuperior closed-loop driving performance. Our code is available at\nhttps://github.com/continental/pred2plan."}
{"id": "2505.05829", "pdf": "https://arxiv.org/pdf/2505.05829", "abs": "https://arxiv.org/abs/2505.05829", "authors": ["Zhiyuan Chen", "Keyi Li", "Yifan Jia", "Le Ye", "Yufei Ma"], "title": "Accelerating Diffusion Transformer via Increment-Calibrated Caching with Channel-Aware Singular Value Decomposition", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": "accepted by CVPR2025", "summary": "Diffusion transformer (DiT) models have achieved remarkable success in image\ngeneration, thanks for their exceptional generative capabilities and\nscalability. Nonetheless, the iterative nature of diffusion models (DMs)\nresults in high computation complexity, posing challenges for deployment.\nAlthough existing cache-based acceleration methods try to utilize the inherent\ntemporal similarity to skip redundant computations of DiT, the lack of\ncorrection may induce potential quality degradation. In this paper, we propose\nincrement-calibrated caching, a training-free method for DiT acceleration,\nwhere the calibration parameters are generated from the pre-trained model\nitself with low-rank approximation. To deal with the possible correction\nfailure arising from outlier activations, we introduce channel-aware Singular\nValue Decomposition (SVD), which further strengthens the calibration effect.\nExperimental results show that our method always achieve better performance\nthan existing naive caching methods with a similar computation resource budget.\nWhen compared with 35-step DDIM, our method eliminates more than 45%\ncomputation and improves IS by 12 at the cost of less than 0.06 FID increase.\nCode is available at https://github.com/ccccczzy/icc."}
{"id": "2505.05665", "pdf": "https://arxiv.org/pdf/2505.05665", "abs": "https://arxiv.org/abs/2505.05665", "authors": ["Neeloy Chakraborty", "John Pohovey", "Melkior Ornik", "Katherine Driggs-Campbell"], "title": "Adaptive Stress Testing Black-Box LLM Planners", "categories": ["cs.RO", "cs.AI", "cs.CL"], "comment": "26 pages, 16 figures, 4 tables", "summary": "Large language models (LLMs) have recently demonstrated success in\ngeneralizing across decision-making tasks including planning, control and\nprediction, but their tendency to hallucinate unsafe and undesired outputs\nposes risks. We argue that detecting such failures is necessary, especially in\nsafety-critical scenarios. Existing black-box methods often detect\nhallucinations by identifying inconsistencies across multiple samples. Many of\nthese approaches typically introduce prompt perturbations like randomizing\ndetail order or generating adversarial inputs, with the intuition that a\nconfident model should produce stable outputs. We first perform a manual case\nstudy showing that other forms of perturbations (e.g., adding noise, removing\nsensor details) cause LLMs to hallucinate in a driving environment. We then\npropose a novel method for efficiently searching the space of prompt\nperturbations using Adaptive Stress Testing (AST) with Monte-Carlo Tree Search\n(MCTS). Our AST formulation enables discovery of scenarios and prompts that\ncause language models to act with high uncertainty. By generating MCTS prompt\nperturbation trees across diverse scenarios, we show that offline analyses can\nbe used at runtime to automatically generate prompts that influence model\nuncertainty, and to inform real-time trust assessments of an LLM."}
{"id": "2505.05834", "pdf": "https://arxiv.org/pdf/2505.05834", "abs": "https://arxiv.org/abs/2505.05834", "authors": ["Chunlai Dong", "Haochao Ying", "Qibo Qiu", "Jinhong Wang", "Danny Chen", "Jian Wu"], "title": "Dual-level Fuzzy Learning with Patch Guidance for Image Ordinal Regression", "categories": ["cs.CV"], "comment": "Accepted by IJCAI 2025", "summary": "Ordinal regression bridges regression and classification by assigning objects\nto ordered classes. While human experts rely on discriminative patch-level\nfeatures for decisions, current approaches are limited by the availability of\nonly image-level ordinal labels, overlooking fine-grained patch-level\ncharacteristics. In this paper, we propose a Dual-level Fuzzy Learning with\nPatch Guidance framework, named DFPG that learns precise feature-based grading\nboundaries from ambiguous ordinal labels, with patch-level supervision.\nSpecifically, we propose patch-labeling and filtering strategies to enable the\nmodel to focus on patch-level features exclusively with only image-level\nordinal labels available. We further design a dual-level fuzzy learning module,\nwhich leverages fuzzy logic to quantitatively capture and handle label\nambiguity from both patch-wise and channel-wise perspectives. Extensive\nexperiments on various image ordinal regression datasets demonstrate the\nsuperiority of our proposed method, further confirming its ability in\ndistinguishing samples from difficult-to-classify categories. The code is\navailable at https://github.com/ZJUMAI/DFPG-ord."}
{"id": "2505.05666", "pdf": "https://arxiv.org/pdf/2505.05666", "abs": "https://arxiv.org/abs/2505.05666", "authors": ["Alexander Most", "Joseph Winjum", "Ayan Biswas", "Shawn Jones", "Nishath Rajiv Ranasinghe", "Dan O'Malley", "Manish Bhattarai"], "title": "Lost in OCR Translation? Vision-Based Approaches to Robust Document Retrieval", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has become a popular technique for\nenhancing the reliability and utility of Large Language Models (LLMs) by\ngrounding responses in external documents. Traditional RAG systems rely on\nOptical Character Recognition (OCR) to first process scanned documents into\ntext. However, even state-of-the-art OCRs can introduce errors, especially in\ndegraded or complex documents. Recent vision-language approaches, such as\nColPali, propose direct visual embedding of documents, eliminating the need for\nOCR. This study presents a systematic comparison between a vision-based RAG\nsystem (ColPali) and more traditional OCR-based pipelines utilizing Llama 3.2\n(90B) and Nougat OCR across varying document qualities. Beyond conventional\nretrieval accuracy metrics, we introduce a semantic answer evaluation benchmark\nto assess end-to-end question-answering performance. Our findings indicate that\nwhile vision-based RAG performs well on documents it has been fine-tuned on,\nOCR-based RAG is better able to generalize to unseen documents of varying\nquality. We highlight the key trade-offs between computational efficiency and\nsemantic accuracy, offering practical guidance for RAG practitioners in\nselecting between OCR-dependent and vision-based document retrieval systems in\nproduction environments."}
{"id": "2505.05845", "pdf": "https://arxiv.org/pdf/2505.05845", "abs": "https://arxiv.org/abs/2505.05845", "authors": ["Guohao Lin", "Shidong Pan", "Rasul Khanbayov", "Changxi Yang", "Ani Khaloian-Sarnaghi", "Andriy Kovryga"], "title": "Automated Knot Detection and Pairing for Wood Analysis in the Timber Industry", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Knots in wood are critical to both aesthetics and structural integrity,\nmaking their detection and pairing essential in timber processing. However,\ntraditional manual annotation was labor-intensive and inefficient,\nnecessitating automation. This paper proposes a lightweight and fully automated\npipeline for knot detection and pairing based on machine learning techniques.\nIn the detection stage, high-resolution surface images of wooden boards were\ncollected using industrial-grade cameras, and a large-scale dataset was\nmanually annotated and preprocessed. After the transfer learning, the YOLOv8l\nachieves an mAP@0.5 of 0.887. In the pairing stage, detected knots were\nanalyzed and paired based on multidimensional feature extraction. A triplet\nneural network was used to map the features into a latent space, enabling\nclustering algorithms to identify and pair corresponding knots. The triplet\nnetwork with learnable weights achieved a pairing accuracy of 0.85. Further\nanalysis revealed that he distances from the knot's start and end points to the\nbottom of the wooden board, and the longitudinal coordinates play crucial roles\nin achieving high pairing accuracy. Our experiments validate the effectiveness\nof the proposed solution, demonstrating the potential of AI in advancing wood\nscience and industry."}
{"id": "2505.05683", "pdf": "https://arxiv.org/pdf/2505.05683", "abs": "https://arxiv.org/abs/2505.05683", "authors": ["Udaya Allani"], "title": "Interactive Diabetes Risk Prediction Using Explainable Machine Learning: A Dash-Based Approach with SHAP, LIME, and Comorbidity Insights", "categories": ["cs.LG", "cs.AI", "I.2.1; I.5.2; J.3"], "comment": "16 pages, 21 figures, submitted as a preprint for academic\n  dissemination", "summary": "This study presents a web-based interactive health risk prediction tool\ndesigned to assess diabetes risk using machine learning models. Built on the\n2015 CDC BRFSS dataset, the study evaluates models including Logistic\nRegression, Random Forest, XGBoost, LightGBM, KNN, and Neural Networks under\noriginal, SMOTE, and undersampling strategies. LightGBM with undersampling\nachieved the best recall, making it ideal for risk detection. The tool\nintegrates SHAP and LIME to explain predictions and highlights comorbidity\ncorrelations using Pearson analysis. A Dash-based UI enables user-friendly\ninteraction with model predictions, personalized suggestions, and feature\ninsights, supporting data-driven health awareness."}
{"id": "2505.05848", "pdf": "https://arxiv.org/pdf/2505.05848", "abs": "https://arxiv.org/abs/2505.05848", "authors": ["Yue Yin", "Enze Tao", "Weijian Deng", "Dylan Campbell"], "title": "RefRef: A Synthetic Dataset and Benchmark for Reconstructing Refractive and Reflective Objects", "categories": ["cs.CV"], "comment": null, "summary": "Modern 3D reconstruction and novel view synthesis approaches have\ndemonstrated strong performance on scenes with opaque Lambertian objects.\nHowever, most assume straight light paths and therefore cannot properly handle\nrefractive and reflective materials. Moreover, datasets specialized for these\neffects are limited, stymieing efforts to evaluate performance and develop\nsuitable techniques. In this work, we introduce a synthetic RefRef dataset and\nbenchmark for reconstructing scenes with refractive and reflective objects from\nposed images. Our dataset has 50 such objects of varying complexity, from\nsingle-material convex shapes to multi-material non-convex shapes, each placed\nin three different background types, resulting in 150 scenes. We also propose\nan oracle method that, given the object geometry and refractive indices,\ncalculates accurate light paths for neural rendering, and an approach based on\nthis that avoids these assumptions. We benchmark these against several\nstate-of-the-art methods and show that all methods lag significantly behind the\noracle, highlighting the challenges of the task and dataset."}
{"id": "2505.05704", "pdf": "https://arxiv.org/pdf/2505.05704", "abs": "https://arxiv.org/abs/2505.05704", "authors": ["Julia Shuieh", "Prasann Singhal", "Apaar Shanker", "John Heyer", "George Pu", "Samuel Denton"], "title": "Assessing Robustness to Spurious Correlations in Post-Training Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "ICLR '25 Workshop on Spurious Correlation and Shortcut Learning", "summary": "Supervised and preference-based fine-tuning techniques have become popular\nfor aligning large language models (LLMs) with user intent and correctness\ncriteria. However, real-world training data often exhibits spurious\ncorrelations -- arising from biases, dataset artifacts, or other \"shortcut\"\nfeatures -- that can compromise a model's performance or generalization. In\nthis paper, we systematically evaluate three post-training algorithms --\nSupervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and KTO\n(Kahneman-Tversky Optimization) -- across a diverse set of synthetic tasks and\nspuriousness conditions. Our tasks span mathematical reasoning, constrained\ninstruction-following, and document-grounded question answering. We vary the\ndegree of spurious correlation (10% vs. 90%) and investigate two forms of\nartifacts: \"Feature Ambiguity\" and \"Distributional Narrowness.\" Our results\nshow that the models often but not always degrade under higher spuriousness.\nThe preference-based methods (DPO/KTO) can demonstrate relative robustness in\nmathematical reasoning tasks. By contrast, SFT maintains stronger performance\nin complex, context-intensive tasks. These findings highlight that no single\npost-training strategy universally outperforms in all scenarios; the best\nchoice depends on the type of target task and the nature of spurious\ncorrelations."}
{"id": "2505.05853", "pdf": "https://arxiv.org/pdf/2505.05853", "abs": "https://arxiv.org/abs/2505.05853", "authors": ["Tongda Xu", "Jiahao Li", "Bin Li", "Yan Wang", "Ya-Qin Zhang", "Yan Lu"], "title": "PICD: Versatile Perceptual Image Compression with Diffusion Rendering", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Recently, perceptual image compression has achieved significant advancements,\ndelivering high visual quality at low bitrates for natural images. However, for\nscreen content, existing methods often produce noticeable artifacts when\ncompressing text. To tackle this challenge, we propose versatile perceptual\nscreen image compression with diffusion rendering (PICD), a codec that works\nwell for both screen and natural images. More specifically, we propose a\ncompression framework that encodes the text and image separately, and renders\nthem into one image using diffusion model. For this diffusion rendering, we\nintegrate conditional information into diffusion models at three distinct\nlevels: 1). Domain level: We fine-tune the base diffusion model using text\ncontent prompts with screen content. 2). Adaptor level: We develop an efficient\nadaptor to control the diffusion model using compressed image and text as\ninput. 3). Instance level: We apply instance-wise guidance to further enhance\nthe decoding process. Empirically, our PICD surpasses existing perceptual\ncodecs in terms of both text accuracy and perceptual quality. Additionally,\nwithout text conditions, our approach serves effectively as a perceptual codec\nfor natural images."}
{"id": "2505.05710", "pdf": "https://arxiv.org/pdf/2505.05710", "abs": "https://arxiv.org/abs/2505.05710", "authors": ["Wooyoung Jeong", "Hyun Jae Park", "Seonghun Jeong", "Jong Wook Jang", "Tae Hoon Lim", "Dae Seoung Kim"], "title": "HyperspectralMAE: The Hyperspectral Imagery Classification Model using Fourier-Encoded Dual-Branch Masked Autoencoder", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": null, "summary": "Hyperspectral imagery provides rich spectral detail but poses unique\nchallenges because of its high dimensionality in both spatial and spectral\ndomains. We propose \\textit{HyperspectralMAE}, a Transformer-based foundation\nmodel for hyperspectral data that employs a \\textit{dual masking} strategy:\nduring pre-training we randomly occlude 50\\% of spatial patches and 50\\% of\nspectral bands. This forces the model to learn representations capable of\nreconstructing missing information across both dimensions. To encode spectral\norder, we introduce learnable harmonic Fourier positional embeddings based on\nwavelength. The reconstruction objective combines mean-squared error (MSE) with\nthe spectral angle mapper (SAM) to balance pixel-level accuracy and\nspectral-shape fidelity.\n  The resulting model contains about $1.8\\times10^{8}$ parameters and produces\n768-dimensional embeddings, giving it sufficient capacity for transfer\nlearning. We pre-trained HyperspectralMAE on two large hyperspectral corpora --\nNASA EO-1 Hyperion ($\\sim$1\\,600 scenes, $\\sim$$3\\times10^{11}$ pixel spectra)\nand DLR EnMAP Level-0 ($\\sim$1\\,300 scenes, $\\sim$$3\\times10^{11}$ pixel\nspectra) -- and fine-tuned it for land-cover classification on the Indian Pines\nbenchmark. HyperspectralMAE achieves state-of-the-art transfer-learning\naccuracy on Indian Pines, confirming that masked dual-dimensional pre-training\nyields robust spectral-spatial representations. These results demonstrate that\ndual masking and wavelength-aware embeddings advance hyperspectral image\nreconstruction and downstream analysis."}
{"id": "2505.05855", "pdf": "https://arxiv.org/pdf/2505.05855", "abs": "https://arxiv.org/abs/2505.05855", "authors": ["Hongyu Rui", "Yinzhe Wu", "Fanwen Wang", "Jiahao Huang", "Liutao Yang", "Zi Wang", "Guang Yang"], "title": "Decoupling Multi-Contrast Super-Resolution: Pairing Unpaired Synthesis with Implicit Representations", "categories": ["cs.CV"], "comment": null, "summary": "Magnetic Resonance Imaging (MRI) is critical for clinical diagnostics but is\noften limited by long acquisition times and low signal-to-noise ratios,\nespecially in modalities like diffusion and functional MRI. The multi-contrast\nnature of MRI presents a valuable opportunity for cross-modal enhancement,\nwhere high-resolution (HR) modalities can serve as references to boost the\nquality of their low-resolution (LR) counterparts-motivating the development of\nMulti-Contrast Super-Resolution (MCSR) techniques. Prior work has shown that\nleveraging complementary contrasts can improve SR performance; however,\neffective feature extraction and fusion across modalities with varying\nresolutions remains a major challenge. Moreover, existing MCSR methods often\nassume fixed resolution settings and all require large, perfectly paired\ntraining datasets-conditions rarely met in real-world clinical environments. To\naddress these challenges, we propose a novel Modular Multi-Contrast\nSuper-Resolution (MCSR) framework that eliminates the need for paired training\ndata and supports arbitrary upscaling. Our method decouples the MCSR task into\ntwo stages: (1) Unpaired Cross-Modal Synthesis (U-CMS), which translates a\nhigh-resolution reference modality into a synthesized version of the target\ncontrast, and (2) Unsupervised Super-Resolution (U-SR), which reconstructs the\nfinal output using implicit neural representations (INRs) conditioned on\nspatial coordinates. This design enables scale-agnostic and anatomically\nfaithful reconstruction by bridging un-paired cross-modal synthesis with\nunsupervised resolution enhancement. Experiments show that our method achieves\nsuperior performance at 4x and 8x upscaling, with improved fidelity and\nanatomical consistency over existing baselines. Our framework demonstrates\nstrong potential for scalable, subject-specific, and data-efficient MCSR in\nreal-world clinical settings."}
{"id": "2505.05738", "pdf": "https://arxiv.org/pdf/2505.05738", "abs": "https://arxiv.org/abs/2505.05738", "authors": ["Yiming Niu", "Jinliang Deng", "Lulu Zhang", "Zimu Zhou", "Yongxin Tong"], "title": "Accurate and Efficient Multivariate Time Series Forecasting via Offline Clustering", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Accurate and efficient multivariate time series (MTS) forecasting is\nessential for applications such as traffic management and weather prediction,\nwhich depend on capturing long-range temporal dependencies and interactions\nbetween entities. Existing methods, particularly those based on Transformer\narchitectures, compute pairwise dependencies across all time steps, leading to\na computational complexity that scales quadratically with the length of the\ninput. To overcome these challenges, we introduce the Forecaster with Offline\nClustering Using Segments (FOCUS), a novel approach to MTS forecasting that\nsimplifies long-range dependency modeling through the use of prototypes\nextracted via offline clustering. These prototypes encapsulate high-level\nevents in the real-world system underlying the data, summarizing the key\ncharacteristics of similar time segments. In the online phase, FOCUS\ndynamically adapts these patterns to the current input and captures\ndependencies between the input segment and high-level events, enabling both\naccurate and efficient forecasting. By identifying prototypes during the\noffline clustering phase, FOCUS reduces the computational complexity of\nmodeling long-range dependencies in the online phase to linear scaling.\nExtensive experiments across diverse benchmarks demonstrate that FOCUS achieves\nstate-of-the-art accuracy while significantly reducing computational costs."}
{"id": "2505.05870", "pdf": "https://arxiv.org/pdf/2505.05870", "abs": "https://arxiv.org/abs/2505.05870", "authors": ["Yimin Zhou", "Yichong Xia", "Bin Chen", "Baoyi An", "Haoqian Wang", "Zhi Wang", "Yaowei Wang", "Zikun Zhou"], "title": "Towards Facial Image Compression with Consistency Preserving Diffusion Prior", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": null, "summary": "With the widespread application of facial image data across various domains,\nthe efficient storage and transmission of facial images has garnered\nsignificant attention. However, the existing learned face image compression\nmethods often produce unsatisfactory reconstructed image quality at low bit\nrates. Simply adapting diffusion-based compression methods to facial\ncompression tasks results in reconstructed images that perform poorly in\ndownstream applications due to insufficient preservation of high-frequency\ninformation. To further explore the diffusion prior in facial image\ncompression, we propose Facial Image Compression with a Stable Diffusion Prior\n(FaSDiff), a method that preserves consistency through frequency enhancement.\nFaSDiff employs a high-frequency-sensitive compressor in an end-to-end\nframework to capture fine image details and produce robust visual prompts.\nAdditionally, we introduce a hybrid low-frequency enhancement module that\ndisentangles low-frequency facial semantics and stably modulates the diffusion\nprior alongside visual prompts. The proposed modules allow FaSDiff to leverage\ndiffusion priors for superior human visual perception while minimizing\nperformance loss in machine vision due to semantic inconsistency. Extensive\nexperiments show that FaSDiff outperforms state-of-the-art methods in balancing\nhuman visual quality and machine vision accuracy. The code will be released\nafter the paper is accepted."}
{"id": "2505.05753", "pdf": "https://arxiv.org/pdf/2505.05753", "abs": "https://arxiv.org/abs/2505.05753", "authors": ["Bo Ai", "Liu Dai", "Nico Bohlinger", "Dichen Li", "Tongzhou Mu", "Zhanxin Wu", "K. Fay", "Henrik I. Christensen", "Jan Peters", "Hao Su"], "title": "Towards Embodiment Scaling Laws in Robot Locomotion", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "32 pages. Project website: https://embodiment-scaling-laws.github.io/", "summary": "Developing generalist agents that can operate across diverse tasks,\nenvironments, and physical embodiments is a grand challenge in robotics and\nartificial intelligence. In this work, we focus on the axis of embodiment and\ninvestigate embodiment scaling laws$\\unicode{x2013}$the hypothesis that\nincreasing the number of training embodiments improves generalization to unseen\nones. Using robot locomotion as a test bed, we procedurally generate a dataset\nof $\\sim$1,000 varied embodiments, spanning humanoids, quadrupeds, and\nhexapods, and train generalist policies capable of handling diverse observation\nand action spaces on random subsets. We find that increasing the number of\ntraining embodiments improves generalization to unseen ones, and scaling\nembodiments is more effective in enabling embodiment-level generalization than\nscaling data on small, fixed sets of embodiments. Notably, our best policy,\ntrained on the full dataset, zero-shot transfers to novel embodiments in the\nreal world, such as Unitree Go2 and H1. These results represent a step toward\ngeneral embodied intelligence, with potential relevance to adaptive control for\nconfigurable robots, co-design of morphology and control, and beyond."}
{"id": "2505.05892", "pdf": "https://arxiv.org/pdf/2505.05892", "abs": "https://arxiv.org/abs/2505.05892", "authors": ["Alexander Lappe", "Martin A. Giese"], "title": "Register and CLS tokens yield a decoupling of local and global features in large ViTs", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Recent work has shown that the attention maps of the widely popular DINOv2\nmodel exhibit artifacts, which hurt both model interpretability and performance\non dense image tasks. These artifacts emerge due to the model repurposing patch\ntokens with redundant local information for the storage of global image\ninformation. To address this problem, additional register tokens have been\nincorporated in which the model can store such information instead. We\ncarefully examine the influence of these register tokens on the relationship\nbetween global and local image features, showing that while register tokens\nyield cleaner attention maps, these maps do not accurately reflect the\nintegration of local image information in large models. Instead, global\ninformation is dominated by information extracted from register tokens, leading\nto a disconnect between local and global features. Inspired by these findings,\nwe show that the CLS token itself, which can be interpreted as a register,\nleads to a very similar phenomenon in models without explicit register tokens.\nOur work shows that care must be taken when interpreting attention maps of\nlarge ViTs. Further, by clearly attributing the faulty behaviour to register\nand CLS tokens, we show a path towards more interpretable vision models."}
{"id": "2505.05756", "pdf": "https://arxiv.org/pdf/2505.05756", "abs": "https://arxiv.org/abs/2505.05756", "authors": ["Antonio Jimeno Yepes", "Pieter Barnard"], "title": "Evolutionary thoughts: integration of large language models and evolutionary algorithms", "categories": ["cs.NE", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have unveiled remarkable capabilities in\nunderstanding and generating both natural language and code, but LLM reasoning\nis prone to hallucination and struggle with complex, novel scenarios, often\ngetting stuck on partial or incorrect solutions. However, the inherent ability\nof Evolutionary Algorithms (EAs) to explore extensive and complex search spaces\nmakes them particularly effective in scenarios where traditional optimization\nmethodologies may falter. However, EAs explore a vast search space when applied\nto complex problems.\n  To address the computational bottleneck of evaluating large populations,\nparticularly crucial for complex evolutionary tasks, we introduce a highly\nefficient evaluation framework. This implementation maintains compatibility\nwith existing primitive definitions, ensuring the generation of valid\nindividuals.\n  Using LLMs, we propose an enhanced evolutionary search strategy that enables\na more focused exploration of expansive solution spaces. LLMs facilitate the\ngeneration of superior candidate solutions, as evidenced by empirical results\ndemonstrating their efficacy in producing improved outcomes."}
{"id": "2505.05895", "pdf": "https://arxiv.org/pdf/2505.05895", "abs": "https://arxiv.org/abs/2505.05895", "authors": ["Benjamin Raphael Ernhofer", "Daniil Prokhorov", "Jannica Langner", "Dominik Bollmann"], "title": "Leveraging Vision-Language Models for Visual Grounding and Analysis of Automotive UI", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Modern automotive infotainment systems require intelligent and adaptive\nsolutions to handle frequent User Interface (UI) updates and diverse design\nvariations. We introduce a vision-language framework for understanding and\ninteracting with automotive infotainment systems, enabling seamless adaptation\nacross different UI designs. To further support research in this field, we\nrelease AutomotiveUI-Bench-4K, an open-source dataset of 998 images with 4,208\nannotations. Additionally, we present a synthetic data pipeline to generate\ntraining data. We fine-tune a Molmo-7B-based model using Low-Rank Adaptation\n(LoRa) and incorporating reasoning generated by our pipeline, along with visual\ngrounding and evaluation capabilities. The fine-tuned Evaluative Large Action\nModel (ELAM) achieves strong performance on AutomotiveUI-Bench-4K (model and\ndataset are available on Hugging Face) and demonstrating strong cross-domain\ngeneralization, including a +5.2% improvement on ScreenSpot over the baseline\nmodel. Notably, our approach achieves 80.4% average accuracy on ScreenSpot,\nclosely matching or even surpassing specialized models for desktop, mobile, and\nweb, such as ShowUI, despite being trained for the infotainment domain. This\nresearch investigates how data collection and subsequent fine-tuning can lead\nto AI-driven progress within automotive UI understanding and interaction. The\napplied method is cost-efficient and fine-tuned models can be deployed on\nconsumer-grade GPUs."}
{"id": "2505.05762", "pdf": "https://arxiv.org/pdf/2505.05762", "abs": "https://arxiv.org/abs/2505.05762", "authors": ["Junhong Chen", "Ziqi Yang", "Haoyuan G Xu", "Dandan Zhang", "George Mylonas"], "title": "Multi-Agent Systems for Robotic Autonomy with LLMs", "categories": ["cs.RO", "cs.AI"], "comment": "11 pages, 2 figures, 5 tables, submitted for publication", "summary": "Since the advent of Large Language Models (LLMs), various research based on\nsuch models have maintained significant academic attention and impact,\nespecially in AI and robotics. In this paper, we propose a multi-agent\nframework with LLMs to construct an integrated system for robotic task\nanalysis, mechanical design, and path generation. The framework includes three\ncore agents: Task Analyst, Robot Designer, and Reinforcement Learning Designer.\nOutputs are formatted as multimodal results, such as code files or technical\nreports, for stronger understandability and usability. To evaluate\ngeneralizability comparatively, we conducted experiments with models from both\nGPT and DeepSeek. Results demonstrate that the proposed system can design\nfeasible robots with control strategies when appropriate task inputs are\nprovided, exhibiting substantial potential for enhancing the efficiency and\naccessibility of robotic system development in research and industrial\napplications."}
{"id": "2505.05901", "pdf": "https://arxiv.org/pdf/2505.05901", "abs": "https://arxiv.org/abs/2505.05901", "authors": ["Hanzhe Liang", "Aoran Wang", "Jie Zhou", "Xin Jin", "Can Gao", "Jinbao Wang"], "title": "Examining the Source of Defects from a Mechanical Perspective for 3D Anomaly Detection", "categories": ["cs.CV", "cs.AI"], "comment": "26 pages", "summary": "In this paper, we go beyond identifying anomalies only in structural terms\nand think about better anomaly detection motivated by anomaly causes. Most\nanomalies are regarded as the result of unpredictable defective forces from\ninternal and external sources, and their opposite forces are sought to correct\nthe anomalies. We introduced a Mechanics Complementary framework for 3D anomaly\ndetection (MC4AD) to generate internal and external Corrective forces for each\npoint. A Diverse Anomaly-Generation (DA-Gen) module is first proposed to\nsimulate various anomalies. Then, we present a Corrective Force Prediction\nNetwork (CFP-Net) with complementary representations for point-level\nrepresentation to simulate the different contributions of internal and external\ncorrective forces. A combined loss was proposed, including a new symmetric loss\nand an overall loss, to constrain the corrective forces properly. As a\nhighlight, we consider 3D anomaly detection in industry more comprehensively,\ncreating a hierarchical quality control strategy based on a three-way decision\nand contributing a dataset named Anomaly-IntraVariance with intraclass variance\nto evaluate the model. On the proposed and existing five datasets, we obtained\nnine state-of-the-art performers with the minimum parameters and the fastest\ninference speed. The source is available at\nhttps://github.com/hzzzzzhappy/MC4AD"}
{"id": "2505.05768", "pdf": "https://arxiv.org/pdf/2505.05768", "abs": "https://arxiv.org/abs/2505.05768", "authors": ["Weiyi Zhang", "Peranut Chotcomwongse", "Yinwen Li", "Pusheng Xu", "Ruijie Yao", "Lianhao Zhou", "Yuxuan Zhou", "Hui Feng", "Qiping Zhou", "Xinyue Wang", "Shoujin Huang", "Zihao Jin", "Florence H. T. Chung", "Shujun Wang", "Yalin Zheng", "Mingguang He", "Danli Shi", "Paisan Ruamviboonsuk"], "title": "Predicting Diabetic Macular Edema Treatment Responses Using OCT: Dataset and Methods of APTOS Competition", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "42 pages,5 tables, 12 figures, challenge report", "summary": "Diabetic macular edema (DME) significantly contributes to visual impairment\nin diabetic patients. Treatment responses to intravitreal therapies vary,\nhighlighting the need for patient stratification to predict therapeutic\nbenefits and enable personalized strategies. To our knowledge, this study is\nthe first to explore pre-treatment stratification for predicting DME treatment\nresponses. To advance this research, we organized the 2nd Asia-Pacific\nTele-Ophthalmology Society (APTOS) Big Data Competition in 2021. The\ncompetition focused on improving predictive accuracy for anti-VEGF therapy\nresponses using ophthalmic OCT images. We provided a dataset containing tens of\nthousands of OCT images from 2,000 patients with labels across four sub-tasks.\nThis paper details the competition's structure, dataset, leading methods, and\nevaluation metrics. The competition attracted strong scientific community\nparticipation, with 170 teams initially registering and 41 reaching the final\nround. The top-performing team achieved an AUC of 80.06%, highlighting the\npotential of AI in personalized DME treatment and clinical decision-making."}
{"id": "2505.05913", "pdf": "https://arxiv.org/pdf/2505.05913", "abs": "https://arxiv.org/abs/2505.05913", "authors": ["Jianjian Yin", "Yi Chen", "Chengyu Li", "Zhichao Zheng", "Yanhui Gu", "Junsheng Zhou"], "title": "DFEN: Dual Feature Equalization Network for Medical Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Current methods for medical image segmentation primarily focus on extracting\ncontextual feature information from the perspective of the whole image. While\nthese methods have shown effective performance, none of them take into account\nthe fact that pixels at the boundary and regions with a low number of class\npixels capture more contextual feature information from other classes, leading\nto misclassification of pixels by unequal contextual feature information. In\nthis paper, we propose a dual feature equalization network based on the hybrid\narchitecture of Swin Transformer and Convolutional Neural Network, aiming to\naugment the pixel feature representations by image-level equalization feature\ninformation and class-level equalization feature information. Firstly, the\nimage-level feature equalization module is designed to equalize the contextual\ninformation of pixels within the image. Secondly, we aggregate regions of the\nsame class to equalize the pixel feature representations of the corresponding\nclass by class-level feature equalization module. Finally, the pixel feature\nrepresentations are enhanced by learning weights for image-level equalization\nfeature information and class-level equalization feature information. In\naddition, Swin Transformer is utilized as both the encoder and decoder, thereby\nbolstering the ability of the model to capture long-range dependencies and\nspatial correlations. We conducted extensive experiments on Breast Ultrasound\nImages (BUSI), International Skin Imaging Collaboration (ISIC2017), Automated\nCardiac Diagnosis Challenge (ACDC) and PH$^2$ datasets. The experimental\nresults demonstrate that our method have achieved state-of-the-art performance.\nOur code is publicly available at https://github.com/JianJianYin/DFEN."}
{"id": "2505.05777", "pdf": "https://arxiv.org/pdf/2505.05777", "abs": "https://arxiv.org/abs/2505.05777", "authors": ["Domenico Cotroneo", "Giuseppe De Rosa", "Pietro Liguori"], "title": "PyResBugs: A Dataset of Residual Python Bugs for Natural Language-Driven Fault Injection", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "This paper presents PyResBugs, a curated dataset of residual bugs, i.e.,\ndefects that persist undetected during traditional testing but later surface in\nproduction, collected from major Python frameworks. Each bug in the dataset is\npaired with its corresponding fault-free (fixed) version and annotated with\nmulti-level natural language (NL) descriptions. These NL descriptions enable\nnatural language-driven fault injection, offering a novel approach to\nsimulating real-world faults in software systems. By bridging the gap between\nsoftware fault injection techniques and real-world representativeness,\nPyResBugs provides researchers with a high-quality resource for advancing\nAI-driven automated testing in Python systems."}
{"id": "2505.05936", "pdf": "https://arxiv.org/pdf/2505.05936", "abs": "https://arxiv.org/abs/2505.05936", "authors": ["Weihong Li", "Xiaoqiong Liu", "Heng Fan", "Libo Zhang"], "title": "CGTrack: Cascade Gating Network with Hierarchical Feature Aggregation for UAV Tracking", "categories": ["cs.CV"], "comment": "Accepted by ICRA 2025", "summary": "Recent advancements in visual object tracking have markedly improved the\ncapabilities of unmanned aerial vehicle (UAV) tracking, which is a critical\ncomponent in real-world robotics applications. While the integration of\nhierarchical lightweight networks has become a prevalent strategy for enhancing\nefficiency in UAV tracking, it often results in a significant drop in network\ncapacity, which further exacerbates challenges in UAV scenarios, such as\nfrequent occlusions and extreme changes in viewing angles. To address these\nissues, we introduce a novel family of UAV trackers, termed CGTrack, which\ncombines explicit and implicit techniques to expand network capacity within a\ncoarse-to-fine framework. Specifically, we first introduce a Hierarchical\nFeature Cascade (HFC) module that leverages the spirit of feature reuse to\nincrease network capacity by integrating the deep semantic cues with the rich\nspatial information, incurring minimal computational costs while enhancing\nfeature representation. Based on this, we design a novel Lightweight Gated\nCenter Head (LGCH) that utilizes gating mechanisms to decouple target-oriented\ncoordinates from previously expanded features, which contain dense local\ndiscriminative information. Extensive experiments on three challenging UAV\ntracking benchmarks demonstrate that CGTrack achieves state-of-the-art\nperformance while running fast. Code will be available at\nhttps://github.com/Nightwatch-Fox11/CGTrack."}
{"id": "2505.05784", "pdf": "https://arxiv.org/pdf/2505.05784", "abs": "https://arxiv.org/abs/2505.05784", "authors": ["Yang Li", "Zhi Chen", "Steve Yang"], "title": "FlowHFT: Flow Policy Induced Optimal High-Frequency Trading under Diverse Market Conditions", "categories": ["q-fin.TR", "cs.AI", "cs.CE", "q-fin.CP"], "comment": "14 pages, 1 figure, 6 tables, 2 algorithms", "summary": "High-frequency trading (HFT) is an investing strategy that continuously\nmonitors market states and places bid and ask orders at millisecond speeds.\nTraditional HFT approaches fit models with historical data and assume that\nfuture market states follow similar patterns. This limits the effectiveness of\nany single model to the specific conditions it was trained for. Additionally,\nthese models achieve optimal solutions only under specific market conditions,\nsuch as assumptions about stock price's stochastic process, stable order flow,\nand the absence of sudden volatility. Real-world markets, however, are dynamic,\ndiverse, and frequently volatile. To address these challenges, we propose the\nFlowHFT, a novel imitation learning framework based on flow matching policy.\nFlowHFT simultaneously learns strategies from numerous expert models, each\nproficient in particular market scenarios. As a result, our framework can\nadaptively adjust investment decisions according to the prevailing market\nstate. Furthermore, FlowHFT incorporates a grid-search fine-tuning mechanism.\nThis allows it to refine strategies and achieve superior performance even in\ncomplex or extreme market scenarios where expert strategies may be suboptimal.\nWe test FlowHFT in multiple market environments. We first show that flow\nmatching policy is applicable in stochastic market environments, thus enabling\nFlowHFT to learn trading strategies under different market conditions. Notably,\nour single framework consistently achieves performance superior to the best\nexpert for each market condition."}
{"id": "2505.05943", "pdf": "https://arxiv.org/pdf/2505.05943", "abs": "https://arxiv.org/abs/2505.05943", "authors": ["Maan Alhazmi", "Abdulrahman Altahhan"], "title": "Achieving 3D Attention via Triplet Squeeze and Excitation Block", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "The emergence of ConvNeXt and its variants has reaffirmed the conceptual and\nstructural suitability of CNN-based models for vision tasks, re-establishing\nthem as key players in image classification in general, and in facial\nexpression recognition (FER) in particular. In this paper, we propose a new set\nof models that build on these advancements by incorporating a new set of\nattention mechanisms that combines Triplet attention with\nSqueeze-and-Excitation (TripSE) in four different variants. We demonstrate the\neffectiveness of these variants by applying them to the ResNet18, DenseNet and\nConvNext architectures to validate their versatility and impact. Our study\nshows that incorporating a TripSE block in these CNN models boosts their\nperformances, particularly for the ConvNeXt architecture, indicating its\nutility. We evaluate the proposed mechanisms and associated models across four\ndatasets, namely CIFAR100, ImageNet, FER2013 and AffectNet datasets, where\nConvNext with TripSE achieves state-of-the-art results with an accuracy of\n\\textbf{78.27\\%} on the popular FER2013 dataset, a new feat for this dataset."}
{"id": "2505.05794", "pdf": "https://arxiv.org/pdf/2505.05794", "abs": "https://arxiv.org/abs/2505.05794", "authors": ["Renjie Li", "Wenjie Wei", "Qi Xin", "Xiaoli Liu", "Sixuan Mao", "Erik Ma", "Zijian Chen", "Malu Zhang", "Haizhou Li", "Zhaoyu Zhang"], "title": "What Is Next for LLMs? Next-Generation AI Computing Hardware Using Photonic Chips", "categories": ["cs.AR", "cs.AI", "cs.NE"], "comment": "36 pages, 22 figures", "summary": "Large language models (LLMs) are rapidly pushing the limits of contemporary\ncomputing hardware. For example, training GPT-3 has been estimated to consume\naround 1300 MWh of electricity, and projections suggest future models may\nrequire city-scale (gigawatt) power budgets. These demands motivate exploration\nof computing paradigms beyond conventional von Neumann architectures. This\nreview surveys emerging photonic hardware optimized for next-generation\ngenerative AI computing. We discuss integrated photonic neural network\narchitectures (e.g., Mach-Zehnder interferometer meshes, lasers,\nwavelength-multiplexed microring resonators) that perform ultrafast matrix\noperations. We also examine promising alternative neuromorphic devices,\nincluding spiking neural network circuits and hybrid spintronic-photonic\nsynapses, which combine memory and processing. The integration of\ntwo-dimensional materials (graphene, TMDCs) into silicon photonic platforms is\nreviewed for tunable modulators and on-chip synaptic elements.\nTransformer-based LLM architectures (self-attention and feed-forward layers)\nare analyzed in this context, identifying strategies and challenges for mapping\ndynamic matrix multiplications onto these novel hardware substrates. We then\ndissect the mechanisms of mainstream LLMs, such as ChatGPT, DeepSeek, and\nLLaMA, highlighting their architectural similarities and differences. We\nsynthesize state-of-the-art components, algorithms, and integration methods,\nhighlighting key advances and open issues in scaling such systems to mega-sized\nLLM models. We find that photonic computing systems could potentially surpass\nelectronic processors by orders of magnitude in throughput and energy\nefficiency, but require breakthroughs in memory, especially for long-context\nwindows and long token sequences, and in storage of ultra-large datasets."}
{"id": "2505.06002", "pdf": "https://arxiv.org/pdf/2505.06002", "abs": "https://arxiv.org/abs/2505.06002", "authors": ["Congqi Cao", "Peiheng Han", "Yueran zhang", "Yating Yu", "Qinyi Lv", "Lingtong Min", "Yanning zhang"], "title": "Task-Adapter++: Task-specific Adaptation with Order-aware Alignment for Few-shot Action Recognition", "categories": ["cs.CV"], "comment": "arXiv admin note: substantial text overlap with arXiv:2408.00249", "summary": "Large-scale pre-trained models have achieved remarkable success in language\nand image tasks, leading an increasing number of studies to explore the\napplication of pre-trained image models, such as CLIP, in the domain of\nfew-shot action recognition (FSAR). However, current methods generally suffer\nfrom several problems: 1) Direct fine-tuning often undermines the\ngeneralization capability of the pre-trained model; 2) The exploration of\ntask-specific information is insufficient in the visual tasks; 3) The semantic\norder information is typically overlooked during text modeling; 4) Existing\ncross-modal alignment techniques ignore the temporal coupling of multimodal\ninformation. To address these, we propose Task-Adapter++, a parameter-efficient\ndual adaptation method for both image and text encoders. Specifically, to make\nfull use of the variations across different few-shot learning tasks, we design\na task-specific adaptation for the image encoder so that the most\ndiscriminative information can be well noticed during feature extraction.\nFurthermore, we leverage large language models (LLMs) to generate detailed\nsequential sub-action descriptions for each action class, and introduce\nsemantic order adapters into the text encoder to effectively model the\nsequential relationships between these sub-actions. Finally, we develop an\ninnovative fine-grained cross-modal alignment strategy that actively maps\nvisual features to reside in the same temporal stage as semantic descriptions.\nExtensive experiments fully demonstrate the effectiveness and superiority of\nthe proposed method, which achieves state-of-the-art performance on 5\nbenchmarks consistently. The code is open-sourced at\nhttps://github.com/Jaulin-Bage/Task-Adapter-pp."}
{"id": "2505.05796", "pdf": "https://arxiv.org/pdf/2505.05796", "abs": "https://arxiv.org/abs/2505.05796", "authors": ["Xinyu Liang", "Frits de Nijs", "Buser Say", "Hao Wang"], "title": "Human-in-the-Loop AI for HVAC Management Enhancing Comfort and Energy Efficiency", "categories": ["eess.SY", "cs.AI", "cs.SY", "math.OC"], "comment": "ACM e-Energy 2025", "summary": "Heating, Ventilation, and Air Conditioning (HVAC) systems account for\napproximately 38% of building energy consumption globally, making them one of\nthe most energy-intensive services. The increasing emphasis on energy\nefficiency and sustainability, combined with the need for enhanced occupant\ncomfort, presents a significant challenge for traditional HVAC systems. These\nsystems often fail to dynamically adjust to real-time changes in electricity\nmarket rates or individual comfort preferences, leading to increased energy\ncosts and reduced comfort. In response, we propose a Human-in-the-Loop (HITL)\nArtificial Intelligence framework that optimizes HVAC performance by\nincorporating real-time user feedback and responding to fluctuating electricity\nprices. Unlike conventional systems that require predefined information about\noccupancy or comfort levels, our approach learns and adapts based on ongoing\nuser input. By integrating the occupancy prediction model with reinforcement\nlearning, the system improves operational efficiency and reduces energy costs\nin line with electricity market dynamics, thereby contributing to demand\nresponse initiatives. Through simulations, we demonstrate that our method\nachieves significant cost reductions compared to baseline approaches while\nmaintaining or enhancing occupant comfort. This feedback-driven approach\nensures personalized comfort control without the need for predefined settings,\noffering a scalable solution that balances individual preferences with economic\nand environmental goals."}
{"id": "2505.06003", "pdf": "https://arxiv.org/pdf/2505.06003", "abs": "https://arxiv.org/abs/2505.06003", "authors": ["Moritz Vandenhirtz", "Julia E. Vogt"], "title": "From Pixels to Perception: Interpretable Predictions via Instance-wise Grouped Feature Selection", "categories": ["cs.CV", "cs.LG"], "comment": "International Conference on Machine Learning", "summary": "Understanding the decision-making process of machine learning models provides\nvaluable insights into the task, the data, and the reasons behind a model's\nfailures. In this work, we propose a method that performs inherently\ninterpretable predictions through the instance-wise sparsification of input\nimages. To align the sparsification with human perception, we learn the masking\nin the space of semantically meaningful pixel regions rather than on\npixel-level. Additionally, we introduce an explicit way to dynamically\ndetermine the required level of sparsity for each instance. We show empirically\non semi-synthetic and natural image datasets that our inherently interpretable\nclassifier produces more meaningful, human-understandable predictions than\nstate-of-the-art benchmarks."}
{"id": "2505.05799", "pdf": "https://arxiv.org/pdf/2505.05799", "abs": "https://arxiv.org/abs/2505.05799", "authors": ["Haojie Duanmu", "Xiuhong Li", "Zhihang Yuan", "Size Zheng", "Jiangfei Duan", "Xingcheng Zhang", "Dahua Lin"], "title": "MxMoE: Mixed-precision Quantization for MoE with Accuracy and Performance Co-Design", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Mixture-of-Experts (MoE) models face deployment challenges due to their large\nparameter counts and computational demands. We explore quantization for MoE\nmodels and highlight two key insights: 1) linear blocks exhibit varying\nquantization sensitivity, and 2) divergent expert activation frequencies create\nheterogeneous computational characteristics. Based on these observations, we\nintroduce MxMoE, a mixed-precision optimization framework for MoE models that\nconsiders both algorithmic and system perspectives. MxMoE navigates the design\nspace defined by parameter sensitivity, expert activation dynamics, and\nhardware resources to derive efficient mixed-precision configurations.\nAdditionally, MxMoE automatically generates optimized mixed-precision GroupGEMM\nkernels, enabling parallel execution of GEMMs with different precisions.\nEvaluations show that MxMoE outperforms existing methods, achieving 2.4 lower\nWikitext-2 perplexity than GPTQ at 2.25-bit and delivering up to 3.4x speedup\nover full precision, as well as up to 29.4% speedup over uniform quantization\nat equivalent accuracy with 5-bit weight-activation quantization. Our code is\navailable at https://github.com/cat538/MxMoE."}
{"id": "2505.06038", "pdf": "https://arxiv.org/pdf/2505.06038", "abs": "https://arxiv.org/abs/2505.06038", "authors": ["Heng Li", "Xiangping Wu", "Qingcai Chen"], "title": "Document Image Rectification Bases on Self-Adaptive Multitask Fusion", "categories": ["cs.CV"], "comment": null, "summary": "Deformed document image rectification is essential for real-world document\nunderstanding tasks, such as layout analysis and text recognition. However,\ncurrent multi-task methods -- such as background removal, 3D coordinate\nprediction, and text line segmentation -- often overlook the complementary\nfeatures between tasks and their interactions. To address this gap, we propose\na self-adaptive learnable multi-task fusion rectification network named\nSalmRec. This network incorporates an inter-task feature aggregation module\nthat adaptively improves the perception of geometric distortions, enhances\nfeature complementarity, and reduces negative interference. We also introduce a\ngating mechanism to balance features both within global tasks and between local\ntasks effectively. Experimental results on two English benchmarks (DIR300 and\nDocUNet) and one Chinese benchmark (DocReal) demonstrate that our method\nsignificantly improves rectification performance. Ablation studies further\nhighlight the positive impact of different tasks on dewarping and the\neffectiveness of our proposed module."}
{"id": "2505.05849", "pdf": "https://arxiv.org/pdf/2505.05849", "abs": "https://arxiv.org/abs/2505.05849", "authors": ["Zhun Wang", "Vincent Siu", "Zhe Ye", "Tianneng Shi", "Yuzhou Nie", "Xuandong Zhao", "Chenguang Wang", "Wenbo Guo", "Dawn Song"], "title": "AgentXploit: End-to-End Redteaming of Black-Box AI Agents", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "The strong planning and reasoning capabilities of Large Language Models\n(LLMs) have fostered the development of agent-based systems capable of\nleveraging external tools and interacting with increasingly complex\nenvironments. However, these powerful features also introduce a critical\nsecurity risk: indirect prompt injection, a sophisticated attack vector that\ncompromises the core of these agents, the LLM, by manipulating contextual\ninformation rather than direct user prompts. In this work, we propose a generic\nblack-box fuzzing framework, AgentXploit, designed to automatically discover\nand exploit indirect prompt injection vulnerabilities across diverse LLM\nagents. Our approach starts by constructing a high-quality initial seed corpus,\nthen employs a seed selection algorithm based on Monte Carlo Tree Search (MCTS)\nto iteratively refine inputs, thereby maximizing the likelihood of uncovering\nagent weaknesses. We evaluate AgentXploit on two public benchmarks, AgentDojo\nand VWA-adv, where it achieves 71% and 70% success rates against agents based\non o3-mini and GPT-4o, respectively, nearly doubling the performance of\nbaseline attacks. Moreover, AgentXploit exhibits strong transferability across\nunseen tasks and internal LLMs, as well as promising results against defenses.\nBeyond benchmark evaluations, we apply our attacks in real-world environments,\nsuccessfully misleading agents to navigate to arbitrary URLs, including\nmalicious sites."}
{"id": "2505.06055", "pdf": "https://arxiv.org/pdf/2505.06055", "abs": "https://arxiv.org/abs/2505.06055", "authors": ["Dongqian Guo", "Wencheng Han", "Pang Lyu", "Yuxi Zhou", "Jianbing Shen"], "title": "Towards Better Cephalometric Landmark Detection with Diffusion Data Generation", "categories": ["cs.CV"], "comment": null, "summary": "Cephalometric landmark detection is essential for orthodontic diagnostics and\ntreatment planning. Nevertheless, the scarcity of samples in data collection\nand the extensive effort required for manual annotation have significantly\nimpeded the availability of diverse datasets. This limitation has restricted\nthe effectiveness of deep learning-based detection methods, particularly those\nbased on large-scale vision models. To address these challenges, we have\ndeveloped an innovative data generation method capable of producing diverse\ncephalometric X-ray images along with corresponding annotations without human\nintervention. To achieve this, our approach initiates by constructing new\ncephalometric landmark annotations using anatomical priors. Then, we employ a\ndiffusion-based generator to create realistic X-ray images that correspond\nclosely with these annotations. To achieve precise control in producing samples\nwith different attributes, we introduce a novel prompt cephalometric X-ray\nimage dataset. This dataset includes real cephalometric X-ray images and\ndetailed medical text prompts describing the images. By leveraging these\ndetailed prompts, our method improves the generation process to control\ndifferent styles and attributes. Facilitated by the large, diverse generated\ndata, we introduce large-scale vision detection models into the cephalometric\nlandmark detection task to improve accuracy. Experimental results demonstrate\nthat training with the generated data substantially enhances the performance.\nCompared to methods without using the generated data, our approach improves the\nSuccess Detection Rate (SDR) by 6.5%, attaining a notable 82.2%. All code and\ndata are available at: https://um-lab.github.io/cepha-generation"}
{"id": "2505.05863", "pdf": "https://arxiv.org/pdf/2505.05863", "abs": "https://arxiv.org/abs/2505.05863", "authors": ["Reiji Suzuki", "Takaya Arita"], "title": "Evolutionary ecology of words", "categories": ["q-bio.PE", "cs.AI", "cs.CL", "92B20"], "comment": "8 pages, 5 figures. Preprint of the paper published in Proceedings of\n  2025 IEEE Symposium on Computational Intelligence in Artificial Life and\n  Cooperative Intelligent Systems (ALIFE-CIS)", "summary": "We propose a model for the evolutionary ecology of words as one attempt to\nextend evolutionary game theory and agent-based models by utilizing the rich\nlinguistic expressions of Large Language Models (LLMs). Our model enables the\nemergence and evolution of diverse and infinite options for interactions among\nagents. Within the population, each agent possesses a short word (or phrase)\ngenerated by an LLM and moves within a spatial environment. When agents become\nadjacent, the outcome of their interaction is determined by the LLM based on\nthe relationship between their words, with the loser's word being replaced by\nthe winner's. Word mutations, also based on LLM outputs, may occur. We\nconducted preliminary experiments assuming that ``strong animal species\" would\nsurvive. The results showed that from an initial population consisting of\nwell-known species, many species emerged both gradually and in a punctuated\nequilibrium manner. Each trial demonstrated the unique evolution of diverse\npopulations, with one type of large species becoming dominant, such as\nterrestrial animals, marine life, or extinct species, which were ecologically\nspecialized and adapted ones across diverse extreme habitats. We also conducted\na long-term experiment with a large population, demonstrating the emergence and\ncoexistence of diverse species."}
{"id": "2505.06068", "pdf": "https://arxiv.org/pdf/2505.06068", "abs": "https://arxiv.org/abs/2505.06068", "authors": ["Kunpeng Qiu", "Zhiqiang Gao", "Zhiying Zhou", "Mingjie Sun", "Yongxin Guo"], "title": "Noise-Consistent Siamese-Diffusion for Medical Image Synthesis and Segmentation", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Deep learning has revolutionized medical image segmentation, yet its full\npotential remains constrained by the paucity of annotated datasets. While\ndiffusion models have emerged as a promising approach for generating synthetic\nimage-mask pairs to augment these datasets, they paradoxically suffer from the\nsame data scarcity challenges they aim to mitigate. Traditional mask-only\nmodels frequently yield low-fidelity images due to their inability to\nadequately capture morphological intricacies, which can critically compromise\nthe robustness and reliability of segmentation models. To alleviate this\nlimitation, we introduce Siamese-Diffusion, a novel dual-component model\ncomprising Mask-Diffusion and Image-Diffusion. During training, a Noise\nConsistency Loss is introduced between these components to enhance the\nmorphological fidelity of Mask-Diffusion in the parameter space. During\nsampling, only Mask-Diffusion is used, ensuring diversity and scalability.\nComprehensive experiments demonstrate the superiority of our method.\nSiamese-Diffusion boosts SANet's mDice and mIoU by 3.6% and 4.4% on the Polyps,\nwhile UNet improves by 1.52% and 1.64% on the ISIC2018. Code is available at\nGitHub."}
{"id": "2505.05869", "pdf": "https://arxiv.org/pdf/2505.05869", "abs": "https://arxiv.org/abs/2505.05869", "authors": ["Hao Xu", "Yuntian Chen", "Rui Cao", "Tianning Tang", "Mengge Du", "Jian Li", "Adrian H. Callaghan", "Dongxiao Zhang"], "title": "Generative Discovery of Partial Differential Equations by Learning from Math Handbooks", "categories": ["cs.LG", "cs.AI", "physics.comp-ph"], "comment": null, "summary": "Data driven discovery of partial differential equations (PDEs) is a promising\napproach for uncovering the underlying laws governing complex systems. However,\npurely data driven techniques face the dilemma of balancing search space with\noptimization efficiency. This study introduces a knowledge guided approach that\nincorporates existing PDEs documented in a mathematical handbook to facilitate\nthe discovery process. These PDEs are encoded as sentence like structures\ncomposed of operators and basic terms, and used to train a generative model,\ncalled EqGPT, which enables the generation of free form PDEs. A loop of\ngeneration evaluation optimization is constructed to autonomously identify the\nmost suitable PDE. Experimental results demonstrate that this framework can\nrecover a variety of PDE forms with high accuracy and computational efficiency,\nparticularly in cases involving complex temporal derivatives or intricate\nspatial terms, which are often beyond the reach of conventional methods. The\napproach also exhibits generalizability to irregular spatial domains and higher\ndimensional settings. Notably, it succeeds in discovering a previously\nunreported PDE governing strongly nonlinear surface gravity waves propagating\ntoward breaking, based on real world experimental data, highlighting its\napplicability to practical scenarios and its potential to support scientific\ndiscovery."}
{"id": "2505.06113", "pdf": "https://arxiv.org/pdf/2505.06113", "abs": "https://arxiv.org/abs/2505.06113", "authors": ["Anupkumar Bochare"], "title": "Camera-Only Bird's Eye View Perception: A Neural Approach to LiDAR-Free Environmental Mapping for Autonomous Vehicles", "categories": ["cs.CV"], "comment": null, "summary": "Autonomous vehicle perception systems have traditionally relied on costly\nLiDAR sensors to generate precise environmental representations. In this paper,\nwe propose a camera-only perception framework that produces Bird's Eye View\n(BEV) maps by extending the Lift-Splat-Shoot architecture. Our method combines\nYOLOv11-based object detection with DepthAnythingV2 monocular depth estimation\nacross multi-camera inputs to achieve comprehensive 360-degree scene\nunderstanding. We evaluate our approach on the OpenLane-V2 and NuScenes\ndatasets, achieving up to 85% road segmentation accuracy and 85-90% vehicle\ndetection rates when compared against LiDAR ground truth, with average\npositional errors limited to 1.2 meters. These results highlight the potential\nof deep learning to extract rich spatial information using only camera inputs,\nenabling cost-efficient autonomous navigation without sacrificing accuracy."}
{"id": "2505.05870", "pdf": "https://arxiv.org/pdf/2505.05870", "abs": "https://arxiv.org/abs/2505.05870", "authors": ["Yimin Zhou", "Yichong Xia", "Bin Chen", "Baoyi An", "Haoqian Wang", "Zhi Wang", "Yaowei Wang", "Zikun Zhou"], "title": "Towards Facial Image Compression with Consistency Preserving Diffusion Prior", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": null, "summary": "With the widespread application of facial image data across various domains,\nthe efficient storage and transmission of facial images has garnered\nsignificant attention. However, the existing learned face image compression\nmethods often produce unsatisfactory reconstructed image quality at low bit\nrates. Simply adapting diffusion-based compression methods to facial\ncompression tasks results in reconstructed images that perform poorly in\ndownstream applications due to insufficient preservation of high-frequency\ninformation. To further explore the diffusion prior in facial image\ncompression, we propose Facial Image Compression with a Stable Diffusion Prior\n(FaSDiff), a method that preserves consistency through frequency enhancement.\nFaSDiff employs a high-frequency-sensitive compressor in an end-to-end\nframework to capture fine image details and produce robust visual prompts.\nAdditionally, we introduce a hybrid low-frequency enhancement module that\ndisentangles low-frequency facial semantics and stably modulates the diffusion\nprior alongside visual prompts. The proposed modules allow FaSDiff to leverage\ndiffusion priors for superior human visual perception while minimizing\nperformance loss in machine vision due to semantic inconsistency. Extensive\nexperiments show that FaSDiff outperforms state-of-the-art methods in balancing\nhuman visual quality and machine vision accuracy. The code will be released\nafter the paper is accepted."}
{"id": "2505.06117", "pdf": "https://arxiv.org/pdf/2505.06117", "abs": "https://arxiv.org/abs/2505.06117", "authors": ["Dongying Li", "Binyi Su", "Hua Zhang", "Yong Li", "Haiyong Chen"], "title": "Photovoltaic Defect Image Generator with Boundary Alignment Smoothing Constraint for Domain Shift Mitigation", "categories": ["cs.CV"], "comment": null, "summary": "Accurate defect detection of photovoltaic (PV) cells is critical for ensuring\nquality and efficiency in intelligent PV manufacturing systems. However, the\nscarcity of rich defect data poses substantial challenges for effective model\ntraining. While existing methods have explored generative models to augment\ndatasets, they often suffer from instability, limited diversity, and domain\nshifts. To address these issues, we propose PDIG, a Photovoltaic Defect Image\nGenerator based on Stable Diffusion (SD). PDIG leverages the strong priors\nlearned from large-scale datasets to enhance generation quality under limited\ndata. Specifically, we introduce a Semantic Concept Embedding (SCE) module that\nincorporates text-conditioned priors to capture the relational concepts between\ndefect types and their appearances. To further enrich the domain distribution,\nwe design a Lightweight Industrial Style Adaptor (LISA), which injects\nindustrial defect characteristics into the SD model through cross-disentangled\nattention. At inference, we propose a Text-Image Dual-Space Constraints (TIDSC)\nmodule, enforcing the quality of generated images via positional consistency\nand spatial smoothing alignment. Extensive experiments demonstrate that PDIG\nachieves superior realism and diversity compared to state-of-the-art methods.\nSpecifically, our approach improves Frechet Inception Distance (FID) by 19.16\npoints over the second-best method and significantly enhances the performance\nof downstream defect detection tasks."}
{"id": "2505.05877", "pdf": "https://arxiv.org/pdf/2505.05877", "abs": "https://arxiv.org/abs/2505.05877", "authors": ["Rong Yin", "Ruyue Liu", "Xiaoshuai Hao", "Xingrui Zhou", "Yong Liu", "Can Ma", "Weiping Wang"], "title": "Multi-Modal Molecular Representation Learning via Structure Awareness", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by IEEE Transactions on Image Processing (TIP) 2025", "summary": "Accurate extraction of molecular representations is a critical step in the\ndrug discovery process. In recent years, significant progress has been made in\nmolecular representation learning methods, among which multi-modal molecular\nrepresentation methods based on images, and 2D/3D topologies have become\nincreasingly mainstream. However, existing these multi-modal approaches often\ndirectly fuse information from different modalities, overlooking the potential\nof intermodal interactions and failing to adequately capture the complex\nhigher-order relationships and invariant features between molecules. To\novercome these challenges, we propose a structure-awareness-based multi-modal\nself-supervised molecular representation pre-training framework (MMSA) designed\nto enhance molecular graph representations by leveraging invariant knowledge\nbetween molecules. The framework consists of two main modules: the multi-modal\nmolecular representation learning module and the structure-awareness module.\nThe multi-modal molecular representation learning module collaboratively\nprocesses information from different modalities of the same molecule to\novercome intermodal differences and generate a unified molecular embedding.\nSubsequently, the structure-awareness module enhances the molecular\nrepresentation by constructing a hypergraph structure to model higher-order\ncorrelations between molecules. This module also introduces a memory mechanism\nfor storing typical molecular representations, aligning them with memory\nanchors in the memory bank to integrate invariant knowledge, thereby improving\nthe model generalization ability. Extensive experiments have demonstrated the\neffectiveness of MMSA, which achieves state-of-the-art performance on the\nMoleculeNet benchmark, with average ROC-AUC improvements ranging from 1.8% to\n9.6% over baseline methods."}
{"id": "2505.06133", "pdf": "https://arxiv.org/pdf/2505.06133", "abs": "https://arxiv.org/abs/2505.06133", "authors": ["Hongming Wang", "Yifeng Wu", "Huimin Huang", "Hongtao Wu", "Jia-Xuan Jiang", "Xiaodong Zhang", "Hao Zheng", "Xian Wu", "Yefeng Zheng", "Jinping Xu", "Jing Cheng"], "title": "BrainSegDMlF: A Dynamic Fusion-enhanced SAM for Brain Lesion Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "The segmentation of substantial brain lesions is a significant and\nchallenging task in the field of medical image segmentation. Substantial brain\nlesions in brain imaging exhibit high heterogeneity, with indistinct boundaries\nbetween lesion regions and normal brain tissue. Small lesions in single slices\nare difficult to identify, making the accurate and reproducible segmentation of\nabnormal regions, as well as their feature description, highly complex.\nExisting methods have the following limitations: 1) They rely solely on\nsingle-modal information for learning, neglecting the multi-modal information\ncommonly used in diagnosis. This hampers the ability to comprehensively acquire\nbrain lesion information from multiple perspectives and prevents the effective\nintegration and utilization of multi-modal data inputs, thereby limiting a\nholistic understanding of lesions. 2) They are constrained by the amount of\ndata available, leading to low sensitivity to small lesions and difficulty in\ndetecting subtle pathological changes. 3) Current SAM-based models rely on\nexternal prompts, which cannot achieve automatic segmentation and, to some\nextent, affect diagnostic efficiency.To address these issues, we have developed\na large-scale fully automated segmentation model specifically designed for\nbrain lesion segmentation, named BrainSegDMLF. This model has the following\nfeatures: 1) Dynamic Modal Interactive Fusion (DMIF) module that processes and\nintegrates multi-modal data during the encoding process, providing the SAM\nencoder with more comprehensive modal information. 2) Layer-by-Layer Upsampling\nDecoder, enabling the model to extract rich low-level and high-level features\neven with limited data, thereby detecting the presence of small lesions. 3)\nAutomatic segmentation masks, allowing the model to generate lesion masks\nautomatically without requiring manual prompts."}
{"id": "2505.05893", "pdf": "https://arxiv.org/pdf/2505.05893", "abs": "https://arxiv.org/abs/2505.05893", "authors": ["Seunghee Han", "Soongyu Choi", "Joo-Young Kim"], "title": "LightNobel: Improving Sequence Length Limitation in Protein Structure Prediction Model via Adaptive Activation Quantization", "categories": ["cs.AR", "cs.AI", "cs.ET", "cs.LG", "B.7; I.2; J.3"], "comment": "To appear in the Proceedings of the 52nd IEEE/ACM International\n  Symposium on Computer Architecture (ISCA 2025)", "summary": "Recent advances in Protein Structure Prediction Models (PPMs), such as\nAlphaFold2 and ESMFold, have revolutionized computational biology by achieving\nunprecedented accuracy in predicting three-dimensional protein folding\nstructures. However, these models face significant scalability challenges,\nparticularly when processing proteins with long amino acid sequences (e.g.,\nsequence length > 1,000). The primary bottleneck that arises from the\nexponential growth in activation sizes is driven by the unique data structure\nin PPM, which introduces an additional dimension that leads to substantial\nmemory and computational demands. These limitations have hindered the effective\nscaling of PPM for real-world applications, such as analyzing large proteins or\ncomplex multimers with critical biological and pharmaceutical relevance.\n  In this paper, we present LightNobel, the first hardware-software co-designed\naccelerator developed to overcome scalability limitations on the sequence\nlength in PPM. At the software level, we propose Token-wise Adaptive Activation\nQuantization (AAQ), which leverages unique token-wise characteristics, such as\ndistogram patterns in PPM activations, to enable fine-grained quantization\ntechniques without compromising accuracy. At the hardware level, LightNobel\nintegrates the multi-precision reconfigurable matrix processing unit (RMPU) and\nversatile vector processing unit (VVPU) to enable the efficient execution of\nAAQ. Through these innovations, LightNobel achieves up to 8.44x, 8.41x speedup\nand 37.29x, 43.35x higher power efficiency over the latest NVIDIA A100 and H100\nGPUs, respectively, while maintaining negligible accuracy loss. It also reduces\nthe peak memory requirement up to 120.05x in PPM, enabling scalable processing\nfor proteins with long sequences."}
{"id": "2505.06152", "pdf": "https://arxiv.org/pdf/2505.06152", "abs": "https://arxiv.org/abs/2505.06152", "authors": ["Wenqi Zeng", "Yuqi Sun", "Chenxi Ma", "Weimin Tan", "Bo Yan"], "title": "MM-Skin: Enhancing Dermatology Vision-Language Model with an Image-Text Dataset Derived from Textbooks", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Medical vision-language models (VLMs) have shown promise as clinical\nassistants across various medical fields. However, specialized dermatology VLM\ncapable of delivering professional and detailed diagnostic analysis remains\nunderdeveloped, primarily due to less specialized text descriptions in current\ndermatology multimodal datasets. To address this issue, we propose MM-Skin, the\nfirst large-scale multimodal dermatology dataset that encompasses 3 imaging\nmodalities, including clinical, dermoscopic, and pathological and nearly 10k\nhigh-quality image-text pairs collected from professional textbooks. In\naddition, we generate over 27k diverse, instruction-following vision question\nanswering (VQA) samples (9 times the size of current largest dermatology VQA\ndataset). Leveraging public datasets and MM-Skin, we developed SkinVL, a\ndermatology-specific VLM designed for precise and nuanced skin disease\ninterpretation. Comprehensive benchmark evaluations of SkinVL on VQA,\nsupervised fine-tuning (SFT) and zero-shot classification tasks across 8\ndatasets, reveal its exceptional performance for skin diseases in comparison to\nboth general and medical VLM models. The introduction of MM-Skin and SkinVL\noffers a meaningful contribution to advancing the development of clinical\ndermatology VLM assistants. MM-Skin is available at\nhttps://github.com/ZwQ803/MM-Skin"}
{"id": "2505.05895", "pdf": "https://arxiv.org/pdf/2505.05895", "abs": "https://arxiv.org/abs/2505.05895", "authors": ["Benjamin Raphael Ernhofer", "Daniil Prokhorov", "Jannica Langner", "Dominik Bollmann"], "title": "Leveraging Vision-Language Models for Visual Grounding and Analysis of Automotive UI", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Modern automotive infotainment systems require intelligent and adaptive\nsolutions to handle frequent User Interface (UI) updates and diverse design\nvariations. We introduce a vision-language framework for understanding and\ninteracting with automotive infotainment systems, enabling seamless adaptation\nacross different UI designs. To further support research in this field, we\nrelease AutomotiveUI-Bench-4K, an open-source dataset of 998 images with 4,208\nannotations. Additionally, we present a synthetic data pipeline to generate\ntraining data. We fine-tune a Molmo-7B-based model using Low-Rank Adaptation\n(LoRa) and incorporating reasoning generated by our pipeline, along with visual\ngrounding and evaluation capabilities. The fine-tuned Evaluative Large Action\nModel (ELAM) achieves strong performance on AutomotiveUI-Bench-4K (model and\ndataset are available on Hugging Face) and demonstrating strong cross-domain\ngeneralization, including a +5.2% improvement on ScreenSpot over the baseline\nmodel. Notably, our approach achieves 80.4% average accuracy on ScreenSpot,\nclosely matching or even surpassing specialized models for desktop, mobile, and\nweb, such as ShowUI, despite being trained for the infotainment domain. This\nresearch investigates how data collection and subsequent fine-tuning can lead\nto AI-driven progress within automotive UI understanding and interaction. The\napplied method is cost-efficient and fine-tuned models can be deployed on\nconsumer-grade GPUs."}
{"id": "2505.06166", "pdf": "https://arxiv.org/pdf/2505.06166", "abs": "https://arxiv.org/abs/2505.06166", "authors": ["Radu Alexandru Rosu", "Keyu Wu", "Yao Feng", "Youyi Zheng", "Michael J. Black"], "title": "DiffLocks: Generating 3D Hair from a Single Image using Diffusion Models", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "We address the task of generating 3D hair geometry from a single image, which\nis challenging due to the diversity of hairstyles and the lack of paired\nimage-to-3D hair data. Previous methods are primarily trained on synthetic data\nand cope with the limited amount of such data by using low-dimensional\nintermediate representations, such as guide strands and scalp-level embeddings,\nthat require post-processing to decode, upsample, and add realism. These\napproaches fail to reconstruct detailed hair, struggle with curly hair, or are\nlimited to handling only a few hairstyles. To overcome these limitations, we\npropose DiffLocks, a novel framework that enables detailed reconstruction of a\nwide variety of hairstyles directly from a single image. First, we address the\nlack of 3D hair data by automating the creation of the largest synthetic hair\ndataset to date, containing 40K hairstyles. Second, we leverage the synthetic\nhair dataset to learn an image-conditioned diffusion-transfomer model that\ngenerates accurate 3D strands from a single frontal image. By using a\npretrained image backbone, our method generalizes to in-the-wild images despite\nbeing trained only on synthetic data. Our diffusion model predicts a scalp\ntexture map in which any point in the map contains the latent code for an\nindividual hair strand. These codes are directly decoded to 3D strands without\npost-processing techniques. Representing individual strands, instead of guide\nstrands, enables the transformer to model the detailed spatial structure of\ncomplex hairstyles. With this, DiffLocks can recover highly curled hair, like\nafro hairstyles, from a single image for the first time. Data and code is\navailable at https://radualexandru.github.io/difflocks/"}
{"id": "2505.05901", "pdf": "https://arxiv.org/pdf/2505.05901", "abs": "https://arxiv.org/abs/2505.05901", "authors": ["Hanzhe Liang", "Aoran Wang", "Jie Zhou", "Xin Jin", "Can Gao", "Jinbao Wang"], "title": "Examining the Source of Defects from a Mechanical Perspective for 3D Anomaly Detection", "categories": ["cs.CV", "cs.AI"], "comment": "26 pages", "summary": "In this paper, we go beyond identifying anomalies only in structural terms\nand think about better anomaly detection motivated by anomaly causes. Most\nanomalies are regarded as the result of unpredictable defective forces from\ninternal and external sources, and their opposite forces are sought to correct\nthe anomalies. We introduced a Mechanics Complementary framework for 3D anomaly\ndetection (MC4AD) to generate internal and external Corrective forces for each\npoint. A Diverse Anomaly-Generation (DA-Gen) module is first proposed to\nsimulate various anomalies. Then, we present a Corrective Force Prediction\nNetwork (CFP-Net) with complementary representations for point-level\nrepresentation to simulate the different contributions of internal and external\ncorrective forces. A combined loss was proposed, including a new symmetric loss\nand an overall loss, to constrain the corrective forces properly. As a\nhighlight, we consider 3D anomaly detection in industry more comprehensively,\ncreating a hierarchical quality control strategy based on a three-way decision\nand contributing a dataset named Anomaly-IntraVariance with intraclass variance\nto evaluate the model. On the proposed and existing five datasets, we obtained\nnine state-of-the-art performers with the minimum parameters and the fastest\ninference speed. The source is available at\nhttps://github.com/hzzzzzhappy/MC4AD"}
{"id": "2505.06217", "pdf": "https://arxiv.org/pdf/2505.06217", "abs": "https://arxiv.org/abs/2505.06217", "authors": ["Pengfei Gu", "Haoteng Tang", "Islam A. Ebeid", "Jose A. Nunez", "Fabian Vazquez", "Diego Adame", "Marcus Zhan", "Huimin Li", "Bin Fu", "Danny Z. Chen"], "title": "Adapting a Segmentation Foundation Model for Medical Image Classification", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in foundation models, such as the Segment Anything Model\n(SAM), have shown strong performance in various vision tasks, particularly\nimage segmentation, due to their impressive zero-shot segmentation\ncapabilities. However, effectively adapting such models for medical image\nclassification is still a less explored topic. In this paper, we introduce a\nnew framework to adapt SAM for medical image classification. First, we utilize\nthe SAM image encoder as a feature extractor to capture segmentation-based\nfeatures that convey important spatial and contextual details of the image,\nwhile freezing its weights to avoid unnecessary overhead during training. Next,\nwe propose a novel Spatially Localized Channel Attention (SLCA) mechanism to\ncompute spatially localized attention weights for the feature maps. The\nfeatures extracted from SAM's image encoder are processed through SLCA to\ncompute attention weights, which are then integrated into deep learning\nclassification models to enhance their focus on spatially relevant or\nmeaningful regions of the image, thus improving classification performance.\nExperimental results on three public medical image classification datasets\ndemonstrate the effectiveness and data-efficiency of our approach."}
{"id": "2505.05916", "pdf": "https://arxiv.org/pdf/2505.05916", "abs": "https://arxiv.org/abs/2505.05916", "authors": ["Yifan Zhou", "Yibo Wang", "Chao Shang"], "title": "IRNN: Innovation-driven Recurrent Neural Network for Time-Series Data Modeling and Prediction", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Many real-world datasets are time series that are sequentially collected and\ncontain rich temporal information. Thus, a common interest in practice is to\ncapture dynamics of time series and predict their future evolutions. To this\nend, the recurrent neural network (RNN) has been a prevalent and effective\nmachine learning option, which admits a nonlinear state-space model\nrepresentation. Motivated by the resemblance between RNN and Kalman filter (KF)\nfor linear state-space models, we propose in this paper Innovation-driven RNN\n(IRNN), a novel RNN architecture tailored to time-series data modeling and\nprediction tasks. By adapting the concept of \"innovation\" from KF to RNN, past\nprediction errors are adopted as additional input signals to update hidden\nstates of RNN and boost prediction performance. Since innovation data depend on\nnetwork parameters, existing training algorithms for RNN do not apply to IRNN\nstraightforwardly. Thus, a tailored training algorithm dubbed input\nupdating-based back-propagation through time (IU-BPTT) is further proposed,\nwhich alternates between updating innovations and optimizing network parameters\nvia gradient descent. Experiments on real-world benchmark datasets show that\nthe integration of innovations into various forms of RNN leads to remarkably\nimproved prediction accuracy of IRNN without increasing the training cost\nsubstantially."}
{"id": "2505.06219", "pdf": "https://arxiv.org/pdf/2505.06219", "abs": "https://arxiv.org/abs/2505.06219", "authors": ["Noah Frahm", "Dongxu Zhao", "Andrea Dunn Beltran", "Ron Alterovitz", "Jan-Michael Frahm", "Junier Oliva", "Roni Sengupta"], "title": "VIN-NBV: A View Introspection Network for Next-Best-View Selection for Resource-Efficient 3D Reconstruction", "categories": ["cs.CV", "cs.RO", "I.2.10; I.2.9"], "comment": "19 pages, 11 figures", "summary": "Next Best View (NBV) algorithms aim to acquire an optimal set of images using\nminimal resources, time, or number of captures to enable efficient 3D\nreconstruction of a scene. Existing approaches often rely on prior scene\nknowledge or additional image captures and often develop policies that maximize\ncoverage. Yet, for many real scenes with complex geometry and self-occlusions,\ncoverage maximization does not lead to better reconstruction quality directly.\nIn this paper, we propose the View Introspection Network (VIN), which is\ntrained to predict the reconstruction quality improvement of views directly,\nand the VIN-NBV policy. A greedy sequential sampling-based policy, where at\neach acquisition step, we sample multiple query views and choose the one with\nthe highest VIN predicted improvement score. We design the VIN to perform\n3D-aware featurization of the reconstruction built from prior acquisitions, and\nfor each query view create a feature that can be decoded into an improvement\nscore. We then train the VIN using imitation learning to predict the\nreconstruction improvement score. We show that VIN-NBV improves reconstruction\nquality by ~30% over a coverage maximization baseline when operating with\nconstraints on the number of acquisitions or the time in motion."}
{"id": "2505.05943", "pdf": "https://arxiv.org/pdf/2505.05943", "abs": "https://arxiv.org/abs/2505.05943", "authors": ["Maan Alhazmi", "Abdulrahman Altahhan"], "title": "Achieving 3D Attention via Triplet Squeeze and Excitation Block", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "The emergence of ConvNeXt and its variants has reaffirmed the conceptual and\nstructural suitability of CNN-based models for vision tasks, re-establishing\nthem as key players in image classification in general, and in facial\nexpression recognition (FER) in particular. In this paper, we propose a new set\nof models that build on these advancements by incorporating a new set of\nattention mechanisms that combines Triplet attention with\nSqueeze-and-Excitation (TripSE) in four different variants. We demonstrate the\neffectiveness of these variants by applying them to the ResNet18, DenseNet and\nConvNext architectures to validate their versatility and impact. Our study\nshows that incorporating a TripSE block in these CNN models boosts their\nperformances, particularly for the ConvNeXt architecture, indicating its\nutility. We evaluate the proposed mechanisms and associated models across four\ndatasets, namely CIFAR100, ImageNet, FER2013 and AffectNet datasets, where\nConvNext with TripSE achieves state-of-the-art results with an accuracy of\n\\textbf{78.27\\%} on the popular FER2013 dataset, a new feat for this dataset."}
{"id": "2505.05477", "pdf": "https://arxiv.org/pdf/2505.05477", "abs": "https://arxiv.org/abs/2505.05477", "authors": ["Sainan xiao", "Wangdong Yang", "Buwen Cao", "Jintao Wu"], "title": "ECGDeDRDNet: A deep learning-based method for Electrocardiogram noise removal using a double recurrent dense network", "categories": ["eess.SP", "cs.CV"], "comment": null, "summary": "Electrocardiogram (ECG) signals are frequently corrupted by noise, such as\nbaseline wander (BW), muscle artifacts (MA), and electrode motion (EM), which\nsignificantly degrade their diagnostic utility. To address this issue, we\npropose ECGDeDRDNet, a deep learning-based ECG Denoising framework leveraging a\nDouble Recurrent Dense Network architecture. In contrast to traditional\napproaches, we introduce a double recurrent scheme to enhance information reuse\nfrom both ECG waveforms and the estimated clean image. For ECG waveform\nprocessing, our basic model employs LSTM layers cascaded with DenseNet blocks.\nThe estimated clean ECG image, obtained by subtracting predicted noise\ncomponents from the noisy input, is iteratively fed back into the model. This\ndual recurrent architecture enables comprehensive utilization of both temporal\nwaveform features and spatial image details, leading to more effective noise\nsuppression. Experimental results on the MIT-BIH dataset demonstrate that our\nmethod achieves superior performance compared to conventional image denoising\nmethods in terms of PSNR and SSIM while also surpassing classical ECG denoising\ntechniques in both SNR and RMSE."}
{"id": "2505.05946", "pdf": "https://arxiv.org/pdf/2505.05946", "abs": "https://arxiv.org/abs/2505.05946", "authors": ["Vytenis Å liogeris", "Povilas DaniuÅ¡is", "ArtÅ«ras Nakvosas"], "title": "Elastic Weight Consolidation for Full-Parameter Continual Pre-Training of Gemma2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "8 pages, 4 figures", "summary": "This technical report describes an experiment on autoregressive pre-training\nof Gemma2 2 billion parameter large language model (LLM) with 10\\% on the\nLithuanian language component of CulturaX from the point of view of continual\nlearning. We apply elastic weight consolidation (EWC) to the full set of the\nmodel's parameters and investigate language understanding benchmarks,\nconsisting of Arc, Belebele, Gsm8K, Hellaswag, MMLU, TruthfulQA, and Winogrande\nsets (both in English and Lithuanian versions), and perplexity benchmarks. We\nempirically demonstrate that EWC regularisation allows us not only to mitigate\ncatastrophic forgetting effects but also that it is potentially beneficial for\nlearning of the new task with LLMs."}
{"id": "2505.05504", "pdf": "https://arxiv.org/pdf/2505.05504", "abs": "https://arxiv.org/abs/2505.05504", "authors": ["Xingyu Jiang", "Ning Gao", "Xiuhui Zhang", "Hongkun Dou", "Shaowen Fu", "Xiaoqing Zhong", "Hongjue Li", "Yue Deng"], "title": "Image Restoration via Multi-domain Learning", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Due to adverse atmospheric and imaging conditions, natural images suffer from\nvarious degradation phenomena. Consequently, image restoration has emerged as a\nkey solution and garnered substantial attention. Although recent Transformer\narchitectures have demonstrated impressive success across various restoration\ntasks, their considerable model complexity poses significant challenges for\nboth training and real-time deployment. Furthermore, instead of investigating\nthe commonalities among different degradations, most existing restoration\nmethods focus on modifying Transformer under limited restoration priors. In\nthis work, we first review various degradation phenomena under multi-domain\nperspective, identifying common priors. Then, we introduce a novel restoration\nframework, which integrates multi-domain learning into Transformer.\nSpecifically, in Token Mixer, we propose a Spatial-Wavelet-Fourier multi-domain\nstructure that facilitates local-region-global multi-receptive field modeling\nto replace vanilla self-attention. Additionally, in Feed-Forward Network, we\nincorporate multi-scale learning to fuse multi-domain features at different\nresolutions. Comprehensive experimental results across ten restoration tasks,\nsuch as dehazing, desnowing, motion deblurring, defocus deblurring, rain\nstreak/raindrop removal, cloud removal, shadow removal, underwater enhancement\nand low-light enhancement, demonstrate that our proposed model outperforms\nstate-of-the-art methods and achieves a favorable trade-off among restoration\nperformance, parameter size, computational cost and inference latency. The code\nis available at: https://github.com/deng-ai-lab/SWFormer."}
{"id": "2505.05965", "pdf": "https://arxiv.org/pdf/2505.05965", "abs": "https://arxiv.org/abs/2505.05965", "authors": ["Abdelfateh Bekkair", "Slimane Bellaouar", "Slimane Oulad-Naoui"], "title": "A Noise-Resilient Semi-Supervised Graph Autoencoder for Overlapping Semantic Community Detection", "categories": ["cs.SI", "cs.AI"], "comment": null, "summary": "Community detection in networks with overlapping structures remains a\nsignificant challenge, particularly in noisy real-world environments where\nintegrating topology, node attributes, and prior information is critical. To\naddress this, we propose a semi-supervised graph autoencoder that combines\ngraph multi-head attention and modularity maximization to robustly detect\noverlapping communities. The model learns semantic representations by fusing\nstructural, attribute, and prior knowledge while explicitly addressing noise in\nnode features. Key innovations include a noise-resistant architecture and a\nsemantic semi-supervised design optimized for community quality through\nmodularity constraints. Experiments demonstrate superior performance the model\noutperforms state-of-the-art methods in overlapping community detection\n(improvements in NMI and F1-score) and exhibits exceptional robustness to\nattribute noise, maintaining stable performance under 60\\% feature corruption.\nThese results highlight the importance of integrating attribute semantics and\nstructural patterns for accurate community discovery in complex networks."}
{"id": "2505.05509", "pdf": "https://arxiv.org/pdf/2505.05509", "abs": "https://arxiv.org/abs/2505.05509", "authors": ["Yi Liu", "Xinyi Liu", "Panwang Xia", "Qiong Wu", "Yi Wan", "Yongjun Zhang"], "title": "StereoINR: Cross-View Geometry Consistent Stereo Super Resolution with Implicit Neural Representation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Stereo image super-resolution (SSR) aims to enhance high-resolution details\nby leveraging information from stereo image pairs. However, existing stereo\nsuper-resolution (SSR) upsampling methods (e.g., pixel shuffle) often overlook\ncross-view geometric consistency and are limited to fixed-scale upsampling. The\nkey issue is that previous upsampling methods use convolution to independently\nprocess deep features of different views, lacking cross-view and non-local\ninformation perception, making it difficult to select beneficial information\nfrom multi-view scenes adaptively. In this work, we propose Stereo Implicit\nNeural Representation (StereoINR), which innovatively models stereo image pairs\nas continuous implicit representations. This continuous representation breaks\nthrough the scale limitations, providing a unified solution for arbitrary-scale\nstereo super-resolution reconstruction of left-right views. Furthermore, by\nincorporating spatial warping and cross-attention mechanisms, StereoINR enables\neffective cross-view information fusion and achieves significant improvements\nin pixel-level geometric consistency. Extensive experiments across multiple\ndatasets show that StereoINR outperforms out-of-training-distribution scale\nupsampling and matches state-of-the-art SSR methods within\ntraining-distribution scales."}
{"id": "2505.05988", "pdf": "https://arxiv.org/pdf/2505.05988", "abs": "https://arxiv.org/abs/2505.05988", "authors": ["JÃ¸rgen Villadsen"], "title": "Minimal Sequent Calculus for Teaching First-Order Logic: Lessons Learned", "categories": ["cs.LO", "cs.AI", "F.4; I.2.3; K.3.1"], "comment": "In Proceedings ThEdu24, arXiv:2505.04677", "summary": "MiniCalc is a web app for teaching first-order logic based on a minimal\nsequent calculus. As an option the proofs can be verified in the Isabelle proof\nassistant. We present the lessons learned using the tool in recent years at our\nuniversity."}
{"id": "2505.05510", "pdf": "https://arxiv.org/pdf/2505.05510", "abs": "https://arxiv.org/abs/2505.05510", "authors": ["Thomas Sommariva", "Simone Calderara", "Angelo Porrello"], "title": "How to Train Your Metamorphic Deep Neural Network", "categories": ["cs.NE", "cs.CV", "cs.LG"], "comment": "14 pages, 7 figures", "summary": "Neural Metamorphosis (NeuMeta) is a recent paradigm for generating neural\nnetworks of varying width and depth. Based on Implicit Neural Representation\n(INR), NeuMeta learns a continuous weight manifold, enabling the direct\ngeneration of compressed models, including those with configurations not seen\nduring training. While promising, the original formulation of NeuMeta proves\neffective only for the final layers of the undelying model, limiting its\nbroader applicability. In this work, we propose a training algorithm that\nextends the capabilities of NeuMeta to enable full-network metamorphosis with\nminimal accuracy degradation. Our approach follows a structured recipe\ncomprising block-wise incremental training, INR initialization, and strategies\nfor replacing batch normalization. The resulting metamorphic networks maintain\ncompetitive accuracy across a wide range of compression ratios, offering a\nscalable solution for adaptable and efficient deployment of deep models. The\ncode is available at: https://github.com/TSommariva/HTTY_NeuMeta."}
{"id": "2505.06023", "pdf": "https://arxiv.org/pdf/2505.06023", "abs": "https://arxiv.org/abs/2505.06023", "authors": ["Qian Qi"], "title": "Universal Approximation Theorem for Deep Q-Learning via FBSDE System", "categories": ["cs.LG", "cs.AI", "math.OC"], "comment": null, "summary": "The approximation capabilities of Deep Q-Networks (DQNs) are commonly\njustified by general Universal Approximation Theorems (UATs) that do not\nleverage the intrinsic structural properties of the optimal Q-function, the\nsolution to a Bellman equation. This paper establishes a UAT for a class of\nDQNs whose architecture is designed to emulate the iterative refinement process\ninherent in Bellman updates. A central element of our analysis is the\npropagation of regularity: while the transformation induced by a single Bellman\noperator application exhibits regularity, for which Backward Stochastic\nDifferential Equations (BSDEs) theory provides analytical tools, the uniform\nregularity of the entire sequence of value iteration iterates--specifically,\ntheir uniform Lipschitz continuity on compact domains under standard Lipschitz\nassumptions on the problem data--is derived from finite-horizon dynamic\nprogramming principles. We demonstrate that layers of a deep residual network,\nconceived as neural operators acting on function spaces, can approximate the\naction of the Bellman operator. The resulting approximation theorem is thus\nintrinsically linked to the control problem's structure, offering a proof\ntechnique wherein network depth directly corresponds to iterations of value\nfunction refinement, accompanied by controlled error propagation. This\nperspective reveals a dynamic systems view of the network's operation on a\nspace of value functions."}
{"id": "2505.05518", "pdf": "https://arxiv.org/pdf/2505.05518", "abs": "https://arxiv.org/abs/2505.05518", "authors": ["Jaeyoung Huh", "Ankur Kapoor", "Young-Ho Kim"], "title": "Guidance for Intra-cardiac Echocardiography Manipulation to Maintain Continuous Therapy Device Tip Visibility", "categories": ["eess.IV", "cs.CV", "cs.RO"], "comment": null, "summary": "Intra-cardiac Echocardiography (ICE) plays a critical role in\nElectrophysiology (EP) and Structural Heart Disease (SHD) interventions by\nproviding real-time visualization of intracardiac structures. However,\nmaintaining continuous visibility of the therapy device tip remains a challenge\ndue to frequent adjustments required during manual ICE catheter manipulation.\nTo address this, we propose an AI-driven tracking model that estimates the\ndevice tip incident angle and passing point within the ICE imaging plane,\nensuring continuous visibility and facilitating robotic ICE catheter control.\n  A key innovation of our approach is the hybrid dataset generation strategy,\nwhich combines clinical ICE sequences with synthetic data augmentation to\nenhance model robustness. We collected ICE images in a water chamber setup,\nequipping both the ICE catheter and device tip with electromagnetic (EM)\nsensors to establish precise ground-truth locations. Synthetic sequences were\ncreated by overlaying catheter tips onto real ICE images, preserving motion\ncontinuity while simulating diverse anatomical scenarios. The final dataset\nconsists of 5,698 ICE-tip image pairs, ensuring comprehensive training\ncoverage.\n  Our model architecture integrates a pretrained ultrasound (US) foundation\nmodel, trained on 37.4M echocardiography images, for feature extraction. A\ntransformer-based network processes sequential ICE frames, leveraging\nhistorical passing points and incident angles to improve prediction accuracy.\n  Experimental results demonstrate that our method achieves 3.32 degree entry\nangle error, 12.76 degree rotation angle error. This AI-driven framework lays\nthe foundation for real-time robotic ICE catheter adjustments, minimizing\noperator workload while ensuring consistent therapy device visibility. Future\nwork will focus on expanding clinical datasets to further enhance model\ngeneralization."}
{"id": "2505.06047", "pdf": "https://arxiv.org/pdf/2505.06047", "abs": "https://arxiv.org/abs/2505.06047", "authors": ["Francesco Spinnato", "Cristiano Landi"], "title": "PYRREGULAR: A Unified Framework for Irregular Time Series, with Classification Benchmarks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Irregular temporal data, characterized by varying recording frequencies,\ndiffering observation durations, and missing values, presents significant\nchallenges across fields like mobility, healthcare, and environmental science.\nExisting research communities often overlook or address these challenges in\nisolation, leading to fragmented tools and methods. To bridge this gap, we\nintroduce a unified framework, and the first standardized dataset repository\nfor irregular time series classification, built on a common array format to\nenhance interoperability. This repository comprises 34 datasets on which we\nbenchmark 12 classifier models from diverse domains and communities. This work\naims to centralize research efforts and enable a more robust evaluation of\nirregular temporal data analysis methods."}
{"id": "2505.05592", "pdf": "https://arxiv.org/pdf/2505.05592", "abs": "https://arxiv.org/abs/2505.05592", "authors": ["Noriaki Hirose", "Lydia Ignatova", "Kyle Stachowicz", "Catherine Glossop", "Sergey Levine", "Dhruv Shah"], "title": "Learning to Drive Anywhere with Model-Based Reannotation11", "categories": ["cs.RO", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "comment": "19 pages, 11 figures, 8 tables", "summary": "Developing broadly generalizable visual navigation policies for robots is a\nsignificant challenge, primarily constrained by the availability of\nlarge-scale, diverse training data. While curated datasets collected by\nresearchers offer high quality, their limited size restricts policy\ngeneralization. To overcome this, we explore leveraging abundant, passively\ncollected data sources, including large volumes of crowd-sourced teleoperation\ndata and unlabeled YouTube videos, despite their potential for lower quality or\nmissing action labels. We propose Model-Based ReAnnotation (MBRA), a framework\nthat utilizes a learned short-horizon, model-based expert model to relabel or\ngenerate high-quality actions for these passive datasets. This relabeled data\nis then distilled into LogoNav, a long-horizon navigation policy conditioned on\nvisual goals or GPS waypoints. We demonstrate that LogoNav, trained using\nMBRA-processed data, achieves state-of-the-art performance, enabling robust\nnavigation over distances exceeding 300 meters in previously unseen indoor and\noutdoor environments. Our extensive real-world evaluations, conducted across a\nfleet of robots (including quadrupeds) in six cities on three continents,\nvalidate the policy's ability to generalize and navigate effectively even\namidst pedestrians in crowded settings."}
{"id": "2505.06085", "pdf": "https://arxiv.org/pdf/2505.06085", "abs": "https://arxiv.org/abs/2505.06085", "authors": ["Hiari Pizzini Cavagna", "Daniele Cesarini", "Andrea Bartolini"], "title": "Assessing Tenstorrent's RISC-V MatMul Acceleration Capabilities", "categories": ["cs.PF", "cs.AI", "cs.AR"], "comment": "Accepted to the Computational Aspects of Deep Learning Workshop at\n  ISC High Performance 2025. To appear in the ISC High Performance 2025\n  Workshop Proceedings", "summary": "The increasing demand for generative AI as Large Language Models (LLMs)\nservices has driven the need for specialized hardware architectures that\noptimize computational efficiency and energy consumption. This paper evaluates\nthe performance of the Tenstorrent Grayskull e75 RISC-V accelerator for basic\nlinear algebra kernels at reduced numerical precision, a fundamental operation\nin LLM computations. We present a detailed characterization of Grayskull's\nexecution model, gridsize, matrix dimensions, data formats, and numerical\nprecision impact computational efficiency. Furthermore, we compare Grayskull's\nperformance against state-of-the-art architectures with tensor acceleration,\nincluding Intel Sapphire Rapids processors and two NVIDIA GPUs (V100 and A100).\nWhilst NVIDIA GPUs dominate raw performance, Grayskull demonstrates a\ncompetitive trade-off between power consumption and computational throughput,\nreaching a peak of 1.55 TFLOPs/Watt with BF16."}
{"id": "2505.05631", "pdf": "https://arxiv.org/pdf/2505.05631", "abs": "https://arxiv.org/abs/2505.05631", "authors": ["Jiachen Tu", "Yaokun Shi", "Fan Lam"], "title": "Score-based Self-supervised MRI Denoising", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Magnetic resonance imaging (MRI) is a powerful noninvasive diagnostic imaging\ntool that provides unparalleled soft tissue contrast and anatomical detail.\nNoise contamination, especially in accelerated and/or low-field acquisitions,\ncan significantly degrade image quality and diagnostic accuracy. Supervised\nlearning based denoising approaches have achieved impressive performance but\nrequire high signal-to-noise ratio (SNR) labels, which are often unavailable.\nSelf-supervised learning holds promise to address the label scarcity issue, but\nexisting self-supervised denoising methods tend to oversmooth fine spatial\nfeatures and often yield inferior performance than supervised methods. We\nintroduce Corruption2Self (C2S), a novel score-based self-supervised framework\nfor MRI denoising. At the core of C2S is a generalized denoising score matching\n(GDSM) loss, which extends denoising score matching to work directly with noisy\nobservations by modeling the conditional expectation of higher-SNR images given\nfurther corrupted observations. This allows the model to effectively learn\ndenoising across multiple noise levels directly from noisy data. Additionally,\nwe incorporate a reparameterization of noise levels to stabilize training and\nenhance convergence, and introduce a detail refinement extension to balance\nnoise reduction with the preservation of fine spatial features. Moreover, C2S\ncan be extended to multi-contrast denoising by leveraging complementary\ninformation across different MRI contrasts. We demonstrate that our method\nachieves state-of-the-art performance among self-supervised methods and\ncompetitive results compared to supervised counterparts across varying noise\nconditions and MRI contrasts on the M4Raw and fastMRI dataset."}
{"id": "2505.06091", "pdf": "https://arxiv.org/pdf/2505.06091", "abs": "https://arxiv.org/abs/2505.06091", "authors": ["Xinxin Li", "Juan Zhang", "Da Li", "Xingyu Liu", "Jin Xu", "Junping Yin"], "title": "UniSymNet: A Unified Symbolic Network Guided by Transformer", "categories": ["cs.LG", "cs.AI", "cs.SC"], "comment": null, "summary": "Symbolic Regression (SR) is a powerful technique for automatically\ndiscovering mathematical expressions from input data. Mainstream SR algorithms\nsearch for the optimal symbolic tree in a vast function space, but the\nincreasing complexity of the tree structure limits their performance. Inspired\nby neural networks, symbolic networks have emerged as a promising new paradigm.\nHowever, most existing symbolic networks still face certain challenges: binary\nnonlinear operators $\\{\\times, \\div\\}$ cannot be naturally extended to\nmultivariate operators, and training with fixed architecture often leads to\nhigher complexity and overfitting. In this work, we propose a Unified Symbolic\nNetwork that unifies nonlinear binary operators into nested unary operators and\ndefine the conditions under which UniSymNet can reduce complexity. Moreover, we\npre-train a Transformer model with a novel label encoding method to guide\nstructural selection, and adopt objective-specific optimization strategies to\nlearn the parameters of the symbolic network. UniSymNet shows high fitting\naccuracy, excellent symbolic solution rate, and relatively low expression\ncomplexity, achieving competitive performance on low-dimensional Standard\nBenchmarks and high-dimensional SRBench."}
{"id": "2505.05643", "pdf": "https://arxiv.org/pdf/2505.05643", "abs": "https://arxiv.org/abs/2505.05643", "authors": ["Mark C. Eid", "Ana I. L. Namburete", "JoÃ£o F. Henriques"], "title": "UltraGauss: Ultrafast Gaussian Reconstruction of 3D Ultrasound Volumes", "categories": ["eess.IV", "cs.CV", "physics.med-ph"], "comment": null, "summary": "Ultrasound imaging is widely used due to its safety, affordability, and\nreal-time capabilities, but its 2D interpretation is highly operator-dependent,\nleading to variability and increased cognitive demand. 2D-to-3D reconstruction\nmitigates these challenges by providing standardized volumetric views, yet\nexisting methods are often computationally expensive, memory-intensive, or\nincompatible with ultrasound physics. We introduce UltraGauss: the first\nultrasound-specific Gaussian Splatting framework, extending view synthesis\ntechniques to ultrasound wave propagation. Unlike conventional\nperspective-based splatting, UltraGauss models probe-plane intersections in 3D,\naligning with acoustic image formation. We derive an efficient rasterization\nboundary formulation for GPU parallelization and introduce a numerically stable\ncovariance parametrization, improving computational efficiency and\nreconstruction accuracy. On real clinical ultrasound data, UltraGauss achieves\nstate-of-the-art reconstructions in 5 minutes, and reaching 0.99 SSIM within 20\nminutes on a single GPU. A survey of expert clinicians confirms UltraGauss'\nreconstructions are the most realistic among competing methods. Our CUDA\nimplementation will be released upon publication."}
{"id": "2505.06108", "pdf": "https://arxiv.org/pdf/2505.06108", "abs": "https://arxiv.org/abs/2505.06108", "authors": ["Lennart Justen"], "title": "LLMs Outperform Experts on Challenging Biology Benchmarks", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "comment": null, "summary": "This study systematically evaluates 27 frontier Large Language Models on\neight diverse biology benchmarks spanning molecular biology, genetics, cloning,\nvirology, and biosecurity. Models from major AI developers released between\nNovember 2022 and April 2025 were assessed through ten independent runs per\nbenchmark. The findings reveal dramatic improvements in biological\ncapabilities. Top model performance increased more than 4-fold on the\nchallenging text-only subset of the Virology Capabilities Test over the study\nperiod, with the top model now performing twice as well as expert virologists.\nSeveral models now match or exceed expert-level performance on other\nchallenging benchmarks, including LAB-Bench CloningScenarios and the biology\nsubsets of GPQA and WMDP. Contrary to expectations, chain-of-thought did not\nsubstantially improve performance over zero-shot evaluation, while extended\nreasoning features in o3-mini and Claude 3.7 Sonnet typically improved\nperformance as predicted by inference scaling. Benchmarks such as PubMedQA and\nthe MMLU and WMDP biology subsets exhibited performance plateaus well below\n100%, suggesting benchmark saturation and errors in the underlying benchmark\ndata. The analysis highlights the need for more sophisticated evaluation\nmethodologies as AI systems continue to advance."}
{"id": "2505.05647", "pdf": "https://arxiv.org/pdf/2505.05647", "abs": "https://arxiv.org/abs/2505.05647", "authors": ["Chin-Cheng Chan", "Justin P. Haldar"], "title": "A New k-Space Model for Non-Cartesian Fourier Imaging", "categories": ["eess.SP", "cs.CV"], "comment": null, "summary": "For the past several decades, it has been popular to reconstruct Fourier\nimaging data using model-based approaches that can easily incorporate physical\nconstraints and advanced regularization/machine learning priors. The most\ncommon modeling approach is to represent the continuous image as a linear\ncombination of shifted \"voxel\" basis functions. Although well-studied and\nwidely-deployed, this voxel-based model is associated with longstanding\nlimitations, including high computational costs, slow convergence, and a\npropensity for artifacts. In this work, we reexamine this model from a fresh\nperspective, identifying new issues that may have been previously overlooked\n(including undesirable approximation, periodicity, and nullspace\ncharacteristics). Our insights motivate us to propose a new model that is more\nresilient to the limitations (old and new) of the previous approach.\nSpecifically, the new model is based on a Fourier-domain basis expansion rather\nthan the standard image-domain voxel-based approach. Illustrative results,\nwhich are presented in the context of non-Cartesian MRI reconstruction,\ndemonstrate that the new model enables improved image quality (reduced\nartifacts) and/or reduced computational complexity (faster computations and\nimproved convergence)."}
{"id": "2505.06110", "pdf": "https://arxiv.org/pdf/2505.06110", "abs": "https://arxiv.org/abs/2505.06110", "authors": ["Jugal Gajjar", "Kaustik Ranaware"], "title": "Multimodal Sentiment Analysis on CMU-MOSEI Dataset using Transformer-based Models", "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 2 figures, 5 tables, and 19 references", "summary": "This project performs multimodal sentiment analysis using the CMU-MOSEI\ndataset, using transformer-based models with early fusion to integrate text,\naudio, and visual modalities. We employ BERT-based encoders for each modality,\nextracting embeddings that are concatenated before classification. The model\nachieves strong performance, with 97.87\\% 7-class accuracy and a 0.9682\nF1-score on the test set, demonstrating the effectiveness of early fusion in\ncapturing cross-modal interactions. The training utilized Adam optimization\n(lr=1e-4), dropout (0.3), and early stopping to ensure generalization and\nrobustness. Results highlight the superiority of transformer architectures in\nmodeling multimodal sentiment, with a low MAE (0.1060) indicating precise\nsentiment intensity prediction. Future work may compare fusion strategies or\nenhance interpretability. This approach utilizes multimodal learning by\neffectively combining linguistic, acoustic, and visual cues for sentiment\nanalysis."}
{"id": "2505.05659", "pdf": "https://arxiv.org/pdf/2505.05659", "abs": "https://arxiv.org/abs/2505.05659", "authors": ["Guilherme Vieira Neto", "Marcos Eduardo Valle"], "title": "V-EfficientNets: Vector-Valued Efficiently Scaled Convolutional Neural Network Models", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "Accepted at International Joint Conference on Neural Networks (IJCNN\n  2025)", "summary": "EfficientNet models are convolutional neural networks optimized for parameter\nallocation by jointly balancing network width, depth, and resolution. Renowned\nfor their exceptional accuracy, these models have become a standard for image\nclassification tasks across diverse computer vision benchmarks. While\ntraditional neural networks learn correlations between feature channels during\ntraining, vector-valued neural networks inherently treat multidimensional data\nas coherent entities, taking for granted the inter-channel relationships. This\npaper introduces vector-valued EfficientNets (V-EfficientNets), a novel\nextension of EfficientNet designed to process arbitrary vector-valued data. The\nproposed models are evaluated on a medical image classification task, achieving\nan average accuracy of 99.46% on the ALL-IDB2 dataset for detecting acute\nlymphoblastic leukemia. V-EfficientNets demonstrate remarkable efficiency,\nsignificantly reducing parameters while outperforming state-of-the-art models,\nincluding the original EfficientNet. The source code is available at\nhttps://github.com/mevalle/v-nets."}
{"id": "2505.06111", "pdf": "https://arxiv.org/pdf/2505.06111", "abs": "https://arxiv.org/abs/2505.06111", "authors": ["Qingwen Bu", "Yanting Yang", "Jisong Cai", "Shenyuan Gao", "Guanghui Ren", "Maoqing Yao", "Ping Luo", "Hongyang Li"], "title": "UniVLA: Learning to Act Anywhere with Task-centric Latent Actions", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Accepted to RSS 2025. Code is available at\n  https://github.com/OpenDriveLab/UniVLA", "summary": "A generalist robot should perform effectively across various environments.\nHowever, most existing approaches heavily rely on scaling action-annotated data\nto enhance their capabilities. Consequently, they are often limited to single\nphysical specification and struggle to learn transferable knowledge across\ndifferent embodiments and environments. To confront these limitations, we\npropose UniVLA, a new framework for learning cross-embodiment\nvision-language-action (VLA) policies. Our key innovation is to derive\ntask-centric action representations from videos with a latent action model.\nThis enables us to exploit extensive data across a wide spectrum of embodiments\nand perspectives. To mitigate the effect of task-irrelevant dynamics, we\nincorporate language instructions and establish a latent action model within\nthe DINO feature space. Learned from internet-scale videos, the generalist\npolicy can be deployed to various robots through efficient latent action\ndecoding. We obtain state-of-the-art results across multiple manipulation and\nnavigation benchmarks, as well as real-robot deployments. UniVLA achieves\nsuperior performance over OpenVLA with less than 1/20 of pretraining compute\nand 1/10 of downstream data. Continuous performance improvements are observed\nas heterogeneous data, even including human videos, are incorporated into the\ntraining pipeline. The results underscore UniVLA's potential to facilitate\nscalable and efficient robot policy learning."}
{"id": "2505.05689", "pdf": "https://arxiv.org/pdf/2505.05689", "abs": "https://arxiv.org/abs/2505.05689", "authors": ["Fuyao Chen", "Yuexi Du", "Tal Zeevi", "Nicha C. Dvornek", "John A. Onofrey"], "title": "Equivariant Imaging Biomarkers for Robust Unsupervised Segmentation of Histopathology", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "Accepted by MIDL 2025", "summary": "Histopathology evaluation of tissue specimens through microscopic examination\nis essential for accurate disease diagnosis and prognosis. However, traditional\nmanual analysis by specially trained pathologists is time-consuming,\nlabor-intensive, cost-inefficient, and prone to inter-rater variability,\npotentially affecting diagnostic consistency and accuracy. As digital pathology\nimages continue to proliferate, there is a pressing need for automated analysis\nto address these challenges. Recent advancements in artificial\nintelligence-based tools such as machine learning (ML) models, have\nsignificantly enhanced the precision and efficiency of analyzing\nhistopathological slides. However, despite their impressive performance, ML\nmodels are invariant only to translation, lacking invariance to rotation and\nreflection. This limitation restricts their ability to generalize effectively,\nparticularly in histopathology, where images intrinsically lack meaningful\norientation. In this study, we develop robust, equivariant histopathological\nbiomarkers through a novel symmetric convolutional kernel via unsupervised\nsegmentation. The approach is validated using prostate tissue micro-array (TMA)\nimages from 50 patients in the Gleason 2019 Challenge public dataset. The\nbiomarkers extracted through this approach demonstrate enhanced robustness and\ngeneralizability against rotation compared to models using standard convolution\nkernels, holding promise for enhancing the accuracy, consistency, and\nrobustness of ML models in digital pathology. Ultimately, this work aims to\nimprove diagnostic and prognostic capabilities of histopathology beyond\nprostate cancer through equivariant imaging."}
{"id": "2505.06118", "pdf": "https://arxiv.org/pdf/2505.06118", "abs": "https://arxiv.org/abs/2505.06118", "authors": ["Jingguo Qu", "Xinyang Han", "Man-Lik Chui", "Yao Pu", "Simon Takadiyi Gunda", "Ziman Chen", "Jing Qin", "Ann Dorothy King", "Winnie Chiu-Wing Chu", "Jing Cai", "Michael Tin-Cheung Ying"], "title": "The Application of Deep Learning for Lymph Node Segmentation: A Systematic Review", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Automatic lymph node segmentation is the cornerstone for advances in computer\nvision tasks for early detection and staging of cancer. Traditional\nsegmentation methods are constrained by manual delineation and variability in\noperator proficiency, limiting their ability to achieve high accuracy. The\nintroduction of deep learning technologies offers new possibilities for\nimproving the accuracy of lymph node image analysis. This study evaluates the\napplication of deep learning in lymph node segmentation and discusses the\nmethodologies of various deep learning architectures such as convolutional\nneural networks, encoder-decoder networks, and transformers in analyzing\nmedical imaging data across different modalities. Despite the advancements, it\nstill confronts challenges like the shape diversity of lymph nodes, the\nscarcity of accurately labeled datasets, and the inadequate development of\nmethods that are robust and generalizable across different imaging modalities.\nTo the best of our knowledge, this is the first study that provides a\ncomprehensive overview of the application of deep learning techniques in lymph\nnode segmentation task. Furthermore, this study also explores potential future\nresearch directions, including multimodal fusion techniques, transfer learning,\nand the use of large-scale pre-trained models to overcome current limitations\nwhile enhancing cancer diagnosis and treatment planning strategies."}
{"id": "2505.05703", "pdf": "https://arxiv.org/pdf/2505.05703", "abs": "https://arxiv.org/abs/2505.05703", "authors": ["Haoyang Pei", "Ding Xia", "Xiang Xu", "William Moore", "Yao Wang", "Hersh Chandarana", "Li Feng"], "title": "Hybrid Learning: A Novel Combination of Self-Supervised and Supervised Learning for MRI Reconstruction without High-Quality Training Reference", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Purpose: Deep learning has demonstrated strong potential for MRI\nreconstruction, but conventional supervised learning methods require\nhigh-quality reference images, which are often unavailable in practice.\nSelf-supervised learning offers an alternative, yet its performance degrades at\nhigh acceleration rates. To overcome these limitations, we propose hybrid\nlearning, a novel two-stage training framework that combines self-supervised\nand supervised learning for robust image reconstruction.\n  Methods: Hybrid learning is implemented in two sequential stages. In the\nfirst stage, self-supervised learning is employed to generate improved images\nfrom noisy or undersampled reference data. These enhanced images then serve as\npseudo-ground truths for the second stage, which uses supervised learning to\nrefine reconstruction performance and support higher acceleration rates. We\nevaluated hybrid learning in two representative applications: (1) accelerated\n0.55T spiral-UTE lung MRI using noisy reference data, and (2) 3D T1 mapping of\nthe brain without access to fully sampled ground truth.\n  Results: For spiral-UTE lung MRI, hybrid learning consistently improved image\nquality over both self-supervised and conventional supervised methods across\ndifferent acceleration rates, as measured by SSIM and NMSE. For 3D T1 mapping,\nhybrid learning achieved superior T1 quantification accuracy across a wide\ndynamic range, outperforming self-supervised learning in all tested conditions.\n  Conclusions: Hybrid learning provides a practical and effective solution for\ntraining deep MRI reconstruction networks when only low-quality or incomplete\nreference data are available. It enables improved image quality and accurate\nquantitative mapping across different applications and field strengths,\nrepresenting a promising technique toward broader clinical deployment of deep\nlearning-based MRI."}
{"id": "2505.06123", "pdf": "https://arxiv.org/pdf/2505.06123", "abs": "https://arxiv.org/abs/2505.06123", "authors": ["Philip Naumann", "Jacob Kauffmann", "GrÃ©goire Montavon"], "title": "Wasserstein Distances Made Explainable: Insights into Dataset Shifts and Transport Phenomena", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Wasserstein distances provide a powerful framework for comparing data\ndistributions. They can be used to analyze processes over time or to detect\ninhomogeneities within data. However, simply calculating the Wasserstein\ndistance or analyzing the corresponding transport map (or coupling) may not be\nsufficient for understanding what factors contribute to a high or low\nWasserstein distance. In this work, we propose a novel solution based on\nExplainable AI that allows us to efficiently and accurately attribute\nWasserstein distances to various data components, including data subgroups,\ninput features, or interpretable subspaces. Our method achieves high accuracy\nacross diverse datasets and Wasserstein distance specifications, and its\npractical utility is demonstrated in two use cases."}
{"id": "2505.05732", "pdf": "https://arxiv.org/pdf/2505.05732", "abs": "https://arxiv.org/abs/2505.05732", "authors": ["Limai Jiang", "Yunpeng Cai"], "title": "Automated Learning of Semantic Embedding Representations for Diffusion Models", "categories": ["cs.LG", "cs.CV"], "comment": "Extended version of the paper published in SDM25", "summary": "Generative models capture the true distribution of data, yielding\nsemantically rich representations. Denoising diffusion models (DDMs) exhibit\nsuperior generative capabilities, though efficient representation learning for\nthem are lacking. In this work, we employ a multi-level denoising autoencoder\nframework to expand the representation capacity of DDMs, which introduces\nsequentially consistent Diffusion Transformers and an additional\ntimestep-dependent encoder to acquire embedding representations on the\ndenoising Markov chain through self-conditional diffusion learning.\nIntuitively, the encoder, conditioned on the entire diffusion process,\ncompresses high-dimensional data into directional vectors in latent under\ndifferent noise levels, facilitating the learning of image embeddings across\nall timesteps. To verify the semantic adequacy of embeddings generated through\nthis approach, extensive experiments are conducted on various datasets,\ndemonstrating that optimally learned embeddings by DDMs surpass\nstate-of-the-art self-supervised representation learning methods in most cases,\nachieving remarkable discriminative semantic representation quality. Our work\njustifies that DDMs are not only suitable for generative tasks, but also\npotentially advantageous for general-purpose deep learning applications."}
{"id": "2505.06136", "pdf": "https://arxiv.org/pdf/2505.06136", "abs": "https://arxiv.org/abs/2505.06136", "authors": ["Yifeng Zhu"], "title": "Efficient Sensorimotor Learning for Open-world Robot Manipulation", "categories": ["cs.RO", "cs.AI"], "comment": "Ph.D. Dissertation", "summary": "This dissertation considers Open-world Robot Manipulation, a manipulation\nproblem where a robot must generalize or quickly adapt to new objects, scenes,\nor tasks for which it has not been pre-programmed or pre-trained. This\ndissertation tackles the problem using a methodology of efficient sensorimotor\nlearning. The key to enabling efficient sensorimotor learning lies in\nleveraging regular patterns that exist in limited amounts of demonstration\ndata. These patterns, referred to as ``regularity,'' enable the data-efficient\nlearning of generalizable manipulation skills. This dissertation offers a new\nperspective on formulating manipulation problems through the lens of\nregularity. Building upon this notion, we introduce three major contributions.\nFirst, we introduce methods that endow robots with object-centric priors,\nallowing them to learn generalizable, closed-loop sensorimotor policies from a\nsmall number of teleoperation demonstrations. Second, we introduce methods that\nconstitute robots' spatial understanding, unlocking their ability to imitate\nmanipulation skills from in-the-wild video observations. Last but not least, we\nintroduce methods that enable robots to identify reusable skills from their\npast experiences, resulting in systems that can continually imitate multiple\ntasks in a sequential manner. Altogether, the contributions of this\ndissertation help lay the groundwork for building general-purpose personal\nrobots that can quickly adapt to new situations or tasks with low-cost data\ncollection and interact easily with humans. By enabling robots to learn and\ngeneralize from limited data, this dissertation takes a step toward realizing\nthe vision of intelligent robotic assistants that can be seamlessly integrated\ninto everyday scenarios."}
{"id": "2505.05736", "pdf": "https://arxiv.org/pdf/2505.05736", "abs": "https://arxiv.org/abs/2505.05736", "authors": ["Da Wu", "Zhanliang Wang", "Quan Nguyen", "Zhuoran Xu", "Kai Wang"], "title": "Multimodal Integrated Knowledge Transfer to Large Language Models through Preference Optimization with Biomedical Applications", "categories": ["q-bio.QM", "cs.CL", "cs.CV", "cs.LG"], "comment": "First Draft", "summary": "The scarcity of high-quality multimodal biomedical data limits the ability to\neffectively fine-tune pretrained Large Language Models (LLMs) for specialized\nbiomedical tasks. To address this challenge, we introduce MINT (Multimodal\nIntegrated kNowledge Transfer), a framework that aligns unimodal large decoder\nmodels with domain-specific decision patterns from multimodal biomedical data\nthrough preference optimization. While MINT supports different optimization\ntechniques, we primarily implement it with the Odds Ratio Preference\nOptimization (ORPO) framework as its backbone. This strategy enables the\naligned LLMs to perform predictive tasks using text-only or image-only inputs\nwhile retaining knowledge learnt from multimodal data. MINT leverages an\nupstream multimodal machine learning (MML) model trained on high-quality\nmultimodal data to transfer domain-specific insights to downstream text-only or\nimage-only LLMs. We demonstrate its effectiveness through two key applications:\n(1) Rare genetic disease prediction from texts, where MINT uses a multimodal\nencoder model, trained on facial photos and clinical notes, to generate a\npreference dataset for aligning a lightweight Llama 3.2-3B-Instruct. Despite\nrelying on text input only, the MINT-derived model outperforms models trained\nwith SFT, RAG, or DPO, and even outperforms Llama 3.1-405B-Instruct. (2) Tissue\ntype classification using cell nucleus images, where MINT uses a\nvision-language foundation model as the preference generator, containing\nknowledge learnt from both text and histopathological images to align\ndownstream image-only models. The resulting MINT-derived model significantly\nimproves the performance of Llama 3.2-Vision-11B-Instruct on tissue type\nclassification. In summary, MINT provides an effective strategy to align\nunimodal LLMs with high-quality multimodal expertise through preference\noptimization."}
{"id": "2505.06150", "pdf": "https://arxiv.org/pdf/2505.06150", "abs": "https://arxiv.org/abs/2505.06150", "authors": ["Ryan Lagasse", "Aidan Kiernans", "Avijit Ghosh", "Shiri Dori-Hacohen"], "title": "A Scaling Law for Token Efficiency in LLM Fine-Tuning Under Fixed Compute Budgets", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce a scaling law for fine-tuning large language models (LLMs) under\nfixed compute budgets that explicitly accounts for data composition.\nConventional approaches measure training data solely by total tokens, yet the\nnumber of examples and their average token length -- what we term \\emph{dataset\nvolume} -- play a decisive role in model performance. Our formulation is tuned\nfollowing established procedures. Experiments on the BRICC dataset\n\\cite{salavati2024reducing} and subsets of the MMLU dataset\n\\cite{hendrycks2021measuringmassivemultitasklanguage}, evaluated under multiple\nsubsampling strategies, reveal that data composition significantly affects\ntoken efficiency. These results motivate refined scaling laws for practical LLM\nfine-tuning in resource-constrained settings."}
{"id": "2505.05768", "pdf": "https://arxiv.org/pdf/2505.05768", "abs": "https://arxiv.org/abs/2505.05768", "authors": ["Weiyi Zhang", "Peranut Chotcomwongse", "Yinwen Li", "Pusheng Xu", "Ruijie Yao", "Lianhao Zhou", "Yuxuan Zhou", "Hui Feng", "Qiping Zhou", "Xinyue Wang", "Shoujin Huang", "Zihao Jin", "Florence H. T. Chung", "Shujun Wang", "Yalin Zheng", "Mingguang He", "Danli Shi", "Paisan Ruamviboonsuk"], "title": "Predicting Diabetic Macular Edema Treatment Responses Using OCT: Dataset and Methods of APTOS Competition", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "42 pages,5 tables, 12 figures, challenge report", "summary": "Diabetic macular edema (DME) significantly contributes to visual impairment\nin diabetic patients. Treatment responses to intravitreal therapies vary,\nhighlighting the need for patient stratification to predict therapeutic\nbenefits and enable personalized strategies. To our knowledge, this study is\nthe first to explore pre-treatment stratification for predicting DME treatment\nresponses. To advance this research, we organized the 2nd Asia-Pacific\nTele-Ophthalmology Society (APTOS) Big Data Competition in 2021. The\ncompetition focused on improving predictive accuracy for anti-VEGF therapy\nresponses using ophthalmic OCT images. We provided a dataset containing tens of\nthousands of OCT images from 2,000 patients with labels across four sub-tasks.\nThis paper details the competition's structure, dataset, leading methods, and\nevaluation metrics. The competition attracted strong scientific community\nparticipation, with 170 teams initially registering and 41 reaching the final\nround. The top-performing team achieved an AUC of 80.06%, highlighting the\npotential of AI in personalized DME treatment and clinical decision-making."}
{"id": "2505.06152", "pdf": "https://arxiv.org/pdf/2505.06152", "abs": "https://arxiv.org/abs/2505.06152", "authors": ["Wenqi Zeng", "Yuqi Sun", "Chenxi Ma", "Weimin Tan", "Bo Yan"], "title": "MM-Skin: Enhancing Dermatology Vision-Language Model with an Image-Text Dataset Derived from Textbooks", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Medical vision-language models (VLMs) have shown promise as clinical\nassistants across various medical fields. However, specialized dermatology VLM\ncapable of delivering professional and detailed diagnostic analysis remains\nunderdeveloped, primarily due to less specialized text descriptions in current\ndermatology multimodal datasets. To address this issue, we propose MM-Skin, the\nfirst large-scale multimodal dermatology dataset that encompasses 3 imaging\nmodalities, including clinical, dermoscopic, and pathological and nearly 10k\nhigh-quality image-text pairs collected from professional textbooks. In\naddition, we generate over 27k diverse, instruction-following vision question\nanswering (VQA) samples (9 times the size of current largest dermatology VQA\ndataset). Leveraging public datasets and MM-Skin, we developed SkinVL, a\ndermatology-specific VLM designed for precise and nuanced skin disease\ninterpretation. Comprehensive benchmark evaluations of SkinVL on VQA,\nsupervised fine-tuning (SFT) and zero-shot classification tasks across 8\ndatasets, reveal its exceptional performance for skin diseases in comparison to\nboth general and medical VLM models. The introduction of MM-Skin and SkinVL\noffers a meaningful contribution to advancing the development of clinical\ndermatology VLM assistants. MM-Skin is available at\nhttps://github.com/ZwQ803/MM-Skin"}
{"id": "2505.05798", "pdf": "https://arxiv.org/pdf/2505.05798", "abs": "https://arxiv.org/abs/2505.05798", "authors": ["Youngjoon Lee", "Jinu Gong", "Joonhyuk Kang"], "title": "Improving Generalizability of Kolmogorov-Arnold Networks via Error-Correcting Output Codes", "categories": ["cs.LG", "cs.CV", "eess.IV", "eess.SP"], "comment": "4 pages", "summary": "Kolmogorov-Arnold Networks (KAN) offer universal function approximation using\nunivariate spline compositions without nonlinear activations. In this work, we\nintegrate Error-Correcting Output Codes (ECOC) into the KAN framework to\ntransform multi-class classification into multiple binary tasks, improving\nrobustness via Hamming-distance decoding. Our proposed KAN with ECOC method\noutperforms vanilla KAN on a challenging blood cell classification dataset,\nachieving higher accuracy under diverse hyperparameter settings. Ablation\nstudies further confirm that ECOC consistently enhances performance across\nFastKAN and FasterKAN variants. These results demonstrate that ECOC integration\nsignificantly boosts KAN generalizability in critical healthcare AI\napplications. To the best of our knowledge, this is the first integration of\nECOC with KAN for enhancing multi-class medical image classification\nperformance."}
{"id": "2505.06175", "pdf": "https://arxiv.org/pdf/2505.06175", "abs": "https://arxiv.org/abs/2505.06175", "authors": ["Zihang Song", "Matteo Zecchin", "Bipin Rajendran", "Osvaldo Simeone"], "title": "Turbo-ICL: In-Context Learning-Based Turbo Equalization", "categories": ["eess.SP", "cs.AI"], "comment": null, "summary": "This paper introduces a novel in-context learning (ICL) framework, inspired\nby large language models (LLMs), for soft-input soft-output channel\nequalization in coded multiple-input multiple-output (MIMO) systems. The\nproposed approach learns to infer posterior symbol distributions directly from\na prompt of pilot signals and decoder feedback. A key innovation is the use of\nprompt augmentation to incorporate extrinsic information from the decoder\noutput as additional context, enabling the ICL model to refine its symbol\nestimates iteratively across turbo decoding iterations. Two model variants,\nbased on Transformer and state-space architectures, are developed and\nevaluated. Extensive simulations demonstrate that, when traditional linear\nassumptions break down, e.g., in the presence of low-resolution quantization,\nICL equalizers consistently outperform conventional model-based baselines, even\nwhen the latter are provided with perfect channel state information. Results\nalso highlight the advantage of Transformer-based models under limited training\ndiversity, as well as the efficiency of state-space models in\nresource-constrained scenarios."}
{"id": "2505.05800", "pdf": "https://arxiv.org/pdf/2505.05800", "abs": "https://arxiv.org/abs/2505.05800", "authors": ["Vineet Bhat", "Yu-Hsiang Lan", "Prashanth Krishnamurthy", "Ramesh Karri", "Farshad Khorrami"], "title": "3D CAVLA: Leveraging Depth and 3D Context to Generalize Vision Language Action Models for Unseen Tasks", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted at the 1st Workshop on 3D LLM/VLA, CVPR 2025", "summary": "Robotic manipulation in 3D requires learning an $N$ degree-of-freedom joint\nspace trajectory of a robot manipulator. Robots must possess semantic and\nvisual perception abilities to transform real-world mappings of their workspace\ninto the low-level control necessary for object manipulation. Recent work has\ndemonstrated the capabilities of fine-tuning large Vision-Language Models\n(VLMs) to learn the mapping between RGB images, language instructions, and\njoint space control. These models typically take as input RGB images of the\nworkspace and language instructions, and are trained on large datasets of\nteleoperated robot demonstrations. In this work, we explore methods to improve\nthe scene context awareness of a popular recent Vision-Language-Action model by\nintegrating chain-of-thought reasoning, depth perception, and task-oriented\nregion of interest detection. Our experiments in the LIBERO simulation\nenvironment show that our proposed model, 3D-CAVLA, improves the success rate\nacross various LIBERO task suites, achieving an average success rate of\n98.1$\\%$. We also evaluate the zero-shot capabilities of our method,\ndemonstrating that 3D scene awareness leads to robust learning and adaptation\nfor completely unseen tasks. 3D-CAVLA achieves an absolute improvement of\n8.8$\\%$ on unseen tasks. We will open-source our code and the unseen tasks\ndataset to promote community-driven research here: https://3d-cavla.github.io"}
{"id": "2505.06186", "pdf": "https://arxiv.org/pdf/2505.06186", "abs": "https://arxiv.org/abs/2505.06186", "authors": ["Massimiliano Pronesti", "Joao Bettencourt-Silva", "Paul Flanagan", "Alessandra Pascale", "Oisin Redmond", "Anya Belz", "Yufang Hou"], "title": "Query-driven Document-level Scientific Evidence Extraction from Biomedical Studies", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Extracting scientific evidence from biomedical studies for clinical research\nquestions (e.g., Does stem cell transplantation improve quality of life in\npatients with medically refractory Crohn's disease compared to placebo?) is a\ncrucial step in synthesising biomedical evidence. In this paper, we focus on\nthe task of document-level scientific evidence extraction for clinical\nquestions with conflicting evidence. To support this task, we create a dataset\ncalled CochraneForest, leveraging forest plots from Cochrane systematic\nreviews. It comprises 202 annotated forest plots, associated clinical research\nquestions, full texts of studies, and study-specific conclusions. Building on\nCochraneForest, we propose URCA (Uniform Retrieval Clustered Augmentation), a\nretrieval-augmented generation framework designed to tackle the unique\nchallenges of evidence extraction. Our experiments show that URCA outperforms\nthe best existing methods by up to 10.3% in F1 score on this task. However, the\nresults also underscore the complexity of CochraneForest, establishing it as a\nchallenging testbed for advancing automated evidence synthesis systems."}
{"id": "2505.05812", "pdf": "https://arxiv.org/pdf/2505.05812", "abs": "https://arxiv.org/abs/2505.05812", "authors": ["Ashkan Pakzad", "Robert Turnbull", "Simon J. Mutch", "Thomas A. Leatham", "Darren Lockie", "Jane Fox", "Beena Kumar", "Daniel HÃ¤sermann", "Christopher J. Hall", "Anton Maksimenko", "Benedicta D. Arhatari", "Yakov I. Nesterets", "Amir Entezam", "Seyedamir T. Taba", "Patrick C. Brennan", "Timur E. Gureyev", "Harry M. Quiney"], "title": "Towards order of magnitude X-ray dose reduction in breast cancer imaging using phase contrast and deep denoising", "categories": ["physics.med-ph", "cs.CV"], "comment": "16 pages, 3 figures, 1 table", "summary": "Breast cancer is the most frequently diagnosed human cancer in the United\nStates at present. Early detection is crucial for its successful treatment.\nX-ray mammography and digital breast tomosynthesis are currently the main\nmethods for breast cancer screening. However, both have known limitations in\nterms of their sensitivity and specificity to breast cancers, while also\nfrequently causing patient discomfort due to the requirement for breast\ncompression. Breast computed tomography is a promising alternative, however, to\nobtain high-quality images, the X-ray dose needs to be sufficiently high. As\nthe breast is highly radiosensitive, dose reduction is particularly important.\nPhase-contrast computed tomography (PCT) has been shown to produce\nhigher-quality images at lower doses and has no need for breast compression. It\nis demonstrated in the present study that, when imaging full fresh mastectomy\nsamples with PCT, deep learning-based image denoising can further reduce the\nradiation dose by a factor of 16 or more, without any loss of image quality.\nThe image quality has been assessed both in terms of objective metrics, such as\nspatial resolution and contrast-to-noise ratio, as well as in an observer study\nby experienced medical imaging specialists and radiologists. This work was\ncarried out in preparation for live patient PCT breast cancer imaging,\ninitially at specialized synchrotron facilities."}
{"id": "2505.06218", "pdf": "https://arxiv.org/pdf/2505.06218", "abs": "https://arxiv.org/abs/2505.06218", "authors": ["Kwan-Yee Lin", "Stella X. Yu"], "title": "Let Humanoids Hike! Integrative Skill Development on Complex Trails", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "CVPR 2025. Project page:\n  https://lego-h-humanoidrobothiking.github.io/", "summary": "Hiking on complex trails demands balance, agility, and adaptive\ndecision-making over unpredictable terrain. Current humanoid research remains\nfragmented and inadequate for hiking: locomotion focuses on motor skills\nwithout long-term goals or situational awareness, while semantic navigation\noverlooks real-world embodiment and local terrain variability. We propose\ntraining humanoids to hike on complex trails, driving integrative skill\ndevelopment across visual perception, decision making, and motor execution. We\ndevelop a learning framework, LEGO-H, that enables a vision-equipped humanoid\nrobot to hike complex trails autonomously. We introduce two technical\ninnovations: 1) A temporal vision transformer variant - tailored into\nHierarchical Reinforcement Learning framework - anticipates future local goals\nto guide movement, seamlessly integrating locomotion with goal-directed\nnavigation. 2) Latent representations of joint movement patterns, combined with\nhierarchical metric learning - enhance Privileged Learning scheme - enable\nsmooth policy transfer from privileged training to onboard execution. These\ncomponents allow LEGO-H to handle diverse physical and environmental challenges\nwithout relying on predefined motion patterns. Experiments across varied\nsimulated trails and robot morphologies highlight LEGO-H's versatility and\nrobustness, positioning hiking as a compelling testbed for embodied autonomy\nand LEGO-H as a baseline for future humanoid development."}
{"id": "2505.05957", "pdf": "https://arxiv.org/pdf/2505.05957", "abs": "https://arxiv.org/abs/2505.05957", "authors": ["Peter RÃ¶seler", "Oliver Schaudt", "Helmut Berg", "Christian Bauckhage", "Matthias Koch"], "title": "Efficient Quantum Convolutional Neural Networks for Image Classification: Overcoming Hardware Constraints", "categories": ["quant-ph", "cs.CV", "cs.LG"], "comment": null, "summary": "While classical convolutional neural networks (CNNs) have revolutionized\nimage classification, the emergence of quantum computing presents new\nopportunities for enhancing neural network architectures. Quantum CNNs (QCNNs)\nleverage quantum mechanical properties and hold potential to outperform\nclassical approaches. However, their implementation on current noisy\nintermediate-scale quantum (NISQ) devices remains challenging due to hardware\nlimitations. In our research, we address this challenge by introducing an\nencoding scheme that significantly reduces the input dimensionality. We\ndemonstrate that a primitive QCNN architecture with 49 qubits is sufficient to\ndirectly process $28\\times 28$ pixel MNIST images, eliminating the need for\nclassical dimensionality reduction pre-processing. Additionally, we propose an\nautomated framework based on expressibility, entanglement, and complexity\ncharacteristics to identify the building blocks of QCNNs, parameterized quantum\ncircuits (PQCs). Our approach demonstrates advantages in accuracy and\nconvergence speed with a similar parameter count compared to both hybrid QCNNs\nand classical CNNs. We validated our experiments on IBM's Heron r2 quantum\nprocessor, achieving $96.08\\%$ classification accuracy, surpassing the\n$71.74\\%$ benchmark of traditional approaches under identical training\nconditions. These results represent one of the first implementations of image\nclassifications on real quantum hardware and validate the potential of quantum\ncomputing in this area."}
{"id": "2505.06020", "pdf": "https://arxiv.org/pdf/2505.06020", "abs": "https://arxiv.org/abs/2505.06020", "authors": ["Shuai Wang", "Ivona Najdenkoska", "Hongyi Zhu", "Stevan Rudinac", "Monika Kackovic", "Nachoem Wijnberg", "Marcel Worring"], "title": "ArtRAG: Retrieval-Augmented Generation with Structured Context for Visual Art Understanding", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Understanding visual art requires reasoning across multiple perspectives --\ncultural, historical, and stylistic -- beyond mere object recognition. While\nrecent multimodal large language models (MLLMs) perform well on general image\ncaptioning, they often fail to capture the nuanced interpretations that fine\nart demands. We propose ArtRAG, a novel, training-free framework that combines\nstructured knowledge with retrieval-augmented generation (RAG) for\nmulti-perspective artwork explanation. ArtRAG automatically constructs an Art\nContext Knowledge Graph (ACKG) from domain-specific textual sources, organizing\nentities such as artists, movements, themes, and historical events into a rich,\ninterpretable graph. At inference time, a multi-granular structured retriever\nselects semantically and topologically relevant subgraphs to guide generation.\nThis enables MLLMs to produce contextually grounded, culturally informed art\ndescriptions. Experiments on the SemArt and Artpedia datasets show that ArtRAG\noutperforms several heavily trained baselines. Human evaluations further\nconfirm that ArtRAG generates coherent, insightful, and culturally enriched\ninterpretations."}
{"id": "2505.06030", "pdf": "https://arxiv.org/pdf/2505.06030", "abs": "https://arxiv.org/abs/2505.06030", "authors": ["Tobias Preintner", "Weixuan Yuan", "Qi Huang", "Adrian KÃ¶nig", "Thomas BÃ¤ck", "Elena Raponi", "Niki van Stein"], "title": "Why Are You Wrong? Counterfactual Explanations for Language Grounding with 3D Objects", "categories": ["cs.AI", "cs.CV", "cs.LG"], "comment": "Accepted at IJCNN 2025", "summary": "Combining natural language and geometric shapes is an emerging research area\nwith multiple applications in robotics and language-assisted design. A crucial\ntask in this domain is object referent identification, which involves selecting\na 3D object given a textual description of the target. Variability in language\ndescriptions and spatial relationships of 3D objects makes this a complex task,\nincreasing the need to better understand the behavior of neural network models\nin this domain. However, limited research has been conducted in this area.\nSpecifically, when a model makes an incorrect prediction despite being provided\nwith a seemingly correct object description, practitioners are left wondering:\n\"Why is the model wrong?\". In this work, we present a method answering this\nquestion by generating counterfactual examples. Our method takes a\nmisclassified sample, which includes two objects and a text description, and\ngenerates an alternative yet similar formulation that would have resulted in a\ncorrect prediction by the model. We have evaluated our approach with data from\nthe ShapeTalk dataset along with three distinct models. Our counterfactual\nexamples maintain the structure of the original description, are semantically\nsimilar and meaningful. They reveal weaknesses in the description, model bias\nand enhance the understanding of the models behavior. Theses insights help\npractitioners to better interact with systems as well as engineers to improve\nmodels."}
{"id": "2505.06079", "pdf": "https://arxiv.org/pdf/2505.06079", "abs": "https://arxiv.org/abs/2505.06079", "authors": ["Shuaiyi Huang", "Mara Levy", "Anubhav Gupta", "Daniel Ekpo", "Ruijie Zheng", "Abhinav Shrivastava"], "title": "TREND: Tri-teaching for Robust Preference-based Reinforcement Learning with Demonstrations", "categories": ["cs.RO", "cs.CV"], "comment": "ICRA 2025", "summary": "Preference feedback collected by human or VLM annotators is often noisy,\npresenting a significant challenge for preference-based reinforcement learning\nthat relies on accurate preference labels. To address this challenge, we\npropose TREND, a novel framework that integrates few-shot expert demonstrations\nwith a tri-teaching strategy for effective noise mitigation. Our method trains\nthree reward models simultaneously, where each model views its small-loss\npreference pairs as useful knowledge and teaches such useful pairs to its peer\nnetwork for updating the parameters. Remarkably, our approach requires as few\nas one to three expert demonstrations to achieve high performance. We evaluate\nTREND on various robotic manipulation tasks, achieving up to 90% success rates\neven with noise levels as high as 40%, highlighting its effective robustness in\nhandling noisy preference feedback. Project page:\nhttps://shuaiyihuang.github.io/publications/TREND."}
{"id": "2505.06105", "pdf": "https://arxiv.org/pdf/2505.06105", "abs": "https://arxiv.org/abs/2505.06105", "authors": ["Xilin Gong", "Yongkai Chen", "Shushan Wu", "Fang Wang", "Ping Ma", "Wenxuan Zhong"], "title": "S2MNet: Speckle-To-Mesh Net for Three-Dimensional Cardiac Morphology Reconstruction via Echocardiogram", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Echocardiogram is the most commonly used imaging modality in cardiac\nassessment duo to its non-invasive nature, real-time capability, and\ncost-effectiveness. Despite its advantages, most clinical echocardiograms\nprovide only two-dimensional views, limiting the ability to fully assess\ncardiac anatomy and function in three dimensions. While three-dimensional\nechocardiography exists, it often suffers from reduced resolution, limited\navailability, and higher acquisition costs. To overcome these challenges, we\npropose a deep learning framework S2MNet that reconstructs continuous and\nhigh-fidelity 3D heart models by integrating six slices of routinely acquired\n2D echocardiogram views. Our method has three advantages. First, our method\navoid the difficulties on training data acquasition by simulate six of 2D\nechocardiogram images from corresponding slices of a given 3D heart mesh.\nSecond, we introduce a deformation field-based method, which avoid spatial\ndiscontinuities or structural artifacts in 3D echocardiogram reconstructions.\nWe validate our method using clinically collected echocardiogram and\ndemonstrate that our estimated left ventricular volume, a key clinical\nindicator of cardiac function, is strongly correlated with the doctor measured\nGLPS, a clinical measurement that should demonstrate a negative correlation\nwith LVE in medical theory. This association confirms the reliability of our\nproposed 3D construction method."}
{"id": "2505.06118", "pdf": "https://arxiv.org/pdf/2505.06118", "abs": "https://arxiv.org/abs/2505.06118", "authors": ["Jingguo Qu", "Xinyang Han", "Man-Lik Chui", "Yao Pu", "Simon Takadiyi Gunda", "Ziman Chen", "Jing Qin", "Ann Dorothy King", "Winnie Chiu-Wing Chu", "Jing Cai", "Michael Tin-Cheung Ying"], "title": "The Application of Deep Learning for Lymph Node Segmentation: A Systematic Review", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Automatic lymph node segmentation is the cornerstone for advances in computer\nvision tasks for early detection and staging of cancer. Traditional\nsegmentation methods are constrained by manual delineation and variability in\noperator proficiency, limiting their ability to achieve high accuracy. The\nintroduction of deep learning technologies offers new possibilities for\nimproving the accuracy of lymph node image analysis. This study evaluates the\napplication of deep learning in lymph node segmentation and discusses the\nmethodologies of various deep learning architectures such as convolutional\nneural networks, encoder-decoder networks, and transformers in analyzing\nmedical imaging data across different modalities. Despite the advancements, it\nstill confronts challenges like the shape diversity of lymph nodes, the\nscarcity of accurately labeled datasets, and the inadequate development of\nmethods that are robust and generalizable across different imaging modalities.\nTo the best of our knowledge, this is the first study that provides a\ncomprehensive overview of the application of deep learning techniques in lymph\nnode segmentation task. Furthermore, this study also explores potential future\nresearch directions, including multimodal fusion techniques, transfer learning,\nand the use of large-scale pre-trained models to overcome current limitations\nwhile enhancing cancer diagnosis and treatment planning strategies."}
{"id": "2505.06123", "pdf": "https://arxiv.org/pdf/2505.06123", "abs": "https://arxiv.org/abs/2505.06123", "authors": ["Philip Naumann", "Jacob Kauffmann", "GrÃ©goire Montavon"], "title": "Wasserstein Distances Made Explainable: Insights into Dataset Shifts and Transport Phenomena", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Wasserstein distances provide a powerful framework for comparing data\ndistributions. They can be used to analyze processes over time or to detect\ninhomogeneities within data. However, simply calculating the Wasserstein\ndistance or analyzing the corresponding transport map (or coupling) may not be\nsufficient for understanding what factors contribute to a high or low\nWasserstein distance. In this work, we propose a novel solution based on\nExplainable AI that allows us to efficiently and accurately attribute\nWasserstein distances to various data components, including data subgroups,\ninput features, or interpretable subspaces. Our method achieves high accuracy\nacross diverse datasets and Wasserstein distance specifications, and its\npractical utility is demonstrated in two use cases."}
{"id": "2505.06176", "pdf": "https://arxiv.org/pdf/2505.06176", "abs": "https://arxiv.org/abs/2505.06176", "authors": ["Niladri Shekhar Dutt", "Duygu Ceylan", "Niloy J. Mitra"], "title": "MonetGPT: Solving Puzzles Enhances MLLMs' Image Retouching Skills", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "Accepted at SIGGRAPH 2025 [ACM Transactions on Graphics]; Project\n  website: https://monetgpt.github.io", "summary": "Retouching is an essential task in post-manipulation of raw photographs.\nGenerative editing, guided by text or strokes, provides a new tool accessible\nto users but can easily change the identity of the original objects in\nunacceptable and unpredictable ways. In contrast, although traditional\nprocedural edits, as commonly supported by photoediting tools (e.g., Gimp,\nLightroom), are conservative, they are still preferred by professionals.\nUnfortunately, professional quality retouching involves many individual\nprocedural editing operations that is challenging to plan for most novices. In\nthis paper, we ask if a multimodal large language model (MLLM) can be taught to\ncritique raw photographs, suggest suitable remedies, and finally realize them\nwith a given set of pre-authored procedural image operations. We demonstrate\nthat MLLMs can be first made aware of the underlying image processing\noperations, by training them to solve specially designed visual puzzles.\nSubsequently, such an operation-aware MLLM can both plan and propose edit\nsequences. To facilitate training, given a set of expert-edited photos, we\nsynthesize a reasoning dataset by procedurally manipulating the expert edits\nand then grounding a pretrained LLM on the visual adjustments, to synthesize\nreasoning for finetuning. The proposed retouching operations are, by\nconstruction, understandable by the users, preserve object details and\nresolution, and can be optionally overridden. We evaluate our setup on a\nvariety of test examples and show advantages, in terms of explainability and\nidentity preservation, over existing generative and other procedural\nalternatives. Code, data, models, and supplementary results can be found via\nour project website at https://monetgpt.github.io."}
{"id": "2505.06185", "pdf": "https://arxiv.org/pdf/2505.06185", "abs": "https://arxiv.org/abs/2505.06185", "authors": ["Kodai Hirata", "Tsuyoshi Okita"], "title": "Brain Hematoma Marker Recognition Using Multitask Learning: SwinTransformer and Swin-Unet", "categories": ["cs.LG", "cs.CV"], "comment": "8 pages,4 figures", "summary": "This paper proposes a method MTL-Swin-Unet which is multi-task learning using\ntransformers for classification and semantic segmentation. For\nspurious-correlation problems, this method allows us to enhance the image\nrepresentation with two other image representations: representation obtained by\nsemantic segmentation and representation obtained by image reconstruction. In\nour experiments, the proposed method outperformed in F-value measure than other\nclassifiers when the test data included slices from the same patient (no\ncovariate shift). Similarly, when the test data did not include slices from the\nsame patient (covariate shift setting), the proposed method outperformed in AUC\nmeasure."}
{"id": "2505.06191", "pdf": "https://arxiv.org/pdf/2505.06191", "abs": "https://arxiv.org/abs/2505.06191", "authors": ["Jiayuan Mao", "Joshua B. Tenenbaum", "Jiajun Wu"], "title": "Neuro-Symbolic Concepts", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.RO"], "comment": "To appear in Communications of the ACM", "summary": "This article presents a concept-centric paradigm for building agents that can\nlearn continually and reason flexibly. The concept-centric agent utilizes a\nvocabulary of neuro-symbolic concepts. These concepts, such as object,\nrelation, and action concepts, are grounded on sensory inputs and actuation\noutputs. They are also compositional, allowing for the creation of novel\nconcepts through their structural combination. To facilitate learning and\nreasoning, the concepts are typed and represented using a combination of\nsymbolic programs and neural network representations. Leveraging such\nneuro-symbolic concepts, the agent can efficiently learn and recombine them to\nsolve various tasks across different domains, ranging from 2D images, videos,\n3D scenes, and robotic manipulation tasks. This concept-centric framework\noffers several advantages, including data efficiency, compositional\ngeneralization, continual learning, and zero-shot transfer."}
{"id": "2505.06210", "pdf": "https://arxiv.org/pdf/2505.06210", "abs": "https://arxiv.org/abs/2505.06210", "authors": ["Diego Adame", "Jose A. Nunez", "Fabian Vazquez", "Nayeli Gurrola", "Huimin Li", "Haoteng Tang", "Bin Fu", "Pengfei Gu"], "title": "Topo-VM-UNetV2: Encoding Topology into Vision Mamba UNet for Polyp Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Convolutional neural network (CNN) and Transformer-based architectures are\ntwo dominant deep learning models for polyp segmentation. However, CNNs have\nlimited capability for modeling long-range dependencies, while Transformers\nincur quadratic computational complexity. Recently, State Space Models such as\nMamba have been recognized as a promising approach for polyp segmentation\nbecause they not only model long-range interactions effectively but also\nmaintain linear computational complexity. However, Mamba-based architectures\nstill struggle to capture topological features (e.g., connected components,\nloops, voids), leading to inaccurate boundary delineation and polyp\nsegmentation. To address these limitations, we propose a new approach called\nTopo-VM-UNetV2, which encodes topological features into the Mamba-based\nstate-of-the-art polyp segmentation model, VM-UNetV2. Our method consists of\ntwo stages: Stage 1: VM-UNetV2 is used to generate probability maps (PMs) for\nthe training and test images, which are then used to compute topology attention\nmaps. Specifically, we first compute persistence diagrams of the PMs, then we\ngenerate persistence score maps by assigning persistence values (i.e., the\ndifference between death and birth times) of each topological feature to its\nbirth location, finally we transform persistence scores into attention weights\nusing the sigmoid function. Stage 2: These topology attention maps are\nintegrated into the semantics and detail infusion (SDI) module of VM-UNetV2 to\nform a topology-guided semantics and detail infusion (Topo-SDI) module for\nenhancing the segmentation results. Extensive experiments on five public polyp\nsegmentation datasets demonstrate the effectiveness of our proposed method. The\ncode will be made publicly available."}
{"id": "2505.06218", "pdf": "https://arxiv.org/pdf/2505.06218", "abs": "https://arxiv.org/abs/2505.06218", "authors": ["Kwan-Yee Lin", "Stella X. Yu"], "title": "Let Humanoids Hike! Integrative Skill Development on Complex Trails", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "CVPR 2025. Project page:\n  https://lego-h-humanoidrobothiking.github.io/", "summary": "Hiking on complex trails demands balance, agility, and adaptive\ndecision-making over unpredictable terrain. Current humanoid research remains\nfragmented and inadequate for hiking: locomotion focuses on motor skills\nwithout long-term goals or situational awareness, while semantic navigation\noverlooks real-world embodiment and local terrain variability. We propose\ntraining humanoids to hike on complex trails, driving integrative skill\ndevelopment across visual perception, decision making, and motor execution. We\ndevelop a learning framework, LEGO-H, that enables a vision-equipped humanoid\nrobot to hike complex trails autonomously. We introduce two technical\ninnovations: 1) A temporal vision transformer variant - tailored into\nHierarchical Reinforcement Learning framework - anticipates future local goals\nto guide movement, seamlessly integrating locomotion with goal-directed\nnavigation. 2) Latent representations of joint movement patterns, combined with\nhierarchical metric learning - enhance Privileged Learning scheme - enable\nsmooth policy transfer from privileged training to onboard execution. These\ncomponents allow LEGO-H to handle diverse physical and environmental challenges\nwithout relying on predefined motion patterns. Experiments across varied\nsimulated trails and robot morphologies highlight LEGO-H's versatility and\nrobustness, positioning hiking as a compelling testbed for embodied autonomy\nand LEGO-H as a baseline for future humanoid development."}
{"id": "2505.06227", "pdf": "https://arxiv.org/pdf/2505.06227", "abs": "https://arxiv.org/abs/2505.06227", "authors": ["Yufan Deng", "Yuhao Zhang", "Chen Geng", "Shangzhe Wu", "Jiajun Wu"], "title": "Anymate: A Dataset and Baselines for Learning 3D Object Rigging", "categories": ["cs.GR", "cs.CV"], "comment": "SIGGRAPH 2025. Project page: https://anymate3d.github.io/", "summary": "Rigging and skinning are essential steps to create realistic 3D animations,\noften requiring significant expertise and manual effort. Traditional attempts\nat automating these processes rely heavily on geometric heuristics and often\nstruggle with objects of complex geometry. Recent data-driven approaches show\npotential for better generality, but are often constrained by limited training\ndata. We present the Anymate Dataset, a large-scale dataset of 230K 3D assets\npaired with expert-crafted rigging and skinning information -- 70 times larger\nthan existing datasets. Using this dataset, we propose a learning-based\nauto-rigging framework with three sequential modules for joint, connectivity,\nand skinning weight prediction. We systematically design and experiment with\nvarious architectures as baselines for each module and conduct comprehensive\nevaluations on our dataset to compare their performance. Our models\nsignificantly outperform existing methods, providing a foundation for comparing\nfuture methods in automated rigging and skinning. Code and dataset can be found\nat https://anymate3d.github.io/."}
