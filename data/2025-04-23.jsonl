{"id": "2504.13842", "pdf": "https://arxiv.org/pdf/2504.13842", "abs": "https://arxiv.org/abs/2504.13842", "authors": ["Johannes K. Fichte", "Markus Hecher"], "title": "The Model Counting Competitions 2021-2023", "categories": ["cs.AI", "cs.DS", "cs.LO"], "comment": null, "summary": "Modern society is full of computational challenges that rely on probabilistic\nreasoning, statistics, and combinatorics. Interestingly, many of these\nquestions can be formulated by encoding them into propositional formulas and\nthen asking for its number of models. With a growing interest in practical\nproblem-solving for tasks that involve model counting, the community\nestablished the Model Counting (MC) Competition in fall of 2019 with its first\niteration in 2020. The competition aims at advancing applications, identifying\nchallenging benchmarks, fostering new solver development, and enhancing\nexisting solvers for model counting problems and their variants. The first\niteration, brought together various researchers, identified challenges, and\ninspired numerous new applications. In this paper, we present a comprehensive\noverview of the 2021-2023 iterations of the Model Counting Competition. We\ndetail its execution and outcomes. The competition comprised four tracks, each\nfocusing on a different variant of the model counting problem. The first track\ncentered on the model counting problem (MC), which seeks the count of models\nfor a given propositional formula. The second track challenged developers to\nsubmit programs capable of solving the weighted model counting problem (WMC).\nThe third track was dedicated to projected model counting (PMC). Finally, we\ninitiated a track that combined projected and weighted model counting (PWMC).\nThe competition continued with a high level of participation, with seven to\nnine solvers submitted in various different version and based on quite\ndiverging techniques."}
{"id": "2504.13924", "pdf": "https://arxiv.org/pdf/2504.13924", "abs": "https://arxiv.org/abs/2504.13924", "authors": ["Akash V. Maharaj", "David Arbour", "Daniel Lee", "Uttaran Bhattacharya", "Anup Rao", "Austin Zane", "Avi Feller", "Kun Qian", "Yunyao Li"], "title": "Evaluation and Incident Prevention in an Enterprise AI Assistant", "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": "7 pages, 5 figures. Accepted at IAAI-25", "summary": "Enterprise AI Assistants are increasingly deployed in domains where accuracy\nis paramount, making each erroneous output a potentially significant incident.\nThis paper presents a comprehensive framework for monitoring, benchmarking, and\ncontinuously improving such complex, multi-component systems under active\ndevelopment by multiple teams. Our approach encompasses three key elements: (1)\na hierarchical ``severity'' framework for incident detection that identifies\nand categorizes errors while attributing component-specific error rates,\nfacilitating targeted improvements; (2) a scalable and principled methodology\nfor benchmark construction, evaluation, and deployment, designed to accommodate\nmultiple development teams, mitigate overfitting risks, and assess the\ndownstream impact of system modifications; and (3) a continual improvement\nstrategy leveraging multidimensional evaluation, enabling the identification\nand implementation of diverse enhancement opportunities. By adopting this\nholistic framework, organizations can systematically enhance the reliability\nand performance of their AI Assistants, ensuring their efficacy in critical\nenterprise environments. We conclude by discussing how this multifaceted\nevaluation approach opens avenues for various classes of enhancements, paving\nthe way for more robust and trustworthy AI systems."}
{"id": "2504.13973", "pdf": "https://arxiv.org/pdf/2504.13973", "abs": "https://arxiv.org/abs/2504.13973", "authors": ["Myke C. Cohen", "David A. Grimm", "Reuth Mirsky", "Xiaoyun Yin"], "title": "Birds of a Different Feather Flock Together: Exploring Opportunities and Challenges in Animal-Human-Machine Teaming", "categories": ["cs.AI", "cs.HC", "cs.MA"], "comment": null, "summary": "Animal-Human-Machine (AHM) teams are a type of hybrid intelligence system\nwherein interactions between a human, AI-enabled machine, and animal members\ncan result in unique capabilities greater than the sum of their parts. This\npaper calls for a systematic approach to studying the design of AHM team\nstructures to optimize performance and overcome limitations in various applied\nsettings. We consider the challenges and opportunities in investigating the\nsynergistic potential of AHM team members by introducing a set of dimensions of\nAHM team functioning to effectively utilize each member's strengths while\ncompensating for individual weaknesses. Using three representative examples of\nsuch teams -- security screening, search-and-rescue, and guide dogs -- the\npaper illustrates how AHM teams can tackle complex tasks. We conclude with open\nresearch directions that this multidimensional approach presents for studying\nhybrid human-AI systems beyond AHM teams."}
{"id": "2504.13988", "pdf": "https://arxiv.org/pdf/2504.13988", "abs": "https://arxiv.org/abs/2504.13988", "authors": ["Herman Cappelen", "Josh Dever"], "title": "Going Whole Hog: A Philosophical Defense of AI Cognition", "categories": ["cs.AI"], "comment": null, "summary": "This work defends the 'Whole Hog Thesis': sophisticated Large Language Models\n(LLMs) like ChatGPT are full-blown linguistic and cognitive agents, possessing\nunderstanding, beliefs, desires, knowledge, and intentions. We argue against\nprevailing methodologies in AI philosophy, rejecting starting points based on\nlow-level computational details ('Just an X' fallacy) or pre-existing theories\nof mind. Instead, we advocate starting with simple, high-level observations of\nLLM behavior (e.g., answering questions, making suggestions) -- defending this\ndata against charges of metaphor, loose talk, or pretense. From these\nobservations, we employ 'Holistic Network Assumptions' -- plausible connections\nbetween mental capacities (e.g., answering implies knowledge, knowledge implies\nbelief, action implies intention) -- to argue for the full suite of cognitive\nstates. We systematically rebut objections based on LLM failures\n(hallucinations, planning/reasoning errors), arguing these don't preclude\nagency, often mirroring human fallibility. We address numerous 'Games of\nLacks', arguing that LLMs do not lack purported necessary conditions for\ncognition (e.g., semantic grounding, embodiment, justification, intrinsic\nintentionality) or that these conditions are not truly necessary, often relying\non anti-discriminatory arguments comparing LLMs to diverse human capacities.\nOur approach is evidential, not functionalist, and deliberately excludes\nconsciousness. We conclude by speculating on the possibility of LLMs possessing\n'alien' contents beyond human conceptual schemes."}
{"id": "2504.13915", "pdf": "https://arxiv.org/pdf/2504.13915", "abs": "https://arxiv.org/abs/2504.13915", "authors": ["Dibyadip Chatterjee", "Edoardo Remelli", "Yale Song", "Bugra Tekin", "Abhay Mittal", "Bharat Bhatnagar", "Necati Cihan Camgöz", "Shreyas Hampali", "Eric Sauser", "Shugao Ma", "Angela Yao", "Fadime Sener"], "title": "Memory-efficient Streaming VideoLLMs for Real-time Procedural Video Understanding", "categories": ["cs.CV"], "comment": "13 pages, 5 figures; https://dibschat.github.io/ProVideLLM", "summary": "We introduce ProVideLLM, an end-to-end framework for real-time procedural\nvideo understanding. ProVideLLM integrates a multimodal cache configured to\nstore two types of tokens - verbalized text tokens, which provide compressed\ntextual summaries of long-term observations, and visual tokens, encoded with\nDETR-QFormer to capture fine-grained details from short-term observations. This\ndesign reduces token count by 22x over existing methods in representing one\nhour of long-term observations while effectively encoding fine-granularity of\nthe present. By interleaving these tokens in our multimodal cache, ProVideLLM\nensures sub-linear scaling of memory and compute with video length, enabling\nper-frame streaming inference at 10 FPS and streaming dialogue at 25 FPS, with\na minimal 2GB GPU memory footprint. ProVideLLM also sets new state-of-the-art\nresults on six procedural tasks across four datasets."}
{"id": "2504.13914", "pdf": "https://arxiv.org/pdf/2504.13914", "abs": "https://arxiv.org/abs/2504.13914", "authors": ["ByteDance Seed", ":", "Jiaze Chen", "Tiantian Fan", "Xin Liu", "Lingjun Liu", "Zhiqi Lin", "Mingxuan Wang", "Chengyi Wang", "Xiangpeng Wei", "Wenyuan Xu", "Yufeng Yuan", "Yu Yue", "Lin Yan", "Qiying Yu", "Xiaochen Zuo", "Chi Zhang", "Ruofei Zhu", "Zhecheng An", "Zhihao Bai", "Yu Bao", "Xingyan Bin", "Jiangjie Chen", "Feng Chen", "Hongmin Chen", "Riwei Chen", "Liangqiang Chen", "Zixin Chen", "Jinsong Chen", "Siyan Chen", "Kaiyuan Chen", "Zhi Chen", "Jin Chen", "Jiecao Chen", "Jinxin Chi", "Weinan Dai", "Ning Dai", "Jiahui Dai", "Shihan Dou", "Yantao Du", "Zhengyin Du", "Jianhui Duan", "Chen Dun", "Ting-Han Fan", "Jiazhan Feng", "Junda Feng", "Ziyuan Feng", "Yuwei Fu", "Wenqi Fu", "Hanjie Fu", "Hao Ge", "Hongyi Guo", "Mingji Han", "Li Han", "Wenhao Hao", "Xintong Hao", "Qianyu He", "Jerry He", "Feng He", "Wen Heng", "Zehua Hong", "Qi Hou", "Liang Hu", "Shengding Hu", "Nan Hu", "Kai Hua", "Qi Huang", "Ziyue Huang", "Hongzhi Huang", "Zihao Huang", "Ting Huang", "Wenhao Huang", "Wei Jia", "Bin Jia", "Xiaoying Jia", "Yuhua Jiang", "Haobin Jiang", "Ziheng Jiang", "Kaihua Jiang", "Chengquan Jiang", "Jianpeng Jiao", "Xiaoran Jin", "Xing Jin", "Xunhao Lai", "Zheng Li", "Xiang Li", "Liyi Li", "Hongkai Li", "Zheng Li", "Shengxian Wan", "Ya Wang", "Yunshui Li", "Chenggang Li", "Niuniu Li", "Siyu Li", "Xi Li", "Xiao Li", "Aoyan Li", "Yuntao Li", "Nianning Liang", "Xinnian Liang", "Haibin Lin", "Weijian Lin", "Ye Lin", "Zhicheng Liu", "Guanlin Liu", "Guanlin Liu", "Chenxiao Liu", "Yan Liu", "Gaohong Liu", "Juncai Liu", "Chundian Liu", "Deyi Liu", "Kaibo Liu", "Siyao Liu", "Qi Liu", "Yongfei Liu", "Kang Liu", "Gan Liu", "Boyi Liu", "Rui Long", "Weiqiang Lou", "Chenwei Lou", "Xiang Luo", "Yao Luo", "Caiping Lv", "Heyang Lv", "Bole Ma", "Qianli Ma", "Hongzhi Ma", "Yiyuan Ma", "Jin Ma", "Wenchang Ma", "Tingting Ma", "Chen Mao", "Qiyang Min", "Zhe Nan", "Guanghan Ning", "Jinxiang Ou", "Haojie Pan", "Renming Pang", "Yanghua Peng", "Tao Peng", "Lihua Qian", "Lihua Qian", "Mu Qiao", "Meng Qu", "Cheng Ren", "Hongbin Ren", "Yong Shan", "Wei Shen", "Ke Shen", "Kai Shen", "Guangming Sheng", "Jinlong Shi", "Wenlei Shi", "Guang Shi", "Shuai Shuai Cao", "Yuxin Song", "Zuquan Song", "Jing Su", "Yifan Sun", "Tao Sun", "Zewei Sun", "Borui Wan", "Zihan Wang", "Xiaohui Wang", "Xi Wang", "Shuguang Wang", "Jun Wang", "Qinlong Wang", "Chenyuan Wang", "Shuai Wang", "Zihan Wang", "Changbao Wang", "Jiaqiang Wang", "Shihang Wang", "Xuwu Wang", "Zaiyuan Wang", "Yuxuan Wang", "Wenqi Wang", "Taiqing Wang", "Chengzhi Wei", "Houmin Wei", "Ziyun Wei", "Shufa Wei", "Zheng Wu", "Yonghui Wu", "Yangjun Wu", "Bohong Wu", "Shuang Wu", "Jingqiao Wu", "Ning Wu", "Shuangzhi Wu", "Jianmin Wu", "Chenguang Xi", "Fan Xia", "Yuqiao Xian", "Liang Xiang", "Boren Xiang", "Bowen Xiao", "Zhen Xiao", "Xia Xiao", "Yongsheng Xiao", "Chao Xin", "Shulin Xin", "Yuwen Xiong", "Jingjing Xu", "Ziwen Xu", "Chenyin Xu", "Jiayi Xu", "Yifan Xu", "Wei Xu", "Yufei Xu", "Shikun Xu", "Shipeng Yan", "Shen Yan", "Qingping Yang", "Xi Yang", "Tianhao Yang", "Yuehang Yang", "Yuan Yang", "Ximing Yang", "Zeyu Yang", "Guang Yang", "Yifan Yang", "Xuesong Yao", "Bairen Yi", "Fan Yin", "Jianian Yin", "Ziqiang Ying", "Xiangyu Yu", "Hongli Yu", "Song Yu", "Menghan Yu", "Huan Yu", "Siyu Yuan", "Jun Yuan", "Yutao Zeng", "Tianyang Zhan", "Zheng Zhang", "Yun Zhang", "Mofan Zhang", "Wang Zhang", "Ru Zhang", "Zhi Zhang", "Tianqi Zhang", "Xinyi Zhang", "Zhexi Zhang", "Sijun Zhang", "Wenqiang Zhang", "Xiangxiang Zhang", "Yongtao Zhang", "Yuyu Zhang", "Ge Zhang", "He Zhang", "Yue Zhang", "Renjie Zheng", "Ningxin Zheng", "Zhuolin Zheng", "Yaowei Zheng", "Chen Zheng", "Xiaoyun Zhi", "Wanjun Zhong", "Cheng Zhong", "Zheng Zhong", "Baoquan Zhong", "Xun Zhou", "Na Zhou", "Huan Zhou", "Hang Zhu", "Defa Zhu", "Wenjia Zhu", "Lei Zuo"], "title": "Seed-Thinking-v1.5: Advancing Superb Reasoning Models with Reinforcement Learning", "categories": ["cs.CL"], "comment": null, "summary": "We introduce Seed-Thinking-v1.5, capable of reasoning through thinking before\nresponding, resulting in improved performance on a wide range of benchmarks.\nSeed-Thinking-v1.5 achieves 86.7 on AIME 2024, 55.0 on Codeforces and 77.3 on\nGPQA, demonstrating excellent reasoning abilities in STEM and coding. Beyond\nreasoning tasks, the method demonstrates notable generalization across diverse\ndomains. For instance, it surpasses DeepSeek R1 by 8% in win rate on\nnon-reasoning tasks, indicating its broader applicability. Compared to other\nstate-of-the-art reasoning models, Seed-Thinking-v1.5 is a Mixture-of-Experts\n(MoE) model with a relatively small size, featuring 20B activated and 200B\ntotal parameters. As part of our effort to assess generalized reasoning, we\ndevelop two internal benchmarks, BeyondAIME and Codeforces, both of which will\nbe publicly released to support future research."}
{"id": "2504.14044", "pdf": "https://arxiv.org/pdf/2504.14044", "abs": "https://arxiv.org/abs/2504.14044", "authors": ["Regan Bolton", "Mohammadreza Sheikhfathollahi", "Simon Parkinson", "Dan Basher", "Howard Parkinson"], "title": "Multi-Stage Retrieval for Operational Technology Cybersecurity Compliance Using Large Language Models: A Railway Casestudy", "categories": ["cs.AI", "cs.CR"], "comment": null, "summary": "Operational Technology Cybersecurity (OTCS) continues to be a dominant\nchallenge for critical infrastructure such as railways. As these systems become\nincreasingly vulnerable to malicious attacks due to digitalization, effective\ndocumentation and compliance processes are essential to protect these\nsafety-critical systems. This paper proposes a novel system that leverages\nLarge Language Models (LLMs) and multi-stage retrieval to enhance the\ncompliance verification process against standards like IEC 62443 and the\nrail-specific IEC 63452. We first evaluate a Baseline Compliance Architecture\n(BCA) for answering OTCS compliance queries, then develop an extended approach\ncalled Parallel Compliance Architecture (PCA) that incorporates additional\ncontext from regulatory standards. Through empirical evaluation comparing\nOpenAI-gpt-4o and Claude-3.5-haiku models in these architectures, we\ndemonstrate that the PCA significantly improves both correctness and reasoning\nquality in compliance verification. Our research establishes metrics for\nresponse correctness, logical reasoning, and hallucination detection,\nhighlighting the strengths and limitations of using LLMs for compliance\nverification in railway cybersecurity. The results suggest that\nretrieval-augmented approaches can significantly improve the efficiency and\naccuracy of compliance assessments, particularly valuable in an industry facing\na shortage of cybersecurity expertise."}
{"id": "2504.13987", "pdf": "https://arxiv.org/pdf/2504.13987", "abs": "https://arxiv.org/abs/2504.13987", "authors": ["Tariq Berrada Ifriqi", "Adriana Romero-Soriano", "Michal Drozdzal", "Jakob Verbeek", "Karteek Alahari"], "title": "Entropy Rectifying Guidance for Diffusion and Flow Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Guidance techniques are commonly used in diffusion and flow models to improve\nimage quality and consistency for conditional generative tasks such as\nclass-conditional and text-to-image generation. In particular, classifier-free\nguidance (CFG) -- the most widely adopted guidance technique -- contrasts\nconditional and unconditional predictions to improve the generated images. This\nresults, however, in trade-offs across quality, diversity and consistency,\nimproving some at the expense of others. While recent work has shown that it is\npossible to disentangle these factors to some extent, such methods come with an\noverhead of requiring an additional (weaker) model, or require more forward\npasses per sampling step. In this paper, we propose Entropy Rectifying Guidance\n(ERG), a simple and effective guidance mechanism based on inference-time\nchanges in the attention mechanism of state-of-the-art diffusion transformer\narchitectures, which allows for simultaneous improvements over image quality,\ndiversity and prompt consistency. ERG is more general than CFG and similar\nguidance techniques, as it extends to unconditional sampling. ERG results in\nsignificant improvements in various generation tasks such as text-to-image,\nclass-conditional and unconditional image generation. We also show that ERG can\nbe seamlessly combined with other recent guidance methods such as CADS and APG,\nfurther boosting generation performance."}
{"id": "2504.14037", "pdf": "https://arxiv.org/pdf/2504.14037", "abs": "https://arxiv.org/abs/2504.14037", "authors": ["Djamila Mohdeb", "Meriem Laifa", "Zineb Guemraoui", "Dalila Behih"], "title": "Uncovering Conspiratorial Narratives within Arabic Online Content", "categories": ["cs.CL", "cs.CY", "cs.SI"], "comment": null, "summary": "This study investigates the spread of conspiracy theories in Arabic digital\nspaces through computational analysis of online content. By combining Named\nEntity Recognition and Topic Modeling techniques, specifically the Top2Vec\nalgorithm, we analyze data from Arabic blogs and Facebook to identify and\nclassify conspiratorial narratives. Our analysis uncovers six distinct\ncategories: gender/feminist, geopolitical, government cover-ups, apocalyptic,\nJudeo-Masonic, and geoengineering. The research highlights how these narratives\nare deeply embedded in Arabic social media discourse, shaped by regional\nhistorical, cultural, and sociopolitical contexts. By applying advanced Natural\nLanguage Processing methods to Arabic content, this study addresses a gap in\nconspiracy theory research, which has traditionally focused on English-language\ncontent or offline data. The findings provide new insights into the\nmanifestation and evolution of conspiracy theories in Arabic digital spaces,\nenhancing our understanding of their role in shaping public discourse in the\nArab world."}
{"id": "2504.14045", "pdf": "https://arxiv.org/pdf/2504.14045", "abs": "https://arxiv.org/abs/2504.14045", "authors": ["Mark Steyvers", "Megan A. K. Peters"], "title": "Metacognition and Uncertainty Communication in Humans and Large Language Models", "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "Metacognition, the capacity to monitor and evaluate one's own knowledge and\nperformance, is foundational to human decision-making, learning, and\ncommunication. As large language models (LLMs) become increasingly embedded in\nhigh-stakes decision contexts, it is critical to assess whether, how, and to\nwhat extent they exhibit metacognitive abilities. Here, we provide an overview\nof current knowledge of LLMs' metacognitive capacities, how they might be\nstudied, and how they relate to our knowledge of metacognition in humans. We\nshow that while humans and LLMs can sometimes appear quite aligned in their\nmetacognitive capacities and behaviors, it is clear many differences remain.\nAttending to these differences is crucial not only for enhancing human-AI\ncollaboration, but also for promoting the development of more capable and\ntrustworthy artificial systems. Finally, we discuss how endowing future LLMs\nwith more sensitive and more calibrated metacognition may also help them\ndevelop new capacities such as more efficient learning, self-direction, and\ncuriosity."}
{"id": "2504.13995", "pdf": "https://arxiv.org/pdf/2504.13995", "abs": "https://arxiv.org/abs/2504.13995", "authors": ["Andrea Amaduzzi", "Pierluigi Zama Ramirez", "Giuseppe Lisanti", "Samuele Salti", "Luigi Di Stefano"], "title": "Scaling LLaNA: Advancing NeRF-Language Understanding Through Large-Scale Training", "categories": ["cs.CV"], "comment": "Under submission. Project page at\n  https://andreamaduzzi.github.io/llana/", "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have shown\nremarkable capabilities in understanding both images and 3D data, yet these\nmodalities face inherent limitations in comprehensively representing object\ngeometry and appearance. Neural Radiance Fields (NeRFs) have emerged as a\npromising alternative, encoding both geometric and photorealistic properties\nwithin the weights of a simple Multi-Layer Perceptron (MLP). This work\ninvestigates the feasibility and effectiveness of ingesting NeRFs into an MLLM.\nWe introduce LLaNA, the first MLLM able to perform new tasks such as NeRF\ncaptioning and Q\\&A, by directly processing the weights of a NeRF's MLP.\nNotably, LLaNA is able to extract information about the represented objects\nwithout the need to render images or materialize 3D data structures. In\naddition, we build the first large-scale NeRF-language dataset, composed by\nmore than 300K NeRFs trained on ShapeNet and Objaverse, with paired textual\nannotations that enable various NeRF-language tasks. Based on this dataset, we\ndevelop a benchmark to evaluate the NeRF understanding capability of our\nmethod. Results show that directly processing NeRF weights leads to better\nperformance on NeRF-Language tasks compared to approaches that rely on either\n2D or 3D representations derived from NeRFs."}
{"id": "2504.14039", "pdf": "https://arxiv.org/pdf/2504.14039", "abs": "https://arxiv.org/abs/2504.14039", "authors": ["Jaime Raldua Veuthey", "Zainab Ali Majid", "Suhas Hariharan", "Jacob Haimes"], "title": "MEQA: A Meta-Evaluation Framework for Question & Answer LLM Benchmarks", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As Large Language Models (LLMs) advance, their potential for widespread\nsocietal impact grows simultaneously. Hence, rigorous LLM evaluations are both\na technical necessity and social imperative. While numerous evaluation\nbenchmarks have been developed, there remains a critical gap in\nmeta-evaluation: effectively assessing benchmarks' quality. We propose MEQA, a\nframework for the meta-evaluation of question and answer (QA) benchmarks, to\nprovide standardized assessments, quantifiable scores, and enable meaningful\nintra-benchmark comparisons. We demonstrate this approach on cybersecurity\nbenchmarks, using human and LLM evaluators, highlighting the benchmarks'\nstrengths and weaknesses. We motivate our choice of test domain by AI models'\ndual nature as powerful defensive tools and security threats."}
{"id": "2504.14047", "pdf": "https://arxiv.org/pdf/2504.14047", "abs": "https://arxiv.org/abs/2504.14047", "authors": ["Junlin Wang", "Shang Zhu", "Jon Saad-Falcon", "Ben Athiwaratkun", "Qingyang Wu", "Jue Wang", "Shuaiwen Leon Song", "Ce Zhang", "Bhuwan Dhingra", "James Zou"], "title": "Think Deep, Think Fast: Investigating Efficiency of Verifier-free Inference-time-scaling Methods", "categories": ["cs.AI"], "comment": null, "summary": "There is intense interest in investigating how inference time compute (ITC)\n(e.g. repeated sampling, refinements, etc) can improve large language model\n(LLM) capabilities. At the same time, recent breakthroughs in reasoning models,\nsuch as Deepseek-R1, unlock the opportunity for reinforcement learning to\nimprove LLM reasoning skills. An in-depth understanding of how ITC interacts\nwith reasoning across different models could provide important guidance on how\nto further advance the LLM frontier. This work conducts a comprehensive\nanalysis of inference-time scaling methods for both reasoning and non-reasoning\nmodels on challenging reasoning tasks. Specifically, we focus our research on\nverifier-free inference time-scaling methods due to its generalizability\nwithout needing a reward model. We construct the Pareto frontier of quality and\nefficiency. We find that non-reasoning models, even with an extremely high\ninference budget, still fall substantially behind reasoning models. For\nreasoning models, majority voting proves to be a robust inference strategy,\ngenerally competitive or outperforming other more sophisticated ITC methods\nlike best-of-N and sequential revisions, while the additional inference compute\noffers minimal improvements. We further perform in-depth analyses of the\nassociation of key response features (length and linguistic markers) with\nresponse quality, with which we can improve the existing ITC methods. We find\nthat correct responses from reasoning models are typically shorter and have\nfewer hedging and thinking markers (but more discourse markers) than the\nincorrect responses."}
{"id": "2504.14011", "pdf": "https://arxiv.org/pdf/2504.14011", "abs": "https://arxiv.org/abs/2504.14011", "authors": ["Fulvio Sanguigni", "Davide Morelli", "Marcella Cornia", "Rita Cucchiara"], "title": "Fashion-RAG: Multimodal Fashion Image Editing via Retrieval-Augmented Generation", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "IJCNN 2025", "summary": "In recent years, the fashion industry has increasingly adopted AI\ntechnologies to enhance customer experience, driven by the proliferation of\ne-commerce platforms and virtual applications. Among the various tasks, virtual\ntry-on and multimodal fashion image editing -- which utilizes diverse input\nmodalities such as text, garment sketches, and body poses -- have become a key\narea of research. Diffusion models have emerged as a leading approach for such\ngenerative tasks, offering superior image quality and diversity. However, most\nexisting virtual try-on methods rely on having a specific garment input, which\nis often impractical in real-world scenarios where users may only provide\ntextual specifications. To address this limitation, in this work we introduce\nFashion Retrieval-Augmented Generation (Fashion-RAG), a novel method that\nenables the customization of fashion items based on user preferences provided\nin textual form. Our approach retrieves multiple garments that match the input\nspecifications and generates a personalized image by incorporating attributes\nfrom the retrieved items. To achieve this, we employ textual inversion\ntechniques, where retrieved garment images are projected into the textual\nembedding space of the Stable Diffusion text encoder, allowing seamless\nintegration of retrieved elements into the generative process. Experimental\nresults on the Dress Code dataset demonstrate that Fashion-RAG outperforms\nexisting methods both qualitatively and quantitatively, effectively capturing\nfine-grained visual details from retrieved garments. To the best of our\nknowledge, this is the first work to introduce a retrieval-augmented generation\napproach specifically tailored for multimodal fashion image editing."}
{"id": "2504.14066", "pdf": "https://arxiv.org/pdf/2504.14066", "abs": "https://arxiv.org/abs/2504.14066", "authors": ["Laerdon Kim"], "title": "A Baseline for Self-state Identification and Classification in Mental Health Data: CLPsych 2025 Task", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to CLPsych Workshop, NAACL 2025", "summary": "We present a baseline for the CLPsych 2025 A.1 task: classifying self-states\nin mental health data taken from Reddit. We use few-shot learning with a 4-bit\nquantized Gemma 2 9B model and a data preprocessing step which first identifies\nrelevant sentences indicating self-state evidence, and then performs a binary\nclassification to determine whether the sentence is evidence of an adaptive or\nmaladaptive self-state. This system outperforms our other method which relies\non an LLM to highlight spans of variable length independently. We attribute the\nperformance of our model to the benefits of this sentence chunking step for two\nreasons: partitioning posts into sentences 1) broadly matches the granularity\nat which self-states were human-annotated and 2) simplifies the task for our\nlanguage model to a binary classification problem. Our system places third out\nof fourteen systems submitted for Task A.1, achieving a test-time recall of\n0.579."}
{"id": "2504.14107", "pdf": "https://arxiv.org/pdf/2504.14107", "abs": "https://arxiv.org/abs/2504.14107", "authors": ["Jennifer Hu", "Michael A. Lepori", "Michael Franke"], "title": "Linking forward-pass dynamics in Transformers and real-time human processing", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Modern AI models are increasingly being used as theoretical tools to study\nhuman cognition. One dominant approach is to evaluate whether human-derived\nmeasures (such as offline judgments or real-time processing) are predicted by a\nmodel's output: that is, the end-product of forward pass(es) through the\nnetwork. At the same time, recent advances in mechanistic interpretability have\nbegun to reveal the internal processes that give rise to model outputs, raising\nthe question of whether models and humans might arrive at outputs using similar\n\"processing strategies\". Here, we investigate the link between real-time\nprocessing in humans and \"layer-time\" dynamics in Transformer models. Across\nfive studies spanning domains and modalities, we test whether the dynamics of\ncomputation in a single forward pass of pre-trained Transformers predict\nsignatures of processing in humans, above and beyond properties of the model's\noutput probability distribution. We consistently find that layer-time dynamics\nprovide additional predictive power on top of output measures. Our results\nsuggest that Transformer processing and human processing may be facilitated or\nimpeded by similar properties of an input stimulus, and this similarity has\nemerged through general-purpose objectives such as next-token prediction or\nimage recognition. Our work suggests a new way of using AI models to study\nhuman cognition: not just as a black box mapping stimuli to responses, but\npotentially also as explicit processing models."}
{"id": "2504.14032", "pdf": "https://arxiv.org/pdf/2504.14032", "abs": "https://arxiv.org/abs/2504.14032", "authors": ["Haiwen Huang", "Anpei Chen", "Volodymyr Havrylov", "Andreas Geiger", "Dan Zhang"], "title": "LoftUp: Learning a Coordinate-Based Feature Upsampler for Vision Foundation Models", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "comment": null, "summary": "Vision foundation models (VFMs) such as DINOv2 and CLIP have achieved\nimpressive results on various downstream tasks, but their limited feature\nresolution hampers performance in applications requiring pixel-level\nunderstanding. Feature upsampling offers a promising direction to address this\nchallenge. In this work, we identify two critical factors for enhancing feature\nupsampling: the upsampler architecture and the training objective. For the\nupsampler architecture, we introduce a coordinate-based cross-attention\ntransformer that integrates the high-resolution images with coordinates and\nlow-resolution VFM features to generate sharp, high-quality features. For the\ntraining objective, we propose constructing high-resolution pseudo-groundtruth\nfeatures by leveraging class-agnostic masks and self-distillation. Our approach\neffectively captures fine-grained details and adapts flexibly to various input\nand feature resolutions. Through experiments, we demonstrate that our approach\nsignificantly outperforms existing feature upsampling techniques across various\ndownstream tasks. Our code is released at https://github.com/andrehuang/loftup."}
{"id": "2504.14089", "pdf": "https://arxiv.org/pdf/2504.14089", "abs": "https://arxiv.org/abs/2504.14089", "authors": ["Kang He", "Kaushik Roy"], "title": "LogicTree: Structured Proof Exploration for Coherent and Rigorous Logical Reasoning with Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average."}
{"id": "2504.14119", "pdf": "https://arxiv.org/pdf/2504.14119", "abs": "https://arxiv.org/abs/2504.14119", "authors": ["Man Ho Lam", "Chaozheng Wang", "Jen-tse Huang", "Michael R. Lyu"], "title": "CODECRASH: Stress Testing LLM Reasoning under Structural and Semantic Perturbations", "categories": ["cs.AI", "cs.SE"], "comment": null, "summary": "Large Language Models (LLMs) have recently showcased strong capabilities in\ncode-related tasks, yet their robustness in code comprehension and reasoning\nremains underexplored. In this paper, we present CodeCrash, a unified benchmark\nthat evaluates LLM robustness under code structural and textual distraction\nperturbations, applied to two established benchmarks -- CRUXEval and\nLiveCodeBench -- across both input and output prediction tasks. We evaluate\nseventeen LLMs using direct and Chain-of-Thought inference to systematically\nanalyze their robustness, identify primary reasons for performance degradation,\nand highlight failure modes. Our findings reveal the fragility of LLMs under\nstructural noise and the inherent reliance on natural language cues,\nhighlighting critical robustness issues of LLMs in code execution and\nunderstanding. Additionally, we examine three Large Reasoning Models (LRMs) and\ndiscover the severe vulnerability of self-reflective reasoning mechanisms that\nlead to reasoning collapse. CodeCrash provides a principled framework for\nstress-testing LLMs in code understanding, offering actionable directions for\nfuture evaluation and benchmarking. The code of CodeCrash and the robustness\nleaderboard are publicly available at https://donaldlamnl.github.io/CodeCrash/ ."}
{"id": "2504.14054", "pdf": "https://arxiv.org/pdf/2504.14054", "abs": "https://arxiv.org/abs/2504.14054", "authors": ["Soroosh Baselizadeh", "Cheuk-To Yu", "Olga Veksler", "Yuri Boykov"], "title": "Occlusion-Ordered Semantic Instance Segmentation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Standard semantic instance segmentation provides useful, but inherently 2D\ninformation from a single image. To enable 3D analysis, one usually integrates\nabsolute monocular depth estimation with instance segmentation. However,\nmonocular depth is a difficult task. Instead, we leverage a simpler\nsingle-image task, occlusion-based relative depth ordering, providing coarser\nbut useful 3D information. We show that relative depth ordering works more\nreliably from occlusions than from absolute depth. We propose to solve the\njoint task of relative depth ordering and segmentation of instances based on\nocclusions. We call this task Occlusion-Ordered Semantic Instance Segmentation\n(OOSIS). We develop an approach to OOSIS that extracts instances and their\nocclusion order simultaneously from oriented occlusion boundaries and semantic\nsegmentation. Unlike popular detect-and-segment framework for instance\nsegmentation, combining occlusion ordering with instance segmentation allows a\nsimple and clean formulation of OOSIS as a labeling problem. As a part of our\nsolution for OOSIS, we develop a novel oriented occlusion boundaries approach\nthat significantly outperforms prior work. We also develop a new joint OOSIS\nmetric based both on instance mask accuracy and correctness of their occlusion\norder. We achieve better performance than strong baselines on KINS and COCOA\ndatasets."}
{"id": "2504.14117", "pdf": "https://arxiv.org/pdf/2504.14117", "abs": "https://arxiv.org/abs/2504.14117", "authors": ["Nusrat Jahan Prottasha", "Upama Roy Chowdhury", "Shetu Mohanto", "Tasfia Nuzhat", "Abdullah As Sami", "Md Shamol Ali", "Md Shohanur Islam Sobuj", "Hafijur Raman", "Md Kowsher", "Ozlem Ozmen Garibay"], "title": "PEFT A2Z: Parameter-Efficient Fine-Tuning Survey for Large Language and Vision Models", "categories": ["cs.CL", "cs.CV"], "comment": "PEFT Survey paper", "summary": "Large models such as Large Language Models (LLMs) and Vision Language Models\n(VLMs) have transformed artificial intelligence, powering applications in\nnatural language processing, computer vision, and multimodal learning. However,\nfully fine-tuning these models remains expensive, requiring extensive\ncomputational resources, memory, and task-specific data. Parameter-Efficient\nFine-Tuning (PEFT) has emerged as a promising solution that allows adapting\nlarge models to downstream tasks by updating only a small portion of\nparameters. This survey presents a comprehensive overview of PEFT techniques,\nfocusing on their motivations, design principles, and effectiveness. We begin\nby analyzing the resource and accessibility challenges posed by traditional\nfine-tuning and highlight key issues, such as overfitting, catastrophic\nforgetting, and parameter inefficiency. We then introduce a structured taxonomy\nof PEFT methods -- grouped into additive, selective, reparameterized, hybrid,\nand unified frameworks -- and systematically compare their mechanisms and\ntrade-offs. Beyond taxonomy, we explore the impact of PEFT across diverse\ndomains, including language, vision, and generative modeling, showing how these\ntechniques offer strong performance with lower resource costs. We also discuss\nimportant open challenges in scalability, interpretability, and robustness, and\nsuggest future directions such as federated learning, domain adaptation, and\ntheoretical grounding. Our goal is to provide a unified understanding of PEFT\nand its growing role in enabling practical, efficient, and sustainable use of\nlarge models."}
{"id": "2504.14123", "pdf": "https://arxiv.org/pdf/2504.14123", "abs": "https://arxiv.org/abs/2504.14123", "authors": ["Mingyu Kim", "Jongwoo Ko", "Mijung Park"], "title": "Bayesian Principles Improve Prompt Learning In Vision-Language Models", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "AISTATS2025", "summary": "Prompt learning is a popular fine-tuning method for vision-language models\ndue to its efficiency. It requires a small number of additional learnable\nparameters while significantly enhancing performance on target tasks. However,\nmost existing methods suffer from overfitting to fine-tuning data, yielding\npoor generalizability. To address this, we propose a new training objective\nfunction based on a Bayesian learning principle to balance adaptability and\ngeneralizability. We derive a prior over the logits, where the mean function is\nparameterized by the pre-trained model, while the posterior corresponds to the\nfine-tuned model. This objective establishes a balance by allowing the\nfine-tuned model to adapt to downstream tasks while remaining close to the\npre-trained model."}
{"id": "2504.14075", "pdf": "https://arxiv.org/pdf/2504.14075", "abs": "https://arxiv.org/abs/2504.14075", "authors": ["Wei Dong", "Yan Min", "Han Zhou", "Jun Chen"], "title": "Towards Scale-Aware Low-Light Enhancement via Structure-Guided Transformer Design", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025 NTIRE Workshop, Structure prior,\n  CNN-Transformer, LLIE", "summary": "Current Low-light Image Enhancement (LLIE) techniques predominantly rely on\neither direct Low-Light (LL) to Normal-Light (NL) mappings or guidance from\nsemantic features or illumination maps. Nonetheless, the intrinsic\nill-posedness of LLIE and the difficulty in retrieving robust semantics from\nheavily corrupted images hinder their effectiveness in extremely low-light\nenvironments. To tackle this challenge, we present SG-LLIE, a new multi-scale\nCNN-Transformer hybrid framework guided by structure priors. Different from\nemploying pre-trained models for the extraction of semantics or illumination\nmaps, we choose to extract robust structure priors based on\nillumination-invariant edge detectors. Moreover, we develop a CNN-Transformer\nHybrid Structure-Guided Feature Extractor (HSGFE) module at each scale with in\nthe UNet encoder-decoder architecture. Besides the CNN blocks which excels in\nmulti-scale feature extraction and fusion, we introduce a Structure-Guided\nTransformer Block (SGTB) in each HSGFE that incorporates structural priors to\nmodulate the enhancement process. Extensive experiments show that our method\nachieves state-of-the-art performance on several LLIE benchmarks in both\nquantitative metrics and visual quality. Our solution ranks second in the NTIRE\n2025 Low-Light Enhancement Challenge. Code is released at\nhttps://github.com/minyan8/imagine."}
{"id": "2504.14150", "pdf": "https://arxiv.org/pdf/2504.14150", "abs": "https://arxiv.org/abs/2504.14150", "authors": ["Katie Matton", "Robert Osazuwa Ness", "John Guttag", "Emre Kıcıman"], "title": "Walk the Talk? Measuring the Faithfulness of Large Language Model Explanations", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "comment": "61 pages, 14 figures, 36 tables", "summary": "Large language models (LLMs) are capable of generating plausible explanations\nof how they arrived at an answer to a question. However, these explanations can\nmisrepresent the model's \"reasoning\" process, i.e., they can be unfaithful.\nThis, in turn, can lead to over-trust and misuse. We introduce a new approach\nfor measuring the faithfulness of LLM explanations. First, we provide a\nrigorous definition of faithfulness. Since LLM explanations mimic human\nexplanations, they often reference high-level concepts in the input question\nthat purportedly influenced the model. We define faithfulness in terms of the\ndifference between the set of concepts that LLM explanations imply are\ninfluential and the set that truly are. Second, we present a novel method for\nestimating faithfulness that is based on: (1) using an auxiliary LLM to modify\nthe values of concepts within model inputs to create realistic counterfactuals,\nand (2) using a Bayesian hierarchical model to quantify the causal effects of\nconcepts at both the example- and dataset-level. Our experiments show that our\nmethod can be used to quantify and discover interpretable patterns of\nunfaithfulness. On a social bias task, we uncover cases where LLM explanations\nhide the influence of social bias. On a medical question answering task, we\nuncover cases where LLM explanations provide misleading claims about which\npieces of evidence influenced the model's decisions."}
{"id": "2504.14126", "pdf": "https://arxiv.org/pdf/2504.14126", "abs": "https://arxiv.org/abs/2504.14126", "authors": ["Saad Hameed", "Basheer Qolomany", "Samir Brahim Belhaouari", "Mohamed Abdallah", "Junaid Qadir", "Ala Al-Fuqaha"], "title": "Large Language Model Enhanced Particle Swarm Optimization for Hyperparameter Tuning for Deep Learning Models", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Determining the ideal architecture for deep learning models, such as the\nnumber of layers and neurons, is a difficult and resource-intensive process\nthat frequently relies on human tuning or computationally costly optimization\napproaches. While Particle Swarm Optimization (PSO) and Large Language Models\n(LLMs) have been individually applied in optimization and deep learning, their\ncombined use for enhancing convergence in numerical optimization tasks remains\nunderexplored. Our work addresses this gap by integrating LLMs into PSO to\nreduce model evaluations and improve convergence for deep learning\nhyperparameter tuning. The proposed LLM-enhanced PSO method addresses the\ndifficulties of efficiency and convergence by using LLMs (particularly\nChatGPT-3.5 and Llama3) to improve PSO performance, allowing for faster\nachievement of target objectives. Our method speeds up search space exploration\nby substituting underperforming particle placements with best suggestions\noffered by LLMs. Comprehensive experiments across three scenarios -- (1)\noptimizing the Rastrigin function, (2) using Long Short-Term Memory (LSTM)\nnetworks for time series regression, and (3) using Convolutional Neural\nNetworks (CNNs) for material classification -- show that the method\nsignificantly improves convergence rates and lowers computational costs.\nDepending on the application, computational complexity is lowered by 20% to 60%\ncompared to traditional PSO methods. Llama3 achieved a 20% to 40% reduction in\nmodel calls for regression tasks, whereas ChatGPT-3.5 reduced model calls by\n60% for both regression and classification tasks, all while preserving accuracy\nand error rates. This groundbreaking methodology offers a very efficient and\neffective solution for optimizing deep learning models, leading to substantial\ncomputational performance improvements across a wide range of applications."}
{"id": "2504.14092", "pdf": "https://arxiv.org/pdf/2504.14092", "abs": "https://arxiv.org/abs/2504.14092", "authors": ["Wei Dong", "Han Zhou", "Seyed Amirreza Mousavi", "Jun Chen"], "title": "Retinex-guided Histogram Transformer for Mask-free Shadow Removal", "categories": ["cs.CV"], "comment": "Accpeted by CVPR 2025 NTIRE Workshop, Retinex Guidance, Histogram\n  Transformer", "summary": "While deep learning methods have achieved notable progress in shadow removal,\nmany existing approaches rely on shadow masks that are difficult to obtain,\nlimiting their generalization to real-world scenes. In this work, we propose\nReHiT, an efficient mask-free shadow removal framework based on a hybrid\nCNN-Transformer architecture guided by Retinex theory. We first introduce a\ndual-branch pipeline to separately model reflectance and illumination\ncomponents, and each is restored by our developed Illumination-Guided Hybrid\nCNN-Transformer (IG-HCT) module. Second, besides the CNN-based blocks that are\ncapable of learning residual dense features and performing multi-scale semantic\nfusion, multi-scale semantic fusion, we develop the Illumination-Guided\nHistogram Transformer Block (IGHB) to effectively handle non-uniform\nillumination and spatially complex shadows. Extensive experiments on several\nbenchmark datasets validate the effectiveness of our approach over existing\nmask-free methods. Trained solely on the NTIRE 2025 Shadow Removal Challenge\ndataset, our solution delivers competitive results with one of the smallest\nparameter sizes and fastest inference speeds among top-ranked entries,\nhighlighting its applicability for real-world applications with limited\ncomputational resources. The code is available at\nhttps://github.com/dongw22/oath."}
{"id": "2504.14154", "pdf": "https://arxiv.org/pdf/2504.14154", "abs": "https://arxiv.org/abs/2504.14154", "authors": ["Zhiyuan Wang", "Qingni Wang", "Yue Zhang", "Tianlong Chen", "Xiaofeng Zhu", "Xiaoshuang Shi", "Kaidi Xu"], "title": "SConU: Selective Conformal Uncertainty in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "comment": null, "summary": "As large language models are increasingly utilized in real-world\napplications, guarantees of task-specific metrics are essential for their\nreliable deployment. Previous studies have introduced various criteria of\nconformal uncertainty grounded in split conformal prediction, which offer\nuser-specified correctness coverage. However, existing frameworks often fail to\nidentify uncertainty data outliers that violate the exchangeability assumption,\nleading to unbounded miscoverage rates and unactionable prediction sets. In\nthis paper, we propose a novel approach termed Selective Conformal Uncertainty\n(SConU), which, for the first time, implements significance tests, by\ndeveloping two conformal p-values that are instrumental in determining whether\na given sample deviates from the uncertainty distribution of the calibration\nset at a specific manageable risk level. Our approach not only facilitates\nrigorous management of miscoverage rates across both single-domain and\ninterdisciplinary contexts, but also enhances the efficiency of predictions.\nFurthermore, we comprehensively analyze the components of the conformal\nprocedures, aiming to approximate conditional coverage, particularly in\nhigh-stakes question-answering tasks."}
{"id": "2504.14128", "pdf": "https://arxiv.org/pdf/2504.14128", "abs": "https://arxiv.org/abs/2504.14128", "authors": ["Christopher Zhang Cui", "Xingdi Yuan", "Ziang Xiao", "Prithviraj Ammanabrolu", "Marc-Alexandre Côté"], "title": "TALES: Text Adventure Learning Environment Suite", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Reasoning is an essential skill to enable Large Language Models (LLMs) to\ninteract with the world. As tasks become more complex, they demand increasingly\nsophisticated and diverse reasoning capabilities for sequential\ndecision-making, requiring structured reasoning over the context history to\ndetermine the next best action. We introduce TALES, a diverse collection of\nsynthetic and human-written text-adventure games designed to challenge and\nevaluate diverse reasoning capabilities. We present results over a range of\nLLMs, open- and closed-weights, performing a qualitative analysis on the top\nperforming models. Despite an impressive showing on synthetic games, even the\ntop LLM-driven agents fail to achieve 15% on games designed for human\nenjoyment. Code and visualization of the experiments can be found at\nhttps://microsoft.github.io/tales."}
{"id": "2504.14096", "pdf": "https://arxiv.org/pdf/2504.14096", "abs": "https://arxiv.org/abs/2504.14096", "authors": ["Yogesh Kulkarni", "Pooyan Fazli"], "title": "VideoPASTA: 7K Preference Pairs That Matter for Video-LLM Alignment", "categories": ["cs.CV"], "comment": null, "summary": "Video-language models (Video-LLMs) excel at understanding video content but\nstruggle with spatial relationships, temporal ordering, and cross-frame\ncontinuity. To address these limitations, we introduce VideoPASTA (Preference\nAlignment with Spatio-Temporal-Cross Frame Adversaries), a framework that\nenhances Video-LLMs through targeted preference optimization. VideoPASTA trains\nmodels to distinguish accurate video representations from carefully generated\nadversarial examples that deliberately violate spatial, temporal, or\ncross-frame relations. By applying Direct Preference Optimization to just 7,020\npreference pairs, VideoPASTA learns robust representations that capture\nfine-grained spatial relationships and long-range temporal dynamics.\nExperiments on standard video benchmarks show significant relative performance\ngains of 3.05% on VideoMME, 1.97% on NeXTQA, and 1.31% on LongVideoBench, over\nthe baseline Qwen2.5-VL model. These results demonstrate that targeted\nalignment, rather than massive pretraining or architectural modifications,\neffectively addresses core video-language challenges. Notably, VideoPASTA\nachieves these improvements without human annotation or captioning, relying on\njust 32-frame sampling, compared to the 96-frame, multi-GPU setups of prior\nwork. This efficiency makes our approach a scalable, plug-and-play solution\nthat seamlessly integrates with existing models while preserving their\ncapabilities."}
{"id": "2504.14165", "pdf": "https://arxiv.org/pdf/2504.14165", "abs": "https://arxiv.org/abs/2504.14165", "authors": ["Ziyan Zhang", "Yang Hou", "Chen Gong", "Zhenghua Li"], "title": "Self-Correction Makes LLMs Better Parsers", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable success across various\nnatural language processing (NLP) tasks. However, recent studies suggest that\nthey still face challenges in performing fundamental NLP tasks essential for\ndeep language understanding, particularly syntactic parsing. In this paper, we\nconduct an in-depth analysis of LLM parsing capabilities, delving into the\nspecific shortcomings of their parsing results. We find that LLMs may stem from\nlimitations to fully leverage grammar rules in existing treebanks, which\nrestricts their capability to generate valid syntactic structures. To help LLMs\nacquire knowledge without additional training, we propose a self-correction\nmethod that leverages grammar rules from existing treebanks to guide LLMs in\ncorrecting previous errors. Specifically, we automatically detect potential\nerrors and dynamically search for relevant rules, offering hints and examples\nto guide LLMs in making corrections themselves. Experimental results on three\ndatasets with various LLMs, demonstrate that our method significantly improves\nperformance in both in-domain and cross-domain settings on the English and\nChinese datasets."}
{"id": "2504.14171", "pdf": "https://arxiv.org/pdf/2504.14171", "abs": "https://arxiv.org/abs/2504.14171", "authors": ["Yangping Chen", "Weijie Shi", "Mengze Li", "Yue Cui", "Hao Chen", "Jia Zhu", "Jiajie Xu"], "title": "Adaptation Method for Misinformation Identification", "categories": ["cs.AI"], "comment": null, "summary": "Multimodal fake news detection plays a crucial role in combating online\nmisinformation. Unfortunately, effective detection methods rely on annotated\nlabels and encounter significant performance degradation when domain shifts\nexist between training (source) and test (target) data. To address the\nproblems, we propose ADOSE, an Active Domain Adaptation (ADA) framework for\nmultimodal fake news detection which actively annotates a small subset of\ntarget samples to improve detection performance. To identify various deceptive\npatterns in cross-domain settings, we design multiple expert classifiers to\nlearn dependencies across different modalities. These classifiers specifically\ntarget the distinct deception patterns exhibited in fake news, where two\nunimodal classifiers capture knowledge errors within individual modalities\nwhile one cross-modal classifier identifies semantic inconsistencies between\ntext and images. To reduce annotation costs from the target domain, we propose\na least-disagree uncertainty selector with a diversity calculator for selecting\nthe most informative samples. The selector leverages prediction disagreement\nbefore and after perturbations by multiple classifiers as an indicator of\nuncertain samples, whose deceptive patterns deviate most from source domains.\nIt further incorporates diversity scores derived from multi-view features to\nensure the chosen samples achieve maximal coverage of target domain features.\nThe extensive experiments on multiple datasets show that ADOSE outperforms\nexisting ADA methods by 2.72\\% $\\sim$ 14.02\\%, indicating the superiority of\nour model."}
{"id": "2504.14108", "pdf": "https://arxiv.org/pdf/2504.14108", "abs": "https://arxiv.org/abs/2504.14108", "authors": ["Zhenyu Yu", "Mohd Yamani Idna Idris", "Pei Wang", "Yuelong Xia"], "title": "Point-Driven Interactive Text and Image Layer Editing Using Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "We present DanceText, a training-free framework for multilingual text editing\nin images, designed to support complex geometric transformations and achieve\nseamless foreground-background integration. While diffusion-based generative\nmodels have shown promise in text-guided image synthesis, they often lack\ncontrollability and fail to preserve layout consistency under non-trivial\nmanipulations such as rotation, translation, scaling, and warping. To address\nthese limitations, DanceText introduces a layered editing strategy that\nseparates text from the background, allowing geometric transformations to be\nperformed in a modular and controllable manner. A depth-aware module is further\nproposed to align appearance and perspective between the transformed text and\nthe reconstructed background, enhancing photorealism and spatial consistency.\nImportantly, DanceText adopts a fully training-free design by integrating\npretrained modules, allowing flexible deployment without task-specific\nfine-tuning. Extensive experiments on the AnyWord-3M benchmark demonstrate that\nour method achieves superior performance in visual quality, especially under\nlarge-scale and complex transformation scenarios."}
{"id": "2504.14175", "pdf": "https://arxiv.org/pdf/2504.14175", "abs": "https://arxiv.org/abs/2504.14175", "authors": ["Yejun Yoon", "Jaeyoon Jung", "Seunghyun Yoon", "Kunwoo Park"], "title": "Hypothetical Documents or Knowledge Leakage? Rethinking LLM-based Query Expansion", "categories": ["cs.CL", "cs.IR"], "comment": "preprint", "summary": "Query expansion methods powered by large language models (LLMs) have\ndemonstrated effectiveness in zero-shot retrieval tasks. These methods assume\nthat LLMs can generate hypothetical documents that, when incorporated into a\nquery vector, enhance the retrieval of real evidence. However, we challenge\nthis assumption by investigating whether knowledge leakage in benchmarks\ncontributes to the observed performance gains. Using fact verification as a\ntestbed, we analyzed whether the generated documents contained information\nentailed by ground truth evidence and assessed their impact on performance. Our\nfindings indicate that performance improvements occurred consistently only for\nclaims whose generated documents included sentences entailed by ground truth\nevidence. This suggests that knowledge leakage may be present in these\nbenchmarks, inflating the perceived performance of LLM-based query expansion\nmethods, particularly in real-world scenarios that require retrieving niche or\nnovel knowledge."}
{"id": "2504.14177", "pdf": "https://arxiv.org/pdf/2504.14177", "abs": "https://arxiv.org/abs/2504.14177", "authors": ["Li He", "He Zhao", "Stephen Wan", "Dadong Wang", "Lina Yao", "Tongliang Liu"], "title": "Direct Advantage Regression: Aligning LLMs with Online AI Reward", "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Online AI Feedback (OAIF) presents a promising alternative to Reinforcement\nLearning from Human Feedback (RLHF) by utilizing online AI preference in\naligning language models (LLMs). However, the straightforward replacement of\nhumans with AI deprives LLMs from learning more fine-grained AI supervision\nbeyond binary signals. In this paper, we propose Direct Advantage Regression\n(DAR), a simple alignment algorithm using online AI reward to optimize policy\nimprovement through weighted supervised fine-tuning. As an RL-free approach,\nDAR maintains theoretical consistency with online RLHF pipelines while\nsignificantly reducing implementation complexity and improving learning\nefficiency. Our empirical results underscore that AI reward is a better form of\nAI supervision consistently achieving higher human-AI agreement as opposed to\nAI preference. Additionally, evaluations using GPT-4-Turbo and MT-bench show\nthat DAR outperforms both OAIF and online RLHF baselines."}
{"id": "2504.14113", "pdf": "https://arxiv.org/pdf/2504.14113", "abs": "https://arxiv.org/abs/2504.14113", "authors": ["Jiyong Kwag", "Alper Yilmaz", "Charles Toth"], "title": "Lightweight Road Environment Segmentation using Vector Quantization", "categories": ["cs.CV"], "comment": null, "summary": "Road environment segmentation plays a significant role in autonomous driving.\nNumerous works based on Fully Convolutional Networks (FCNs) and Transformer\narchitectures have been proposed to leverage local and global contextual\nlearning for efficient and accurate semantic segmentation. In both\narchitectures, the encoder often relies heavily on extracting continuous\nrepresentations from the image, which limits the ability to represent\nmeaningful discrete information. To address this limitation, we propose\nsegmentation of the autonomous driving environment using vector quantization.\nVector quantization offers three primary advantages for road environment\nsegmentation. (1) Each continuous feature from the encoder is mapped to a\ndiscrete vector from the codebook, helping the model discover distinct features\nmore easily than with complex continuous features. (2) Since a discrete feature\nacts as compressed versions of the encoder's continuous features, they also\ncompress noise or outliers, enhancing the image segmentation task. (3) Vector\nquantization encourages the latent space to form coarse clusters of continuous\nfeatures, forcing the model to group similar features, making the learned\nrepresentations more structured for the decoding process. In this work, we\ncombined vector quantization with the lightweight image segmentation model\nMobileUNETR and used it as a baseline model for comparison to demonstrate its\nefficiency. Through experiments, we achieved 77.0 % mIoU on Cityscapes,\noutperforming the baseline by 2.9 % without increasing the model's initial size\nor complexity."}
{"id": "2504.14194", "pdf": "https://arxiv.org/pdf/2504.14194", "abs": "https://arxiv.org/abs/2504.14194", "authors": ["Xinlin Zhuang", "Jiahui Peng", "Ren Ma", "Yinfan Wang", "Tianyi Bai", "Xingjian Wei", "Jiantao Qiu", "Chi Zhang", "Ying Qian", "Conghui He"], "title": "Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models", "categories": ["cs.CL"], "comment": "Under review", "summary": "The composition of pre-training datasets for large language models (LLMs)\nremains largely undisclosed, hindering transparency and efforts to optimize\ndata quality, a critical driver of model performance. Current data selection\nmethods, such as natural language quality assessments, diversity-based filters,\nand classifier-based approaches, are limited by single-dimensional evaluation\nor redundancy-focused strategies. To address these gaps, we propose PRRC to\nevaluate data quality across Professionalism, Readability, Reasoning, and\nCleanliness. We further introduce Meta-rater, a multi-dimensional data\nselection method that integrates these dimensions with existing quality metrics\nthrough learned optimal weightings. Meta-rater employs proxy models to train a\nregression model that predicts validation loss, enabling the identification of\noptimal combinations of quality scores. Experiments demonstrate that Meta-rater\ndoubles convergence speed for 1.3B parameter models and improves downstream\ntask performance by 3.23, with scalable benefits observed in 3.3B models\ntrained on 100B tokens. Additionally, we release the annotated SlimPajama-627B\ndataset, labeled across 25 quality metrics (including PRRC), to advance\nresearch in data-centric LLM development. Our work establishes that holistic,\nmulti-dimensional quality integration significantly outperforms conventional\nsingle-dimension approaches, offering a scalable paradigm for enhancing\npre-training efficiency and model capability."}
{"id": "2504.14191", "pdf": "https://arxiv.org/pdf/2504.14191", "abs": "https://arxiv.org/abs/2504.14191", "authors": ["Yansheng Qiu", "Haoquan Zhang", "Zhaopan Xu", "Ming Li", "Diping Song", "Zheng Wang", "Kaipeng Zhang"], "title": "AI Idea Bench 2025: AI Research Idea Generation Benchmark", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large-scale Language Models (LLMs) have revolutionized human-AI interaction\nand achieved significant success in the generation of novel ideas. However,\ncurrent assessments of idea generation overlook crucial factors such as\nknowledge leakage in LLMs, the absence of open-ended benchmarks with grounded\ntruth, and the limited scope of feasibility analysis constrained by prompt\ndesign. These limitations hinder the potential of uncovering groundbreaking\nresearch ideas. In this paper, we present AI Idea Bench 2025, a framework\ndesigned to quantitatively evaluate and compare the ideas generated by LLMs\nwithin the domain of AI research from diverse perspectives. The framework\ncomprises a comprehensive dataset of 3,495 AI papers and their associated\ninspired works, along with a robust evaluation methodology. This evaluation\nsystem gauges idea quality in two dimensions: alignment with the ground-truth\ncontent of the original papers and judgment based on general reference\nmaterial. AI Idea Bench 2025's benchmarking system stands to be an invaluable\nresource for assessing and comparing idea-generation techniques, thereby\nfacilitating the automation of scientific discovery."}
{"id": "2504.14129", "pdf": "https://arxiv.org/pdf/2504.14129", "abs": "https://arxiv.org/abs/2504.14129", "authors": ["Yaning Zhang", "Jiahe Zhang", "Chunjie Ma", "Weili Guan", "Tian Gan", "Zan Gao"], "title": "BMRL: Bi-Modal Guided Multi-Perspective Representation Learning for Zero-Shot Deepfake Attribution", "categories": ["cs.CV"], "comment": null, "summary": "The challenge of tracing the source attribution of forged faces has gained\nsignificant attention due to the rapid advancement of generative models.\nHowever, existing deepfake attribution (DFA) works primarily focus on the\ninteraction among various domains in vision modality, and other modalities such\nas texts and face parsing are not fully explored. Besides, they tend to fail to\nassess the generalization performance of deepfake attributors to unseen\ngenerators in a fine-grained manner. In this paper, we propose a novel bi-modal\nguided multi-perspective representation learning (BMRL) framework for zero-shot\ndeepfake attribution (ZS-DFA), which facilitates effective traceability to\nunseen generators. Specifically, we design a multi-perspective visual encoder\n(MPVE) to explore general deepfake attribution visual characteristics across\nthree views (i.e., image, noise, and edge). We devise a novel parsing encoder\nto focus on global face attribute embeddings, enabling parsing-guided DFA\nrepresentation learning via vision-parsing matching. A language encoder is\nproposed to capture fine-grained language embeddings, facilitating\nlanguage-guided general visual forgery representation learning through\nvision-language alignment. Additionally, we present a novel deepfake\nattribution contrastive center (DFACC) loss, to pull relevant generators closer\nand push irrelevant ones away, which can be introduced into DFA models to\nenhance traceability. Experimental results demonstrate that our method\noutperforms the state-of-the-art on the ZS-DFA task through various protocols\nevaluation."}
{"id": "2504.14203", "pdf": "https://arxiv.org/pdf/2504.14203", "abs": "https://arxiv.org/abs/2504.14203", "authors": ["Jian Zhang", "Tianqing Zhang", "Qi Li", "Hongwei Wang"], "title": "EIoU-EMC: A Novel Loss for Domain-specific Nested Entity Recognition", "categories": ["cs.CL", "cs.IR"], "comment": "Accepted by SIGIR'2025", "summary": "In recent years, research has mainly focused on the general NER task. There\nstill have some challenges with nested NER task in the specific domains.\nSpecifically, the scenarios of low resource and class imbalance impede the wide\napplication for biomedical and industrial domains. In this study, we design a\nnovel loss EIoU-EMC, by enhancing the implement of Intersection over Union loss\nand Multiclass loss. Our proposed method specially leverages the information of\nentity boundary and entity classification, thereby enhancing the model's\ncapacity to learn from a limited number of data samples. To validate the\nperformance of this innovative method in enhancing NER task, we conducted\nexperiments on three distinct biomedical NER datasets and one dataset\nconstructed by ourselves from industrial complex equipment maintenance\ndocuments. Comparing to strong baselines, our method demonstrates the\ncompetitive performance across all datasets. During the experimental analysis,\nour proposed method exhibits significant advancements in entity boundary\nrecognition and entity classification. Our code are available here."}
{"id": "2504.14209", "pdf": "https://arxiv.org/pdf/2504.14209", "abs": "https://arxiv.org/abs/2504.14209", "authors": ["Xiangkai Ma", "Xiaobin Hong", "Wenzhong Li", "Sanglu Lu"], "title": "Pets: General Pattern Assisted Architecture For Time Series Analysis", "categories": ["cs.AI"], "comment": null, "summary": "Time series analysis has found widespread applications in areas such as\nweather forecasting, anomaly detection, and healthcare. However, real-world\nsequential data often exhibit a superimposed state of various fluctuation\npatterns, including hourly, daily, and monthly frequencies. Traditional\ndecomposition techniques struggle to effectively disentangle these multiple\nfluctuation patterns from the seasonal components, making time series analysis\nchallenging. Surpassing the existing multi-period decoupling paradigms, this\npaper introduces a novel perspective based on energy distribution within the\ntemporal-spectrum space. By adaptively quantifying observed sequences into\ncontinuous frequency band intervals, the proposed approach reconstructs\nfluctuation patterns across diverse periods without relying on domain-specific\nprior knowledge. Building upon this innovative strategy, we propose Pets, an\nenhanced architecture that is adaptable to arbitrary model structures. Pets\nintegrates a Fluctuation Pattern Assisted (FPA) module and a Context-Guided\nMixture of Predictors (MoP). The FPA module facilitates information fusion\namong diverse fluctuation patterns by capturing their dependencies and\nprogressively modeling these patterns as latent representations at each layer.\nMeanwhile, the MoP module leverages these compound pattern representations to\nguide and regulate the reconstruction of distinct fluctuations hierarchically.\nPets achieves state-of-the-art performance across various tasks, including\nforecasting, imputation, anomaly detection, and classification, while\ndemonstrating strong generalization and robustness."}
{"id": "2504.14131", "pdf": "https://arxiv.org/pdf/2504.14131", "abs": "https://arxiv.org/abs/2504.14131", "authors": ["Ole-Christian Galbo Engstrøm", "Michela Albano-Gaglio", "Erik Schou Dreier", "Yamine Bouzembrak", "Maria Font-i-Furnols", "Puneet Mishra", "Kim Steenstrup Pedersen"], "title": "Transforming hyperspectral images into chemical maps: A new deep learning based approach to hyperspectral image processing", "categories": ["cs.CV", "cs.LG", "q-bio.QM"], "comment": null, "summary": "Current approaches to chemical map generation from hyperspectral images are\nbased on models such as partial least squares (PLS) regression, generating\npixel-wise predictions that do not consider spatial context and suffer from a\nhigh degree of noise. This study proposes an end-to-end deep learning approach\nusing a modified version of U-Net and a custom loss function to directly obtain\nchemical maps from hyperspectral images, skipping all intermediate steps\nrequired for traditional pixel-wise analysis. We compare the U-Net with the\ntraditional PLS regression on a real dataset of pork belly samples with\nassociated mean fat reference values. The U-Net obtains a test set root mean\nsquared error of between 9% and 13% lower than that of PLS regression on the\ntask of mean fat prediction. At the same time, U-Net generates fine detail\nchemical maps where 99.91% of the variance is spatially correlated. Conversely,\nonly 2.53% of the variance in the PLS-generated chemical maps is spatially\ncorrelated, indicating that each pixel-wise prediction is largely independent\nof neighboring pixels. Additionally, while the PLS-generated chemical maps\ncontain predictions far beyond the physically possible range of 0-100%, U-Net\nlearns to stay inside this range. Thus, the findings of this study indicate\nthat U-Net is superior to PLS for chemical map generation."}
{"id": "2504.14212", "pdf": "https://arxiv.org/pdf/2504.14212", "abs": "https://arxiv.org/abs/2504.14212", "authors": ["Takuma Udagawa", "Yang Zhao", "Hiroshi Kanayama", "Bishwaranjan Bhattacharjee"], "title": "Bias Analysis and Mitigation through Protected Attribute Detection and Regard Classification", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) acquire general linguistic knowledge from\nmassive-scale pretraining. However, pretraining data mainly comprised of\nweb-crawled texts contain undesirable social biases which can be perpetuated or\neven amplified by LLMs. In this study, we propose an efficient yet effective\nannotation pipeline to investigate social biases in the pretraining corpora.\nOur pipeline consists of protected attribute detection to identify diverse\ndemographics, followed by regard classification to analyze the language\npolarity towards each attribute. Through our experiments, we demonstrate the\neffect of our bias analysis and mitigation measures, focusing on Common Crawl\nas the most representative pretraining corpus."}
{"id": "2504.14232", "pdf": "https://arxiv.org/pdf/2504.14232", "abs": "https://arxiv.org/abs/2504.14232", "authors": ["Antoun Yaacoub", "Jérôme Da-Rugna", "Zainab Assaghir"], "title": "Assessing AI-Generated Questions' Alignment with Cognitive Frameworks in Educational Assessment", "categories": ["cs.AI", "cs.CL"], "comment": "This paper was presented in the 17th Int. Conf. on Computer Science\n  and Information Technology (ICCSIT 2024), Dubai, United Arab Emirates, 2024,\n  Oct. 23-25. IT's now in production to be published in the International\n  Journal of Computer Theory and Engineering", "summary": "This study evaluates the integration of Bloom's Taxonomy into OneClickQuiz,\nan Artificial Intelligence (AI) driven plugin for automating Multiple-Choice\nQuestion (MCQ) generation in Moodle. Bloom's Taxonomy provides a structured\nframework for categorizing educational objectives into hierarchical cognitive\nlevels. Our research investigates whether incorporating this taxonomy can\nimprove the alignment of AI-generated questions with specific cognitive\nobjectives. We developed a dataset of 3691 questions categorized according to\nBloom's levels and employed various classification models-Multinomial Logistic\nRegression, Naive Bayes, Linear Support Vector Classification (SVC), and a\nTransformer-based model (DistilBERT)-to evaluate their effectiveness in\ncategorizing questions. Our results indicate that higher Bloom's levels\ngenerally correlate with increased question length, Flesch-Kincaid Grade Level\n(FKGL), and Lexical Density (LD), reflecting the increased complexity of higher\ncognitive demands. Multinomial Logistic Regression showed varying accuracy\nacross Bloom's levels, performing best for \"Knowledge\" and less accurately for\nhigher-order levels. Merging higher-level categories improved accuracy for\ncomplex cognitive tasks. Naive Bayes and Linear SVC also demonstrated effective\nclassification for lower levels but struggled with higher-order tasks.\nDistilBERT achieved the highest performance, significantly improving\nclassification of both lower and higher-order cognitive levels, achieving an\noverall validation accuracy of 91%. This study highlights the potential of\nintegrating Bloom's Taxonomy into AI-driven assessment tools and underscores\nthe advantages of advanced models like DistilBERT for enhancing educational\ncontent generation."}
{"id": "2504.14132", "pdf": "https://arxiv.org/pdf/2504.14132", "abs": "https://arxiv.org/abs/2504.14132", "authors": ["Xuanhua Yin", "Dingxin Zhang", "Jianhui Yu", "Weidong Cai"], "title": "HFBRI-MAE: Handcrafted Feature Based Rotation-Invariant Masked Autoencoder for 3D Point Cloud Analysis", "categories": ["cs.CV"], "comment": "12 pages, 9 figures, accepted by IJCNN 2025", "summary": "Self-supervised learning (SSL) has demonstrated remarkable success in 3D\npoint cloud analysis, particularly through masked autoencoders (MAEs). However,\nexisting MAE-based methods lack rotation invariance, leading to significant\nperformance degradation when processing arbitrarily rotated point clouds in\nreal-world scenarios. To address this limitation, we introduce Handcrafted\nFeature-Based Rotation-Invariant Masked Autoencoder (HFBRI-MAE), a novel\nframework that refines the MAE design with rotation-invariant handcrafted\nfeatures to ensure stable feature learning across different orientations. By\nleveraging both rotation-invariant local and global features for token\nembedding and position embedding, HFBRI-MAE effectively eliminates rotational\ndependencies while preserving rich geometric structures. Additionally, we\nredefine the reconstruction target to a canonically aligned version of the\ninput, mitigating rotational ambiguities. Extensive experiments on ModelNet40,\nScanObjectNN, and ShapeNetPart demonstrate that HFBRI-MAE consistently\noutperforms existing methods in object classification, segmentation, and\nfew-shot learning, highlighting its robustness and strong generalization\nability in real-world 3D applications."}
{"id": "2504.14218", "pdf": "https://arxiv.org/pdf/2504.14218", "abs": "https://arxiv.org/abs/2504.14218", "authors": ["Junchi Yao", "Shu Yang", "Jianhua Xu", "Lijie Hu", "Mengdi Li", "Di Wang"], "title": "Understanding the Repeat Curse in Large Language Models from a Feature Perspective", "categories": ["cs.CL"], "comment": "Submitted to ACL 2025", "summary": "Large language models (LLMs) have made remarkable progress in various\ndomains, yet they often suffer from repetitive text generation, a phenomenon we\nrefer to as the \"Repeat Curse\". While previous studies have proposed decoding\nstrategies to mitigate repetition, the underlying mechanism behind this issue\nremains insufficiently explored. In this work, we investigate the root causes\nof repetition in LLMs through the lens of mechanistic interpretability.\nInspired by recent advances in Sparse Autoencoders (SAEs), which enable\nmonosemantic feature extraction, we propose a novel approach, \"Duplicatus\nCharm\", to induce and analyze the Repeat Curse. Our method systematically\nidentifies \"Repetition Features\" -the key model activations responsible for\ngenerating repetitive outputs. First, we locate the layers most involved in\nrepetition through logit analysis. Next, we extract and stimulate relevant\nfeatures using SAE-based activation manipulation. To validate our approach, we\nconstruct a repetition dataset covering token and paragraph level repetitions\nand introduce an evaluation pipeline to quantify the influence of identified\nrepetition features. Furthermore, by deactivating these features, we have\neffectively mitigated the Repeat Curse."}
{"id": "2504.14239", "pdf": "https://arxiv.org/pdf/2504.14239", "abs": "https://arxiv.org/abs/2504.14239", "authors": ["Yuhang Liu", "Pengxiang Li", "Congkai Xie", "Xavier Hu", "Xiaotian Han", "Shengyu Zhang", "Hongxia Yang", "Fei Wu"], "title": "InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners", "categories": ["cs.AI", "cs.CL"], "comment": "10 pages, 3 figures, work in progress", "summary": "Multimodal Large Language Models (MLLMs) have powered Graphical User\nInterface (GUI) Agents, showing promise in automating tasks on computing\ndevices. Recent works have begun exploring reasoning in GUI tasks with\nencouraging results. However, many current approaches rely on manually designed\nreasoning templates, which may result in reasoning that is not sufficiently\nrobust and adaptive for complex GUI environments. Meanwhile, some existing\nagents continue to operate as Reactive Actors, relying primarily on implicit\nreasoning that may lack sufficient depth for GUI tasks demanding planning and\nerror recovery. We argue that advancing these agents requires a shift from\nreactive acting towards acting based on deliberate reasoning. To facilitate\nthis transformation, we introduce InfiGUI-R1, an MLLM-based GUI agent developed\nthrough our Actor2Reasoner framework, a reasoning-centric, two-stage training\napproach designed to progressively evolve agents from Reactive Actors to\nDeliberative Reasoners. The first stage, Reasoning Injection, focuses on\nestablishing a basic reasoner. We employ Spatial Reasoning Distillation to\ntransfer cross-modal spatial reasoning capabilities from teacher models to\nMLLMs through trajectories with explicit reasoning steps, enabling models to\nintegrate GUI visual-spatial information with logical reasoning before action\ngeneration. The second stage, Deliberation Enhancement, refines the basic\nreasoner into a deliberative one using Reinforcement Learning. This stage\nintroduces two approaches: Sub-goal Guidance, which rewards models for\ngenerating accurate intermediate sub-goals, and Error Recovery Scenario\nConstruction, which creates failure-and-recovery training scenarios from\nidentified prone-to-error steps. Experimental results show InfiGUI-R1 achieves\nstrong performance in GUI grounding and trajectory tasks. Resources at\nhttps://github.com/Reallm-Labs/InfiGUI-R1."}
{"id": "2504.14137", "pdf": "https://arxiv.org/pdf/2504.14137", "abs": "https://arxiv.org/abs/2504.14137", "authors": ["Hangyu Liu", "Bo Peng", "Pengxiang Ding", "Donglin Wang"], "title": "Rethinking Target Label Conditioning in Adversarial Attacks: A 2D Tensor-Guided Generative Approach", "categories": ["cs.CV"], "comment": "12 pages, 4 figures", "summary": "Compared to single-target adversarial attacks, multi-target attacks have\ngarnered significant attention due to their ability to generate adversarial\nimages for multiple target classes simultaneously. Existing generative\napproaches for multi-target attacks mainly analyze the effect of the use of\ntarget labels on noise generation from a theoretical perspective, lacking\npractical validation and comprehensive summarization. To address this gap, we\nfirst identify and validate that the semantic feature quality and quantity are\ncritical factors affecting the transferability of targeted attacks: 1) Feature\nquality refers to the structural and detailed completeness of the implanted\ntarget features, as deficiencies may result in the loss of key discriminative\ninformation; 2) Feature quantity refers to the spatial sufficiency of the\nimplanted target features, as inadequacy limits the victim model's attention to\nthis feature. Based on these findings, we propose the 2D Tensor-Guided\nAdversarial Fusion (2D-TGAF) framework, which leverages the powerful generative\ncapabilities of diffusion models to encode target labels into two-dimensional\nsemantic tensors for guiding adversarial noise generation. Additionally, we\ndesign a novel masking strategy tailored for the training process, ensuring\nthat parts of the generated noise retain complete semantic information about\nthe target class. Extensive experiments on the standard ImageNet dataset\ndemonstrate that 2D-TGAF consistently surpasses state-of-the-art methods in\nattack success rates, both on normally trained models and across various\ndefense mechanisms."}
{"id": "2504.14223", "pdf": "https://arxiv.org/pdf/2504.14223", "abs": "https://arxiv.org/abs/2504.14223", "authors": ["Michael Färber", "Parisa Aghdam", "Kyuri Im", "Mario Tawfelis", "Hardik Ghoshal"], "title": "SimplifyMyText: An LLM-Based System for Inclusive Plain Language Text Simplification", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "accepted at ECIR 2025", "summary": "Text simplification is essential for making complex content accessible to\ndiverse audiences who face comprehension challenges. Yet, the limited\navailability of simplified materials creates significant barriers to personal\nand professional growth and hinders social inclusion. Although researchers have\nexplored various methods for automatic text simplification, none fully leverage\nlarge language models (LLMs) to offer tailored customization for different\ntarget groups and varying levels of simplicity. Moreover, despite its proven\nbenefits for both consumers and organizations, the well-established practice of\nplain language remains underutilized. In this paper, we\nhttps://simplifymytext.org, the first system designed to produce plain language\ncontent from multiple input formats, including typed text and file uploads,\nwith flexible customization options for diverse audiences. We employ GPT-4 and\nLlama-3 and evaluate outputs across multiple metrics. Overall, our work\ncontributes to research on automatic text simplification and highlights the\nimportance of tailored communication in promoting inclusivity."}
{"id": "2504.14241", "pdf": "https://arxiv.org/pdf/2504.14241", "abs": "https://arxiv.org/abs/2504.14241", "authors": ["Chengming Wang", "Dongyao Jia", "Wei Wang", "Dong Ngoduy", "Bei Peng", "Jianping Wang"], "title": "A Knowledge-Informed Deep Learning Paradigm for Generalizable and Stability-Optimized Car-Following Models", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "Car-following models (CFMs) are fundamental to traffic flow analysis and\nautonomous driving. Although calibrated physics-based and trained data-driven\nCFMs can replicate human driving behavior, their reliance on specific datasets\nlimits generalization across diverse scenarios and reduces reliability in\nreal-world deployment. Moreover, these models typically focus on behavioral\nfidelity and do not support the explicit optimization of local and string\nstability, which are increasingly important for the safe and efficient\noperation of autonomous vehicles (AVs). To address these limitations, we\npropose a Knowledge-Informed Deep Learning (KIDL) paradigm that distills the\ngeneralization capabilities of pre-trained Large Language Models (LLMs) into a\nlightweight and stability-aware neural architecture. LLMs are used to extract\nfundamental car-following knowledge beyond dataset-specific patterns, and this\nknowledge is transferred to a reliable, tractable, and computationally\nefficient model through knowledge distillation. KIDL also incorporates\nstability constraints directly into its training objective, ensuring that the\nresulting model not only emulates human-like behavior but also satisfies the\nlocal and string stability requirements essential for real-world AV deployment.\nWe evaluate KIDL on the real-world NGSIM and HighD datasets, comparing its\nperformance with representative physics-based, data-driven, and hybrid CFMs.\nBoth empirical and theoretical results consistently demonstrate KIDL's superior\nbehavioral generalization and traffic flow stability, offering a robust and\nscalable solution for next-generation traffic systems."}
{"id": "2504.14138", "pdf": "https://arxiv.org/pdf/2504.14138", "abs": "https://arxiv.org/abs/2504.14138", "authors": ["Ghodsiyeh Rostami", "Po-Han Chen", "Mahdi S. Hosseini"], "title": "Segment Any Crack: Deep Semantic Segmentation Adaptation for Crack Detection", "categories": ["cs.CV"], "comment": null, "summary": "Image-based crack detection algorithms are increasingly in demand in\ninfrastructure monitoring, as early detection of cracks is of paramount\nimportance for timely maintenance planning. While deep learning has\nsignificantly advanced crack detection algorithms, existing models often\nrequire extensive labeled datasets and high computational costs for\nfine-tuning, limiting their adaptability across diverse conditions. This study\nintroduces an efficient selective fine-tuning strategy, focusing on tuning\nnormalization components, to enhance the adaptability of segmentation models\nfor crack detection. The proposed method is applied to the Segment Anything\nModel (SAM) and five well-established segmentation models. Experimental results\ndemonstrate that selective fine-tuning of only normalization parameters\noutperforms full fine-tuning and other common fine-tuning techniques in both\nperformance and computational efficiency, while improving generalization. The\nproposed approach yields a SAM-based model, Segment Any Crack (SAC), achieving\na 61.22\\% F1-score and 44.13\\% IoU on the OmniCrack30k benchmark dataset, along\nwith the highest performance across three zero-shot datasets and the lowest\nstandard deviation. The results highlight the effectiveness of the adaptation\napproach in improving segmentation accuracy while significantly reducing\ncomputational overhead."}
{"id": "2504.14225", "pdf": "https://arxiv.org/pdf/2504.14225", "abs": "https://arxiv.org/abs/2504.14225", "authors": ["Bowen Jiang", "Zhuoqun Hao", "Young-Min Cho", "Bryan Li", "Yuan Yuan", "Sihao Chen", "Lyle Ungar", "Camillo J. Taylor", "Dan Roth"], "title": "Know Me, Respond to Me: Benchmarking LLMs for Dynamic User Profiling and Personalized Responses at Scale", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have emerged as personalized assistants for\nusers across a wide range of tasks -- from offering writing support to\ndelivering tailored recommendations or consultations. Over time, the\ninteraction history between a user and an LLM can provide extensive information\nabout an individual's traits and preferences. However, open questions remain on\nhow well LLMs today can effectively leverage such history to (1) internalize\nthe user's inherent traits and preferences, (2) track how the user profiling\nand preferences evolve over time, and (3) generate personalized responses\naccordingly in new scenarios.\n  In this work, we introduce the PERSONAMEM benchmark. PERSONAMEM features\ncurated user profiles with over 180 simulated user-LLM interaction histories,\neach containing up to 60 sessions of multi-turn conversations across 15\nreal-world tasks that require personalization. Given an in-situ user query,\ni.e. query issued by the user from the first-person perspective, we evaluate\nLLM chatbots' ability to identify the most suitable response according to the\ncurrent state of the user's profile. We observe that current LLMs still\nstruggle to recognize the dynamic evolution in users' profiles over time\nthrough direct prompting approaches. As a consequence, LLMs often fail to\ndeliver responses that align with users' current situations and preferences,\nwith frontier models such as GPT-4.1, o4-mini, GPT-4.5, o1, or Gemini-2.0\nachieving only around 50% overall accuracy, suggesting room for improvement. We\nhope that PERSONAMEM, along with the user profile and conversation simulation\npipeline, can facilitate future research in the development of truly user-aware\nchatbots. Code and data are available at github.com/bowen-upenn/PersonaMem."}
{"id": "2504.14248", "pdf": "https://arxiv.org/pdf/2504.14248", "abs": "https://arxiv.org/abs/2504.14248", "authors": ["Li Shijiao", "Ma Zhipeng", "He Huajun", "Chen Haiyue"], "title": "Rethinking Traffic Flow Forecasting: From Transition to Generatation", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Traffic flow prediction plays an important role in Intelligent Transportation\nSystems in traffic management and urban planning. There have been extensive\nsuccessful works in this area. However, these approaches focus only on\nmodelling the flow transition and ignore the flow generation process, which\nmanifests itself in two ways: (i) The models are based on Markovian\nassumptions, ignoring the multi-periodicity of the flow generation in nodes.\n(ii) The same structure is designed to encode both the transition and\ngeneration processes, ignoring the differences between them. To address these\nproblems, we propose an Effective Multi-Branch Similarity Transformer for\nTraffic Flow Prediction, namely EMBSFormer. Through data analysis, we find that\nthe factors affecting traffic flow include node-level traffic generation and\ngraph-level traffic transition, which describe the multi-periodicity and\ninteraction pattern of nodes, respectively. Specifically, to capture traffic\ngeneration patterns, we propose a similarity analysis module that supports\nmulti-branch encoding to dynamically expand significant cycles. For traffic\ntransition, we employ a temporal and spatial self-attention mechanism to\nmaintain global node interactions, and use GNN and time conv to model local\nnode interactions, respectively. Model performance is evaluated on three\nreal-world datasets on both long-term and short-term prediction tasks.\nExperimental results show that EMBSFormer outperforms baselines on both tasks.\nMoreover, compared to models based on flow transition modelling (e.g. GMAN,\n513k), the variant of EMBSFormer(93K) only uses 18\\% of the parameters,\nachieving the same performance."}
{"id": "2504.14139", "pdf": "https://arxiv.org/pdf/2504.14139", "abs": "https://arxiv.org/abs/2504.14139", "authors": ["Hai Pham-Ngoc", "De Nguyen-Van", "Dung Vu-Tien", "Phuong Le-Hong"], "title": "ThyroidEffi 1.0: A Cost-Effective System for High-Performance Multi-Class Thyroid Carcinoma Classification", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Background: Automated classification of thyroid fine needle aspiration biopsy\n(FNAB) images faces challenges in limited data, inter-observer variability, and\ncomputational cost. Efficient, interpretable models are crucial for clinical\nsupport. Objective: To develop and externally validate a deep learning system\nfor the multi-class classification of thyroid FNAB images into three key\ncategories that directly guide post-biopsy treatment decisions in Vietnam:\nbenign (B2), suspicious for malignancy (B5), and malignant (B6), while\nachieving high diagnostic accuracy with low computational overhead. Methods:\nOur framework features: (1) YOLOv10-based cell cluster detection for\ninformative sub-region extraction and noise reduction; (2) a curriculum\nlearning-inspired protocol sequencing localized crops to full images for\nmulti-scale feature capture; (3) adaptive lightweight EfficientNetB0 (4\nmillions parameters) selection balancing performance and efficiency; and (4) a\nTransformer-inspired module for multi-scale, multi-region analysis. External\nvalidation used 1,015 independent FNAB images. Results: ThyroidEffi Basic\nachieved a macro F1 of 89.19\\% and AUCs of 0.98 (B2), 0.95 (B5), and 0.96 (B6)\non the internal test set. External validation yielded AUCs of 0.9495 (B2),\n0.7436 (B5), and 0.8396 (B6). ThyroidEffi Premium improved macro F1 to 89.77\\%.\nGrad-CAM highlighted key diagnostic regions, confirming interpretability. The\nsystem processed 1000 cases in 30 seconds, demonstrating feasibility on widely\naccessible hardware like a 12-core CPU. Conclusions: This work demonstrates\nthat high-accuracy, interpretable thyroid FNAB image classification is\nachievable with minimal computational demands."}
{"id": "2504.14287", "pdf": "https://arxiv.org/pdf/2504.14287", "abs": "https://arxiv.org/abs/2504.14287", "authors": ["Demetris Paschalides", "George Pallis", "Marios D. Dikaiakos"], "title": "Probing the Subtle Ideological Manipulation of Large Language Models", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Large Language Models (LLMs) have transformed natural language processing,\nbut concerns have emerged about their susceptibility to ideological\nmanipulation, particularly in politically sensitive areas. Prior work has\nfocused on binary Left-Right LLM biases, using explicit prompts and fine-tuning\non political QA datasets. In this work, we move beyond this binary approach to\nexplore the extent to which LLMs can be influenced across a spectrum of\npolitical ideologies, from Progressive-Left to Conservative-Right. We introduce\na novel multi-task dataset designed to reflect diverse ideological positions\nthrough tasks such as ideological QA, statement ranking, manifesto cloze\ncompletion, and Congress bill comprehension. By fine-tuning three LLMs-Phi-2,\nMistral, and Llama-3-on this dataset, we evaluate their capacity to adopt and\nexpress these nuanced ideologies. Our findings indicate that fine-tuning\nsignificantly enhances nuanced ideological alignment, while explicit prompts\nprovide only minor refinements. This highlights the models' susceptibility to\nsubtle ideological manipulation, suggesting a need for more robust safeguards\nto mitigate these risks."}
{"id": "2504.14274", "pdf": "https://arxiv.org/pdf/2504.14274", "abs": "https://arxiv.org/abs/2504.14274", "authors": ["Zhengxi Lu", "Shizhuo Cheng", "Yuru Jiang", "Yan Zhang", "Min Zhang"], "title": "ProtPainter: Draw or Drag Protein via Topology-guided Diffusion", "categories": ["cs.AI"], "comment": "Published as a conference paper at ICLR 2025", "summary": "Recent advances in protein backbone generation have achieved promising\nresults under structural, functional, or physical constraints. However,\nexisting methods lack the flexibility for precise topology control, limiting\nnavigation of the backbone space. We present ProtPainter, a diffusion-based\napproach for generating protein backbones conditioned on 3D curves. ProtPainter\nfollows a two-stage process: curve-based sketching and sketch-guided backbone\ngeneration. For the first stage, we propose CurveEncoder, which predicts\nsecondary structure annotations from a curve to parametrize sketch generation.\nFor the second stage, the sketch guides the generative process in Denoising\nDiffusion Probabilistic Modeling (DDPM) to generate backbones. During this\nprocess, we further introduce a fusion scheduling scheme, Helix-Gating, to\ncontrol the scaling factors. To evaluate, we propose the first benchmark for\ntopology-conditioned protein generation, introducing Protein Restoration Task\nand a new metric, self-consistency Topology Fitness (scTF). Experiments\ndemonstrate ProtPainter's ability to generate topology-fit (scTF > 0.8) and\ndesignable (scTM > 0.5) backbones, with drawing and dragging tasks showcasing\nits flexibility and versatility."}
{"id": "2504.14151", "pdf": "https://arxiv.org/pdf/2504.14151", "abs": "https://arxiv.org/abs/2504.14151", "authors": ["Sergio Arnaud", "Paul McVay", "Ada Martin", "Arjun Majumdar", "Krishna Murthy Jatavallabhula", "Phillip Thomas", "Ruslan Partsey", "Daniel Dugas", "Abha Gejji", "Alexander Sax", "Vincent-Pierre Berges", "Mikael Henaff", "Ayush Jain", "Ang Cao", "Ishita Prasad", "Mrinal Kalakrishnan", "Michael Rabbat", "Nicolas Ballas", "Mido Assran", "Oleksandr Maksymets", "Aravind Rajeswaran", "Franziska Meier"], "title": "Locate 3D: Real-World Object Localization via Self-Supervised Learning in 3D", "categories": ["cs.CV", "cs.AI", "cs.RO", "I.2.10; I.2.6; I.2.9; I.3.7; I.4.6; I.4.8"], "comment": null, "summary": "We present LOCATE 3D, a model for localizing objects in 3D scenes from\nreferring expressions like \"the small coffee table between the sofa and the\nlamp.\" LOCATE 3D sets a new state-of-the-art on standard referential grounding\nbenchmarks and showcases robust generalization capabilities. Notably, LOCATE 3D\noperates directly on sensor observation streams (posed RGB-D frames), enabling\nreal-world deployment on robots and AR devices. Key to our approach is 3D-JEPA,\na novel self-supervised learning (SSL) algorithm applicable to sensor point\nclouds. It takes as input a 3D pointcloud featurized using 2D foundation models\n(CLIP, DINO). Subsequently, masked prediction in latent space is employed as a\npretext task to aid the self-supervised learning of contextualized pointcloud\nfeatures. Once trained, the 3D-JEPA encoder is finetuned alongside a\nlanguage-conditioned decoder to jointly predict 3D masks and bounding boxes.\nAdditionally, we introduce LOCATE 3D DATASET, a new dataset for 3D referential\ngrounding, spanning multiple capture setups with over 130K annotations. This\nenables a systematic study of generalization capabilities as well as a stronger\nmodel."}
{"id": "2504.14321", "pdf": "https://arxiv.org/pdf/2504.14321", "abs": "https://arxiv.org/abs/2504.14321", "authors": ["Xingyu Li", "Chen Gong", "Guohong Fu"], "title": "Multimodal Coreference Resolution for Chinese Social Media Dialogues: Dataset and Benchmark Approach", "categories": ["cs.CL"], "comment": null, "summary": "Multimodal coreference resolution (MCR) aims to identify mentions referring\nto the same entity across different modalities, such as text and visuals, and\nis essential for understanding multimodal content. In the era of rapidly\ngrowing mutimodal content and social media, MCR is particularly crucial for\ninterpreting user interactions and bridging text-visual references to improve\ncommunication and personalization. However, MCR research for real-world\ndialogues remains unexplored due to the lack of sufficient data resources.To\naddress this gap, we introduce TikTalkCoref, the first Chinese multimodal\ncoreference dataset for social media in real-world scenarios, derived from the\npopular Douyin short-video platform. This dataset pairs short videos with\ncorresponding textual dialogues from user comments and includes manually\nannotated coreference clusters for both person mentions in the text and the\ncoreferential person head regions in the corresponding video frames. We also\npresent an effective benchmark approach for MCR, focusing on the celebrity\ndomain, and conduct extensive experiments on our dataset, providing reliable\nbenchmark results for this newly constructed dataset. We will release the\nTikTalkCoref dataset to facilitate future research on MCR for real-world social\nmedia dialogues."}
{"id": "2504.14282", "pdf": "https://arxiv.org/pdf/2504.14282", "abs": "https://arxiv.org/abs/2504.14282", "authors": ["Ze Zhao", "Bin Lu", "Xiaoying Gan", "Gu Tang", "Luoyi Fu", "Xinbing Wang"], "title": "CHAINSFORMER: Numerical Reasoning on Knowledge Graphs from a Chain Perspective", "categories": ["cs.AI", "cs.LG"], "comment": "Accepted to ICDE 2025", "summary": "Reasoning over Knowledge Graphs (KGs) plays a pivotal role in knowledge graph\ncompletion or question answering systems, providing richer and more accurate\ntriples and attributes. As numerical attributes become increasingly essential\nin characterizing entities and relations in KGs, the ability to reason over\nthese attributes has gained significant importance. Existing graph-based\nmethods such as Graph Neural Networks (GNNs) and Knowledge Graph Embeddings\n(KGEs), primarily focus on aggregating homogeneous local neighbors and\nimplicitly embedding diverse triples. However, these approaches often fail to\nfully leverage the potential of logical paths within the graph, limiting their\neffectiveness in exploiting the reasoning process. To address these\nlimitations, we propose ChainsFormer, a novel chain-based framework designed to\nsupport numerical reasoning. Chainsformer not only explicitly constructs\nlogical chains but also expands the reasoning depth to multiple hops.\nSpecially, we introduces Relation-Attribute Chains (RA-Chains), a specialized\nlogic chain, to model sequential reasoning patterns. ChainsFormer captures the\nstep-by-step nature of multi-hop reasoning along RA-Chains by employing\nsequential in-context learning. To mitigate the impact of noisy chains, we\npropose a hyperbolic affinity scoring mechanism that selects relevant logic\nchains in a variable-resolution space. Furthermore, ChainsFormer incorporates\nan attention-based numerical reasoner to identify critical reasoning paths,\nenhancing both reasoning accuracy and transparency. Experimental results\ndemonstrate that ChainsFormer significantly outperforms state-of-the-art\nmethods, achieving up to a 20.0% improvement in performance. The\nimplementations are available at\nhttps://github.com/zhaodazhuang2333/ChainsFormer."}
{"id": "2504.14178", "pdf": "https://arxiv.org/pdf/2504.14178", "abs": "https://arxiv.org/abs/2504.14178", "authors": ["Yijie Li", "Hewei Wang", "Jiayi Zhang", "Jinjiang You", "Jinfeng Xu", "Puzhen Wu", "Yunzhong Xiao", "Soumyabrata Dev"], "title": "Segregation and Context Aggregation Network for Real-time Cloud Segmentation", "categories": ["cs.CV"], "comment": "15 pages", "summary": "Cloud segmentation from intensity images is a pivotal task in atmospheric\nscience and computer vision, aiding weather forecasting and climate analysis.\nGround-based sky/cloud segmentation extracts clouds from images for further\nfeature analysis. Existing methods struggle to balance segmentation accuracy\nand computational efficiency, limiting real-world deployment on edge devices,\nso we introduce SCANet, a novel lightweight cloud segmentation model featuring\nSegregation and Context Aggregation Module (SCAM), which refines rough\nsegmentation maps into weighted sky and cloud features processed separately.\nSCANet achieves state-of-the-art performance while drastically reducing\ncomputational complexity. SCANet-large (4.29M) achieves comparable accuracy to\nstate-of-the-art methods with 70.9% fewer parameters. Meanwhile, SCANet-lite\n(90K) delivers 1390 fps in FP16, surpassing real-time standards. Additionally,\nwe propose an efficient pre-training strategy that enhances performance even\nwithout ImageNet pre-training."}
{"id": "2504.14366", "pdf": "https://arxiv.org/pdf/2504.14366", "abs": "https://arxiv.org/abs/2504.14366", "authors": ["Patrick Haller", "Jonas Golde", "Alan Akbik"], "title": "Empirical Evaluation of Knowledge Distillation from Transformers to Subquadratic Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Knowledge distillation is a widely used technique for compressing large\nlanguage models (LLMs) by training a smaller student model to mimic a larger\nteacher model. Typically, both the teacher and student are Transformer-based\narchitectures, leveraging softmax attention for sequence modeling. However, the\nquadratic complexity of self-attention at inference time remains a significant\nbottleneck, motivating the exploration of subquadratic alternatives such as\nstructured state-space models (SSMs), linear attention, and recurrent\narchitectures. In this work, we systematically evaluate the transferability of\nknowledge distillation from a Transformer teacher to nine subquadratic student\narchitectures. Our study aims to determine which subquadratic model best aligns\nwith the teacher's learned representations and how different architectural\nconstraints influence the distillation process. We also investigate the impact\nof intelligent initialization strategies, including matrix mixing and\nquery-key-value (QKV) copying, on the adaptation process. Our empirical results\non multiple NLP benchmarks provide insights into the trade-offs between\nefficiency and performance, highlighting key factors for successful knowledge\ntransfer to subquadratic architectures."}
{"id": "2504.14298", "pdf": "https://arxiv.org/pdf/2504.14298", "abs": "https://arxiv.org/abs/2504.14298", "authors": ["Xiucheng Wang", "Zhongsheng Fang", "Nan Cheng"], "title": "RadioDiff-Inverse: Diffusion Enhanced Bayesian Inverse Estimation for ISAC Radio Map Construction", "categories": ["cs.AI"], "comment": "12 pages, 7 figures", "summary": "Radio maps (RMs) are essential for environment-aware communication and\nsensing, providing location-specific wireless channel information. Existing RM\nconstruction methods often rely on precise environmental data and base station\n(BS) locations, which are not always available in dynamic or privacy-sensitive\nenvironments. While sparse measurement techniques reduce data collection, the\nimpact of noise in sparse data on RM accuracy is not well understood. This\npaper addresses these challenges by formulating RM construction as a Bayesian\ninverse problem under coarse environmental knowledge and noisy sparse\nmeasurements. Although maximum a posteriori (MAP) filtering offers an optimal\nsolution, it requires a precise prior distribution of the RM, which is\ntypically unavailable. To solve this, we propose RadioDiff-Inverse, a\ndiffusion-enhanced Bayesian inverse estimation framework that uses an\nunconditional generative diffusion model to learn the RM prior. This approach\nnot only reconstructs the spatial distribution of wireless channel features but\nalso enables environmental structure perception, such as building outlines, and\nlocation of BS just relay on pathloss, through integrated sensing and\ncommunication (ISAC). Remarkably, RadioDiff-Inverse is training-free,\nleveraging a pre-trained model from Imagenet without task-specific fine-tuning,\nwhich significantly reduces the training cost of using generative large model\nin wireless networks. Experimental results demonstrate that RadioDiff-Inverse\nachieves state-of-the-art performance in accuracy of RM construction and\nenvironmental reconstruction, and robustness against noisy sparse sampling."}
{"id": "2504.14200", "pdf": "https://arxiv.org/pdf/2504.14200", "abs": "https://arxiv.org/abs/2504.14200", "authors": ["Huiyi Chen", "Jiawei Peng", "Kaihua Tang", "Xin Geng", "Xu Yang"], "title": "Enhancing Multimodal In-Context Learning for Image Classification through Coreset Optimization", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 5 figures", "summary": "In-context learning (ICL) enables Large Vision-Language Models (LVLMs) to\nadapt to new tasks without parameter updates, using a few demonstrations from a\nlarge support set. However, selecting informative demonstrations leads to high\ncomputational and memory costs. While some methods explore selecting a small\nand representative coreset in the text classification, evaluating all support\nset samples remains costly, and discarded samples lead to unnecessary\ninformation loss. These methods may also be less effective for image\nclassification due to differences in feature spaces. Given these limitations,\nwe propose Key-based Coreset Optimization (KeCO), a novel framework that\nleverages untapped data to construct a compact and informative coreset. We\nintroduce visual features as keys within the coreset, which serve as the anchor\nfor identifying samples to be updated through different selection strategies.\nBy leveraging untapped samples from the support set, we update the keys of\nselected coreset samples, enabling the randomly initialized coreset to evolve\ninto a more informative coreset under low computational cost. Through extensive\nexperiments on coarse-grained and fine-grained image classification benchmarks,\nwe demonstrate that KeCO effectively enhances ICL performance for image\nclassification task, achieving an average improvement of more than 20\\%.\nNotably, we evaluate KeCO under a simulated online scenario, and the strong\nperformance in this scenario highlights the practical value of our framework\nfor resource-constrained real-world scenarios."}
{"id": "2504.14367", "pdf": "https://arxiv.org/pdf/2504.14367", "abs": "https://arxiv.org/abs/2504.14367", "authors": ["Gabriel Machado Santos", "Rita Maria da Silva Julia", "Marcelo Zanchetta do Nascimento"], "title": "Diverse Prompts: Illuminating the Prompt Space of Large Language Models with MAP-Elites", "categories": ["cs.CL", "cs.AI"], "comment": "8 pages Accepted for publication in IEEE CEC 2025", "summary": "Prompt engineering is essential for optimizing large language models (LLMs),\nyet the link between prompt structures and task performance remains\nunderexplored. This work introduces an evolutionary approach that combines\ncontext-free grammar (CFG) with the MAP-Elites algorithm to systematically\nexplore the prompt space. Our method prioritizes quality and diversity,\ngenerating high-performing and structurally varied prompts while analyzing\ntheir alignment with diverse tasks by varying traits such as the number of\nexamples (shots) and reasoning depth. By systematically mapping the phenotypic\nspace, we reveal how structural variations influence LLM performance, offering\nactionable insights for task-specific and adaptable prompt design. Evaluated on\nseven BigBench Lite tasks across multiple LLMs, our results underscore the\ncritical interplay of quality and diversity, advancing the effectiveness and\nversatility of LLMs."}
{"id": "2504.14325", "pdf": "https://arxiv.org/pdf/2504.14325", "abs": "https://arxiv.org/abs/2504.14325", "authors": ["Alessio Buscemi", "Daniele Proverbio", "Alessandro Di Stefano", "The Anh Han", "German Castignani", "Pietro Liò"], "title": "FAIRGAME: a Framework for AI Agents Bias Recognition using Game Theory", "categories": ["cs.AI"], "comment": null, "summary": "Letting AI agents interact in multi-agent applications adds a layer of\ncomplexity to the interpretability and prediction of AI outcomes, with profound\nimplications for their trustworthy adoption in research and society. Game\ntheory offers powerful models to capture and interpret strategic interaction\namong agents, but requires the support of reproducible, standardized and\nuser-friendly IT frameworks to enable comparison and interpretation of results.\nTo this end, we present FAIRGAME, a Framework for AI Agents Bias Recognition\nusing Game Theory. We describe its implementation and usage, and we employ it\nto uncover biased outcomes in popular games among AI agents, depending on the\nemployed Large Language Model (LLM) and used language, as well as on the\npersonality trait or strategic knowledge of the agents. Overall, FAIRGAME\nallows users to reliably and easily simulate their desired games and scenarios\nand compare the results across simulation campaigns and with game-theoretic\npredictions, enabling the systematic discovery of biases, the anticipation of\nemerging behavior out of strategic interplays, and empowering further research\ninto strategic decision-making using LLM agents."}
{"id": "2504.14202", "pdf": "https://arxiv.org/pdf/2504.14202", "abs": "https://arxiv.org/abs/2504.14202", "authors": ["Zichuan Liu", "Liming Jiang", "Qing Yan", "Yumin Jia", "Hao Kang", "Xin Lu"], "title": "Learning Joint ID-Textual Representation for ID-Preserving Image Synthesis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We propose a novel framework for ID-preserving generation using a multi-modal\nencoding strategy rather than injecting identity features via adapters into\npre-trained models. Our method treats identity and text as a unified\nconditioning input. To achieve this, we introduce FaceCLIP, a multi-modal\nencoder that learns a joint embedding space for both identity and textual\nsemantics. Given a reference face and a text prompt, FaceCLIP produces a\nunified representation that encodes both identity and text, which conditions a\nbase diffusion model to generate images that are identity-consistent and\ntext-aligned. We also present a multi-modal alignment algorithm to train\nFaceCLIP, using a loss that aligns its joint representation with face, text,\nand image embedding spaces. We then build FaceCLIP-SDXL, an ID-preserving image\nsynthesis pipeline by integrating FaceCLIP with Stable Diffusion XL (SDXL).\nCompared to prior methods, FaceCLIP-SDXL enables photorealistic portrait\ngeneration with better identity preservation and textual relevance. Extensive\nexperiments demonstrate its quantitative and qualitative superiority."}
{"id": "2504.14452", "pdf": "https://arxiv.org/pdf/2504.14452", "abs": "https://arxiv.org/abs/2504.14452", "authors": ["Tong Chen", "Faeze Brahman", "Jiacheng Liu", "Niloofar Mireshghallah", "Weijia Shi", "Pang Wei Koh", "Luke Zettlemoyer", "Hannaneh Hajishirzi"], "title": "ParaPO: Aligning Language Models to Reduce Verbatim Reproduction of Pre-training Data", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Language models (LMs) can memorize and reproduce segments from their\npretraining data verbatim even in non-adversarial settings, raising concerns\nabout copyright, plagiarism, privacy, and creativity. We introduce Paraphrase\nPreference Optimization (ParaPO), a post-training method that fine-tunes LMs to\nreduce unintentional regurgitation while preserving their overall utility.\nParaPO trains LMs to prefer paraphrased versions of memorized segments over the\noriginal verbatim content from the pretraining data. To maintain the ability to\nrecall famous quotations when appropriate, we develop a variant of ParaPO that\nuses system prompts to control regurgitation behavior. In our evaluation on\nLlama3.1-8B, ParaPO consistently reduces regurgitation across all tested\ndatasets (e.g., reducing the regurgitation metric from 17.3 to 12.9 in creative\nwriting), whereas unlearning methods used in prior work to mitigate\nregurgitation are less effective outside their targeted unlearned domain (from\n17.3 to 16.9). When applied to the instruction-tuned Tulu3-8B model, ParaPO\nwith system prompting successfully preserves famous quotation recall while\nreducing unintentional regurgitation (from 8.7 to 6.3 in creative writing) when\nprompted not to regurgitate. In contrast, without ParaPO tuning, prompting the\nmodel not to regurgitate produces only a marginal reduction (8.7 to 8.4)."}
{"id": "2504.14350", "pdf": "https://arxiv.org/pdf/2504.14350", "abs": "https://arxiv.org/abs/2504.14350", "authors": ["Yi Sun", "Han Wang", "Jiaqiang Li", "Jiacheng Liu", "Xiangyu Li", "Hao Wen", "Huiwen Zheng", "Yan Liang", "Yuanchun Li", "Yunxin Liu"], "title": "Time's Up! An Empirical Study of LLM Reasoning Ability Under Output Length Constraint", "categories": ["cs.AI"], "comment": null, "summary": "Recent work has demonstrated the remarkable potential of Large Language\nModels (LLMs) in test-time scaling. By making the models think before\nanswering, they are able to achieve much higher accuracy with extra inference\ncomputation. However, in many real-world scenarios, models are used under time\nconstraints, where an answer should be given to the user within a certain\noutput length. It is unclear whether and how the reasoning abilities of LLMs\nremain effective under such constraints. We take a first look at this problem\nby conducting an in-depth empirical study. Specifically, we test more than 25\nLLMs on common reasoning datasets under a wide range of output length budgets,\nand we analyze the correlation between the inference accuracy and various\nproperties including model type, model size, prompt style, etc. We also\nconsider the mappings between the token budgets and the actual on-device\nlatency budgets. The results have demonstrated several interesting findings\nregarding the budget-aware LLM reasoning that differ from the unconstrained\nsituation, e.g. the optimal choices of model sizes and prompts change under\ndifferent budgets. These findings offer practical guidance for users to deploy\nLLMs under real-world latency constraints."}
{"id": "2504.14221", "pdf": "https://arxiv.org/pdf/2504.14221", "abs": "https://arxiv.org/abs/2504.14221", "authors": ["Wenbing Zhu", "Lidong Wang", "Ziqing Zhou", "Chengjie Wang", "Yurui Pan", "Ruoyi Zhang", "Zhuhao Chen", "Linjie Cheng", "Bin-Bin Gao", "Jiangning Zhang", "Zhenye Gan", "Yuxie Wang", "Yulong Chen", "Shuguang Qian", "Mingmin Chi", "Bo Peng", "Lizhuang Ma"], "title": "Real-IAD D3: A Real-World 2D/Pseudo-3D/3D Dataset for Industrial Anomaly Detection", "categories": ["cs.CV"], "comment": "13 pages. Dataset and code: https://realiad4ad.github.io/Real-IAD D3", "summary": "The increasing complexity of industrial anomaly detection (IAD) has\npositioned multimodal detection methods as a focal area of machine vision\nresearch. However, dedicated multimodal datasets specifically tailored for IAD\nremain limited. Pioneering datasets like MVTec 3D have laid essential\ngroundwork in multimodal IAD by incorporating RGB+3D data, but still face\nchallenges in bridging the gap with real industrial environments due to\nlimitations in scale and resolution. To address these challenges, we introduce\nReal-IAD D3, a high-precision multimodal dataset that uniquely incorporates an\nadditional pseudo3D modality generated through photometric stereo, alongside\nhigh-resolution RGB images and micrometer-level 3D point clouds. Real-IAD D3\nfeatures finer defects, diverse anomalies, and greater scale across 20\ncategories, providing a challenging benchmark for multimodal IAD Additionally,\nwe introduce an effective approach that integrates RGB, point cloud, and\npseudo-3D depth information to leverage the complementary strengths of each\nmodality, enhancing detection performance. Our experiments highlight the\nimportance of these modalities in boosting detection robustness and overall IAD\nperformance. The dataset and code are publicly accessible for research purposes\nat https://realiad4ad.github.io/Real-IAD D3"}
{"id": "2504.14462", "pdf": "https://arxiv.org/pdf/2504.14462", "abs": "https://arxiv.org/abs/2504.14462", "authors": ["Armin Toroghi", "Willis Guo", "Scott Sanner"], "title": "CoLoTa: A Dataset for Entity-based Commonsense Reasoning over Long-Tail Knowledge", "categories": ["cs.CL"], "comment": null, "summary": "The rise of Large Language Models (LLMs) has redefined the AI landscape,\nparticularly due to their ability to encode factual and commonsense knowledge,\nand their outstanding performance in tasks requiring reasoning. Despite these\nadvances, hallucinations and reasoning errors remain a significant barrier to\ntheir deployment in high-stakes settings. In this work, we observe that even\nthe most prominent LLMs, such as OpenAI-o1, suffer from high rates of reasoning\nerrors and hallucinations on tasks requiring commonsense reasoning over\nobscure, long-tail entities. To investigate this limitation, we present a new\ndataset for Commonsense reasoning over Long-Tail entities (CoLoTa), that\nconsists of 3,300 queries from question answering and claim verification tasks\nand covers a diverse range of commonsense reasoning skills. We remark that\nCoLoTa can also serve as a Knowledge Graph Question Answering (KGQA) dataset\nsince the support of knowledge required to answer its queries is present in the\nWikidata knowledge graph. However, as opposed to existing KGQA benchmarks that\nmerely focus on factoid questions, our CoLoTa queries also require commonsense\nreasoning. Our experiments with strong LLM-based KGQA methodologies indicate\ntheir severe inability to answer queries involving commonsense reasoning.\nHence, we propose CoLoTa as a novel benchmark for assessing both (i) LLM\ncommonsense reasoning capabilities and their robustness to hallucinations on\nlong-tail entities and (ii) the commonsense reasoning capabilities of KGQA\nmethods."}
{"id": "2504.14356", "pdf": "https://arxiv.org/pdf/2504.14356", "abs": "https://arxiv.org/abs/2504.14356", "authors": ["Masoud Ataei", "Edrin Hasaj", "Jacob Gipp", "Sepideh Forouzi"], "title": "Mathematical Programming Models for Exact and Interpretable Formulation of Neural Networks", "categories": ["cs.AI", "math.OC"], "comment": null, "summary": "This paper presents a unified mixed-integer programming framework for\ntraining sparse and interpretable neural networks. We develop exact\nformulations for both fully connected and convolutional architectures by\nmodeling nonlinearities such as ReLU activations through binary variables and\nencoding structural sparsity via filter- and layer-level pruning constraints.\nThe resulting models integrate parameter learning, architecture selection, and\nstructural regularization within a single optimization problem, yielding\nglobally optimal solutions with respect to a composite objective that balances\nprediction accuracy, weight sparsity, and architectural compactness. The\nmixed-integer programming formulation accommodates piecewise-linear operations,\nincluding max pooling and activation gating, and permits precise enforcement of\nlogic-based or domain-specific constraints. By incorporating considerations of\ninterpretability, sparsity, and verifiability directly into the training\nprocess, the proposed framework bridges a range of research areas including\nexplainable artificial intelligence, symbolic reasoning, and formal\nverification."}
{"id": "2504.14224", "pdf": "https://arxiv.org/pdf/2504.14224", "abs": "https://arxiv.org/abs/2504.14224", "authors": ["Yongguang Li", "Jindong Li", "Qi Wang", "Qianli Xing", "Runliang Niu", "Shengsheng Wang", "Menglin Yang"], "title": "Revisiting CLIP for SF-OSDA: Unleashing Zero-Shot Potential with Adaptive Threshold and Training-Free Feature Filtering", "categories": ["cs.CV"], "comment": null, "summary": "Source-Free Unsupervised Open-Set Domain Adaptation (SF-OSDA) methods using\nCLIP face significant issues: (1) while heavily dependent on domain-specific\nthreshold selection, existing methods employ simple fixed thresholds,\nunderutilizing CLIP's zero-shot potential in SF-OSDA scenarios; and (2)\noverlook intrinsic class tendencies while employing complex training to enforce\nfeature separation, incurring deployment costs and feature shifts that\ncompromise CLIP's generalization ability. To address these issues, we propose\nCLIPXpert, a novel SF-OSDA approach that integrates two key components: an\nadaptive thresholding strategy and an unknown class feature filtering module.\nSpecifically, the Box-Cox GMM-Based Adaptive Thresholding (BGAT) module\ndynamically determines the optimal threshold by estimating sample score\ndistributions, balancing known class recognition and unknown class sample\ndetection. Additionally, the Singular Value Decomposition (SVD)-Based\nUnknown-Class Feature Filtering (SUFF) module reduces the tendency of unknown\nclass samples towards known classes, improving the separation between known and\nunknown classes. Experiments show that our source-free and training-free method\noutperforms state-of-the-art trained approach UOTA by 1.92% on the DomainNet\ndataset, achieves SOTA-comparable performance on datasets such as Office-Home,\nand surpasses other SF-OSDA methods. This not only validates the effectiveness\nof our proposed method but also highlights CLIP's strong zero-shot potential\nfor SF-OSDA tasks."}
{"id": "2504.14468", "pdf": "https://arxiv.org/pdf/2504.14468", "abs": "https://arxiv.org/abs/2504.14468", "authors": ["Yijun Liu"], "title": "sEEG-based Encoding for Sentence Retrieval: A Contrastive Learning Approach to Brain-Language Alignment", "categories": ["cs.CL", "cs.LG", "eess.SP", "q-bio.NC"], "comment": "Accepted for poster presentation at the CVPR 2025 Workshop on\n  Multimodal Foundation Models (MMFM3)", "summary": "Interpreting neural activity through meaningful latent representations\nremains a complex and evolving challenge at the intersection of neuroscience\nand artificial intelligence. We investigate the potential of multimodal\nfoundation models to align invasive brain recordings with natural language. We\npresent SSENSE, a contrastive learning framework that projects single-subject\nstereo-electroencephalography (sEEG) signals into the sentence embedding space\nof a frozen CLIP model, enabling sentence-level retrieval directly from brain\nactivity. SSENSE trains a neural encoder on spectral representations of sEEG\nusing InfoNCE loss, without fine-tuning the text encoder. We evaluate our\nmethod on time-aligned sEEG and spoken transcripts from a naturalistic\nmovie-watching dataset. Despite limited data, SSENSE achieves promising\nresults, demonstrating that general-purpose language representations can serve\nas effective priors for neural decoding."}
{"id": "2504.14379", "pdf": "https://arxiv.org/pdf/2504.14379", "abs": "https://arxiv.org/abs/2504.14379", "authors": ["Andrew Lee", "Lihao Sun", "Chris Wendler", "Fernanda Viégas", "Martin Wattenberg"], "title": "The Geometry of Self-Verification in a Task-Specific Reasoning Model", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "How do reasoning models verify their own answers? We study this question by\ntraining a model using DeepSeek R1's recipe on the CountDown task. We leverage\nthe fact that preference tuning leads to mode collapse, resulting in a model\nthat always produces highly structured and easily parse-able chain-of-thought\nsequences. With this setup, we do a top-down and bottom-up analysis to\nreverse-engineer how the model verifies its outputs. Our top-down analysis\nreveals Gated Linear Unit (GLU) weights encoding verification-related tokens,\nsuch as ``success'' or ``incorrect'', which activate according to the\ncorrectness of the model's reasoning steps. Our bottom-up analysis reveals that\n``previous-token heads'' are mainly responsible for model verification. Our\nanalyses meet in the middle: drawing inspiration from inter-layer communication\nchannels, we use the identified GLU vectors to localize as few as three\nattention heads that can disable model verification, pointing to a necessary\ncomponent of a potentially larger verification circuit."}
{"id": "2504.14231", "pdf": "https://arxiv.org/pdf/2504.14231", "abs": "https://arxiv.org/abs/2504.14231", "authors": ["Johannes Spoecklberger", "Wei Lin", "Pedro Hermosilla", "Sivan Doveh", "Horst Possegger", "M. Jehanzeb Mirza"], "title": "Exploring Modality Guidance to Enhance VFM-based Feature Fusion for UDA in 3D Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Vision Foundation Models (VFMs) have become a de facto choice for many\ndownstream vision tasks, like image classification, image segmentation, and\nobject localization. However, they can also provide significant utility for\ndownstream 3D tasks that can leverage the cross-modal information (e.g., from\npaired image data). In our work, we further explore the utility of VFMs for\nadapting from a labeled source to unlabeled target data for the task of\nLiDAR-based 3D semantic segmentation. Our method consumes paired 2D-3D (image\nand point cloud) data and relies on the robust (cross-domain) features from a\nVFM to train a 3D backbone on a mix of labeled source and unlabeled target\ndata. At the heart of our method lies a fusion network that is guided by both\nthe image and point cloud streams, with their relative contributions adjusted\nbased on the target domain. We extensively compare our proposed methodology\nwith different state-of-the-art methods in several settings and achieve strong\nperformance gains. For example, achieving an average improvement of 6.5 mIoU\n(over all tasks), when compared with the previous state-of-the-art."}
{"id": "2504.14482", "pdf": "https://arxiv.org/pdf/2504.14482", "abs": "https://arxiv.org/abs/2504.14482", "authors": ["Xiang Li", "Duyi Pan", "Hongru Xiao", "Jiale Han", "Jing Tang", "Jiabao Ma", "Wei Wang", "Bo Cheng"], "title": "DialogueAgents: A Hybrid Agent-Based Speech Synthesis Framework for Multi-Party Dialogue", "categories": ["cs.CL", "cs.SD"], "comment": "Accepted by ICME 2025. Dataset and code are publicly available:\n  [https://github.com/uirlx/DialogueAgents](https://github.com/uirlx/DialogueAgents)", "summary": "Speech synthesis is crucial for human-computer interaction, enabling natural\nand intuitive communication. However, existing datasets involve high\nconstruction costs due to manual annotation and suffer from limited character\ndiversity, contextual scenarios, and emotional expressiveness. To address these\nissues, we propose DialogueAgents, a novel hybrid agent-based speech synthesis\nframework, which integrates three specialized agents -- a script writer, a\nspeech synthesizer, and a dialogue critic -- to collaboratively generate\ndialogues. Grounded in a diverse character pool, the framework iteratively\nrefines dialogue scripts and synthesizes speech based on speech review,\nboosting emotional expressiveness and paralinguistic features of the\nsynthesized dialogues. Using DialogueAgent, we contribute MultiTalk, a\nbilingual, multi-party, multi-turn speech dialogue dataset covering diverse\ntopics. Extensive experiments demonstrate the effectiveness of our framework\nand the high quality of the MultiTalk dataset. We release the dataset and code\nhttps://github.com/uirlx/DialogueAgents to facilitate future research on\nadvanced speech synthesis models and customized data generation."}
{"id": "2504.14448", "pdf": "https://arxiv.org/pdf/2504.14448", "abs": "https://arxiv.org/abs/2504.14448", "authors": ["Ali Arslan Yousaf", "Umair Rehman", "Muhammad Umair Danish"], "title": "Seeing Through Risk: A Symbolic Approximation of Prospect Theory", "categories": ["cs.AI", "cs.LG", "math.OC", "stat.ME"], "comment": null, "summary": "We propose a novel symbolic modeling framework for decision-making under risk\nthat merges interpretability with the core insights of Prospect Theory. Our\napproach replaces opaque utility curves and probability weighting functions\nwith transparent, effect-size-guided features. We mathematically formalize the\nmethod, demonstrate its ability to replicate well-known framing and\nloss-aversion phenomena, and provide an end-to-end empirical validation on\nsynthetic datasets. The resulting model achieves competitive predictive\nperformance while yielding clear coefficients mapped onto psychological\nconstructs, making it suitable for applications ranging from AI safety to\neconomic policy analysis."}
{"id": "2504.14238", "pdf": "https://arxiv.org/pdf/2504.14238", "abs": "https://arxiv.org/abs/2504.14238", "authors": ["Lu Pan", "Yu-Hsuan Huang", "Hongxia Xie", "Cheng Zhang", "Hongwei Zhao", "Hong-Han Shuai", "Wen-Huang Cheng"], "title": "Single Document Image Highlight Removal via A Large-Scale Real-World Dataset and A Location-Aware Network", "categories": ["cs.CV"], "comment": "main paper with 8 pages, conference", "summary": "Reflective documents often suffer from specular highlights under ambient\nlighting, severely hindering text readability and degrading overall visual\nquality. Although recent deep learning methods show promise in highlight\nremoval, they remain suboptimal for document images, primarily due to the lack\nof dedicated datasets and tailored architectural designs. To tackle these\nchallenges, we present DocHR14K, a large-scale real-world dataset comprising\n14,902 high-resolution image pairs across six document categories and various\nlighting conditions. To the best of our knowledge, this is the first\nhigh-resolution dataset for document highlight removal that captures a wide\nrange of real-world lighting conditions. Additionally, motivated by the\nobservation that the residual map between highlighted and clean images\nnaturally reveals the spatial structure of highlight regions, we propose a\nsimple yet effective Highlight Location Prior (HLP) to estimate highlight masks\nwithout human annotations. Building on this prior, we present the\nLocation-Aware Laplacian Pyramid Highlight Removal Network (L2HRNet), which\neffectively removes highlights by leveraging estimated priors and incorporates\ndiffusion module to restore details. Extensive experiments demonstrate that\nDocHR14K improves highlight removal under diverse lighting conditions. Our\nL2HRNet achieves state-of-the-art performance across three benchmark datasets,\nincluding a 5.01\\% increase in PSNR and a 13.17\\% reduction in RMSE on\nDocHR14K."}
{"id": "2504.14492", "pdf": "https://arxiv.org/pdf/2504.14492", "abs": "https://arxiv.org/abs/2504.14492", "authors": ["Yichen Li", "Zhiting Fan", "Ruizhe Chen", "Xiaotang Gai", "Luqi Gong", "Yan Zhang", "Zuozhu Liu"], "title": "FairSteer: Inference Time Debiasing for LLMs with Dynamic Activation Steering", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are prone to capturing biases from training\ncorpus, leading to potential negative social impacts. Existing prompt-based\ndebiasing methods exhibit instability due to their sensitivity to prompt\nchanges, while fine-tuning-based techniques incur substantial computational\noverhead and catastrophic forgetting. In this paper, we propose FairSteer, a\nnovel inference-time debiasing framework without requiring customized prompt\ndesign or model retraining. Motivated by the linear representation hypothesis,\nour preliminary investigation demonstrates that fairness-related features can\nbe encoded into separable directions in the hidden activation space. FairSteer\noperates in three steps: biased activation detection, debiasing steering vector\n(DSV) computation, and dynamic activation steering. Specifically, it first\ntrains a lightweight linear classifier to detect bias signatures in\nactivations, and then computes DSVs as intervention directions derived from\nsmall contrastive prompt pairs. Subsequently, it performs debiasing by\nadjusting activations with DSVs in the inference stage. Comprehensive\nevaluation with six LLMs demonstrates the superiority of FairSteer across\nquestion-answering, counterfactual input evaluation and open-ended text\ngeneration tasks. Code will be released."}
{"id": "2504.14520", "pdf": "https://arxiv.org/pdf/2504.14520", "abs": "https://arxiv.org/abs/2504.14520", "authors": ["Ahsan Bilal", "Muhammad Ahmed Mohsin", "Muhammad Umer", "Muhammad Awais Khan Bangash", "Muhammad Ali Jamshed"], "title": "Meta-Thinking in LLMs via Multi-Agent Reinforcement Learning: A Survey", "categories": ["cs.AI", "cs.CL"], "comment": "Submitted to IEEE Transactions on Artificial Intelligence", "summary": "This survey explores the development of meta-thinking capabilities in Large\nLanguage Models (LLMs) from a Multi-Agent Reinforcement Learning (MARL)\nperspective. Meta-thinking self-reflection, assessment, and control of thinking\nprocesses is an important next step in enhancing LLM reliability, flexibility,\nand performance, particularly for complex or high-stakes tasks. The survey\nbegins by analyzing current LLM limitations, such as hallucinations and the\nlack of internal self-assessment mechanisms. It then talks about newer methods,\nincluding RL from human feedback (RLHF), self-distillation, and\nchain-of-thought prompting, and each of their limitations. The crux of the\nsurvey is to talk about how multi-agent architectures, namely supervisor-agent\nhierarchies, agent debates, and theory of mind frameworks, can emulate\nhuman-like introspective behavior and enhance LLM robustness. By exploring\nreward mechanisms, self-play, and continuous learning methods in MARL, this\nsurvey gives a comprehensive roadmap to building introspective, adaptive, and\ntrustworthy LLMs. Evaluation metrics, datasets, and future research avenues,\nincluding neuroscience-inspired architectures and hybrid symbolic reasoning,\nare also discussed."}
{"id": "2504.14240", "pdf": "https://arxiv.org/pdf/2504.14240", "abs": "https://arxiv.org/abs/2504.14240", "authors": ["Xie Liang", "Gao Wei", "Zhenghui Ming", "Li Ge"], "title": "ROI-Guided Point Cloud Geometry Compression Towards Human and Machine Vision", "categories": ["cs.CV", "cs.MM"], "comment": "10 pages, 5 figures", "summary": "Point cloud data is pivotal in applications like autonomous driving, virtual\nreality, and robotics. However, its substantial volume poses significant\nchallenges in storage and transmission. In order to obtain a high compression\nratio, crucial semantic details usually confront severe damage, leading to\ndifficulties in guaranteeing the accuracy of downstream tasks. To tackle this\nproblem, we are the first to introduce a novel Region of Interest (ROI)-guided\nPoint Cloud Geometry Compression (RPCGC) method for human and machine vision.\nOur framework employs a dual-branch parallel structure, where the base layer\nencodes and decodes a simplified version of the point cloud, and the\nenhancement layer refines this by focusing on geometry details. Furthermore,\nthe residual information of the enhancement layer undergoes refinement through\nan ROI prediction network. This network generates mask information, which is\nthen incorporated into the residuals, serving as a strong supervision signal.\nAdditionally, we intricately apply these mask details in the Rate-Distortion\n(RD) optimization process, with each point weighted in the distortion\ncalculation. Our loss function includes RD loss and detection loss to better\nguide point cloud encoding for the machine. Experiment results demonstrate that\nRPCGC achieves exceptional compression performance and better detection\naccuracy (10% gain) than some learning-based compression methods at high\nbitrates in ScanNet and SUN RGB-D datasets."}
{"id": "2504.14496", "pdf": "https://arxiv.org/pdf/2504.14496", "abs": "https://arxiv.org/abs/2504.14496", "authors": ["Zijian Wang", "Chang Xu"], "title": "Functional Abstraction of Knowledge Recall in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Pre-trained transformer large language models (LLMs) demonstrate strong\nknowledge recall capabilities. This paper investigates the knowledge recall\nmechanism in LLMs by abstracting it into a functional structure. We propose\nthat during knowledge recall, the model's hidden activation space implicitly\nentails a function execution process where specific activation vectors align\nwith functional components (Input argument, Function body, and Return values).\nSpecifically, activation vectors of relation-related tokens define a mapping\nfunction from subjects to objects, with subject-related token activations\nserving as input arguments and object-related token activations as return\nvalues. For experimental verification, we first design a patching-based\nknowledge-scoring algorithm to identify knowledge-aware activation vectors as\nindependent functional components. Then, we conduct counter-knowledge testing\nto examine the independent functional effects of each component on knowledge\nrecall outcomes. From this functional perspective, we improve the contextual\nknowledge editing approach augmented by activation patching. By rewriting\nincoherent activations in context, we enable improved short-term memory\nretention for new knowledge prompting."}
{"id": "2504.14523", "pdf": "https://arxiv.org/pdf/2504.14523", "abs": "https://arxiv.org/abs/2504.14523", "authors": ["Gabriela Ben Melech Stan", "Estelle Aflalo", "Avinash Madasu", "Vasudev Lal", "Phillip Howard"], "title": "Learning from Reasoning Failures via Synthetic Data Generation", "categories": ["cs.AI"], "comment": null, "summary": "Training models on synthetic data has emerged as an increasingly important\nstrategy for improving the performance of generative AI. This approach is\nparticularly helpful for large multimodal models (LMMs) due to the relative\nscarcity of high-quality paired image-text data compared to language-only data.\nWhile a variety of methods have been proposed for generating large multimodal\ndatasets, they do not tailor the synthetic data to address specific\ndeficiencies in the reasoning abilities of LMMs which will be trained with the\ngenerated dataset. In contrast, humans often learn in a more efficient manner\nby seeking out examples related to the types of reasoning where they have\nfailed previously. Inspired by this observation, we propose a new approach for\nsynthetic data generation which is grounded in the analysis of an existing\nLMM's reasoning failures. Our methodology leverages frontier models to\nautomatically analyze errors produced by a weaker LMM and propose new examples\nwhich can be used to correct the reasoning failure via additional training,\nwhich are then further filtered to ensure high quality. We generate a large\nmultimodal instruction tuning dataset containing over 553k examples using our\napproach and conduct extensive experiments demonstrating its utility for\nimproving the performance of LMMs on multiple downstream tasks. Our results\nshow that models trained on our synthetic data can even exceed the performance\nof LMMs trained on an equivalent amount of additional real data, demonstrating\nthe high value of generating synthetic data targeted to specific reasoning\nfailure modes in LMMs. We will make our dataset and code publicly available."}
{"id": "2504.14245", "pdf": "https://arxiv.org/pdf/2504.14245", "abs": "https://arxiv.org/abs/2504.14245", "authors": ["Yikun Ji", "Yan Hong", "Jiahui Zhan", "Haoxing Chen", "jun lan", "Huijia Zhu", "Weiqiang Wang", "Liqing Zhang", "Jianfu Zhang"], "title": "Towards Explainable Fake Image Detection with Multi-Modal Large Language Models", "categories": ["cs.CV", "cs.CL", "I.2.7; I.2.10"], "comment": null, "summary": "Progress in image generation raises significant public security concerns. We\nargue that fake image detection should not operate as a \"black box\". Instead,\nan ideal approach must ensure both strong generalization and transparency.\nRecent progress in Multi-modal Large Language Models (MLLMs) offers new\nopportunities for reasoning-based AI-generated image detection. In this work,\nwe evaluate the capabilities of MLLMs in comparison to traditional detection\nmethods and human evaluators, highlighting their strengths and limitations.\nFurthermore, we design six distinct prompts and propose a framework that\nintegrates these prompts to develop a more robust, explainable, and\nreasoning-driven detection system. The code is available at\nhttps://github.com/Gennadiyev/mllm-defake."}
{"id": "2504.14530", "pdf": "https://arxiv.org/pdf/2504.14530", "abs": "https://arxiv.org/abs/2504.14530", "authors": ["Zhijing Jin"], "title": "Causality for Natural Language Processing", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": "PhD Thesis 2024", "summary": "Causal reasoning is a cornerstone of human intelligence and a critical\ncapability for artificial systems aiming to achieve advanced understanding and\ndecision-making. This thesis delves into various dimensions of causal reasoning\nand understanding in large language models (LLMs). It encompasses a series of\nstudies that explore the causal inference skills of LLMs, the mechanisms behind\ntheir performance, and the implications of causal and anticausal learning for\nnatural language processing (NLP) tasks. Additionally, it investigates the\napplication of causal reasoning in text-based computational social science,\nspecifically focusing on political decision-making and the evaluation of\nscientific impact through citations. Through novel datasets, benchmark tasks,\nand methodological frameworks, this work identifies key challenges and\nopportunities to improve the causal capabilities of LLMs, providing a\ncomprehensive foundation for future research in this evolving field."}
{"id": "2504.14556", "pdf": "https://arxiv.org/pdf/2504.14556", "abs": "https://arxiv.org/abs/2504.14556", "authors": ["Yousef Emami", "Hao Gao", "SeyedSina Nabavirazani", "Luis Almeida"], "title": "LLM-Enabled In-Context Learning for Data Collection Scheduling in UAV-assisted Sensor Networks", "categories": ["cs.AI", "cs.ET", "cs.LG", "cs.RO", "53-01", "C.2"], "comment": "8 pages, 7 figures,", "summary": "Unmanned Aerial Vehicles (UAVs) are increasingly being used in various\nprivate and commercial applications, e.g. traffic control, package delivery,\nand Search and Rescue (SAR) operations. Machine Learning (ML) methods used in\nUAV-assisted Sensor Networks (UASNETs) and especially in Deep Reinforcement\nLearning (DRL) face challenges such as complex and lengthy model training, gaps\nbetween simulation and reality, and low sample efficiency, which conflict with\nthe urgency of emergencies such as SAR operations. This paper proposes\nIn-Context Learning (ICL)-based Data Collection Scheduling (ICLDC) scheme, as\nan alternative to DRL in emergencies. The UAV collects and transmits logged\nsensory data, to an LLM, to generate a task description in natural language,\nfrom which it obtains a data collection schedule to be executed by the UAV. The\nsystem continuously adapts by adding feedback to task descriptions and\nutilizing feedback for future decisions. This method is tested against\njailbreaking attacks, where task description is manipulated to undermine\nnetwork performance, highlighting the vulnerability of LLMs to such attacks.\nThe proposed ICLDC outperforms the Maximum Channel Gain by reducing cumulative\npacket loss by approximately 56\\%. ICLDC presents a promising direction for\nintelligent scheduling and control in UAV-assisted data collection."}
{"id": "2504.14249", "pdf": "https://arxiv.org/pdf/2504.14249", "abs": "https://arxiv.org/abs/2504.14249", "authors": ["Bin Ren", "Eduard Zamfir", "Zongwei Wu", "Yawei Li", "Yidi Li", "Danda Pani Paudel", "Radu Timofte", "Ming-Hsuan Yang", "Luc Van Gool", "Nicu Sebe"], "title": "Any Image Restoration via Efficient Spatial-Frequency Degradation Adaptation", "categories": ["cs.CV"], "comment": "Efficient All in One Image Restoration", "summary": "Restoring any degraded image efficiently via just one model has become\nincreasingly significant and impactful, especially with the proliferation of\nmobile devices. Traditional solutions typically involve training dedicated\nmodels per degradation, resulting in inefficiency and redundancy. More recent\napproaches either introduce additional modules to learn visual prompts,\nsignificantly increasing model size, or incorporate cross-modal transfer from\nlarge language models trained on vast datasets, adding complexity to the system\narchitecture. In contrast, our approach, termed AnyIR, takes a unified path\nthat leverages inherent similarity across various degradations to enable both\nefficient and comprehensive restoration through a joint embedding mechanism,\nwithout scaling up the model or relying on large language models.Specifically,\nwe examine the sub-latent space of each input, identifying key components and\nreweighting them first in a gated manner. To fuse the intrinsic degradation\nawareness and the contextualized attention, a spatial-frequency parallel fusion\nstrategy is proposed for enhancing spatial-aware local-global interactions and\nenriching the restoration details from the frequency perspective. Extensive\nbenchmarking in the all-in-one restoration setting confirms AnyIR's SOTA\nperformance, reducing model complexity by around 82\\% in parameters and 85\\% in\nFLOPs. Our code will be available at our Project page\n(https://amazingren.github.io/AnyIR/)"}
{"id": "2504.14538", "pdf": "https://arxiv.org/pdf/2504.14538", "abs": "https://arxiv.org/abs/2504.14538", "authors": ["Yiting Ran", "Xintao Wang", "Tian Qiu", "Jiaqing Liang", "Yanghua Xiao", "Deqing Yang"], "title": "BookWorld: From Novels to Interactive Agent Societies for Creative Story Generation", "categories": ["cs.CL"], "comment": "19 pages, 4 figures", "summary": "Recent advances in large language models (LLMs) have enabled social\nsimulation through multi-agent systems. Prior efforts focus on agent societies\ncreated from scratch, assigning agents with newly defined personas. However,\nsimulating established fictional worlds and characters remain largely\nunderexplored, despite its significant practical value. In this paper, we\nintroduce BookWorld, a comprehensive system for constructing and simulating\nbook-based multi-agent societies. BookWorld's design covers comprehensive\nreal-world intricacies, including diverse and dynamic characters, fictional\nworldviews, geographical constraints and changes, e.t.c. BookWorld enables\ndiverse applications including story generation, interactive games and social\nsimulation, offering novel ways to extend and explore beloved fictional works.\nThrough extensive experiments, we demonstrate that BookWorld generates\ncreative, high-quality stories while maintaining fidelity to the source books,\nsurpassing previous methods with a win rate of 75.36%. The code of this paper\ncan be found at the project page: https://bookworld2025.github.io/."}
{"id": "2504.14596", "pdf": "https://arxiv.org/pdf/2504.14596", "abs": "https://arxiv.org/abs/2504.14596", "authors": ["Kei Itoh"], "title": "Toward the Axiomatization of Intelligence: Structure, Time, and Existence", "categories": ["cs.AI", "cs.NE"], "comment": "37 pages, 4 tables, in English, in Japanese", "summary": "This study aims to construct an axiomatic definition of intelligence within a\nmeta-framework that defines the method of definition, addressing intelligence\nas an inherently naive and polysemous concept. Initially, we formalize a\nset-theoretic representation of the universe as the domain wherein intelligence\nexists and characterize intelligence as a structure that involves temporal\nevolution and interaction with other sets. Starting from a naive definition of\nintelligence as \"an entity possessing structures for externally inputting,\ninternally processing, and externally outputting information or matter,\" we\naxiomatically reformulate it within this set-theoretical depiction of the\nuniverse. Applying this axiomatic definition, we compare and interpret three\nexamples -- Hebbian non-optimized neural networks (NNs),\nbackpropagation-optimized NNs, and biological reflexive systems -- in terms of\ntheir intelligence, structural properties, and biological plausibility.\nFurthermore, by extending our definition into a categorical framework, we\nintroduce two categories, \"Time Category\" and \"Intelligence Category,\" along\nwith the functorial relationships between them, demonstrating the potential to\nrepresent changes and mimicry relationships among intelligent systems\nabstractly. Additionally, since intelligence, as defined herein, functions\neffectively only when accompanied by temporal interactions, we introduce the\nconcept of \"activity\" and explore how activity-based conditions influence\nclassifications and interpretations of intelligence. Finally, we suggest that\nour definitional methodology is not limited to intelligence alone, but can be\nsimilarly applied to other concepts, such as consciousness and emotion,\nadvocating for their formal reinterpretation through the same procedural steps:\ndefining a universal representation, selecting naive definitions, and axiomatic\nformalization."}
{"id": "2504.14253", "pdf": "https://arxiv.org/pdf/2504.14253", "abs": "https://arxiv.org/abs/2504.14253", "authors": ["Yifan Wang", "Jie Gui", "Xinli Shi", "Linqing Gui", "Yuan Yan Tang", "James Tin-Yau Kwok"], "title": "ColorVein: Colorful Cancelable Vein Biometrics", "categories": ["cs.CV"], "comment": null, "summary": "Vein recognition technologies have become one of the primary solutions for\nhigh-security identification systems. However, the issue of biometric\ninformation leakage can still pose a serious threat to user privacy and\nanonymity. Currently, there is no cancelable biometric template generation\nscheme specifically designed for vein biometrics. Therefore, this paper\nproposes an innovative cancelable vein biometric generation scheme: ColorVein.\nUnlike previous cancelable template generation schemes, ColorVein does not\ndestroy the original biometric features and introduces additional color\ninformation to grayscale vein images. This method significantly enhances the\ninformation density of vein images by transforming static grayscale information\ninto dynamically controllable color representations through interactive\ncolorization. ColorVein allows users/administrators to define a controllable\npseudo-random color space for grayscale vein images by editing the position,\nnumber, and color of hint points, thereby generating protected cancelable\ntemplates. Additionally, we propose a new secure center loss to optimize the\ntraining process of the protected feature extraction model, effectively\nincreasing the feature distance between enrolled users and any potential\nimpostors. Finally, we evaluate ColorVein's performance on all types of vein\nbiometrics, including recognition performance, unlinkability, irreversibility,\nand revocability, and conduct security and privacy analyses. ColorVein achieves\ncompetitive performance compared with state-of-the-art methods."}
{"id": "2504.14597", "pdf": "https://arxiv.org/pdf/2504.14597", "abs": "https://arxiv.org/abs/2504.14597", "authors": ["Lingrui Mei", "Shenghua Liu", "Yiwei Wang", "Baolong Bi", "Yuyao Ge", "Jun Wan", "Yurong Wu", "Xueqi Cheng"], "title": "a1: Steep Test-time Scaling Law via Environment Augmented Generation", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have made remarkable breakthroughs in reasoning,\nyet continue to struggle with hallucinations, logical errors, and inability to\nself-correct during complex multi-step tasks. Current approaches like\nchain-of-thought prompting offer limited reasoning capabilities that fail when\nprecise step validation is required. We propose Environment Augmented\nGeneration (EAG), a framework that enhances LLM reasoning through: (1)\nreal-time environmental feedback validating each reasoning step, (2) dynamic\nbranch exploration for investigating alternative solution paths when faced with\nerrors, and (3) experience-based learning from successful reasoning\ntrajectories. Unlike existing methods, EAG enables deliberate backtracking and\nstrategic replanning through tight integration of execution feedback with\nbranching exploration. Our a1-32B model achieves state-of-the-art performance\namong similar-sized models across all benchmarks, matching larger models like\no1 on competition mathematics while outperforming comparable models by up to\n24.4 percentage points. Analysis reveals EAG's distinctive scaling pattern:\ninitial token investment in environment interaction yields substantial\nlong-term performance dividends, with advantages amplifying proportionally to\ntask complexity. EAG's theoretical framework demonstrates how environment\ninteractivity and systematic branch exploration together establish a new\nparadigm for reliable machine reasoning, particularly for problems requiring\nprecise multi-step calculation and logical verification."}
{"id": "2504.14603", "pdf": "https://arxiv.org/pdf/2504.14603", "abs": "https://arxiv.org/abs/2504.14603", "authors": ["Chaoyun Zhang", "He Huang", "Chiming Ni", "Jian Mu", "Si Qin", "Shilin He", "Lu Wang", "Fangkai Yang", "Pu Zhao", "Chao Du", "Liqun Li", "Yu Kang", "Zhao Jiang", "Suzhen Zheng", "Rujia Wang", "Jiaxu Qian", "Minghua Ma", "Jian-Guang Lou", "Qingwei Lin", "Saravan Rajmohan", "Dongmei Zhang"], "title": "UFO2: The Desktop AgentOS", "categories": ["cs.AI", "cs.HC", "cs.OS"], "comment": "The source code of UFO2 is publicly available at\n  https://github.com/microsoft/UFO/, with comprehensive documentation provided\n  at https://microsoft.github.io/UFO/", "summary": "Recent Computer-Using Agents (CUAs), powered by multimodal large language\nmodels (LLMs), offer a promising direction for automating complex desktop\nworkflows through natural language. However, most existing CUAs remain\nconceptual prototypes, hindered by shallow OS integration, fragile\nscreenshot-based interaction, and disruptive execution.\n  We present UFO2, a multiagent AgentOS for Windows desktops that elevates CUAs\ninto practical, system-level automation. UFO2 features a centralized HostAgent\nfor task decomposition and coordination, alongside a collection of\napplication-specialized AppAgent equipped with native APIs, domain-specific\nknowledge, and a unified GUI--API action layer. This architecture enables\nrobust task execution while preserving modularity and extensibility. A hybrid\ncontrol detection pipeline fuses Windows UI Automation (UIA) with vision-based\nparsing to support diverse interface styles. Runtime efficiency is further\nenhanced through speculative multi-action planning, reducing per-step LLM\noverhead. Finally, a Picture-in-Picture (PiP) interface enables automation\nwithin an isolated virtual desktop, allowing agents and users to operate\nconcurrently without interference.\n  We evaluate UFO2 across over 20 real-world Windows applications,\ndemonstrating substantial improvements in robustness and execution accuracy\nover prior CUAs. Our results show that deep OS integration unlocks a scalable\npath toward reliable, user-aligned desktop automation."}
{"id": "2504.14254", "pdf": "https://arxiv.org/pdf/2504.14254", "abs": "https://arxiv.org/abs/2504.14254", "authors": ["Jie Wang", "Nana Yu", "Zihao Zhang", "Yahong Han"], "title": "Visual Consensus Prompting for Co-Salient Object Detection", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Existing co-salient object detection (CoSOD) methods generally employ a\nthree-stage architecture (i.e., encoding, consensus extraction & dispersion,\nand prediction) along with a typical full fine-tuning paradigm. Although they\nyield certain benefits, they exhibit two notable limitations: 1) This\narchitecture relies on encoded features to facilitate consensus extraction, but\nthe meticulously extracted consensus does not provide timely guidance to the\nencoding stage. 2) This paradigm involves globally updating all parameters of\nthe model, which is parameter-inefficient and hinders the effective\nrepresentation of knowledge within the foundation model for this task.\nTherefore, in this paper, we propose an interaction-effective and\nparameter-efficient concise architecture for the CoSOD task, addressing two key\nlimitations. It introduces, for the first time, a parameter-efficient prompt\ntuning paradigm and seamlessly embeds consensus into the prompts to formulate\ntask-specific Visual Consensus Prompts (VCP). Our VCP aims to induce the frozen\nfoundation model to perform better on CoSOD tasks by formulating task-specific\nvisual consensus prompts with minimized tunable parameters. Concretely, the\nprimary insight of the purposeful Consensus Prompt Generator (CPG) is to\nenforce limited tunable parameters to focus on co-salient representations and\ngenerate consensus prompts. The formulated Consensus Prompt Disperser (CPD)\nleverages consensus prompts to form task-specific visual consensus prompts,\nthereby arousing the powerful potential of pre-trained models in addressing\nCoSOD tasks. Extensive experiments demonstrate that our concise VCP outperforms\n13 cutting-edge full fine-tuning models, achieving the new state of the art\n(with 6.8% improvement in F_m metrics on the most challenging CoCA dataset).\nSource code has been available at https://github.com/WJ-CV/VCP."}
{"id": "2504.14619", "pdf": "https://arxiv.org/pdf/2504.14619", "abs": "https://arxiv.org/abs/2504.14619", "authors": ["Yuri Balashov", "Alex Balashov", "Shiho Fukuda Koski"], "title": "Translation Analytics for Freelancers: I. Introduction, Data Preparation, Baseline Evaluations", "categories": ["cs.CL"], "comment": "28 pages, 4 figures. Accepted at the MT Summit, University of Geneva,\n  June 2025", "summary": "This is the first in a series of papers exploring the rapidly expanding new\nopportunities arising from recent progress in language technologies for\nindividual translators and language service providers with modest resources.\nThe advent of advanced neural machine translation systems, large language\nmodels, and their integration into workflows via computer-assisted translation\ntools and translation management systems have reshaped the translation\nlandscape. These advancements enable not only translation but also quality\nevaluation, error spotting, glossary generation, and adaptation to\ndomain-specific needs, creating new technical opportunities for freelancers. In\nthis series, we aim to empower translators with actionable methods to harness\nthese advancements. Our approach emphasizes Translation Analytics, a suite of\nevaluation techniques traditionally reserved for large-scale industry\napplications but now becoming increasingly available for smaller-scale users.\nThis first paper introduces a practical framework for adapting automatic\nevaluation metrics -- such as BLEU, chrF, TER, and COMET -- to freelancers'\nneeds. We illustrate the potential of these metrics using a trilingual corpus\nderived from a real-world project in the medical domain and provide statistical\nanalysis correlating human evaluations with automatic scores. Our findings\nemphasize the importance of proactive engagement with emerging technologies to\nnot only adapt but thrive in the evolving professional environment."}
{"id": "2504.14624", "pdf": "https://arxiv.org/pdf/2504.14624", "abs": "https://arxiv.org/abs/2504.14624", "authors": ["Polina Gordienko", "Christoph Jansen", "Thomas Augustin", "Martin Rechenauer"], "title": "Consensus in Motion: A Case of Dynamic Rationality of Sequential Learning in Probability Aggregation", "categories": ["cs.AI"], "comment": "Submitted to the International Conference on Modeling Decisions for\n  Artificial Intelligence (MDAI 2025)", "summary": "We propose a framework for probability aggregation based on propositional\nprobability logic. Unlike conventional judgment aggregation, which focuses on\nstatic rationality, our model addresses dynamic rationality by ensuring that\ncollective beliefs update consistently with new information. We show that any\nconsensus-compatible and independent aggregation rule on a non-nested agenda is\nnecessarily linear. Furthermore, we provide sufficient conditions for a fair\nlearning process, where individuals initially agree on a specified subset of\npropositions known as the common ground, and new information is restricted to\nthis shared foundation. This guarantees that updating individual judgments via\nBayesian conditioning-whether performed before or after aggregation-yields the\nsame collective belief. A distinctive feature of our framework is its treatment\nof sequential decision-making, which allows new information to be incorporated\nprogressively through multiple stages while maintaining the established common\nground. We illustrate our findings with a running example in a political\nscenario concerning healthcare and immigration policies."}
{"id": "2504.14260", "pdf": "https://arxiv.org/pdf/2504.14260", "abs": "https://arxiv.org/abs/2504.14260", "authors": ["Liu Xiao", "Li Zhiyuan", "Lin Yueyu"], "title": "Cross-attention for State-based model RWKV-7", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "We introduce CrossWKV, a novel cross-attention mechanism for the state-based\nRWKV-7 model, designed to enhance the expressive power of text-to-image\ngeneration. Leveraging RWKV-7's linear-complexity Weighted Key-Value (WKV)\narchitecture, CrossWKV integrates text and image modalities in a single pass,\nutilizing a generalized delta rule with vector-valued gating and low-rank\nadaptations (LoRA) to achieve superior cross-modal alignment. Unlike\nTransformer-based models, CrossWKV's non-diagonal, input-dependent transition\nmatrix enables it to represent complex functions beyond the $\\mathrm{TC}^0$\ncomplexity class, including all regular languages, as demonstrated by its\nability to perform state-tracking tasks like $S_5$ permutation modeling.\nEvaluated within the Diffusion in RWKV-7 (DIR-7) on datasets such as LAION-5B\nand ImageNet, CrossWKV achieves a Frechet Inception Distance (FID) of 2.88 and\na CLIP score of 0.33 on ImageNet 256x256, matching state-of-the-art performance\nwhile offering robust generalization across diverse prompts. The model's\nenhanced expressivity, combined with constant memory usage and linear scaling,\npositions it as a powerful solution for advanced cross-modal tasks, with\npotential applications in high-resolution generation and dynamic state\nmanipulation.Code at https://github.com/TorchRWKV/flash-linear-attention"}
{"id": "2504.14620", "pdf": "https://arxiv.org/pdf/2504.14620", "abs": "https://arxiv.org/abs/2504.14620", "authors": ["Hongming Tan", "Shaoxiong Zhan", "Fengwei Jia", "Hai-Tao Zheng", "Wai Kin Chan"], "title": "A Hierarchical Framework for Measuring Scientific Paper Innovation via Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Measuring scientific paper innovation is both important and challenging.\nExisting content-based methods often overlook the full-paper context, fail to\ncapture the full scope of innovation, and lack generalization. We propose\nHSPIM, a hierarchical and training-free framework based on large language\nmodels (LLMs). It introduces a Paper-to-Sections-to-QAs decomposition to assess\ninnovation. We segment the text by section titles and use zero-shot LLM\nprompting to implement section classification, question-answering (QA)\naugmentation, and weighted novelty scoring. The generated QA pair focuses on\nsection-level innovation and serves as additional context to improve the LLM\nscoring. For each chunk, the LLM outputs a novelty score and a confidence\nscore. We use confidence scores as weights to aggregate novelty scores into a\npaper-level innovation score. To further improve performance, we propose a\ntwo-layer question structure consisting of common and section-specific\nquestions, and apply a genetic algorithm to optimize the question-prompt\ncombinations. Comprehensive experiments on scientific conference paper datasets\nshow that HSPIM outperforms baseline methods in effectiveness, generalization,\nand interpretability."}
{"id": "2504.14650", "pdf": "https://arxiv.org/pdf/2504.14650", "abs": "https://arxiv.org/abs/2504.14650", "authors": ["Yuting Huang", "Leilei Ding", "Zhipeng Tang", "Tianfu Wang", "Xinrui Lin", "Wuyang Zhang", "Mingxiao Ma", "Yanyong Zhang"], "title": "A Framework for Benchmarking and Aligning Task-Planning Safety in LLM-Based Embodied Agents", "categories": ["cs.AI"], "comment": "16 pages, 10 figures", "summary": "Large Language Models (LLMs) exhibit substantial promise in enhancing\ntask-planning capabilities within embodied agents due to their advanced\nreasoning and comprehension. However, the systemic safety of these agents\nremains an underexplored frontier. In this study, we present Safe-BeAl, an\nintegrated framework for the measurement (SafePlan-Bench) and alignment\n(Safe-Align) of LLM-based embodied agents' behaviors. SafePlan-Bench\nestablishes a comprehensive benchmark for evaluating task-planning safety,\nencompassing 2,027 daily tasks and corresponding environments distributed\nacross 8 distinct hazard categories (e.g., Fire Hazard). Our empirical analysis\nreveals that even in the absence of adversarial inputs or malicious intent,\nLLM-based agents can exhibit unsafe behaviors. To mitigate these hazards, we\npropose Safe-Align, a method designed to integrate physical-world safety\nknowledge into LLM-based embodied agents while maintaining task-specific\nperformance. Experiments across a variety of settings demonstrate that\nSafe-BeAl provides comprehensive safety validation, improving safety by 8.55 -\n15.22%, compared to embodied agents based on GPT-4, while ensuring successful\ntask completion."}
{"id": "2504.14267", "pdf": "https://arxiv.org/pdf/2504.14267", "abs": "https://arxiv.org/abs/2504.14267", "authors": ["Li Yu", "Xuanzhe Sun", "Wei Zhou", "Moncef Gabbouj"], "title": "Text-Audio-Visual-conditioned Diffusion Model for Video Saliency Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Video saliency prediction is crucial for downstream applications, such as\nvideo compression and human-computer interaction. With the flourishing of\nmultimodal learning, researchers started to explore multimodal video saliency\nprediction, including audio-visual and text-visual approaches. Auditory cues\nguide the gaze of viewers to sound sources, while textual cues provide semantic\nguidance for understanding video content. Integrating these complementary cues\ncan improve the accuracy of saliency prediction. Therefore, we attempt to\nsimultaneously analyze visual, auditory, and textual modalities in this paper,\nand propose TAVDiff, a Text-Audio-Visual-conditioned Diffusion Model for video\nsaliency prediction. TAVDiff treats video saliency prediction as an image\ngeneration task conditioned on textual, audio, and visual inputs, and predicts\nsaliency maps through stepwise denoising. To effectively utilize text, a large\nmultimodal model is used to generate textual descriptions for video frames and\nintroduce a saliency-oriented image-text response (SITR) mechanism to generate\nimage-text response maps. It is used as conditional information to guide the\nmodel to localize the visual regions that are semantically related to the\ntextual description. Regarding the auditory modality, it is used as another\nconditional information for directing the model to focus on salient regions\nindicated by sounds. At the same time, since the diffusion transformer (DiT)\ndirectly concatenates the conditional information with the timestep, which may\naffect the estimation of the noise level. To achieve effective conditional\nguidance, we propose Saliency-DiT, which decouples the conditional information\nfrom the timestep. Experimental results show that TAVDiff outperforms existing\nmethods, improving 1.03\\%, 2.35\\%, 2.71\\% and 0.33\\% on SIM, CC, NSS and AUC-J\nmetrics, respectively."}
{"id": "2504.14630", "pdf": "https://arxiv.org/pdf/2504.14630", "abs": "https://arxiv.org/abs/2504.14630", "authors": ["Rondik Hadi Abdulrahman", "Hossein Hassani"], "title": "Automatic Text Summarization (ATS) for Research Documents in Sorani Kurdish", "categories": ["cs.CL"], "comment": "18 pages, 11 figures, 8 tables", "summary": "Extracting concise information from scientific documents aids learners,\nresearchers, and practitioners. Automatic Text Summarization (ATS), a key\nNatural Language Processing (NLP) application, automates this process. While\nATS methods exist for many languages, Kurdish remains underdeveloped due to\nlimited resources. This study develops a dataset and language model based on\n231 scientific papers in Sorani Kurdish, collected from four academic\ndepartments in two universities in the Kurdistan Region of Iraq (KRI),\naveraging 26 pages per document. Using Sentence Weighting and Term\nFrequency-Inverse Document Frequency (TF-IDF) algorithms, two experiments were\nconducted, differing in whether the conclusions were included. The average word\ncount was 5,492.3 in the first experiment and 5,266.96 in the second. Results\nwere evaluated manually and automatically using ROUGE-1, ROUGE-2, and ROUGE-L\nmetrics, with the best accuracy reaching 19.58%. Six experts conducted manual\nevaluations using three criteria, with results varying by document. This\nresearch provides valuable resources for Kurdish NLP researchers to advance ATS\nand related fields."}
{"id": "2504.14706", "pdf": "https://arxiv.org/pdf/2504.14706", "abs": "https://arxiv.org/abs/2504.14706", "authors": ["Shin-nosuke Ishikawa", "Atsushi Yoshino"], "title": "AI with Emotions: Exploring Emotional Expressions in Large Language Models", "categories": ["cs.AI"], "comment": "14 pages, 8 figures, accepted to the Natural Language Processing for\n  Digital Humanities (NLP4DH) workshop at NAACL 2025", "summary": "The human-level performance of Large Language Models (LLMs) across various\ntasks has raised expectations for the potential of Artificial Intelligence (AI)\nto possess emotions someday. To explore the capability of current LLMs to\nexpress emotions in their outputs, we conducted an experiment using several\nLLMs (OpenAI GPT, Google Gemini, Meta Llama3, and Cohere Command R+) to\nrole-play as agents answering questions with specified emotional states. We\ndefined the emotional states using Russell's Circumplex model, a\nwell-established framework that characterizes emotions along the\nsleepy-activated (arousal) and pleasure-displeasure (valence) axes. We chose\nthis model for its simplicity, utilizing two continuous parameters, which\nallows for better controllability in applications involving continuous changes\nin emotional states. The responses generated were evaluated using a sentiment\nanalysis model, independent of the LLMs, trained on the GoEmotions dataset. The\nevaluation showed that the emotional states of the generated answers were\nconsistent with the specifications, demonstrating the LLMs' capability for\nemotional expression. This indicates the potential for LLM-based AI agents to\nsimulate emotions, opening up a wide range of applications for emotion-based\ninteractions, such as advisors or consultants who can provide advice or\nopinions with a personal touch."}
{"id": "2504.14278", "pdf": "https://arxiv.org/pdf/2504.14278", "abs": "https://arxiv.org/abs/2504.14278", "authors": ["Shang Zhang", "Yuke Hou", "Guoqiang Gong", "Ruoyan Xiong", "Yue Zhang"], "title": "RAMCT: Novel Region-adaptive Multi-channel Tracker with Iterative Tikhonov Regularization for Thermal Infrared Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Correlation filter (CF)-based trackers have gained significant attention for\ntheir computational efficiency in thermal infrared (TIR) target tracking.\nHowever, ex-isting methods struggle with challenges such as low-resolution\nimagery, occlu-sion, background clutter, and target deformation, which severely\nimpact tracking performance. To overcome these limitations, we propose RAMCT, a\nregion-adaptive sparse correlation filter tracker that integrates multi-channel\nfeature opti-mization with an adaptive regularization strategy. Firstly, we\nrefine the CF learn-ing process by introducing a spatially adaptive binary\nmask, which enforces spar-sity in the target region while dynamically\nsuppressing background interference. Secondly, we introduce generalized\nsingular value decomposition (GSVD) and propose a novel GSVD-based\nregion-adaptive iterative Tikhonov regularization method. This enables flexible\nand robust optimization across multiple feature channels, improving resilience\nto occlusion and background variations. Thirdly, we propose an online\noptimization strategy with dynamic discrepancy-based pa-rameter adjustment.\nThis mechanism facilitates real time adaptation to target and background\nvariations, thereby improving tracking accuracy and robustness. Ex-tensive\nexperiments on LSOTB-TIR, PTB-TIR, VOT-TIR2015, and VOT-TIR2017 benchmarks\ndemonstrate that RAMCT outperforms other state-of-the-art trackers in terms of\naccuracy and robustness."}
{"id": "2504.14633", "pdf": "https://arxiv.org/pdf/2504.14633", "abs": "https://arxiv.org/abs/2504.14633", "authors": ["Soo-joon Choi", "Ji-jun Park"], "title": "Harnessing Generative LLMs for Enhanced Financial Event Entity Extraction Performance", "categories": ["cs.CL"], "comment": null, "summary": "Financial event entity extraction is a crucial task for analyzing market\ndynamics and building financial knowledge graphs, yet it presents significant\nchallenges due to the specialized language and complex structures in financial\ntexts. Traditional approaches often rely on sequence labeling models, which can\nstruggle with long-range dependencies and the inherent complexity of extracting\nmultiple, potentially overlapping entities. Motivated by the advanced language\nunderstanding and generative capabilities of Large Language Models (LLMs), we\npropose a novel method that reframes financial event entity extraction as a\ntext-to-structured-output generation task. Our approach involves fine-tuning a\npre-trained LLM using Parameter-Efficient Fine-Tuning (PEFT) to directly\ngenerate a structured representation, such as a JSON object, containing the\nextracted entities and their precise character spans from the input text. We\nevaluate our method on the challenging CCKS 2019 Financial Event Entity\nExtraction dataset, comparing its performance against strong sequence labeling\nbaselines, including SEBERTNets and sebertNets. Experimental results\ndemonstrate that our generative LLM method achieves a new state-of-the-art F1\nscore on this benchmark, significantly outperforming previous methods. Through\ndetailed quantitative analysis across event types, entity types, and instance\ncomplexity, as well as human evaluation, we show that our approach is more\neffective at handling the nuances of financial text and extracting high-quality\nentities. This work validates the potential of applying generative LLMs\ndirectly to complex, domain-specific information extraction tasks requiring\nstructured output."}
{"id": "2504.14773", "pdf": "https://arxiv.org/pdf/2504.14773", "abs": "https://arxiv.org/abs/2504.14773", "authors": ["Haoming Li", "Zhaoliang Chen", "Jonathan Zhang", "Fei Liu"], "title": "PLANET: A Collection of Benchmarks for Evaluating LLMs' Planning Capabilities", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "comment": "10 pages", "summary": "Planning is central to agents and agentic AI. The ability to plan, e.g.,\ncreating travel itineraries within a budget, holds immense potential in both\nscientific and commercial contexts. Moreover, optimal plans tend to require\nfewer resources compared to ad-hoc methods. To date, a comprehensive\nunderstanding of existing planning benchmarks appears to be lacking. Without\nit, comparing planning algorithms' performance across domains or selecting\nsuitable algorithms for new scenarios remains challenging. In this paper, we\nexamine a range of planning benchmarks to identify commonly used testbeds for\nalgorithm development and highlight potential gaps. These benchmarks are\ncategorized into embodied environments, web navigation, scheduling, games and\npuzzles, and everyday task automation. Our study recommends the most\nappropriate benchmarks for various algorithms and offers insights to guide\nfuture benchmark development."}
{"id": "2504.14280", "pdf": "https://arxiv.org/pdf/2504.14280", "abs": "https://arxiv.org/abs/2504.14280", "authors": ["Jindong Li", "Yongguang Li", "Yali Fu", "Jiahong Liu", "Yixin Liu", "Menglin Yang", "Irwin King"], "title": "CLIP-Powered Domain Generalization and Domain Adaptation: A Comprehensive Survey", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "As machine learning evolves, domain generalization (DG) and domain adaptation\n(DA) have become crucial for enhancing model robustness across diverse\nenvironments. Contrastive Language-Image Pretraining (CLIP) plays a significant\nrole in these tasks, offering powerful zero-shot capabilities that allow models\nto perform effectively in unseen domains. However, there remains a significant\ngap in the literature, as no comprehensive survey currently exists that\nsystematically explores the applications of CLIP in DG and DA, highlighting the\nnecessity for this review. This survey presents a comprehensive review of\nCLIP's applications in DG and DA. In DG, we categorize methods into optimizing\nprompt learning for task alignment and leveraging CLIP as a backbone for\neffective feature extraction, both enhancing model adaptability. For DA, we\nexamine both source-available methods utilizing labeled source data and\nsource-free approaches primarily based on target domain data, emphasizing\nknowledge transfer mechanisms and strategies for improved performance across\ndiverse contexts. Key challenges, including overfitting, domain diversity, and\ncomputational efficiency, are addressed, alongside future research\nopportunities to advance robustness and efficiency in practical applications.\nBy synthesizing existing literature and pinpointing critical gaps, this survey\nprovides valuable insights for researchers and practitioners, proposing\ndirections for effectively leveraging CLIP to enhance methodologies in domain\ngeneralization and adaptation. Ultimately, this work aims to foster innovation\nand collaboration in the quest for more resilient machine learning models that\ncan perform reliably across diverse real-world scenarios. A more up-to-date\nversion of the papers is maintained at:\nhttps://github.com/jindongli-Ai/Survey_on_CLIP-Powered_Domain_Generalization_and_Adaptation."}
{"id": "2504.14657", "pdf": "https://arxiv.org/pdf/2504.14657", "abs": "https://arxiv.org/abs/2504.14657", "authors": ["Yihan Lin", "Zhirong Bella Yu", "Simon Lee"], "title": "A Case Study Exploring the Current Landscape of Synthetic Medical Record Generation with Commercial LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at the Conference of Health, Inference, Learning (CHIL 2025)\n  in Berkeley, CA. To appear in PMLR later in 2025", "summary": "Synthetic Electronic Health Records (EHRs) offer a valuable opportunity to\ncreate privacy preserving and harmonized structured data, supporting numerous\napplications in healthcare. Key benefits of synthetic data include precise\ncontrol over the data schema, improved fairness and representation of patient\npopulations, and the ability to share datasets without concerns about\ncompromising real individuals privacy. Consequently, the AI community has\nincreasingly turned to Large Language Models (LLMs) to generate synthetic data\nacross various domains. However, a significant challenge in healthcare is\nensuring that synthetic health records reliably generalize across different\nhospitals, a long standing issue in the field. In this work, we evaluate the\ncurrent state of commercial LLMs for generating synthetic data and investigate\nmultiple aspects of the generation process to identify areas where these models\nexcel and where they fall short. Our main finding from this work is that while\nLLMs can reliably generate synthetic health records for smaller subsets of\nfeatures, they struggle to preserve realistic distributions and correlations as\nthe dimensionality of the data increases, ultimately limiting their ability to\ngeneralize across diverse hospital settings."}
{"id": "2504.14810", "pdf": "https://arxiv.org/pdf/2504.14810", "abs": "https://arxiv.org/abs/2504.14810", "authors": ["Jucheng Hu", "Surong Yang", "Dongzhan Zhou", "Lijun Wu"], "title": "DONOD: Robust and Generalizable Instruction Fine-Tuning for LLMs via Model-Intrinsic Dataset Pruning", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Ad-hoc instruction fine-tuning of large language models (LLMs) is widely\nadopted for domain-specific adaptation. While domain-specific supervised\nfine-tuning (SFT) is effective and efficient, it often weakens cross-domain\ngeneralization and struggles with noisy training data. To address these\nchallenges, we propose DONOD, a lightweight model-intrinsic data pruning\nmethod. Our approach evaluates data using two model-parameter-based metrics:\nDelta of Norm (DON), which captures the cumulative influence on model weights,\nand Norm of Delta (NOD), which quantifies weight instability. Moreover, by\nemploying the Technique for Order of Preference by Similarity to Ideal Solution\n(TOPSIS) algorithm, we effectively filter noisy, unlearnable, and\ngeneralization-harming samples without relying on auxiliary models during the\nSFT process. Experiments on mathematical tasks demonstrate that data selected\nby DONOD achieve superior fine-tuning efficiency and improved robustness\nagainst noisy data. By filtering out 70% of the full dataset, we improve\ntarget-domain accuracy by 14.90% and cross-domain accuracy by 5.67%. Meanwhile,\nour selected data present superior cross-architecture generalization. Data\npruned by smaller models (e.g., Llama 3.1-8B) generalize effectively on larger\nmodels (e.g., Llama 2-13B). Compared to existing related methodologies, DONOD\ndemonstrates comparable or superior performance while remaining\ndataset-agnostic, enabling broader applicability."}
{"id": "2504.14289", "pdf": "https://arxiv.org/pdf/2504.14289", "abs": "https://arxiv.org/abs/2504.14289", "authors": ["Shang Zhang", "Yujie Cui", "Ruoyan Xiong", "Huanbin Zhang"], "title": "ISTD-YOLO: A Multi-Scale Lightweight High-Performance Infrared Small Target Detection Algorithm", "categories": ["cs.CV"], "comment": null, "summary": "Aiming at the detection difficulties of infrared images such as complex\nbackground, low signal-to-noise ratio, small target size and weak brightness, a\nlightweight infrared small target detection algorithm ISTD-YOLO based on\nimproved YOLOv7 was proposed. Firstly, the YOLOv7 network structure was\nlightweight reconstructed, and a three-scale lightweight network architecture\nwas designed. Then, the ELAN-W module of the model neck network is replaced by\nVoV-GSCSP to reduce the computational cost and the complexity of the network\nstructure. Secondly, a parameter-free attention mechanism was introduced into\nthe neck network to enhance the relevance of local con-text information.\nFinally, the Normalized Wasserstein Distance (NWD) was used to optimize the\ncommonly used IoU index to enhance the localization and detection accuracy of\nsmall targets. Experimental results show that compared with YOLOv7 and the\ncurrent mainstream algorithms, ISTD-YOLO can effectively improve the detection\neffect, and all indicators are effectively improved, which can achieve\nhigh-quality detection of infrared small targets."}
{"id": "2504.14669", "pdf": "https://arxiv.org/pdf/2504.14669", "abs": "https://arxiv.org/abs/2504.14669", "authors": ["Wei Zou", "Sen Yang", "Yu Bao", "Shujian Huang", "Jiajun Chen", "Shanbo Cheng"], "title": "Trans-Zero: Self-Play Incentivizes Large Language Models for Multilingual Translation Without Parallel Data", "categories": ["cs.CL"], "comment": "11 pages, 4 figures", "summary": "The rise of Large Language Models (LLMs) has reshaped machine translation\n(MT), but multilingual MT still relies heavily on parallel data for supervised\nfine-tuning (SFT), facing challenges like data scarcity for low-resource\nlanguages and catastrophic forgetting. To address these issues, we propose\nTRANS-ZERO, a self-play framework that leverages only monolingual data and the\nintrinsic multilingual knowledge of LLM. TRANS-ZERO combines Genetic\nMonte-Carlo Tree Search (G-MCTS) with preference optimization, achieving strong\ntranslation performance that rivals supervised methods. Experiments demonstrate\nthat this approach not only matches the performance of models trained on\nlarge-scale parallel data but also excels in non-English translation\ndirections. Further analysis reveals that G-MCTS itself significantly enhances\ntranslation quality by exploring semantically consistent candidates through\niterative translations, providing a robust foundation for the framework's\nsuccuss."}
{"id": "2504.14838", "pdf": "https://arxiv.org/pdf/2504.14838", "abs": "https://arxiv.org/abs/2504.14838", "authors": ["Yizhou Chen", "Yawen Liu", "Xuesi Wang", "Qingtao Yu", "Guangda Huzhang", "Anxiang Zeng", "Han Yu", "Zhiming Zhou"], "title": "Establishing Reliability Metrics for Reward Models in Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "The reward model (RM) that represents human preferences plays a crucial role\nin optimizing the outputs of large language models (LLMs), e.g., through\nreinforcement learning from human feedback (RLHF) or rejection sampling.\nHowever, a long challenge for RM is its uncertain reliability, i.e., LLM\noutputs with higher rewards may not align with actual human preferences.\nCurrently, there is a lack of a convincing metric to quantify the reliability\nof RMs. To bridge this gap, we propose the \\textit{\\underline{R}eliable at\n\\underline{$\\eta$}} (RETA) metric, which directly measures the reliability of\nan RM by evaluating the average quality (scored by an oracle) of the top $\\eta$\nquantile responses assessed by an RM. On top of RETA, we present an integrated\nbenchmarking pipeline that allows anyone to evaluate their own RM without\nincurring additional Oracle labeling costs. Extensive experimental studies\ndemonstrate the superior stability of RETA metric, providing solid evaluations\nof the reliability of various publicly available and proprietary RMs. When\ndealing with an unreliable RM, we can use the RETA metric to identify the\noptimal quantile from which to select the responses."}
{"id": "2504.14290", "pdf": "https://arxiv.org/pdf/2504.14290", "abs": "https://arxiv.org/abs/2504.14290", "authors": ["Shouwei Ruan", "Zhenyu Wu", "Yao Huang", "Ruochen Zhang", "Yitong Sun", "Caixin Kang", "Xingxing Wei"], "title": "Towards NSFW-Free Text-to-Image Generation via Safety-Constraint Direct Preference Optimization", "categories": ["cs.CV"], "comment": "10 pages, 6 figures", "summary": "Ensuring the safety of generated content remains a fundamental challenge for\nText-to-Image (T2I) generation. Existing studies either fail to guarantee\ncomplete safety under potentially harmful concepts or struggle to balance\nsafety with generation quality. To address these issues, we propose\nSafety-Constrained Direct Preference Optimization (SC-DPO), a novel framework\nfor safety alignment in T2I models. SC-DPO integrates safety constraints into\nthe general human preference calibration, aiming to maximize the likelihood of\ngenerating human-preferred samples while minimizing the safety cost of the\ngenerated outputs. In SC-DPO, we introduce a safety cost model to accurately\nquantify harmful levels for images, and train it effectively using the proposed\ncontrastive learning and cost anchoring objectives. To apply SC-DPO for\neffective T2I safety alignment, we constructed SCP-10K, a safety-constrained\npreference dataset containing rich harmful concepts, which blends\nsafety-constrained preference pairs under both harmful and clean instructions,\nfurther mitigating the trade-off between safety and sample quality.\nAdditionally, we propose a Dynamic Focusing Mechanism (DFM) for SC-DPO,\npromoting the model's learning of difficult preference pair samples. Extensive\nexperiments demonstrate that SC-DPO outperforms existing methods, effectively\ndefending against various NSFW content while maintaining optimal sample quality\nand human preference alignment. Additionally, SC-DPO exhibits resilience\nagainst adversarial prompts designed to generate harmful content."}
{"id": "2504.14690", "pdf": "https://arxiv.org/pdf/2504.14690", "abs": "https://arxiv.org/abs/2504.14690", "authors": ["Mehrnoush Shamsfard", "Zahra Saaberi", "Mostafa Karimi manesh", "Seyed Mohammad Hossein Hashemi", "Zahra Vatankhah", "Motahareh Ramezani", "Niki Pourazin", "Tara Zare", "Maryam Azimi", "Sarina Chitsaz", "Sama Khoraminejad", "Morteza Mahdavi Mortazavi", "Mohammad Mahdi Chizari", "Sahar Maleki", "Seyed Soroush Majd", "Mostafa Masumi", "Sayed Ali Musavi Khoeini", "Amir Mohseni", "Sogol Alipour"], "title": "FarsEval-PKBETS: A new diverse benchmark for evaluating Persian large language models", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7; E.0"], "comment": "24 pages, 3 figures, 3 tables", "summary": "Research on evaluating and analyzing large language models (LLMs) has been\nextensive for resource-rich languages such as English, yet their performance in\nlanguages such as Persian has received considerably less attention. This paper\nintroduces FarsEval-PKBETS benchmark, a subset of FarsEval project for\nevaluating large language models in Persian. This benchmark consists of 4000\nquestions and answers in various formats, including multiple choice, short\nanswer and descriptive responses. It covers a wide range of domains and\ntasks,including medicine, law, religion, Persian language, encyclopedic\nknowledge, human preferences, social knowledge, ethics and bias, text\ngeneration, and respecting others' rights. This bechmark incorporates\nlinguistics, cultural, and local considerations relevant to the Persian\nlanguage and Iran. To ensure the questions are challenging for current LLMs,\nthree models -- Llama3-70B, PersianMind, and Dorna -- were evaluated using this\nbenchmark. Their average accuracy was below 50%, meaning they provided fully\ncorrect answers to fewer than half of the questions. These results indicate\nthat current language models are still far from being able to solve this\nbenchmark"}
{"id": "2504.14858", "pdf": "https://arxiv.org/pdf/2504.14858", "abs": "https://arxiv.org/abs/2504.14858", "authors": ["Jiaqi Wei", "Hao Zhou", "Xiang Zhang", "Di Zhang", "Zijie Qiu", "Wei Wei", "Jinzhe Li", "Wanli Ouyang", "Siqi Sun"], "title": "AlignRAG: An Adaptable Framework for Resolving Misalignments in Retrieval-Aware Reasoning of RAG", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Retrieval-augmented generation (RAG) has emerged as a foundational paradigm\nfor knowledge-grounded text generation. However, existing RAG pipelines often\nfail to ensure that the reasoning trajectories align with the evidential\nconstraints imposed by retrieved content. In this paper, we reframe RAG as a\nproblem of retrieval-aware reasoning and identify a core challenge: reasoning\nmisalignment-the mismatch between a model's reasoning trajectory and the\nretrieved evidence. To address this challenge, we propose AlignRAG, a novel\ntest-time framework that mitigates reasoning misalignment through iterative\nCritique-Driven Alignment (CDA) steps. In contrast to prior approaches that\nrely on static training or post-hoc selection, AlignRAG actively refines\nreasoning trajectories during inference by enforcing fine-grained alignment\nwith evidence. Our framework introduces a new paradigm for retrieval-aware\nreasoning by: (1) constructing context-rich training corpora; (2) generating\ncontrastive critiques from preference-aware reasoning trajectories; (3)\ntraining a dedicated \\textit{Critic Language Model (CLM)} to identify reasoning\nmisalignments; and (4) applying CDA steps to optimize reasoning trajectories\niteratively. Empirical results demonstrate that AlignRAG consistently\noutperforms all baselines and could integrate as a plug-and-play module into\nexisting RAG pipelines without further changes. By reconceptualizing RAG as a\nstructured reasoning trajectory and establishing the test-time framework for\ncorrecting reasoning misalignments in RAG, AlignRAG provides practical\nadvancements for retrieval-aware generation."}
{"id": "2504.14294", "pdf": "https://arxiv.org/pdf/2504.14294", "abs": "https://arxiv.org/abs/2504.14294", "authors": ["Pourya Shamsolmoali", "Masoumeh Zareapoor", "Huiyu Zhou", "Michael Felsberg", "Dacheng Tao", "Xuelong Li"], "title": "From Missing Pieces to Masterpieces: Image Completion with Context-Adaptive Diffusion", "categories": ["cs.CV"], "comment": "Accepted in TPAMI", "summary": "Image completion is a challenging task, particularly when ensuring that\ngenerated content seamlessly integrates with existing parts of an image. While\nrecent diffusion models have shown promise, they often struggle with\nmaintaining coherence between known and unknown (missing) regions. This issue\narises from the lack of explicit spatial and semantic alignment during the\ndiffusion process, resulting in content that does not smoothly integrate with\nthe original image. Additionally, diffusion models typically rely on global\nlearned distributions rather than localized features, leading to\ninconsistencies between the generated and existing image parts. In this work,\nwe propose ConFill, a novel framework that introduces a Context-Adaptive\nDiscrepancy (CAD) model to ensure that intermediate distributions of known and\nunknown regions are closely aligned throughout the diffusion process. By\nincorporating CAD, our model progressively reduces discrepancies between\ngenerated and original images at each diffusion step, leading to contextually\naligned completion. Moreover, ConFill uses a new Dynamic Sampling mechanism\nthat adaptively increases the sampling rate in regions with high reconstruction\ncomplexity. This approach enables precise adjustments, enhancing detail and\nintegration in restored areas. Extensive experiments demonstrate that ConFill\noutperforms current methods, setting a new benchmark in image completion."}
{"id": "2504.14692", "pdf": "https://arxiv.org/pdf/2504.14692", "abs": "https://arxiv.org/abs/2504.14692", "authors": ["Songtao Jiang", "Yuan Wang", "Sibo Song", "Yan Zhang", "Zijie Meng", "Bohan Lei", "Jian Wu", "Jimeng Sun", "Zuozhu Liu"], "title": "OmniV-Med: Scaling Medical Vision-Language Model for Universal Visual Understanding", "categories": ["cs.CL"], "comment": null, "summary": "The practical deployment of medical vision-language models (Med-VLMs)\nnecessitates seamless integration of textual data with diverse visual\nmodalities, including 2D/3D images and videos, yet existing models typically\nemploy separate encoders for different modalities. To address this limitation,\nwe present OmniV-Med, a unified framework for multimodal medical understanding.\nOur technical contributions are threefold: First, we construct\nOmniV-Med-Instruct, a comprehensive multimodal medical dataset containing 252K\ninstructional samples spanning 14 medical image modalities and 11 clinical\ntasks. Second, we devise a rotary position-adaptive encoder that processes\nmulti-resolution 2D/3D images and videos within a unified architecture,\ndiverging from conventional modality-specific encoders. Third, we introduce a\nmedical-aware token pruning mechanism that exploits spatial-temporal redundancy\nin volumetric data (e.g., consecutive CT slices) and medical videos,\neffectively reducing 60\\% of visual tokens without performance degradation.\nEmpirical evaluations demonstrate that OmniV-Med-7B achieves state-of-the-art\nperformance on 7 benchmarks spanning 2D/3D medical imaging and video\nunderstanding tasks. Notably, our lightweight variant (OmniV-Med-1.5B) attains\ncomparable performance while requiring only 8 RTX3090 GPUs for training and\nsupporting efficient long-video inference. Data, code and model will be\nreleased."}
{"id": "2504.14870", "pdf": "https://arxiv.org/pdf/2504.14870", "abs": "https://arxiv.org/abs/2504.14870", "authors": ["Hongru Wang", "Cheng Qian", "Wanjun Zhong", "Xiusi Chen", "Jiahao Qiu", "Shijue Huang", "Bowen Jin", "Mengdi Wang", "Kam-Fai Wong", "Heng Ji"], "title": "OTC: Optimal Tool Calls via Reinforcement Learning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Tool-integrated reasoning (TIR) augments large language models (LLMs) with\nthe ability to invoke external tools, such as search engines and code\ninterpreters, to solve tasks beyond the capabilities of language-only\nreasoning. While reinforcement learning (RL) has shown promise in improving TIR\nby optimizing final answer correctness, existing approaches often overlook the\nefficiency and cost associated with tool usage. This can lead to suboptimal\nbehavior, including excessive tool calls that increase computational and\nfinancial overhead, or insufficient tool use that compromises answer quality.\nIn this work, we propose Optimal Tool Call-controlled Policy Optimization\n(OTC-PO), a simple yet effective RL-based framework that encourages models to\nproduce accurate answers with minimal tool calls. Our method introduces a\ntool-integrated reward that jointly considers correctness and tool efficiency,\npromoting high tool productivity. We instantiate this framework within both\nProximal Policy Optimization (PPO) and Group Relative Preference Optimization\n(GRPO), resulting in OTC-PPO and OTC-GRPO. Experiments with Qwen-2.5 and\nQwen-Math across multiple QA benchmarks show that our approach reduces tool\ncalls by up to 73.1\\% and improves tool productivity by up to 229.4\\%, while\nmaintaining comparable answer accuracy. To the best of our knowledge, this is\nthe first RL-based framework that explicitly optimizes tool-use efficiency in\nTIR."}
{"id": "2504.14301", "pdf": "https://arxiv.org/pdf/2504.14301", "abs": "https://arxiv.org/abs/2504.14301", "authors": ["Nazia Aslam", "Kamal Nasrollahi"], "title": "Balancing Privacy and Action Performance: A Penalty-Driven Approach to Image Anonymization", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "Accepted to CVPRW 2025", "summary": "The rapid development of video surveillance systems for object detection,\ntracking, activity recognition, and anomaly detection has revolutionized our\nday-to-day lives while setting alarms for privacy concerns. It isn't easy to\nstrike a balance between visual privacy and action recognition performance in\nmost computer vision models. Is it possible to safeguard privacy without\nsacrificing performance? It poses a formidable challenge, as even minor privacy\nenhancements can lead to substantial performance degradation. To address this\nchallenge, we propose a privacy-preserving image anonymization technique that\noptimizes the anonymizer using penalties from the utility branch, ensuring\nimproved action recognition performance while minimally affecting privacy\nleakage. This approach addresses the trade-off between minimizing privacy\nleakage and maintaining high action performance. The proposed approach is\nprimarily designed to align with the regulatory standards of the EU AI Act and\nGDPR, ensuring the protection of personally identifiable information while\nmaintaining action performance. To the best of our knowledge, we are the first\nto introduce a feature-based penalty scheme that exclusively controls the\naction features, allowing freedom to anonymize private attributes. Extensive\nexperiments were conducted to validate the effectiveness of the proposed\nmethod. The results demonstrate that applying a penalty to anonymizer from\nutility branch enhances action performance while maintaining nearly consistent\nprivacy leakage across different penalty settings."}
{"id": "2504.14707", "pdf": "https://arxiv.org/pdf/2504.14707", "abs": "https://arxiv.org/abs/2504.14707", "authors": ["Ratna Kandala", "Katie Hoemann"], "title": "Evaluating BERTopic on Open-Ended Data: A Case Study with Belgian Dutch Daily Narratives", "categories": ["cs.CL"], "comment": null, "summary": "This study explores BERTopic's potential for modeling open-ended Belgian\nDutch daily narratives, contrasting its performance with Latent Dirichlet\nAllocation (LDA) and KMeans. Although LDA scores well on certain automated\nmetrics, human evaluations reveal semantically irrelevant co-occurrences,\nhighlighting the limitations of purely statistic-based methods. In contrast,\nBERTopic's reliance on contextual embeddings yields culturally resonant themes,\nunderscoring the importance of hybrid evaluation frameworks that account for\nmorphologically rich languages. KMeans performed less coherently than prior\nresearch suggested, pointing to the unique challenges posed by personal\nnarratives. Our findings emphasize the need for robust generalization in NLP\nmodels, especially in underrepresented linguistic contexts."}
{"id": "2504.14928", "pdf": "https://arxiv.org/pdf/2504.14928", "abs": "https://arxiv.org/abs/2504.14928", "authors": ["Yao Shi", "Rongkeng Liang", "Yong Xu"], "title": "EducationQ: Evaluating LLMs' Teaching Capabilities Through Multi-Agent Dialogue Framework", "categories": ["cs.AI", "cs.CE", "cs.CL", "cs.CY", "cs.HC"], "comment": null, "summary": "Large language models (LLMs) increasingly serve as educational tools, yet\nevaluating their teaching capabilities remains challenging due to the\nresource-intensive, context-dependent, and methodologically complex nature of\nteacher-student interactions. We introduce EducationQ, a multi-agent dialogue\nframework that efficiently assesses teaching capabilities through simulated\ndynamic educational scenarios, featuring specialized agents for teaching,\nlearning, and evaluation. Testing 14 LLMs across major AI Organizations\n(OpenAI, Meta, Google, Anthropic, and others) on 1,498 questions spanning 13\ndisciplines and 10 difficulty levels reveals that teaching effectiveness does\nnot correlate linearly with model scale or general reasoning capabilities -\nwith some smaller open-source models outperforming larger commercial\ncounterparts in teaching contexts. This finding highlights a critical gap in\ncurrent evaluations that prioritize knowledge recall over interactive pedagogy.\nOur mixed-methods evaluation, combining quantitative metrics with qualitative\nanalysis and expert case studies, identifies distinct pedagogical strengths\nemployed by top-performing models (e.g., sophisticated questioning strategies,\nadaptive feedback mechanisms). Human expert evaluations show 78% agreement with\nour automated qualitative analysis of effective teaching behaviors, validating\nour methodology. EducationQ demonstrates that LLMs-as-teachers require\nspecialized optimization beyond simple scaling, suggesting next-generation\neducational AI prioritize targeted enhancement of specific pedagogical\neffectiveness."}
{"id": "2504.14306", "pdf": "https://arxiv.org/pdf/2504.14306", "abs": "https://arxiv.org/abs/2504.14306", "authors": ["Yitao Zhao", "Sen Lei", "Nanqing Liu", "Heng-Chao Li", "Turgay Celik", "Qing Zhu"], "title": "Exploring Generalizable Pre-training for Real-world Change Detection via Geometric Estimation", "categories": ["cs.CV"], "comment": "Submitted to IEEE TGRS", "summary": "As an essential procedure in earth observation system, change detection (CD)\naims to reveal the spatial-temporal evolution of the observation regions. A key\nprerequisite for existing change detection algorithms is aligned geo-references\nbetween multi-temporal images by fine-grained registration. However, in the\nmajority of real-world scenarios, a prior manual registration is required\nbetween the original images, which significantly increases the complexity of\nthe CD workflow. In this paper, we proposed a self-supervision motivated CD\nframework with geometric estimation, called \"MatchCD\". Specifically, the\nproposed MatchCD framework utilizes the zero-shot capability to optimize the\nencoder with self-supervised contrastive representation, which is reused in the\ndownstream image registration and change detection to simultaneously handle the\nbi-temporal unalignment and object change issues. Moreover, unlike the\nconventional change detection requiring segmenting the full-frame image into\nsmall patches, our MatchCD framework can directly process the original\nlarge-scale image (e.g., 6K*4K resolutions) with promising performance. The\nperformance in multiple complex scenarios with significant geometric distortion\ndemonstrates the effectiveness of our proposed framework."}
{"id": "2504.14738", "pdf": "https://arxiv.org/pdf/2504.14738", "abs": "https://arxiv.org/abs/2504.14738", "authors": ["Reya Vir", "Shreya Shankar", "Harrison Chase", "Will Fu-Hinthorn", "Aditya Parameswaran"], "title": "PROMPTEVALS: A Dataset of Assertions and Guardrails for Custom Production Large Language Model Pipelines", "categories": ["cs.CL"], "comment": "Accepted to NAACL 2025 Main Conference", "summary": "Large language models (LLMs) are increasingly deployed in specialized\nproduction data processing pipelines across diverse domains -- such as finance,\nmarketing, and e-commerce. However, when running them in production across many\ninputs, they often fail to follow instructions or meet developer expectations.\nTo improve reliability in these applications, creating assertions or guardrails\nfor LLM outputs to run alongside the pipelines is essential. Yet, determining\nthe right set of assertions that capture developer requirements for a task is\nchallenging. In this paper, we introduce PROMPTEVALS, a dataset of 2087 LLM\npipeline prompts with 12623 corresponding assertion criteria, sourced from\ndevelopers using our open-source LLM pipeline tools. This dataset is 5x larger\nthan previous collections. Using a hold-out test split of PROMPTEVALS as a\nbenchmark, we evaluated closed- and open-source models in generating relevant\nassertions. Notably, our fine-tuned Mistral and Llama 3 models outperform\nGPT-4o by 20.93% on average, offering both reduced latency and improved\nperformance. We believe our dataset can spur further research in LLM\nreliability, alignment, and prompt engineering."}
{"id": "2504.14947", "pdf": "https://arxiv.org/pdf/2504.14947", "abs": "https://arxiv.org/abs/2504.14947", "authors": ["Xiaojun Yuan", "Haoming Ma", "Yinuo Huang", "Zhoufan Hua", "Yong Zuo", "Zhi Ding"], "title": "Generative Semantic Communications: Principles and Practices", "categories": ["cs.AI", "eess.IV", "eess.SP"], "comment": null, "summary": "Semantic communication leverages artificial intelligence (AI) technologies to\nextract semantic information from data for efficient transmission, theraby\nsignificantly reducing communication cost. With the evolution towards\nartificial general intelligence (AGI), the increasing demands for AGI services\npose new challenges to semantic communication. In response, we propose a new\nparadigm for AGI-driven communications, called generative semantic\ncommunication (GSC), which utilizes advanced AI technologies such as foundation\nmodels and generative models. We first describe the basic concept of GSC and\nits difference from existing semantic communications, and then introduce a\ngeneral framework of GSC, followed by two case studies to verify the advantages\nof GSC in AGI-driven applications. Finally, open challenges and new research\ndirections are discussed to stimulate this line of research and pave the way\nfor practical applications."}
{"id": "2504.14309", "pdf": "https://arxiv.org/pdf/2504.14309", "abs": "https://arxiv.org/abs/2504.14309", "authors": ["Ruoyan Xiong", "Huanbin Zhang", "Shentao Wang", "Hui He", "Yuke Hou", "Yue Zhang", "Yujie Cui", "Huipan Guan", "Shang Zhang"], "title": "FGSGT: Saliency-Guided Siamese Network Tracker Based on Key Fine-Grained Feature Information for Thermal Infrared Target Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Thermal infrared (TIR) images typically lack detailed features and have low\ncontrast, making it challenging for conventional feature extraction models to\ncapture discriminative target characteristics. As a result, trackers are often\naffected by interference from visually similar objects and are susceptible to\ntracking drift. To address these challenges, we propose a novel saliency-guided\nSiamese network tracker based on key fine-grained feature infor-mation. First,\nwe introduce a fine-grained feature parallel learning convolu-tional block with\na dual-stream architecture and convolutional kernels of varying sizes. This\ndesign captures essential global features from shallow layers, enhances feature\ndiversity, and minimizes the loss of fine-grained in-formation typically\nencountered in residual connections. In addition, we propose a multi-layer\nfine-grained feature fusion module that uses bilinear matrix multiplication to\neffectively integrate features across both deep and shallow layers. Next, we\nintroduce a Siamese residual refinement block that corrects saliency map\nprediction errors using residual learning. Combined with deep supervision, this\nmechanism progressively refines predictions, ap-plying supervision at each\nrecursive step to ensure consistent improvements in accuracy. Finally, we\npresent a saliency loss function to constrain the sali-ency predictions,\ndirecting the network to focus on highly discriminative fi-ne-grained features.\nExtensive experiment results demonstrate that the pro-posed tracker achieves\nthe highest precision and success rates on the PTB-TIR and LSOTB-TIR\nbenchmarks. It also achieves a top accuracy of 0.78 on the VOT-TIR 2015\nbenchmark and 0.75 on the VOT-TIR 2017 benchmark."}
{"id": "2504.14766", "pdf": "https://arxiv.org/pdf/2504.14766", "abs": "https://arxiv.org/abs/2504.14766", "authors": ["Saniya Karwa", "Navpreet Singh"], "title": "Disentangling Linguistic Features with Dimension-Wise Analysis of Vector Embeddings", "categories": ["cs.CL"], "comment": null, "summary": "Understanding the inner workings of neural embeddings, particularly in models\nsuch as BERT, remains a challenge because of their high-dimensional and opaque\nnature. This paper proposes a framework for uncovering the specific dimensions\nof vector embeddings that encode distinct linguistic properties (LPs). We\nintroduce the Linguistically Distinct Sentence Pairs (LDSP-10) dataset, which\nisolates ten key linguistic features such as synonymy, negation, tense, and\nquantity. Using this dataset, we analyze BERT embeddings with various methods,\nincluding the Wilcoxon signed-rank test, mutual information, and recursive\nfeature elimination, to identify the most influential dimensions for each LP.\nWe introduce a new metric, the Embedding Dimension Impact (EDI) score, which\nquantifies the relevance of each embedding dimension to a LP. Our findings show\nthat certain properties, such as negation and polarity, are robustly encoded in\nspecific dimensions, while others, like synonymy, exhibit more complex\npatterns. This study provides insights into the interpretability of embeddings,\nwhich can guide the development of more transparent and optimized language\nmodels, with implications for model bias mitigation and the responsible\ndeployment of AI systems."}
{"id": "2504.14964", "pdf": "https://arxiv.org/pdf/2504.14964", "abs": "https://arxiv.org/abs/2504.14964", "authors": ["Emir Catir", "Robin Claesson", "Rodothea Myrsini Tsoupidi"], "title": "Evaluating Code Generation of LLMs in Advanced Computer Science Problems", "categories": ["cs.AI", "cs.CY"], "comment": null, "summary": "Large Language Models (LLMs), such as GitHub Copilot and ChatGPT have become\npopular among programming students. Students use LLMs to assist them in\nprogramming courses, including generating source code. Previous work has\nevaluated the ability of LLMs in solving introductory-course programming\nassignments. The results have shown that LLMs are highly effective in\ngenerating code for introductory Computer Science (CS) courses. However, there\nis a gap in research on evaluating LLMs' ability to generate code that solves\nadvanced programming assignments. In this work, we evaluate the ability of four\nLLM tools to solve programming assignments from advanced CS courses in three\npopular programming languages, Java, Python, and C. We manually select 12\nproblems, three problems from introductory courses as the baseline and nine\nprogramming assignments from second- and third-year CS courses. To evaluate the\nLLM-generated code, we generate a test suite of 1000 test cases per problem and\nanalyze the program output. Our evaluation shows that although LLMs are highly\neffective in generating source code for introductory programming courses,\nsolving advanced programming assignments is more challenging. Nonetheless, in\nmany cases, LLMs identify the base problem and provide partial solutions that\nmay be useful to CS students. Furthermore, our results may provide useful\nguidance for teachers of advanced programming courses on how to design\nprogramming assignments."}
{"id": "2504.14311", "pdf": "https://arxiv.org/pdf/2504.14311", "abs": "https://arxiv.org/abs/2504.14311", "authors": ["Ruoyan Xiong", "Yuke Hou", "Princess Retor Torboh", "Hui He", "Huanbin Zhang", "Yue Zhang", "Yanpin Wang", "Huipan Guan", "Shang Zhang"], "title": "DCFG: Diverse Cross-Channel Fine-Grained Feature Learning and Progressive Fusion Siamese Tracker for Thermal Infrared Target Tracking", "categories": ["cs.CV"], "comment": null, "summary": "To address the challenge of capturing highly discriminative features in\nther-mal infrared (TIR) tracking, we propose a novel Siamese tracker based on\ncross-channel fine-grained feature learning and progressive fusion. First, we\nintroduce a cross-channel fine-grained feature learning network that employs\nmasks and suppression coefficients to suppress dominant target features,\nen-abling the tracker to capture more detailed and subtle information. The\nnet-work employs a channel rearrangement mechanism to enhance efficient\nin-formation flow, coupled with channel equalization to reduce parameter count.\nAdditionally, we incorporate layer-by-layer combination units for ef-fective\nfeature extraction and fusion, thereby minimizing parameter redun-dancy and\ncomputational complexity. The network further employs feature redirection and\nchannel shuffling strategies to better integrate fine-grained details. Second,\nwe propose a specialized cross-channel fine-grained loss function designed to\nguide feature groups toward distinct discriminative re-gions of the target,\nthus improving overall target representation. This loss function includes an\ninter-channel loss term that promotes orthogonality be-tween channels,\nmaximizing feature diversity and facilitating finer detail capture. Extensive\nexperiments demonstrate that our proposed tracker achieves the highest\naccuracy, scoring 0.81 on the VOT-TIR 2015 and 0.78 on the VOT-TIR 2017\nbenchmark, while also outperforming other methods across all evaluation metrics\non the LSOTB-TIR and PTB-TIR benchmarks."}
{"id": "2504.14772", "pdf": "https://arxiv.org/pdf/2504.14772", "abs": "https://arxiv.org/abs/2504.14772", "authors": ["Luyang Fang", "Xiaowei Yu", "Jiazhang Cai", "Yongkai Chen", "Shushan Wu", "Zhengliang Liu", "Zhenyuan Yang", "Haoran Lu", "Xilin Gong", "Yufang Liu", "Terry Ma", "Wei Ruan", "Ali Abbasi", "Jing Zhang", "Tao Wang", "Ehsan Latif", "Wei Liu", "Wei Zhang", "Soheil Kolouri", "Xiaoming Zhai", "Dajiang Zhu", "Wenxuan Zhong", "Tianming Liu", "Ping Ma"], "title": "Knowledge Distillation and Dataset Distillation of Large Language Models: Emerging Trends, Challenges, and Future Directions", "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": null, "summary": "The exponential growth of Large Language Models (LLMs) continues to highlight\nthe need for efficient strategies to meet ever-expanding computational and data\ndemands. This survey provides a comprehensive analysis of two complementary\nparadigms: Knowledge Distillation (KD) and Dataset Distillation (DD), both\naimed at compressing LLMs while preserving their advanced reasoning\ncapabilities and linguistic diversity. We first examine key methodologies in\nKD, such as task-specific alignment, rationale-based training, and\nmulti-teacher frameworks, alongside DD techniques that synthesize compact,\nhigh-impact datasets through optimization-based gradient matching, latent space\nregularization, and generative synthesis. Building on these foundations, we\nexplore how integrating KD and DD can produce more effective and scalable\ncompression strategies. Together, these approaches address persistent\nchallenges in model scalability, architectural heterogeneity, and the\npreservation of emergent LLM abilities. We further highlight applications\nacross domains such as healthcare and education, where distillation enables\nefficient deployment without sacrificing performance. Despite substantial\nprogress, open challenges remain in preserving emergent reasoning and\nlinguistic diversity, enabling efficient adaptation to continually evolving\nteacher models and datasets, and establishing comprehensive evaluation\nprotocols. By synthesizing methodological innovations, theoretical foundations,\nand practical insights, our survey charts a path toward sustainable,\nresource-efficient LLMs through the tighter integration of KD and DD\nprinciples."}
{"id": "2504.15046", "pdf": "https://arxiv.org/pdf/2504.15046", "abs": "https://arxiv.org/abs/2504.15046", "authors": ["Shilin Zhang", "Zican Hu", "Wenhao Wu", "Xinyi Xie", "Jianxiang Tang", "Chunlin Chen", "Daoyi Dong", "Yu Cheng", "Zhenhong Sun", "Zhi Wang"], "title": "Text-to-Decision Agent: Learning Generalist Policies from Natural Language Supervision", "categories": ["cs.AI"], "comment": "18 pages, 8 figures", "summary": "RL systems usually tackle generalization by inferring task beliefs from\nhigh-quality samples or warmup explorations. The restricted form limits their\ngenerality and usability since these supervision signals are expensive and even\ninfeasible to acquire in advance for unseen tasks. Learning directly from the\nraw text about decision tasks is a promising alternative to leverage a much\nbroader source of supervision. In the paper, we propose Text-to-Decision Agent\n(T2DA), a simple and scalable framework that supervises generalist policy\nlearning with natural language. We first introduce a generalized world model to\nencode multi-task decision data into a dynamics-aware embedding space. Then,\ninspired by CLIP, we predict which textual description goes with which decision\nembedding, effectively bridging their semantic gap via contrastive\nlanguage-decision pre-training and aligning the text embeddings to comprehend\nthe environment dynamics. After training the text-conditioned generalist\npolicy, the agent can directly realize zero-shot text-to-decision generation in\nresponse to language instructions. Comprehensive experiments on MuJoCo and\nMeta-World benchmarks show that T2DA facilitates high-capacity zero-shot\ngeneralization and outperforms various types of baselines."}
{"id": "2504.14335", "pdf": "https://arxiv.org/pdf/2504.14335", "abs": "https://arxiv.org/abs/2504.14335", "authors": ["Zhengbo Zhang", "Yuxi Zhou", "Duo Peng", "Joo-Hwee Lim", "Zhigang Tu", "De Wen Soh", "Lin Geng Foo"], "title": "Visual Prompting for One-shot Controllable Video Editing without Inversion", "categories": ["cs.CV", "cs.AI"], "comment": "accepted by cvpr2025", "summary": "One-shot controllable video editing (OCVE) is an important yet challenging\ntask, aiming to propagate user edits that are made -- using any image editing\ntool -- on the first frame of a video to all subsequent frames, while ensuring\ncontent consistency between edited frames and source frames. To achieve this,\nprior methods employ DDIM inversion to transform source frames into latent\nnoise, which is then fed into a pre-trained diffusion model, conditioned on the\nuser-edited first frame, to generate the edited video. However, the DDIM\ninversion process accumulates errors, which hinder the latent noise from\naccurately reconstructing the source frames, ultimately compromising content\nconsistency in the generated edited frames. To overcome it, our method\neliminates the need for DDIM inversion by performing OCVE through a novel\nperspective based on visual prompting. Furthermore, inspired by consistency\nmodels that can perform multi-step consistency sampling to generate a sequence\nof content-consistent images, we propose a content consistency sampling (CCS)\nto ensure content consistency between the generated edited frames and the\nsource frames. Moreover, we introduce a temporal-content consistency sampling\n(TCS) based on Stein Variational Gradient Descent to ensure temporal\nconsistency across the edited frames. Extensive experiments validate the\neffectiveness of our approach."}
{"id": "2504.14804", "pdf": "https://arxiv.org/pdf/2504.14804", "abs": "https://arxiv.org/abs/2504.14804", "authors": ["Jiaxin GUO", "Xiaoyu Chen", "Zhiqiang Rao", "Jinlong Yang", "Zongyao Li", "Hengchao Shang", "Daimeng Wei", "Hao Yang"], "title": "Automatic Evaluation Metrics for Document-level Translation: Overview, Challenges and Trends", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "With the rapid development of deep learning technologies, the field of\nmachine translation has witnessed significant progress, especially with the\nadvent of large language models (LLMs) that have greatly propelled the\nadvancement of document-level translation. However, accurately evaluating the\nquality of document-level translation remains an urgent issue. This paper first\nintroduces the development status of document-level translation and the\nimportance of evaluation, highlighting the crucial role of automatic evaluation\nmetrics in reflecting translation quality and guiding the improvement of\ntranslation systems. It then provides a detailed analysis of the current state\nof automatic evaluation schemes and metrics, including evaluation methods with\nand without reference texts, as well as traditional metrics, Model-based\nmetrics and LLM-based metrics. Subsequently, the paper explores the challenges\nfaced by current evaluation methods, such as the lack of reference diversity,\ndependence on sentence-level alignment information, and the bias, inaccuracy,\nand lack of interpretability of the LLM-as-a-judge method. Finally, the paper\nlooks ahead to the future trends in evaluation methods, including the\ndevelopment of more user-friendly document-level evaluation methods and more\nrobust LLM-as-a-judge methods, and proposes possible research directions, such\nas reducing the dependency on sentence-level information, introducing\nmulti-level and multi-granular evaluation approaches, and training models\nspecifically for machine translation evaluation. This study aims to provide a\ncomprehensive analysis of automatic evaluation for document-level translation\nand offer insights into future developments."}
{"id": "2504.15075", "pdf": "https://arxiv.org/pdf/2504.15075", "abs": "https://arxiv.org/abs/2504.15075", "authors": ["Van Thuy Hoang", "Hyeon-Ju Jeon", "O-Joun Lee"], "title": "Mitigating Degree Bias in Graph Representation Learning with Learnable Structural Augmentation and Structural Self-Attention", "categories": ["cs.AI", "cs.LG"], "comment": "Accepted at IEEE TNSE", "summary": "Graph Neural Networks (GNNs) update node representations through message\npassing, which is primarily based on the homophily principle, assuming that\nadjacent nodes share similar features. However, in real-world graphs with\nlong-tailed degree distributions, high-degree nodes dominate message passing,\ncausing a degree bias where low-degree nodes remain under-represented due to\ninadequate messages. The main challenge in addressing degree bias is how to\ndiscover non-adjacent nodes to provide additional messages to low-degree nodes\nwhile reducing excessive messages for high-degree nodes. Nevertheless,\nexploiting non-adjacent nodes to provide valuable messages is challenging, as\nit could generate noisy information and disrupt the original graph structures.\nTo solve it, we propose a novel Degree Fairness Graph Transformer, named\nDegFairGT, to mitigate degree bias by discovering structural similarities\nbetween non-adjacent nodes through learnable structural augmentation and\nstructural self-attention. Our key idea is to exploit non-adjacent nodes with\nsimilar roles in the same community to generate informative edges under our\naugmentation, which could provide informative messages between nodes with\nsimilar roles while ensuring that the homophily principle is maintained within\nthe community. To enable DegFairGT to learn such structural similarities, we\nthen propose a structural self-attention to capture the similarities between\nnode pairs. To preserve global graph structures and prevent graph augmentation\nfrom hindering graph structure, we propose a Self-Supervised Learning task to\npreserve p-step transition probability and regularize graph augmentation.\nExtensive experiments on six datasets showed that DegFairGT outperformed\nstate-of-the-art baselines in degree fairness analysis, node classification,\nand node clustering tasks."}
{"id": "2504.14337", "pdf": "https://arxiv.org/pdf/2504.14337", "abs": "https://arxiv.org/abs/2504.14337", "authors": ["Josef Taher", "Eric Hyyppä", "Matti Hyyppä", "Klaara Salolahti", "Xiaowei Yu", "Leena Matikainen", "Antero Kukko", "Matti Lehtomäki", "Harri Kaartinen", "Sopitta Thurachen", "Paula Litkey", "Ville Luoma", "Markus Holopainen", "Gefei Kong", "Hongchao Fan", "Petri Rönnholm", "Antti Polvivaara", "Samuli Junttila", "Mikko Vastaranta", "Stefano Puliti", "Rasmus Astrup", "Joel Kostensalo", "Mari Myllymäki", "Maksymilian Kulicki", "Krzysztof Stereńczak", "Raul de Paula Pires", "Ruben Valbuena", "Juan Pedro Carbonell-Rivera", "Jesús Torralba", "Yi-Chen Chen", "Lukas Winiwarter", "Markus Hollaus", "Gottfried Mandlburger", "Narges Takhtkeshha", "Fabio Remondino", "Maciej Lisiewicz", "Bartłomiej Kraszewski", "Xinlian Liang", "Jianchang Chen", "Eero Ahokas", "Kirsi Karila", "Eugeniu Vezeteu", "Petri Manninen", "Roope Näsi", "Heikki Hyyti", "Siiri Pyykkönen", "Peilun Hu", "Juha Hyyppä"], "title": "Multispectral airborne laser scanning for tree species classification: a benchmark of machine learning and deep learning algorithms", "categories": ["cs.CV"], "comment": null, "summary": "Climate-smart and biodiversity-preserving forestry demands precise\ninformation on forest resources, extending to the individual tree level.\nMultispectral airborne laser scanning (ALS) has shown promise in automated\npoint cloud processing and tree segmentation, but challenges remain in\nidentifying rare tree species and leveraging deep learning techniques. This\nstudy addresses these gaps by conducting a comprehensive benchmark of machine\nlearning and deep learning methods for tree species classification. For the\nstudy, we collected high-density multispectral ALS data (>1000 pts/m$^2$) at\nthree wavelengths using the FGI-developed HeliALS system, complemented by\nexisting Optech Titan data (35 pts/m$^2$), to evaluate the species\nclassification accuracy of various algorithms in a test site located in\nSouthern Finland. Based on 5261 test segments, our findings demonstrate that\npoint-based deep learning methods, particularly a point transformer model,\noutperformed traditional machine learning and image-based deep learning\napproaches on high-density multispectral point clouds. For the high-density ALS\ndataset, a point transformer model provided the best performance reaching an\noverall (macro-average) accuracy of 87.9% (74.5%) with a training set of 1065\nsegments and 92.0% (85.1%) with 5000 training segments. The best image-based\ndeep learning method, DetailView, reached an overall (macro-average) accuracy\nof 84.3% (63.9%), whereas a random forest (RF) classifier achieved an overall\n(macro-average) accuracy of 83.2% (61.3%). Importantly, the overall\nclassification accuracy of the point transformer model on the HeliALS data\nincreased from 73.0% with no spectral information to 84.7% with single-channel\nreflectance, and to 87.9% with spectral information of all the three channels."}
{"id": "2504.14808", "pdf": "https://arxiv.org/pdf/2504.14808", "abs": "https://arxiv.org/abs/2504.14808", "authors": ["Mario M. Kubek", "Shiraj Pokharel", "Thomas Böhme", "Emma L. McDaniel", "Herwig Unger", "Armin R. Mikler"], "title": "On Self-improving Token Embeddings", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "68T50, 68T07", "I.2.6; I.2.7; H.3.3"], "comment": "18 pages, 4 figures, 3 tables, accepted at the 2025 25th\n  International Conference on Innovations for Community Services (I4CS), June\n  11 - 13, Munich, Germany, 2025", "summary": "This article introduces a novel and fast method for refining pre-trained\nstatic word or, more generally, token embeddings. By incorporating the\nembeddings of neighboring tokens in text corpora, it continuously updates the\nrepresentation of each token, including those without pre-assigned embeddings.\nThis approach effectively addresses the out-of-vocabulary problem, too.\nOperating independently of large language models and shallow neural networks,\nit enables versatile applications such as corpus exploration, conceptual\nsearch, and word sense disambiguation. The method is designed to enhance token\nrepresentations within topically homogeneous corpora, where the vocabulary is\nrestricted to a specific domain, resulting in more meaningful embeddings\ncompared to general-purpose pre-trained vectors. As an example, the methodology\nis applied to explore storm events and their impacts on infrastructure and\ncommunities using narratives from a subset of the NOAA Storm Events database.\nThe article also demonstrates how the approach improves the representation of\nstorm-related terms over time, providing valuable insights into the evolving\nnature of disaster narratives."}
{"id": "2504.15125", "pdf": "https://arxiv.org/pdf/2504.15125", "abs": "https://arxiv.org/abs/2504.15125", "authors": ["Ruben Laukkonen", "Fionn Inglis", "Shamil Chandaria", "Lars Sandved-Smith", "Jakob Hohwy", "Jonathan Gold", "Adam Elwood"], "title": "Contemplative Wisdom for Superalignment", "categories": ["cs.AI"], "comment": null, "summary": "As artificial intelligence (AI) improves, traditional alignment strategies\nmay falter in the face of unpredictable self-improvement, hidden subgoals, and\nthe sheer complexity of intelligent systems. Rather than externally\nconstraining behavior, we advocate designing AI with intrinsic morality built\ninto its cognitive architecture and world model. Inspired by contemplative\nwisdom traditions, we show how four axiomatic principles can instil a resilient\nWise World Model in AI systems. First, mindfulness enables self-monitoring and\nrecalibration of emergent subgoals. Second, emptiness forestalls dogmatic goal\nfixation and relaxes rigid priors. Third, non-duality dissolves adversarial\nself-other boundaries. Fourth, boundless care motivates the universal reduction\nof suffering. We find that prompting AI to reflect on these principles improves\nperformance on the AILuminate Benchmark using GPT-4o, particularly when\ncombined. We offer detailed implementation strategies for state-of-the-art\nmodels, including contemplative architectures, constitutions, and reinforcement\nof chain-of-thought. For future systems, the active inference framework may\noffer the self-organizing and dynamic coupling capabilities needed to enact\nthese insights in embodied agents. This interdisciplinary approach offers a\nself-correcting and resilient alternative to prevailing brittle control\nschemes."}
{"id": "2504.14348", "pdf": "https://arxiv.org/pdf/2504.14348", "abs": "https://arxiv.org/abs/2504.14348", "authors": ["Le Wang", "Zonghao Ying", "Tianyuan Zhang", "Siyuan Liang", "Shengshan Hu", "Mingchuan Zhang", "Aishan Liu", "Xianglong Liu"], "title": "Manipulating Multimodal Agents via Cross-Modal Prompt Injection", "categories": ["cs.CV"], "comment": "17 pages, 5 figures", "summary": "The emergence of multimodal large language models has redefined the agent\nparadigm by integrating language and vision modalities with external data\nsources, enabling agents to better interpret human instructions and execute\nincreasingly complex tasks. However, in this work, we identify a critical yet\npreviously overlooked security vulnerability in multimodal agents: cross-modal\nprompt injection attacks. To exploit this vulnerability, we propose\nCrossInject, a novel attack framework in which attackers embed adversarial\nperturbations across multiple modalities to align with target malicious\ncontent, allowing external instructions to hijack the agent's decision-making\nprocess and execute unauthorized tasks. Our approach consists of two key\ncomponents. First, we introduce Visual Latent Alignment, where we optimize\nadversarial features to the malicious instructions in the visual embedding\nspace based on a text-to-image generative model, ensuring that adversarial\nimages subtly encode cues for malicious task execution. Subsequently, we\npresent Textual Guidance Enhancement, where a large language model is leveraged\nto infer the black-box defensive system prompt through adversarial meta\nprompting and generate an malicious textual command that steers the agent's\noutput toward better compliance with attackers' requests. Extensive experiments\ndemonstrate that our method outperforms existing injection attacks, achieving\nat least a +26.4% increase in attack success rates across diverse tasks.\nFurthermore, we validate our attack's effectiveness in real-world multimodal\nautonomous agents, highlighting its potential implications for safety-critical\napplications."}
{"id": "2504.14856", "pdf": "https://arxiv.org/pdf/2504.14856", "abs": "https://arxiv.org/abs/2504.14856", "authors": ["Jiajun Shen", "Tong Zhou", "Yubo Chen", "Delai Qiu", "Shengping Liu", "Kang Liu", "Jun Zhao"], "title": "Transparentize the Internal and External Knowledge Utilization in LLMs with Trustworthy Citation", "categories": ["cs.CL"], "comment": "19 pages, 14 figures", "summary": "While hallucinations of large language models could been alleviated through\nretrieval-augmented generation and citation generation, how the model utilizes\ninternal knowledge is still opaque, and the trustworthiness of its generated\nanswers remains questionable. In this work, we introduce Context-Prior\nAugmented Citation Generation task, requiring models to generate citations\nconsidering both external and internal knowledge while providing trustworthy\nreferences, with 5 evaluation metrics focusing on 3 aspects: answer\nhelpfulness, citation faithfulness, and trustworthiness. We introduce RAEL, the\nparadigm for our task, and also design INTRALIGN, an integrated method\ncontaining customary data generation and an alignment algorithm. Our\nexperimental results show that our method achieves a better cross-scenario\nperformance with regard to other baselines. Our extended experiments further\nreveal that retrieval quality, question types, and model knowledge have\nconsiderable influence on the trustworthiness in citation generation."}
{"id": "2504.15146", "pdf": "https://arxiv.org/pdf/2504.15146", "abs": "https://arxiv.org/abs/2504.15146", "authors": ["Wei Zhou", "Ailiya Borjigin", "Cong He"], "title": "Behavioral Universe Network (BUN): A Behavioral Information-Based Framework for Complex Systems", "categories": ["cs.AI"], "comment": "17 pages, 1 figure", "summary": "Modern digital ecosystems feature complex, dynamic interactions among\nautonomous entities across diverse domains. Traditional models often separate\nagents and objects, lacking a unified foundation to capture their interactive\nbehaviors. This paper introduces the Behavioral Universe Network (BUN), a\ntheoretical framework grounded in the Agent-Interaction-Behavior (AIB)\nformalism. BUN treats subjects (active agents), objects (resources), and\nbehaviors (operations) as first-class entities, all governed by a shared\nBehavioral Information Base (BIB). We detail the AIB core concepts and\ndemonstrate how BUN leverages information-driven triggers, semantic enrichment,\nand adaptive rules to coordinate multi-agent systems. We highlight key\nbenefits: enhanced behavior analysis, strong adaptability, and cross-domain\ninteroperability. We conclude by positioning BUN as a promising foundation for\nnext-generation digital governance and intelligent applications."}
{"id": "2504.14359", "pdf": "https://arxiv.org/pdf/2504.14359", "abs": "https://arxiv.org/abs/2504.14359", "authors": ["Kyle Buettner", "Jacob Emmerson", "Adriana Kovashka"], "title": "A Multimodal Recaptioning Framework to Account for Perceptual Diversity in Multilingual Vision-Language Modeling", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "There are many ways to describe, name, and group objects when captioning an\nimage. Differences are evident when speakers come from diverse cultures due to\nthe unique experiences that shape perception. Machine translation of captions\nhas pushed multilingual capabilities in vision-language models (VLMs), but data\ncomes mainly from English speakers, indicating a perceptual bias and lack of\nmodel flexibility. In this work, we address this challenge and outline a\ndata-efficient framework to instill multilingual VLMs with greater\nunderstanding of perceptual diversity. We specifically propose an LLM-based,\nmultimodal recaptioning strategy that alters the object descriptions of English\ncaptions before translation. The greatest benefits are demonstrated in a\ntargeted multimodal mechanism guided by native speaker data. By adding produced\nrewrites as augmentations in training, we improve on German and Japanese\ntext-image retrieval cases studies (up to +3.5 mean recall overall, +4.7 on\nnon-native error cases). We further propose a mechanism to analyze the specific\nobject description differences across datasets, and we offer insights into\ncross-dataset and cross-language generalization."}
{"id": "2504.14871", "pdf": "https://arxiv.org/pdf/2504.14871", "abs": "https://arxiv.org/abs/2504.14871", "authors": ["Teppei Suzuki", "Ryokan Ri", "Sho Takase"], "title": "Natural Fingerprints of Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) often exhibit biases -- systematic deviations\nfrom expected norms -- in their outputs. These range from overt issues, such as\nunfair responses, to subtler patterns that can reveal which model produced\nthem. We investigate the factors that give rise to identifiable characteristics\nin LLMs. Since LLMs model training data distribution, it is reasonable that\ndifferences in training data naturally lead to the characteristics. However,\nour findings reveal that even when LLMs are trained on the exact same data, it\nis still possible to distinguish the source model based on its generated text.\nWe refer to these unintended, distinctive characteristics as natural\nfingerprints. By systematically controlling training conditions, we show that\nthe natural fingerprints can emerge from subtle differences in the training\nprocess, such as parameter sizes, optimization settings, and even random seeds.\nWe believe that understanding natural fingerprints offers new insights into the\norigins of unintended bias and ways for improving control over LLM behavior."}
{"id": "2504.15188", "pdf": "https://arxiv.org/pdf/2504.15188", "abs": "https://arxiv.org/abs/2504.15188", "authors": ["Yizhu Jiao", "Xuchao Zhang", "Zhaoyang Wang", "Yubo Ma", "Zhun Deng", "Rujia Wang", "Chetan Bansal", "Saravan Rajmohan", "Jiawei Han", "Huaxiu Yao"], "title": "Synergistic Weak-Strong Collaboration by Aligning Preferences", "categories": ["cs.AI"], "comment": null, "summary": "Current Large Language Models (LLMs) excel in general reasoning yet struggle\nwith specialized tasks requiring proprietary or domain-specific knowledge.\nFine-tuning large models for every niche application is often infeasible due to\nblack-box constraints and high computational overhead. To address this, we\npropose a collaborative framework that pairs a specialized weak model with a\ngeneral strong model. The weak model, tailored to specific domains, produces\ninitial drafts and background information, while the strong model leverages its\nadvanced reasoning to refine these drafts, extending LLMs' capabilities to\ncritical yet specialized tasks. To optimize this collaboration, we introduce a\ncollaborative feedback to fine-tunes the weak model, which quantifies the\ninfluence of the weak model's contributions in the collaboration procedure and\nestablishes preference pairs to guide preference tuning of the weak model. We\nvalidate our framework through experiments on three domains. We find that the\ncollaboration significantly outperforms each model alone by leveraging\ncomplementary strengths. Moreover, aligning the weak model with the\ncollaborative preference further enhances overall performance."}
{"id": "2504.14371", "pdf": "https://arxiv.org/pdf/2504.14371", "abs": "https://arxiv.org/abs/2504.14371", "authors": ["Peixi Wu", "Bosong Chai", "Menghua Zheng", "Wei Li", "Zhangchi Hu", "Jie Chen", "Zheyu Zhang", "Hebei Li", "Xiaoyan Sun"], "title": "Efficient Spiking Point Mamba for Point Cloud Analysis", "categories": ["cs.CV"], "comment": null, "summary": "Bio-inspired Spiking Neural Networks (SNNs) provide an energy-efficient way\nto extract 3D spatio-temporal features. However, existing 3D SNNs have\nstruggled with long-range dependencies until the recent emergence of Mamba,\nwhich offers superior computational efficiency and sequence modeling\ncapability. In this work, we propose Spiking Point Mamba (SPM), the first\nMamba-based SNN in the 3D domain. Due to the poor performance of simply\ntransferring Mamba to 3D SNNs, SPM is designed to utilize both the sequence\nmodeling capabilities of Mamba and the temporal feature extraction of SNNs.\nSpecifically, we first introduce Hierarchical Dynamic Encoding (HDE), an\nimproved direct encoding method that effectively introduces dynamic temporal\nmechanism, thereby facilitating temporal interactions. Then, we propose a\nSpiking Mamba Block (SMB), which builds upon Mamba while learning\ninter-time-step features and minimizing information loss caused by spikes.\nFinally, to further enhance model performance, we adopt an asymmetric SNN-ANN\narchitecture for spike-based pre-training and finetune. Compared with the\nprevious state-of-the-art SNN models, SPM improves OA by +6.2%, +6.1%, and\n+7.4% on three variants of ScanObjectNN, and boosts instance mIOU by +1.9% on\nShapeNetPart. Meanwhile, its energy consumption is at least 3.5x lower than\nthat of its ANN counterpart. The code will be made publicly available."}
{"id": "2504.14891", "pdf": "https://arxiv.org/pdf/2504.14891", "abs": "https://arxiv.org/abs/2504.14891", "authors": ["Aoran Gan", "Hao Yu", "Kai Zhang", "Qi Liu", "Wenyu Yan", "Zhenya Huang", "Shiwei Tong", "Guoping Hu"], "title": "Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey", "categories": ["cs.CL"], "comment": "18 pages, 5 figures", "summary": "Recent advancements in Retrieval-Augmented Generation (RAG) have\nrevolutionized natural language processing by integrating Large Language Models\n(LLMs) with external information retrieval, enabling accurate, up-to-date, and\nverifiable text generation across diverse applications. However, evaluating RAG\nsystems presents unique challenges due to their hybrid architecture that\ncombines retrieval and generation components, as well as their dependence on\ndynamic knowledge sources in the LLM era. In response, this paper provides a\ncomprehensive survey of RAG evaluation methods and frameworks, systematically\nreviewing traditional and emerging evaluation approaches, for system\nperformance, factual accuracy, safety, and computational efficiency in the LLM\nera. We also compile and categorize the RAG-specific datasets and evaluation\nframeworks, conducting a meta-analysis of evaluation practices in high-impact\nRAG research. To the best of our knowledge, this work represents the most\ncomprehensive survey for RAG evaluation, bridging traditional and LLM-driven\nmethods, and serves as a critical resource for advancing RAG development."}
{"id": "2504.15211", "pdf": "https://arxiv.org/pdf/2504.15211", "abs": "https://arxiv.org/abs/2504.15211", "authors": ["Yanan Long"], "title": "Position: Bayesian Statistics Facilitates Stakeholder Participation in Evaluation of Generative AI", "categories": ["cs.AI", "stat.AP"], "comment": "To be presented at ACM CHI 2025 workshop STAIG", "summary": "The evaluation of Generative AI (GenAI) systems plays a critical role in\npublic policy and decision-making, yet existing methods are often limited by\nreliance on benchmark-driven, point-estimate comparisons that fail to capture\nuncertainty and broader societal impacts. This paper argues for the use of\nBayesian statistics as a principled framework to address these challenges.\nBayesian methods enable the integration of domain expertise through prior\nelicitation, allow for continuous learning from new data, and provide robust\nuncertainty quantification via posterior inference. We demonstrate how Bayesian\ninference can be applied to GenAI evaluation, particularly in incorporating\nstakeholder perspectives to enhance fairness, transparency, and reliability.\nFurthermore, we discuss Bayesian workflows as an iterative process for model\nvalidation and refinement, ensuring robust assessments of GenAI systems in\ndynamic, real-world contexts."}
{"id": "2504.14386", "pdf": "https://arxiv.org/pdf/2504.14386", "abs": "https://arxiv.org/abs/2504.14386", "authors": ["Md Abtahi Majeed Chowdhury", "Md Rifat Ur Rahman", "Akil Ahmad Taki"], "title": "LOOPE: Learnable Optimal Patch Order in Positional Embeddings for Vision Transformers", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Positional embeddings (PE) play a crucial role in Vision Transformers (ViTs)\nby providing spatial information otherwise lost due to the permutation\ninvariant nature of self attention. While absolute positional embeddings (APE)\nhave shown theoretical advantages over relative positional embeddings (RPE),\nparticularly due to the ability of sinusoidal functions to preserve spatial\ninductive biases like monotonicity and shift invariance, a fundamental\nchallenge arises when mapping a 2D grid to a 1D sequence. Existing methods have\nmostly overlooked or never explored the impact of patch ordering in positional\nembeddings. To address this, we propose LOOPE, a learnable patch-ordering\nmethod that optimizes spatial representation for a given set of frequencies,\nproviding a principled approach to patch order optimization. Empirical results\nshow that our PE significantly improves classification accuracy across various\nViT architectures. To rigorously evaluate the effectiveness of positional\nembeddings, we introduce the \"Three Cell Experiment\", a novel benchmarking\nframework that assesses the ability of PEs to retain relative and absolute\npositional information across different ViT architectures. Unlike standard\nevaluations, which typically report a performance gap of 4 to 6% between models\nwith and without PE, our method reveals a striking 30 to 35% difference,\noffering a more sensitive diagnostic tool to measure the efficacy of PEs. Our\nexperimental analysis confirms that the proposed LOOPE demonstrates enhanced\neffectiveness in retaining both relative and absolute positional information."}
{"id": "2504.14905", "pdf": "https://arxiv.org/pdf/2504.14905", "abs": "https://arxiv.org/abs/2504.14905", "authors": ["Yingming Zheng", "Xiaoliang Liu", "Peng Wu", "Li Pan"], "title": "CRAVE: A Conflicting Reasoning Approach for Explainable Claim Verification Using LLMs", "categories": ["cs.CL"], "comment": null, "summary": "The rapid spread of misinformation, driven by digital media and AI-generated\ncontent, has made automatic claim verification essential. Traditional methods,\nwhich depend on expert-annotated evidence, are labor-intensive and not\nscalable. Although recent automated systems have improved, they still struggle\nwith complex claims that require nuanced reasoning. To address this, we propose\nCRAVE, a Conflicting Reasoning Approach for explainable claim VErification,\nthat verify the complex claims based on the conflicting rationales reasoned by\nlarge language models (LLMs). Specifically, CRAVE introduces a three-module\nframework. Ambiguity Elimination enchanced Evidence Retrieval module performs\nambiguity elimination and entity-based search to gather relevant evidence\nrelated to claim verification from external sources like Wikipedia. Conflicting\nPerspective Reasoning and Preliminary Judgment module with LLMs adopts LLMs to\nreason rationales with conflicting stances about claim verification from\nretrieved evidence across four dimensions, i.e., direct evidence, semantic\nrelationships, linguistic patterns, and logical reasoning and make a\npreliminary judgment. Finally, Small Language Model (SLM) based Judge module is\nfine-tuned to make use of preliminary judgment from LLMs to assess the\nconfidence of the conflicting rationales and make a final authenticity\njudgment. This methodology allows CRAVE to capture subtle inconsistencies in\ncomplex claims, improving both the accuracy and transparency of claim\nverification. Extensive experiments on two public claim verification datasets\ndemonstrate that our CRAVE model achieves much better performance than\nstate-of-the-art methods and exhibits a superior capacity for finding relevant\nevidence and explaining the model predictions. The code is provided at\nhttps://github.com/8zym/CRAVE."}
{"id": "2504.15228", "pdf": "https://arxiv.org/pdf/2504.15228", "abs": "https://arxiv.org/abs/2504.15228", "authors": ["Maxime Robeyns", "Martin Szummer", "Laurence Aitchison"], "title": "A Self-Improving Coding Agent", "categories": ["cs.AI"], "comment": "Published at an ICLR 2025 workshop on Scaling Self-Improving\n  Foundation Models", "summary": "We demonstrate that an LLM coding agent, equipped with basic coding tools,\ncan autonomously edit itself, and thereby improve its performance on benchmark\ntasks. We find performance gains from 17% to 53% on a random subset of SWE\nBench Verified, with additional performance gains on LiveCodeBench, as well as\nsynthetically generated agent benchmarks. Our work represents an advancement in\nthe automated and open-ended design of agentic systems, and provides a\nreference agent framework for those seeking to post-train LLMs on tool use and\nother agentic tasks."}
{"id": "2504.14391", "pdf": "https://arxiv.org/pdf/2504.14391", "abs": "https://arxiv.org/abs/2504.14391", "authors": ["Rahul Thapa", "Andrew Li", "Qingyang Wu", "Bryan He", "Yuki Sahashi", "Christina Binder", "Angela Zhang", "Ben Athiwaratkun", "Shuaiwen Leon Song", "David Ouyang", "James Zou"], "title": "How Well Can General Vision-Language Models Learn Medicine By Watching Public Educational Videos?", "categories": ["cs.CV"], "comment": null, "summary": "Publicly available biomedical videos, such as those on YouTube, serve as\nvaluable educational resources for medical students. Unlike standard machine\nlearning datasets, these videos are designed for human learners, often mixing\nmedical imagery with narration, explanatory diagrams, and contextual framing.\nIn this work, we investigate whether such pedagogically rich, yet\nnon-standardized and heterogeneous videos can effectively teach general-domain\nvision-language models biomedical knowledge. To this end, we introduce\nOpenBiomedVi, a biomedical video instruction tuning dataset comprising 1031\nhours of video-caption and Q/A pairs, curated through a multi-step\nhuman-in-the-loop pipeline. Diverse biomedical video datasets are rare, and\nOpenBiomedVid fills an important gap by providing instruction-style supervision\ngrounded in real-world educational content. Surprisingly, despite the informal\nand heterogeneous nature of these videos, the fine-tuned Qwen-2-VL models\nexhibit substantial performance improvements across most benchmarks. The 2B\nmodel achieves gains of 98.7% on video tasks, 71.2% on image tasks, and 0.2% on\ntext tasks. The 7B model shows improvements of 37.09% on video and 11.2% on\nimage tasks, with a slight degradation of 2.7% on text tasks compared to their\nrespective base models. To address the lack of standardized biomedical video\nevaluation datasets, we also introduce two new expert curated benchmarks,\nMIMICEchoQA and SurgeryVideoQA. On these benchmarks, the 2B model achieves\ngains of 99.1% and 98.1%, while the 7B model shows gains of 22.5% and 52.1%,\nrespectively, demonstrating the models' ability to generalize and perform\nbiomedical video understanding on cleaner and more standardized datasets than\nthose seen during training. These results suggest that educational videos\ncreated for human learning offer a surprisingly effective training signal for\nbiomedical VLMs."}
{"id": "2504.14963", "pdf": "https://arxiv.org/pdf/2504.14963", "abs": "https://arxiv.org/abs/2504.14963", "authors": ["Rui Ribeiro", "Luísa Coheur", "Joao P. Carvalho"], "title": "Speaker Fuzzy Fingerprints: Benchmarking Text-Based Identification in Multiparty Dialogues", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.LG", "cs.NE"], "comment": "Paper accepted at the FUZZY IEEE 2025 conference", "summary": "Speaker identification using voice recordings leverages unique acoustic\nfeatures, but this approach fails when only textual data is available. Few\napproaches have attempted to tackle the problem of identifying speakers solely\nfrom text, and the existing ones have primarily relied on traditional methods.\nIn this work, we explore the use of fuzzy fingerprints from large pre-trained\nmodels to improve text-based speaker identification. We integrate\nspeaker-specific tokens and context-aware modeling, demonstrating that\nconversational context significantly boosts accuracy, reaching 70.6% on the\nFriends dataset and 67.7% on the Big Bang Theory dataset. Additionally, we show\nthat fuzzy fingerprints can approximate full fine-tuning performance with fewer\nhidden units, offering improved interpretability. Finally, we analyze ambiguous\nutterances and propose a mechanism to detect speaker-agnostic lines. Our\nfindings highlight key challenges and provide insights for future improvements\nin text-based speaker identification."}
{"id": "2504.15252", "pdf": "https://arxiv.org/pdf/2504.15252", "abs": "https://arxiv.org/abs/2504.15252", "authors": ["Tue Vo", "Lakshay Sharma", "Tuan Dinh", "Khuong Dinh", "Trang Nguyen", "Trung Phan", "Minh Do", "Duong Vu"], "title": "SuoiAI: Building a Dataset for Aquatic Invertebrates in Vietnam", "categories": ["cs.AI", "cs.CV", "cs.LG"], "comment": "Published as a workshop paper at \"Tackling Climate Change with\n  Machine Learning\", ICLR 2025", "summary": "Understanding and monitoring aquatic biodiversity is critical for ecological\nhealth and conservation efforts. This paper proposes SuoiAI, an end-to-end\npipeline for building a dataset of aquatic invertebrates in Vietnam and\nemploying machine learning (ML) techniques for species classification. We\noutline the methods for data collection, annotation, and model training,\nfocusing on reducing annotation effort through semi-supervised learning and\nleveraging state-of-the-art object detection and classification models. Our\napproach aims to overcome challenges such as data scarcity, fine-grained\nclassification, and deployment in diverse environmental conditions."}
{"id": "2504.14395", "pdf": "https://arxiv.org/pdf/2504.14395", "abs": "https://arxiv.org/abs/2504.14395", "authors": ["Chung-En", "Yu", "Hsuan-Chih", "Chen", "Brian Jalaian", "Nathaniel D. Bastian"], "title": "Hydra: An Agentic Reasoning Approach for Enhancing Adversarial Robustness and Mitigating Hallucinations in Vision-Language Models", "categories": ["cs.CV", "cs.AI", "cs.MA"], "comment": null, "summary": "To develop trustworthy Vision-Language Models (VLMs), it is essential to\naddress adversarial robustness and hallucination mitigation, both of which\nimpact factual accuracy in high-stakes applications such as defense and\nhealthcare. Existing methods primarily focus on either adversarial defense or\nhallucination post-hoc correction, leaving a gap in unified robustness\nstrategies. We introduce \\textbf{Hydra}, an adaptive agentic framework that\nenhances plug-in VLMs through iterative reasoning, structured critiques, and\ncross-model verification, improving both resilience to adversarial\nperturbations and intrinsic model errors. Hydra employs an Action-Critique\nLoop, where it retrieves and critiques visual information, leveraging\nChain-of-Thought (CoT) and In-Context Learning (ICL) techniques to refine\noutputs dynamically. Unlike static post-hoc correction methods, Hydra adapts to\nboth adversarial manipulations and intrinsic model errors, making it robust to\nmalicious perturbations and hallucination-related inaccuracies. We evaluate\nHydra on four VLMs, three hallucination benchmarks, two adversarial attack\nstrategies, and two adversarial defense methods, assessing performance on both\nclean and adversarial inputs. Results show that Hydra surpasses plug-in VLMs\nand state-of-the-art (SOTA) dehallucination methods, even without explicit\nadversarial defenses, demonstrating enhanced robustness and factual\nconsistency. By bridging adversarial resistance and hallucination mitigation,\nHydra provides a scalable, training-free solution for improving the reliability\nof VLMs in real-world applications."}
{"id": "2504.14969", "pdf": "https://arxiv.org/pdf/2504.14969", "abs": "https://arxiv.org/abs/2504.14969", "authors": ["Xiaodong Yang"], "title": "Evaluating LLMs on Chinese Topic Constructions: A Research Proposal Inspired by Tian et al. (2024)", "categories": ["cs.CL"], "comment": null, "summary": "This paper proposes a framework for evaluating large language models (LLMs)\non Chinese topic constructions, focusing on their sensitivity to island\nconstraints. Drawing inspiration from Tian et al. (2024), we outline an\nexperimental design for testing LLMs' grammatical knowledge of Mandarin syntax.\nWhile no experiments have been conducted yet, this proposal aims to provide a\nfoundation for future studies and invites feedback on the methodology."}
{"id": "2504.15257", "pdf": "https://arxiv.org/pdf/2504.15257", "abs": "https://arxiv.org/abs/2504.15257", "authors": ["Hongcheng Gao", "Yue Liu", "Yufei He", "Longxu Dou", "Chao Du", "Zhijie Deng", "Bryan Hooi", "Min Lin", "Tianyu Pang"], "title": "FlowReasoner: Reinforcing Query-Level Meta-Agents", "categories": ["cs.AI"], "comment": null, "summary": "This paper proposes a query-level meta-agent named FlowReasoner to automate\nthe design of query-level multi-agent systems, i.e., one system per user query.\nOur core idea is to incentivize a reasoning-based meta-agent via external\nexecution feedback. Concretely, by distilling DeepSeek R1, we first endow the\nbasic reasoning ability regarding the generation of multi-agent systems to\nFlowReasoner. Then, we further enhance it via reinforcement learning (RL) with\nexternal execution feedback. A multi-purpose reward is designed to guide the RL\ntraining from aspects of performance, complexity, and efficiency. In this\nmanner, FlowReasoner is enabled to generate a personalized multi-agent system\nfor each user query via deliberative reasoning. Experiments on both engineering\nand competition code benchmarks demonstrate the superiority of FlowReasoner.\nRemarkably, it surpasses o1-mini by 10.52% accuracy across three benchmarks.\nThe code is available at https://github.com/sail-sg/FlowReasoner."}
{"id": "2504.14396", "pdf": "https://arxiv.org/pdf/2504.14396", "abs": "https://arxiv.org/abs/2504.14396", "authors": ["Minho Park", "Taewoong Kang", "Jooyeol Yun", "Sungwon Hwang", "Jaegul Choo"], "title": "SphereDiff: Tuning-free Omnidirectional Panoramic Image and Video Generation via Spherical Latent Representation", "categories": ["cs.CV"], "comment": null, "summary": "The increasing demand for AR/VR applications has highlighted the need for\nhigh-quality 360-degree panoramic content. However, generating high-quality\n360-degree panoramic images and videos remains a challenging task due to the\nsevere distortions introduced by equirectangular projection (ERP). Existing\napproaches either fine-tune pretrained diffusion models on limited ERP datasets\nor attempt tuning-free methods that still rely on ERP latent representations,\nleading to discontinuities near the poles. In this paper, we introduce\nSphereDiff, a novel approach for seamless 360-degree panoramic image and video\ngeneration using state-of-the-art diffusion models without additional tuning.\nWe define a spherical latent representation that ensures uniform distribution\nacross all perspectives, mitigating the distortions inherent in ERP. We extend\nMultiDiffusion to spherical latent space and propose a spherical latent\nsampling method to enable direct use of pretrained diffusion models. Moreover,\nwe introduce distortion-aware weighted averaging to further improve the\ngeneration quality in the projection process. Our method outperforms existing\napproaches in generating 360-degree panoramic content while maintaining high\nfidelity, making it a robust solution for immersive AR/VR applications. The\ncode is available here. https://github.com/pmh9960/SphereDiff"}
{"id": "2504.14992", "pdf": "https://arxiv.org/pdf/2504.14992", "abs": "https://arxiv.org/abs/2504.14992", "authors": ["Bohong Wu", "Shen Yan", "Sijun Zhang", "Jianqiao Lu", "Yutao Zeng", "Ya Wang", "Xun Zhou"], "title": "Efficient Pretraining Length Scaling", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large language models have demonstrated the effectiveness\nof length scaling during post-training, yet its potential in pre-training\nremains underexplored. We present the Parallel Hidden Decoding Transformer\n(\\textit{PHD}-Transformer), a novel framework that enables efficient length\nscaling during pre-training while maintaining inference efficiency.\n\\textit{PHD}-Transformer achieves this through an innovative KV cache\nmanagement strategy that distinguishes between original tokens and hidden\ndecoding tokens. By retaining only the KV cache of original tokens for\nlong-range dependencies while immediately discarding hidden decoding tokens\nafter use, our approach maintains the same KV cache size as the vanilla\ntransformer while enabling effective length scaling. To further enhance\nperformance, we introduce two optimized variants: \\textit{PHD-SWA} employs\nsliding window attention to preserve local dependencies, while\n\\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\nimprovements across multiple benchmarks."}
{"id": "2504.15261", "pdf": "https://arxiv.org/pdf/2504.15261", "abs": "https://arxiv.org/abs/2504.15261", "authors": ["Mohammad Beheshti", "Lovedeep Gondara", "Iris Zachary"], "title": "Leveraging Language Models for Automated Patient Record Linkage", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Objective: Healthcare data fragmentation presents a major challenge for\nlinking patient data, necessitating robust record linkage to integrate patient\nrecords from diverse sources. This study investigates the feasibility of\nleveraging language models for automated patient record linkage, focusing on\ntwo key tasks: blocking and matching. Materials and Methods: We utilized\nreal-world healthcare data from the Missouri Cancer Registry and Research\nCenter, linking patient records from two independent sources using\nprobabilistic linkage as a baseline. A transformer-based model, RoBERTa, was\nfine-tuned for blocking using sentence embeddings. For matching, several\nlanguage models were experimented under fine-tuned and zero-shot settings,\nassessing their performance against ground truth labels. Results: The\nfine-tuned blocking model achieved a 92% reduction in the number of candidate\npairs while maintaining near-perfect recall. In the matching task, fine-tuned\nMistral-7B achieved the best performance with only 6 incorrect predictions.\nAmong zero-shot models, Mistral-Small-24B performed best, with a total of 55\nincorrect predictions. Discussion: Fine-tuned language models achieved strong\nperformance in patient record blocking and matching with minimal errors.\nHowever, they remain less accurate and efficient than a hybrid rule-based and\nprobabilistic approach for blocking. Additionally, reasoning models like\nDeepSeek-R1 are impractical for large-scale record linkage due to high\ncomputational costs. Conclusion: This study highlights the potential of\nlanguage models for automating patient record linkage, offering improved\nefficiency by eliminating the manual efforts required to perform patient record\nlinkage. Overall, language models offer a scalable solution that can enhance\ndata integration, reduce manual effort, and support disease surveillance and\nresearch."}
{"id": "2504.14423", "pdf": "https://arxiv.org/pdf/2504.14423", "abs": "https://arxiv.org/abs/2504.14423", "authors": ["Qiang Chen", "Xiao Wang", "Haowen Wang", "Bo Jiang", "Lin Zhu", "Dawei Zhang", "Yonghong Tian", "Jin Tang"], "title": "Adversarial Attack for RGB-Event based Visual Object Tracking", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Visual object tracking is a crucial research topic in the fields of computer\nvision and multi-modal fusion. Among various approaches, robust visual tracking\nthat combines RGB frames with Event streams has attracted increasing attention\nfrom researchers. While striving for high accuracy and efficiency in tracking,\nit is also important to explore how to effectively conduct adversarial attacks\nand defenses on RGB-Event stream tracking algorithms, yet research in this area\nremains relatively scarce. To bridge this gap, in this paper, we propose a\ncross-modal adversarial attack algorithm for RGB-Event visual tracking. Because\nof the diverse representations of Event streams, and given that Event voxels\nand frames are more commonly used, this paper will focus on these two\nrepresentations for an in-depth study. Specifically, for the RGB-Event voxel,\nwe first optimize the perturbation by adversarial loss to generate RGB frame\nadversarial examples. For discrete Event voxel representations, we propose a\ntwo-step attack strategy, more in detail, we first inject Event voxels into the\ntarget region as initialized adversarial examples, then, conduct a\ngradient-guided optimization by perturbing the spatial location of the Event\nvoxels. For the RGB-Event frame based tracking, we optimize the cross-modal\nuniversal perturbation by integrating the gradient information from multimodal\ndata. We evaluate the proposed approach against attacks on three widely used\nRGB-Event Tracking datasets, i.e., COESOT, FE108, and VisEvent. Extensive\nexperiments show that our method significantly reduces the performance of the\ntracker across numerous datasets in both unimodal and multimodal scenarios. The\nsource code will be released on\nhttps://github.com/Event-AHU/Adversarial_Attack_Defense"}
{"id": "2504.15013", "pdf": "https://arxiv.org/pdf/2504.15013", "abs": "https://arxiv.org/abs/2504.15013", "authors": ["Yow-Fu Liou", "Yu-Chien Tang", "An-Zi Yen"], "title": "Stay Hungry, Stay Foolish: On the Extended Reading Articles Generation with LLMs", "categories": ["cs.CL"], "comment": "Accepted by iRAISE@AAAI2025", "summary": "The process of creating educational materials is both time-consuming and\ndemanding for educators. This research explores the potential of Large Language\nModels (LLMs) to streamline this task by automating the generation of extended\nreading materials and relevant course suggestions. Using the TED-Ed Dig Deeper\nsections as an initial exploration, we investigate how supplementary articles\ncan be enriched with contextual knowledge and connected to additional learning\nresources. Our method begins by generating extended articles from video\ntranscripts, leveraging LLMs to include historical insights, cultural examples,\nand illustrative anecdotes. A recommendation system employing semantic\nsimilarity ranking identifies related courses, followed by an LLM-based\nrefinement process to enhance relevance. The final articles are tailored to\nseamlessly integrate these recommendations, ensuring they remain cohesive and\ninformative. Experimental evaluations demonstrate that our model produces\nhigh-quality content and accurate course suggestions, assessed through metrics\nsuch as Hit Rate, semantic similarity, and coherence. Our experimental analysis\nhighlight the nuanced differences between the generated and existing materials,\nunderscoring the model's capacity to offer more engaging and accessible\nlearning experiences. This study showcases how LLMs can bridge the gap between\ncore content and supplementary learning, providing students with additional\nrecommended resources while also assisting teachers in designing educational\nmaterials."}
{"id": "2504.15275", "pdf": "https://arxiv.org/pdf/2504.15275", "abs": "https://arxiv.org/abs/2504.15275", "authors": ["Jie Cheng", "Ruixi Qiao", "Lijun Li", "Chao Guo", "Junle Wang", "Gang Xiong", "Yisheng Lv", "Fei-Yue Wang"], "title": "Stop Summation: Min-Form Credit Assignment Is All Process Reward Model Needs for Reasoning", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Process reward models (PRMs) have proven effective for test-time scaling of\nLarge Language Models (LLMs) on challenging reasoning tasks. However, reward\nhacking issues with PRMs limit their successful application in reinforcement\nfine-tuning. In this paper, we identify the main cause of PRM-induced reward\nhacking: the canonical summation-form credit assignment in reinforcement\nlearning (RL), which defines the value as cumulative gamma-decayed future\nrewards, easily induces LLMs to hack steps with high rewards. To address this,\nwe propose PURE: Process sUpervised Reinforcement lEarning. The key innovation\nof PURE is a min-form credit assignment that formulates the value function as\nthe minimum of future rewards. This method significantly alleviates reward\nhacking by limiting the value function range and distributing advantages more\nreasonably. Through extensive experiments on 3 base models, we show that\nPRM-based approaches enabling min-form credit assignment achieve comparable\nreasoning performance to verifiable reward-based methods within only 30% steps.\nIn contrast, the canonical sum-form credit assignment collapses training even\nat the beginning! Additionally, when we supplement PRM-based fine-tuning with\njust 10% verifiable rewards, we further alleviate reward hacking and produce\nthe best fine-tuned model based on Qwen2.5-Math-7B in our experiments,\nachieving 82.5% accuracy on AMC23 and 53.3% average accuracy across 5\nbenchmarks. Moreover, we summarize the observed reward hacking cases and\nanalyze the causes of training collapse. Code and models are available at\nhttps://github.com/CJReinforce/PURE."}
{"id": "2504.14429", "pdf": "https://arxiv.org/pdf/2504.14429", "abs": "https://arxiv.org/abs/2504.14429", "authors": ["Ahmad Khalil", "Mahmoud Khalil", "Alioune Ngom"], "title": "ResNetVLLM-2: Addressing ResNetVLLM's Multi-Modal Hallucinations", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have transformed natural language processing\n(NLP) tasks, but they suffer from hallucination, generating plausible yet\nfactually incorrect content. This issue extends to Video-Language Models\n(VideoLLMs), where textual descriptions may inaccurately represent visual\ncontent, resulting in multi-modal hallucinations. In this paper, we address\nhallucination in ResNetVLLM, a video-language model combining ResNet visual\nencoders with LLMs. We introduce a two-step protocol: (1) a faithfulness\ndetection strategy that uses a modified Lynx model to assess semantic alignment\nbetween generated captions and ground-truth video references, and (2) a\nhallucination mitigation strategy using Retrieval-Augmented Generation (RAG)\nwith an ad-hoc knowledge base dynamically constructed during inference. Our\nenhanced model, ResNetVLLM-2, reduces multi-modal hallucinations by\ncross-verifying generated content against external knowledge, improving factual\nconsistency. Evaluation on the ActivityNet-QA benchmark demonstrates a\nsubstantial accuracy increase from 54.8% to 65.3%, highlighting the\neffectiveness of our hallucination detection and mitigation strategies in\nenhancing video-language model reliability."}
{"id": "2504.15022", "pdf": "https://arxiv.org/pdf/2504.15022", "abs": "https://arxiv.org/abs/2504.15022", "authors": ["Muhammad Uzair Ul Haq", "Davide Rigoni", "Alessandro Sperduti"], "title": "LLMs as Data Annotators: How Close Are We to Human Performance", "categories": ["cs.CL"], "comment": "27 pages, 4 figures", "summary": "In NLP, fine-tuning LLMs is effective for various applications but requires\nhigh-quality annotated data. However, manual annotation of data is\nlabor-intensive, time-consuming, and costly. Therefore, LLMs are increasingly\nused to automate the process, often employing in-context learning (ICL) in\nwhich some examples related to the task are given in the prompt for better\nperformance. However, manually selecting context examples can lead to\ninefficiencies and suboptimal model performance. This paper presents\ncomprehensive experiments comparing several LLMs, considering different\nembedding models, across various datasets for the Named Entity Recognition\n(NER) task. The evaluation encompasses models with approximately $7$B and $70$B\nparameters, including both proprietary and non-proprietary models. Furthermore,\nleveraging the success of Retrieval-Augmented Generation (RAG), it also\nconsiders a method that addresses the limitations of ICL by automatically\nretrieving contextual examples, thereby enhancing performance. The results\nhighlight the importance of selecting the appropriate LLM and embedding model,\nunderstanding the trade-offs between LLM sizes and desired performance, and the\nnecessity to direct research efforts towards more challenging datasets."}
{"id": "2504.13848", "pdf": "https://arxiv.org/pdf/2504.13848", "abs": "https://arxiv.org/abs/2504.13848", "authors": ["Janet Rafner", "Ryan Q. Guloy", "Eden W. Wen", "Catherine M. Chiodo", "Jacob Sherson"], "title": "From Interaction to Collaboration: How Hybrid Intelligence Enhances Chatbot Feedback", "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.ET"], "comment": "14 pages", "summary": "Generative AI (GenAI) chatbots are becoming increasingly integrated into\nvirtual assistant technologies, yet their success hinges on the ability to\ngather meaningful user feedback to improve interaction quality, system\noutcomes, and overall user acceptance. Successful chatbot interactions can\nenable organizations to build long-term relationships with their customers and\nusers, supporting customer loyalty and furthering the organization's goals.\nThis study explores the impact of two distinct narratives and feedback\ncollection mechanisms on user engagement and feedback behavior: a standard\nAI-focused interaction versus a hybrid intelligence (HI) framed interaction.\nInitial findings indicate that while small-scale survey measures allowed for no\nsignificant differences in user willingness to leave feedback, use the system,\nor trust the system, participants exposed to the HI narrative statistically\nsignificantly provided more detailed feedback. These initial findings offer\ninsights into designing effective feedback systems for GenAI virtual\nassistants, balancing user effort with system improvement potential."}
{"id": "2504.14432", "pdf": "https://arxiv.org/pdf/2504.14432", "abs": "https://arxiv.org/abs/2504.14432", "authors": ["Ahmad Khalil", "Mahmoud Khalil", "Alioune Ngom"], "title": "ResNetVLLM -- Multi-modal Vision LLM for the Video Understanding Task", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In this paper, we introduce ResNetVLLM (ResNet Vision LLM), a novel\ncross-modal framework for zero-shot video understanding that integrates a\nResNet-based visual encoder with a Large Language Model (LLM. ResNetVLLM\naddresses the challenges associated with zero-shot video models by avoiding\nreliance on pre-trained video understanding models and instead employing a\nnon-pretrained ResNet to extract visual features. This design ensures the model\nlearns visual and semantic representations within a unified architecture,\nenhancing its ability to generate accurate and contextually relevant textual\ndescriptions from video inputs. Our experimental results demonstrate that\nResNetVLLM achieves state-of-the-art performance in zero-shot video\nunderstanding (ZSVU) on several benchmarks, including MSRVTT-QA, MSVD-QA,\nTGIF-QA FrameQA, and ActivityNet-QA."}
{"id": "2504.15027", "pdf": "https://arxiv.org/pdf/2504.15027", "abs": "https://arxiv.org/abs/2504.15027", "authors": ["Chengyu Wang", "Junbing Yan", "Yuanhao Yue", "Jun Huang"], "title": "DistilQwen2.5: Industrial Practices of Training Distilled Open Lightweight Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Enhancing computational efficiency and reducing deployment costs for large\nlanguage models (LLMs) have become critical challenges in various\nresource-constrained scenarios. In this work, we present DistilQwen2.5, a\nfamily of distilled, lightweight LLMs derived from the public Qwen2.5 models.\nThese distilled models exhibit enhanced instruction-following capabilities\ncompared to the original models based on a series of distillation techniques\nthat incorporate knowledge from much larger LLMs. In our industrial practice,\nwe first leverage powerful proprietary LLMs with varying capacities as\nmulti-agent teachers to select, rewrite, and refine instruction-response pairs\nthat are more suitable for student LLMs to learn. After standard fine-tuning,\nwe further leverage a computationally efficient model fusion approach that\nenables student models to progressively integrate fine-grained hidden knowledge\nfrom their teachers. Experimental evaluations demonstrate that the distilled\nmodels possess significantly stronger capabilities than their original\ncheckpoints. Additionally, we present use cases to illustrate the applications\nof our framework in real-world scenarios. To facilitate practical use, we have\nreleased all the DistilQwen2.5 models to the open-source community."}
{"id": "2504.13853", "pdf": "https://arxiv.org/pdf/2504.13853", "abs": "https://arxiv.org/abs/2504.13853", "authors": ["Pingfei Zhu", "Chenyang Zhao", "Haishi Zhao", "Bo Yang"], "title": "GenShin:geometry-enhanced structural graph embodies binding pose can better predicting compound-protein interaction affinity", "categories": ["q-bio.BM", "cs.AI"], "comment": "11 pages, 3 figures", "summary": "AI-powered drug discovery typically relies on the successful prediction of\ncompound-protein interactions, which are pivotal for the evaluation of designed\ncompound molecules in structure-based drug design and represent a core\nchallenge in the field.\n  However, accurately predicting compound-protein affinity via regression\nmodels usually requires adequate-binding pose, which are derived from costly\nand complex experimental methods or time-consuming simulations with docking\nsoftware. In response, we have introduced the GenShin model, which constructs a\ngeometry-enhanced structural graph module that separately extracts additional\nfeatures from proteins and compounds. Consequently, it attains an accuracy on\npar with mainstream models in predicting compound-protein affinities, while\neliminating the need for adequate-binding pose as input. Our experimental\nfindings demonstrate that the GenShin model vastly outperforms other models\nthat rely on non-input docking conformations, achieving, or in some cases even\nexceeding, the performance of those requiring adequate-binding pose. Further\nexperiments indicate that our GenShin model is more robust to\ninadequate-binding pose, affirming its higher suitability for real-world drug\ndiscovery scenarios. We hope our work will inspire more endeavors to bridge the\ngap between AI models and practical drug discovery challenges."}
{"id": "2504.14445", "pdf": "https://arxiv.org/pdf/2504.14445", "abs": "https://arxiv.org/abs/2504.14445", "authors": ["Mingya Zhang", "Liang Wang", "Limei Gu", "Tingsheng Ling", "Xianping Tao"], "title": "WT-BCP: Wavelet Transform based Bidirectional Copy-Paste for Semi-Supervised Medical Image Segmentation", "categories": ["cs.CV"], "comment": "6 pages", "summary": "Semi-supervised medical image segmentation (SSMIS) shows promise in reducing\nreliance on scarce labeled medical data. However, SSMIS field confronts\nchallenges such as distribution mismatches between labeled and unlabeled data,\nartificial perturbations causing training biases, and inadequate use of raw\nimage information, especially low-frequency (LF) and high-frequency (HF)\ncomponents.To address these challenges, we propose a Wavelet Transform based\nBidirectional Copy-Paste SSMIS framework, named WT-BCP, which improves upon the\nMean Teacher approach. Our method enhances unlabeled data understanding by\ncopying random crops between labeled and unlabeled images and employs WT to\nextract LF and HF details.We propose a multi-input and multi-output model named\nXNet-Plus, to receive the fused information after WT. Moreover, consistency\ntraining among multiple outputs helps to mitigate learning biases introduced by\nartificial perturbations. During consistency training, the mixed images\nresulting from WT are fed into both models, with the student model's output\nbeing supervised by pseudo-labels and ground-truth. Extensive experiments\nconducted on 2D and 3D datasets confirm the effectiveness of our model.Code:\nhttps://github.com/simzhangbest/WT-BCP."}
{"id": "2504.15047", "pdf": "https://arxiv.org/pdf/2504.15047", "abs": "https://arxiv.org/abs/2504.15047", "authors": ["Quy-Anh Dang", "Chris Ngo", "Truong-Son Hy"], "title": "RainbowPlus: Enhancing Adversarial Prompt Generation via Evolutionary Quality-Diversity Search", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) exhibit remarkable capabilities but are\nsusceptible to adversarial prompts that exploit vulnerabilities to produce\nunsafe or biased outputs. Existing red-teaming methods often face scalability\nchallenges, resource-intensive requirements, or limited diversity in attack\nstrategies. We propose RainbowPlus, a novel red-teaming framework rooted in\nevolutionary computation, enhancing adversarial prompt generation through an\nadaptive quality-diversity (QD) search that extends classical evolutionary\nalgorithms like MAP-Elites with innovations tailored for language models. By\nemploying a multi-element archive to store diverse high-quality prompts and a\ncomprehensive fitness function to evaluate multiple prompts concurrently,\nRainbowPlus overcomes the constraints of single-prompt archives and pairwise\ncomparisons in prior QD methods like Rainbow Teaming. Experiments comparing\nRainbowPlus to QD methods across six benchmark datasets and four open-source\nLLMs demonstrate superior attack success rate (ASR) and diversity\n(Diverse-Score $\\approx 0.84$), generating up to 100 times more unique prompts\n(e.g., 10,418 vs. 100 for Ministral-8B-Instruct-2410). Against nine\nstate-of-the-art methods on the HarmBench dataset with twelve LLMs (ten\nopen-source, two closed-source), RainbowPlus achieves an average ASR of 81.1%,\nsurpassing AutoDAN-Turbo by 3.9%, and is 9 times faster (1.45 vs. 13.50 hours).\nOur open-source implementation fosters further advancements in LLM safety,\noffering a scalable tool for vulnerability assessment. Code and resources are\npublicly available at https://github.com/knoveleng/rainbowplus, supporting\nreproducibility and future research in LLM red-teaming."}
{"id": "2504.13856", "pdf": "https://arxiv.org/pdf/2504.13856", "abs": "https://arxiv.org/abs/2504.13856", "authors": ["Andrew Silva", "Pradyumna Tambwekar", "Mariah Schrum", "Matthew Gombolay"], "title": "Towards Balancing Preference and Performance through Adaptive Personalized Explainability", "categories": ["cs.HC", "cs.AI", "cs.RO"], "comment": "20 pages, 19 figures, HRI 2024", "summary": "As robots and digital assistants are deployed in the real world, these agents\nmust be able to communicate their decision-making criteria to build trust,\nimprove human-robot teaming, and enable collaboration. While the field of\nexplainable artificial intelligence (xAI) has made great strides to enable such\ncommunication, these advances often assume that one xAI approach is ideally\nsuited to each problem (e.g., decision trees to explain how to triage patients\nin an emergency or feature-importance maps to explain radiology reports). This\nfails to recognize that users have diverse experiences or preferences for\ninteraction modalities. In this work, we present two user-studies set in a\nsimulated autonomous vehicle (AV) domain. We investigate (1) population-level\npreferences for xAI and (2) personalization strategies for providing robot\nexplanations. We find significant differences between xAI modes (language\nexplanations, feature-importance maps, and decision trees) in both preference\n(p < 0.01) and performance (p < 0.05). We also observe that a participant's\npreferences do not always align with their performance, motivating our\ndevelopment of an adaptive personalization strategy to balance the two. We show\nthat this strategy yields significant performance gains (p < 0.05), and we\nconclude with a discussion of our findings and implications for xAI in\nhuman-robot interactions."}
{"id": "2504.14446", "pdf": "https://arxiv.org/pdf/2504.14446", "abs": "https://arxiv.org/abs/2504.14446", "authors": ["Carlos Caetano", "Gabriel O. dos Santos", "Caio Petrucci", "Artur Barros", "Camila Laranjeira", "Leo S. F. Ribeiro", "Júlia F. de Mendonça", "Jefersson A. dos Santos", "Sandra Avila"], "title": "Neglected Risks: The Disturbing Reality of Children's Images in Datasets and the Urgent Call for Accountability", "categories": ["cs.CV", "cs.CY", "cs.LG"], "comment": "ACM Conference on Fairness, Accountability, and Transparency (FAccT\n  2025)", "summary": "Including children's images in datasets has raised ethical concerns,\nparticularly regarding privacy, consent, data protection, and accountability.\nThese datasets, often built by scraping publicly available images from the\nInternet, can expose children to risks such as exploitation, profiling, and\ntracking. Despite the growing recognition of these issues, approaches for\naddressing them remain limited. We explore the ethical implications of using\nchildren's images in AI datasets and propose a pipeline to detect and remove\nsuch images. As a use case, we built the pipeline on a Vision-Language Model\nunder the Visual Question Answering task and tested it on the #PraCegoVer\ndataset. We also evaluate the pipeline on a subset of 100,000 images from the\nOpen Images V7 dataset to assess its effectiveness in detecting and removing\nimages of children. The pipeline serves as a baseline for future research,\nproviding a starting point for more comprehensive tools and methodologies.\nWhile we leverage existing models trained on potentially problematic data, our\ngoal is to expose and address this issue. We do not advocate for training or\ndeploying such models, but instead call for urgent community reflection and\naction to protect children's rights. Ultimately, we aim to encourage the\nresearch community to exercise - more than an additional - care in creating new\ndatasets and to inspire the development of tools to protect the fundamental\nrights of vulnerable groups, particularly children."}
{"id": "2504.15052", "pdf": "https://arxiv.org/pdf/2504.15052", "abs": "https://arxiv.org/abs/2504.15052", "authors": ["Joachim Minder", "Guillaume Wisniewski", "Natalie Kübler"], "title": "Testing LLMs' Capabilities in Annotating Translations Based on an Error Typology Designed for LSP Translation: First Experiments with ChatGPT", "categories": ["cs.CL", "eess.AS"], "comment": "Accepted for publication in the proceedings of MT Summit 2025", "summary": "This study investigates the capabilities of large language models (LLMs),\nspecifically ChatGPT, in annotating MT outputs based on an error typology. In\ncontrast to previous work focusing mainly on general language, we explore\nChatGPT's ability to identify and categorise errors in specialised\ntranslations. By testing two different prompts and based on a customised error\ntypology, we compare ChatGPT annotations with human expert evaluations of\ntranslations produced by DeepL and ChatGPT itself. The results show that, for\ntranslations generated by DeepL, recall and precision are quite high. However,\nthe degree of accuracy in error categorisation depends on the prompt's specific\nfeatures and its level of detail, ChatGPT performing very well with a detailed\nprompt. When evaluating its own translations, ChatGPT achieves significantly\npoorer results, revealing limitations with self-assessment. These results\nhighlight both the potential and the limitations of LLMs for translation\nevaluation, particularly in specialised domains. Our experiments pave the way\nfor future research on open-source LLMs, which could produce annotations of\ncomparable or even higher quality. In the future, we also aim to test the\npractical effectiveness of this automated evaluation in the context of\ntranslation training, particularly by optimising the process of human\nevaluation by teachers and by exploring the impact of annotations by LLMs on\nstudents' post-editing and translation learning."}
{"id": "2504.13858", "pdf": "https://arxiv.org/pdf/2504.13858", "abs": "https://arxiv.org/abs/2504.13858", "authors": ["Felix Haag"], "title": "The Effect of Explainable AI-based Decision Support on Human Task Performance: A Meta-Analysis", "categories": ["cs.HC", "cs.AI"], "comment": "Published in the Proceedings of the Twenty-Third Annual Pre-ICIS\n  Workshop on HCI Research in MIS, Bangkok, Thailand, December 15th, 2024", "summary": "The desirable properties of explanations in information systems have fueled\nthe demands for transparency in artificial intelligence (AI) outputs. To\naddress these demands, the field of explainable AI (XAI) has put forth methods\nthat can support human decision-making by explaining AI outputs. However,\ncurrent empirical works present inconsistent findings on whether such\nexplanations help to improve users' task performance in decision support\nsystems (DSS). In this paper, we conduct a meta-analysis to explore how XAI\naffects human performance in classification tasks. Our results show an\nimprovement in task performance through XAI-based decision support, though\nexplanations themselves are not the decisive driver for this improvement. The\nanalysis reveals that the studies' risk of bias moderates the effect of\nexplanations in AI, while the explanation type appears to play only a\nnegligible role. Our findings contribute to the human computer interaction\nfield by enhancing the understanding of human-XAI collaboration in DSS."}
{"id": "2504.14450", "pdf": "https://arxiv.org/pdf/2504.14450", "abs": "https://arxiv.org/abs/2504.14450", "authors": ["Weizhi Nie", "Zichun Zhang", "Weijie Wang", "Bruno Lepri", "Anan Liu", "Nicu Seb"], "title": "Causal Disentanglement for Robust Long-tail Medical Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Counterfactual medical image generation effectively addresses data scarcity\nand enhances the interpretability of medical images. However, due to the\ncomplex and diverse pathological features of medical images and the imbalanced\nclass distribution in medical data, generating high-quality and diverse medical\nimages from limited data is significantly challenging. Additionally, to fully\nleverage the information in limited data, such as anatomical structure\ninformation and generate more structurally stable medical images while avoiding\ndistortion or inconsistency. In this paper, in order to enhance the clinical\nrelevance of generated data and improve the interpretability of the model, we\npropose a novel medical image generation framework, which generates independent\npathological and structural features based on causal disentanglement and\nutilizes text-guided modeling of pathological features to regulate the\ngeneration of counterfactual images. First, we achieve feature separation\nthrough causal disentanglement and analyze the interactions between features.\nHere, we introduce group supervision to ensure the independence of pathological\nand identity features. Second, we leverage a diffusion model guided by\npathological findings to model pathological features, enabling the generation\nof diverse counterfactual images. Meanwhile, we enhance accuracy by leveraging\na large language model to extract lesion severity and location from medical\nreports. Additionally, we improve the performance of the latent diffusion model\non long-tailed categories through initial noise optimization."}
{"id": "2504.15093", "pdf": "https://arxiv.org/pdf/2504.15093", "abs": "https://arxiv.org/abs/2504.15093", "authors": ["K. Wong", "B. Wu", "S. Bulathwela", "M. Cukurova"], "title": "Rethinking the Potential of Multimodality in Collaborative Problem Solving Diagnosis with Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted for 26th International Conference on Artificial Intelligence\n  in Education (AIED 2025), 22 - 26 July 2025, Palermo, Italy. 17 pages, 1\n  figure", "summary": "Detecting collaborative and problem-solving behaviours from digital traces to\ninterpret students' collaborative problem solving (CPS) competency is a\nlong-term goal in the Artificial Intelligence in Education (AIEd) field.\nAlthough multimodal data and advanced models are argued to have the potential\nto detect complex CPS behaviours, empirical evidence on their value remains\nlimited with some contrasting evidence. In this study, we investigated the\npotential of multimodal data to improve model performance in diagnosing 78\nsecondary school students' CPS subskills and indicators in authentic\neducational settings. In particular, text embeddings from verbal data and\nacoustic embeddings from audio data were used in a multimodal classification\nmodel for CPS diagnosis. Both unimodal and multimodal transformer-based models\noutperformed traditional models in detecting CPS classes. Although the\ninclusion of multimodality did not improve the performance of traditional\nunimodal models, its integration into transformer-based models demonstrated\nimproved performance for diagnosing social-cognitive CPS classes compared to\nunimodal transformer-based models. Based on the results, the paper argues that\nmultimodality and the selection of a particular modelling technique should not\nbe taken for granted to achieve the best performance in the automated detection\nof every CPS subskill and indicator. Rather, their value is limited to certain\ntypes of CPS indicators, affected by the complexity of the labels, and\ndependent on the composition of indicators in the dataset. We conclude the\npaper by discussing the required nuance when considering the value of LLMs and\nmultimodality in automated CPS diagnosis, highlighting the need for human-AI\ncomplementarity, and proposing the exploration of relevant model architectures\nand techniques to improve CPS diagnosis in authentic educational contexts."}
{"id": "2504.13859", "pdf": "https://arxiv.org/pdf/2504.13859", "abs": "https://arxiv.org/abs/2504.13859", "authors": ["Phillip Driscoll", "Priyanka Kumar"], "title": "DoYouTrustAI: A Tool to Teach Students About AI Misinformation and Prompt Engineering", "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": null, "summary": "AI, especially Large Language Models (LLMs) like ChatGPT, have rapidly\ndeveloped and gained widespread adoption in the past five years, shifting user\npreference from traditional search engines. However, the generative nature of\nLLMs raises concerns about presenting misinformation as fact. To address this,\nwe developed a web-based application that helps K-12 students enhance critical\nthinking by identifying misleading information in LLM responses about major\nhistorical figures. In this paper, we describe the implementation and design\ndetails of the DoYouTrustAI tool, which can be used to provide an interactive\nlesson which teaches students about the dangers of misinformation and how\nbelievable generative AI can make it seem. The DoYouTrustAI tool utilizes\nprompt engineering to present the user with AI generated summaries about the\nlife of a historical figure. These summaries can be either accurate accounts of\nthat persons life, or an intentionally misleading alteration of their history.\nThe user is tasked with determining the validity of the statement without\nexternal resources. Our research questions for this work were:(RQ1) How can we\ndesign a tool that teaches students about the dangers of misleading information\nand of how misinformation can present itself in LLM responses? (RQ2) Can we\npresent prompt engineering as a topic that is easily understandable for\nstudents? Our findings highlight the need to correct misleading information\nbefore users retain it. Our tool lets users select familiar individuals for\ntesting to reduce random guessing and presents misinformation alongside known\nfacts to maintain believability. It also provides pre-configured prompt\ninstructions to show how different prompts affect AI responses. Together, these\nfeatures create a controlled environment where users learn the importance of\nverifying AI responses and understanding prompt engineering."}
{"id": "2504.14460", "pdf": "https://arxiv.org/pdf/2504.14460", "abs": "https://arxiv.org/abs/2504.14460", "authors": ["Junyan Su", "Baozhu Zhao", "Xiaohan Zhang", "Qi Liu"], "title": "Metamon-GS: Enhancing Representability with Variance-Guided Densification and Light Encoding", "categories": ["cs.CV"], "comment": null, "summary": "The introduction of 3D Gaussian Splatting (3DGS) has advanced novel view\nsynthesis by utilizing Gaussians to represent scenes. Encoding Gaussian point\nfeatures with anchor embeddings has significantly enhanced the performance of\nnewer 3DGS variants. While significant advances have been made, it is still\nchallenging to boost rendering performance. Feature embeddings have difficulty\naccurately representing colors from different perspectives under varying\nlighting conditions, which leads to a washed-out appearance. Another reason is\nthe lack of a proper densification strategy that prevents Gaussian point growth\nin thinly initialized areas, resulting in blurriness and needle-shaped\nartifacts. To address them, we propose Metamon-GS, from innovative viewpoints\nof variance-guided densification strategy and multi-level hash grid. The\ndensification strategy guided by variance specifically targets Gaussians with\nhigh gradient variance in pixels and compensates for the importance of regions\nwith extra Gaussians to improve reconstruction. The latter studies implicit\nglobal lighting conditions and accurately interprets color from different\nperspectives and feature embeddings. Our thorough experiments on publicly\navailable datasets show that Metamon-GS surpasses its baseline model and\nprevious versions, delivering superior quality in rendering novel views."}
{"id": "2504.15120", "pdf": "https://arxiv.org/pdf/2504.15120", "abs": "https://arxiv.org/abs/2504.15120", "authors": ["Khalil Hennara", "Sara Chrouf", "Mohamed Motaism Hamed", "Zeina Aldallal", "Omar Hadid", "Safwan AlModhayan"], "title": "Kuwain 1.5B: An Arabic SLM via Language Injection", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Enhancing existing models with new knowledge is a crucial aspect of AI\ndevelopment. This paper introduces a novel method for integrating a new\nlanguage into a large language model (LLM). Our approach successfully\nincorporates a previously unseen target language into an existing LLM without\ncompromising its prior knowledge. We trained a tiny model with 1.5 billion\nparameters named Kuwain by injecting the Arabic language into a small\nopen-source model mainly trained in English. Our method demonstrates\nsignificant improvements in Arabic language performance, with an average 8%\nimprovement across various benchmarks, while retaining the model's existing\nknowledge with a minimum amount of the original model's data. This offers a\ncost-effective alternative to training a comprehensive model in both English\nand Arabic. The results highlight the potential for efficient, targeted\nlanguage model expansion without extensive retraining or resource-intensive\nprocesses."}
{"id": "2504.13865", "pdf": "https://arxiv.org/pdf/2504.13865", "abs": "https://arxiv.org/abs/2504.13865", "authors": ["Fei Tang", "Haolei Xu", "Hang Zhang", "Siqi Chen", "Xingyu Wu", "Yongliang Shen", "Wenqi Zhang", "Guiyang Hou", "Zeqi Tan", "Yuchen Yan", "Kaitao Song", "Jian Shao", "Weiming Lu", "Jun Xiao", "Yueting Zhuang"], "title": "A Survey on (M)LLM-Based GUI Agents", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Graphical User Interface (GUI) Agents have emerged as a transformative\nparadigm in human-computer interaction, evolving from rule-based automation\nscripts to sophisticated AI-driven systems capable of understanding and\nexecuting complex interface operations. This survey provides a comprehensive\nexamination of the rapidly advancing field of LLM-based GUI Agents,\nsystematically analyzing their architectural foundations, technical components,\nand evaluation methodologies. We identify and analyze four fundamental\ncomponents that constitute modern GUI Agents: (1) perception systems that\nintegrate text-based parsing with multimodal understanding for comprehensive\ninterface comprehension; (2) exploration mechanisms that construct and maintain\nknowledge bases through internal modeling, historical experience, and external\ninformation retrieval; (3) planning frameworks that leverage advanced reasoning\nmethodologies for task decomposition and execution; and (4) interaction systems\nthat manage action generation with robust safety controls. Through rigorous\nanalysis of these components, we reveal how recent advances in large language\nmodels and multimodal learning have revolutionized GUI automation across\ndesktop, mobile, and web platforms. We critically examine current evaluation\nframeworks, highlighting methodological limitations in existing benchmarks\nwhile proposing directions for standardization. This survey also identifies key\ntechnical challenges, including accurate element localization, effective\nknowledge retrieval, long-horizon planning, and safety-aware execution control,\nwhile outlining promising research directions for enhancing GUI Agents'\ncapabilities. Our systematic review provides researchers and practitioners with\na thorough understanding of the field's current state and offers insights into\nfuture developments in intelligent interface automation."}
{"id": "2504.14467", "pdf": "https://arxiv.org/pdf/2504.14467", "abs": "https://arxiv.org/abs/2504.14467", "authors": ["Jiachen Li", "Qing Xie", "Xiaohan Yu", "Hongyun Wang", "Jinyu Xu", "Yongjian Liu", "Yongsheng Gao"], "title": "LGD: Leveraging Generative Descriptions for Zero-Shot Referring Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Zero-shot referring image segmentation aims to locate and segment the target\nregion based on a referring expression, with the primary challenge of aligning\nand matching semantics across visual and textual modalities without training.\nPrevious works address this challenge by utilizing Vision-Language Models and\nmask proposal networks for region-text matching. However, this paradigm may\nlead to incorrect target localization due to the inherent ambiguity and\ndiversity of free-form referring expressions. To alleviate this issue, we\npresent LGD (Leveraging Generative Descriptions), a framework that utilizes the\nadvanced language generation capabilities of Multi-Modal Large Language Models\nto enhance region-text matching performance in Vision-Language Models.\nSpecifically, we first design two kinds of prompts, the attribute prompt and\nthe surrounding prompt, to guide the Multi-Modal Large Language Models in\ngenerating descriptions related to the crucial attributes of the referent\nobject and the details of surrounding objects, referred to as attribute\ndescription and surrounding description, respectively. Secondly, three\nvisual-text matching scores are introduced to evaluate the similarity between\ninstance-level visual features and textual features, which determines the mask\nmost associated with the referring expression. The proposed method achieves new\nstate-of-the-art performance on three public datasets RefCOCO, RefCOCO+ and\nRefCOCOg, with maximum improvements of 9.97% in oIoU and 11.29% in mIoU\ncompared to previous methods."}
{"id": "2504.15133", "pdf": "https://arxiv.org/pdf/2504.15133", "abs": "https://arxiv.org/abs/2504.15133", "authors": ["Ziwen Xu", "Shuxun Wang", "Kewei Xu", "Haoming Xu", "Mengru Wang", "Xinle Deng", "Yunzhi Yao", "Guozhou Zheng", "Huajun Chen", "Ningyu Zhang"], "title": "EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "comment": "Work in progress. Demo:\n  https://zjunlp.github.io/project/EasyEdit2/video; code:\n  https://github.com/zjunlp/EasyEdit", "summary": "In this paper, we introduce EasyEdit2, a framework designed to enable\nplug-and-play adjustability for controlling Large Language Model (LLM)\nbehaviors. EasyEdit2 supports a wide range of test-time interventions,\nincluding safety, sentiment, personality, reasoning patterns, factuality, and\nlanguage features. Unlike its predecessor, EasyEdit2 features a new\narchitecture specifically designed for seamless model steering. It comprises\nkey modules such as the steering vector generator and the steering vector\napplier, which enable automatic generation and application of steering vectors\nto influence the model's behavior without modifying its parameters. One of the\nmain advantages of EasyEdit2 is its ease of use-users do not need extensive\ntechnical knowledge. With just a single example, they can effectively guide and\nadjust the model's responses, making precise control both accessible and\nefficient. Empirically, we report model steering performance across different\nLLMs, demonstrating the effectiveness of these techniques. We have released the\nsource code on GitHub at https://github.com/zjunlp/EasyEdit along with a\ndemonstration notebook. In addition, we provide a demo video at\nhttps://zjunlp.github.io/project/EasyEdit2/video for a quick introduction."}
{"id": "2504.13866", "pdf": "https://arxiv.org/pdf/2504.13866", "abs": "https://arxiv.org/abs/2504.13866", "authors": ["Aleksa Marusic", "Sao Mai Nguyen", "Adriana Tapus"], "title": "Skeleton-Based Transformer for Classification of Errors and Better Feedback in Low Back Pain Physical Rehabilitation Exercises", "categories": ["cs.HC", "cs.AI", "cs.CV", "cs.RO"], "comment": "ICORR 2025 - 19th IEEE/RAS-EMBS International Conference on\n  Rehabilitation Robotics, INTERNATIONAL CONSORTIUM FOR REHABILITATION\n  ROBOTICS, May 2025, Michigan, USA, United States", "summary": "Physical rehabilitation exercises suggested by healthcare professionals can\nhelp recovery from various musculoskeletal disorders and prevent re-injury.\nHowever, patients' engagement tends to decrease over time without direct\nsupervision, which is why there is a need for an automated monitoring system.\nIn recent years, there has been great progress in quality assessment of\nphysical rehabilitation exercises. Most of them only provide a binary\nclassification if the performance is correct or incorrect, and a few provide a\ncontinuous score. This information is not sufficient for patients to improve\ntheir performance. In this work, we propose an algorithm for error\nclassification of rehabilitation exercises, thus making the first step toward\nmore detailed feedback to patients. We focus on skeleton-based exercise\nassessment, which utilizes human pose estimation to evaluate motion. Inspired\nby recent algorithms for quality assessment during rehabilitation exercises, we\npropose a Transformer-based model for the described classification. Our model\nis inspired by the HyperFormer method for human action recognition, and adapted\nto our problem and dataset. The evaluation is done on the KERAAL dataset, as it\nis the only medical dataset with clear error labels for the exercises, and our\nmodel significantly surpasses state-of-the-art methods. Furthermore, we bridge\nthe gap towards better feedback to the patients by presenting a way to\ncalculate the importance of joints for each exercise."}
{"id": "2504.14470", "pdf": "https://arxiv.org/pdf/2504.14470", "abs": "https://arxiv.org/abs/2504.14470", "authors": ["Jingjing Ren", "Wenbo Li", "Zhongdao Wang", "Haoze Sun", "Bangzhen Liu", "Haoyu Chen", "Jiaqi Xu", "Aoxue Li", "Shifeng Zhang", "Bin Shao", "Yong Guo", "Lei Zhu"], "title": "Turbo2K: Towards Ultra-Efficient and High-Quality 2K Video Synthesis", "categories": ["cs.CV"], "comment": "Webpage at https://jingjingrenabc.github.io/turbo2k/", "summary": "Demand for 2K video synthesis is rising with increasing consumer expectations\nfor ultra-clear visuals. While diffusion transformers (DiTs) have demonstrated\nremarkable capabilities in high-quality video generation, scaling them to 2K\nresolution remains computationally prohibitive due to quadratic growth in\nmemory and processing costs. In this work, we propose Turbo2K, an efficient and\npractical framework for generating detail-rich 2K videos while significantly\nimproving training and inference efficiency. First, Turbo2K operates in a\nhighly compressed latent space, reducing computational complexity and memory\nfootprint, making high-resolution video synthesis feasible. However, the high\ncompression ratio of the VAE and limited model size impose constraints on\ngenerative quality. To mitigate this, we introduce a knowledge distillation\nstrategy that enables a smaller student model to inherit the generative\ncapacity of a larger, more powerful teacher model. Our analysis reveals that,\ndespite differences in latent spaces and architectures, DiTs exhibit structural\nsimilarities in their internal representations, facilitating effective\nknowledge transfer. Second, we design a hierarchical two-stage synthesis\nframework that first generates multi-level feature at lower resolutions before\nguiding high-resolution video generation. This approach ensures structural\ncoherence and fine-grained detail refinement while eliminating redundant\nencoding-decoding overhead, further enhancing computational efficiency.Turbo2K\nachieves state-of-the-art efficiency, generating 5-second, 24fps, 2K videos\nwith significantly reduced computational cost. Compared to existing methods,\nTurbo2K is up to 20$\\times$ faster for inference, making high-resolution video\ngeneration more scalable and practical for real-world applications."}
{"id": "2504.15160", "pdf": "https://arxiv.org/pdf/2504.15160", "abs": "https://arxiv.org/abs/2504.15160", "authors": ["Joan C. Timoneda"], "title": "The Synthetic Imputation Approach: Generating Optimal Synthetic Texts For Underrepresented Categories In Supervised Classification Tasks", "categories": ["cs.CL"], "comment": null, "summary": "Encoder-decoder Large Language Models (LLMs), such as BERT and RoBERTa,\nrequire that all categories in an annotation task be sufficiently represented\nin the training data for optimal performance. However, it is often difficult to\nfind sufficient examples for all categories in a task when building a\nhigh-quality training set. In this article, I describe this problem and propose\na solution, the synthetic imputation approach. Leveraging a generative LLM\n(GPT-4o), this approach generates synthetic texts based on careful prompting\nand five original examples drawn randomly with replacement from the sample.\nThis approach ensures that new synthetic texts are sufficiently different from\nthe original texts to reduce overfitting, but retain the underlying substantive\nmeaning of the examples to maximize out-of-sample performance. With 75 original\nexamples or more, synthetic imputation's performance is on par with a full\nsample of original texts, and overfitting remains low, predictable and\ncorrectable with 50 original samples. The synthetic imputation approach\nprovides a novel role for generative LLMs in research and allows applied\nresearchers to balance their datasets for best performance."}
{"id": "2504.13868", "pdf": "https://arxiv.org/pdf/2504.13868", "abs": "https://arxiv.org/abs/2504.13868", "authors": ["Yun Wan", "Yoram M Kalman"], "title": "Using Generative AI Personas Increases Collective Diversity in Human Ideation", "categories": ["cs.HC", "cs.AI", "I.2.7, H.5.0, H.4.0"], "comment": null, "summary": "This study challenges the widely-reported tradeoff between generative AI's\n(GenAI) contribution to creative outcomes and decreased diversity of these\noutcomes. We modified the design of such a study, by Doshi and Hauser (2024),\nin which participants wrote short stories either aided or unaided by GenAI plot\nideas[1]. In the modified study, plot ideas were generated through ten unique\nGenAI \"personas\" with diverse traits (e.g. cultural backgrounds, thinking\nstyles, genre preferences), creating a pool of 300 story plots. While plot\nideas from any individual persona showed high similarity (average cosine\nsimilarity of 0.92), ideas across different personas exhibited substantial\nvariation (average similarity of 0.20). When human participants wrote stories\nbased on these diverse plot ideas, their collective outputs maintained the same\nlevel of diversity as stories written without GenAI assistance, effectively\neliminating the diversity reduction observed in [1]. Traditional text analytics\nfurther revealed that GenAI-assisted stories featured greater diversity in\ndescriptive and emotional language compared to purely human-generated stories\nwithout GenAI assistance. Our findings demonstrate that introducing diversity\nat the AI input stage through distinct personas can preserve and potentially\nenhance the collective diversity of human creative outputs when collaborating\nwith GenAI."}
{"id": "2504.14471", "pdf": "https://arxiv.org/pdf/2504.14471", "abs": "https://arxiv.org/abs/2504.14471", "authors": ["Yichi Zhang", "Qianqian Yang"], "title": "Efficient Implicit Neural Compression of Point Clouds via Learnable Activation in Latent Space", "categories": ["cs.CV"], "comment": "8 pages", "summary": "Implicit Neural Representations (INRs), also known as neural fields, have\nemerged as a powerful paradigm in deep learning, parameterizing continuous\nspatial fields using coordinate-based neural networks. In this paper, we\npropose \\textbf{PICO}, an INR-based framework for static point cloud\ncompression. Unlike prevailing encoder-decoder paradigms, we decompose the\npoint cloud compression task into two separate stages: geometry compression and\nattribute compression, each with distinct INR optimization objectives. Inspired\nby Kolmogorov-Arnold Networks (KANs), we introduce a novel network\narchitecture, \\textbf{LeAFNet}, which leverages learnable activation functions\nin the latent space to better approximate the target signal's implicit\nfunction. By reformulating point cloud compression as neural parameter\ncompression, we further improve compression efficiency through quantization and\nentropy coding. Experimental results demonstrate that \\textbf{LeAFNet}\noutperforms conventional MLPs in INR-based point cloud compression.\nFurthermore, \\textbf{PICO} achieves superior geometry compression performance\ncompared to the current MPEG point cloud compression standard, yielding an\naverage improvement of $4.92$ dB in D1 PSNR. In joint geometry and attribute\ncompression, our approach exhibits highly competitive results, with an average\nPCQM gain of $2.7 \\times 10^{-3}$."}
{"id": "2504.15168", "pdf": "https://arxiv.org/pdf/2504.15168", "abs": "https://arxiv.org/abs/2504.15168", "authors": ["Qilin Tian"], "title": "On true empty category", "categories": ["cs.CL"], "comment": null, "summary": "According to Chomsky (1981, 1986), empty categories consist of PRO, pro,\ntrace, and variable. However, some empty object positions seem to be\nincompatible with extant empty categories. Given this, Li (2007a, 2007b, 2014)\nand Li & Wei (2014) raise the true empty category hypothesis, which holds that\ntrue empty category is only an empty position with category and Case features.\nAs a last resort option, it is used mainly to meet the subcatgorization of a\nverb. This assumption is ingenious, and if proved to be true, it will exert a\ngreat impact on the study of UG. In this paper, we evaluate their evidence from\ntopicalization and demonstrate that it can be accounted for without invoking\ntrue empty category."}
{"id": "2504.13871", "pdf": "https://arxiv.org/pdf/2504.13871", "abs": "https://arxiv.org/abs/2504.13871", "authors": ["Yuanjun Feng", "Vivek Chodhary", "Yash Raj Shrestha"], "title": "Human aversion? Do AI Agents Judge Identity More Harshly Than Performance", "categories": ["cs.HC", "cs.AI", "econ.GN", "q-fin.EC"], "comment": null, "summary": "This study examines the understudied role of algorithmic evaluation of human\njudgment in hybrid decision-making systems, a critical gap in management\nresearch. While extant literature focuses on human reluctance to follow\nalgorithmic advice, we reverse the perspective by investigating how AI agents\nbased on large language models (LLMs) assess and integrate human input. Our\nwork addresses a pressing managerial constraint: firms barred from deploying\nLLMs directly due to privacy concerns can still leverage them as mediating\ntools (for instance, anonymized outputs or decision pipelines) to guide\nhigh-stakes choices like pricing or discounts without exposing proprietary\ndata. Through a controlled prediction task, we analyze how an LLM-based AI\nagent weights human versus algorithmic predictions. We find that the AI system\nsystematically discounts human advice, penalizing human errors more severely\nthan algorithmic errors--a bias exacerbated when the agent's identity (human vs\nAI) is disclosed and the human is positioned second. These results reveal a\ndisconnect between AI-generated trust metrics and the actual influence of human\njudgment, challenging assumptions about equitable human-AI collaboration. Our\nfindings offer three key contributions. First, we identify a reverse algorithm\naversion phenomenon, where AI agents undervalue human input despite comparable\nerror rates. Second, we demonstrate how disclosure and positional bias interact\nto amplify this effect, with implications for system design. Third, we provide\na framework for indirect LLM deployment that balances predictive power with\ndata privacy. For practitioners, this research emphasize the need to audit AI\nweighting mechanisms, calibrate trust dynamics, and strategically design\ndecision sequences in human-AI systems."}
{"id": "2504.14481", "pdf": "https://arxiv.org/pdf/2504.14481", "abs": "https://arxiv.org/abs/2504.14481", "authors": ["Guoyi Zhang", "Siyang Chen", "Guangsheng Xu", "Han Wang", "Xiaohu Zhang"], "title": "Vision-Centric Representation-Efficient Fine-Tuning for Robust Universal Foreground Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Foreground segmentation is crucial for scene understanding, yet\nparameter-efficient fine-tuning (PEFT) of vision foundation models (VFMs) often\nfails in complex scenarios, such as camouflage and infrared imagery. We\nattribute this challenge to the inherent texture bias in VFMs, which is\nexacerbated during fine-tuning and limits generalization in texture-sparse\nenvironments. To address this, we propose Ladder Shape-bias Representation\nSide-tuning (LSR-ST), a lightweight PEFT framework that enhances model\nrobustness by introducing shape-biased inductive priors. LSR-ST captures\nshape-aware features using a simple HDConv Block, which integrates large-kernel\nattention and residual learning. The method satisfies three key conditions for\ninducing shape bias: large receptive fields, multi-order feature interactions,\nand sparse connectivity. Our analysis reveals that these improvements stem from\nrepresentation efficiency-the ability to extract task-relevant, structurally\ngrounded features while minimizing redundancy. We formalize this concept via\nInformation Bottleneck theory and advocate for it as a key PEFT objective.\nUnlike traditional NLP paradigms that focus on optimizing parameters and\nmemory, visual tasks require models that extract task-defined semantics, rather\nthan just relying on pre-encoded features. This shift enables our approach to\nmove beyond conventional trade-offs, offering more robust and generalizable\nsolutions for vision tasks. With minimal changes to SAM2-UNet, LSR-ST achieves\nconsistent improvements across 17 datasets and 6 tasks using only 4.719M\ntrainable parameters. These results highlight the potential of representation\nefficiency for robust and adaptable VFMs within complex visual environments."}
{"id": "2504.15205", "pdf": "https://arxiv.org/pdf/2504.15205", "abs": "https://arxiv.org/abs/2504.15205", "authors": ["Nandan Thakur", "Ronak Pradeep", "Shivani Upadhyay", "Daniel Campos", "Nick Craswell", "Jimmy Lin"], "title": "Support Evaluation for the TREC 2024 RAG Track: Comparing Human versus LLM Judges", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Accepted at SIGIR 2025 (short)", "summary": "Retrieval-augmented generation (RAG) enables large language models (LLMs) to\ngenerate answers with citations from source documents containing \"ground\ntruth\", thereby reducing system hallucinations. A crucial factor in RAG\nevaluation is \"support\", whether the information in the cited documents\nsupports the answer. To this end, we conducted a large-scale comparative study\nof 45 participant submissions on 36 topics to the TREC 2024 RAG Track,\ncomparing an automatic LLM judge (GPT-4o) against human judges for support\nassessment. We considered two conditions: (1) fully manual assessments from\nscratch and (2) manual assessments with post-editing of LLM predictions. Our\nresults indicate that for 56% of the manual from-scratch assessments, human and\nGPT-4o predictions match perfectly (on a three-level scale), increasing to 72%\nin the manual with post-editing condition. Furthermore, by carefully analyzing\nthe disagreements in an unbiased study, we found that an independent human\njudge correlates better with GPT-4o than a human judge, suggesting that LLM\njudges can be a reliable alternative for support assessment. To conclude, we\nprovide a qualitative analysis of human and GPT-4o errors to help guide future\niterations of support assessment."}
{"id": "2504.13877", "pdf": "https://arxiv.org/pdf/2504.13877", "abs": "https://arxiv.org/abs/2504.13877", "authors": ["Ionut Anghel", "Tudor Cioara", "Roberta Bevilacqua", "Federico Barbarossa", "Terje Grimstad", "Riitta Hellman", "Arnor Solberg", "Lars Thomas Boye", "Ovidiu Anchidin", "Ancuta Nemes", "Camilla Gabrielsen"], "title": "New care pathways for supporting transitional care from hospitals to home using AI and personalized digital assistance", "categories": ["cs.HC", "cs.AI"], "comment": "submitted to journal (under review)", "summary": "Transitional care may play a vital role for the sustainability of Europe\nfuture healthcare system, offering solutions for relocating patient care from\nhospital to home therefore addressing the growing demand for medical care as\nthe population is ageing. However, to be effective, it is essential to\nintegrate innovative Information and Communications Technology technologies to\nensure that patients with comorbidities experience a smooth and coordinated\ntransition from hospitals or care centers to home, thereby reducing the risk of\nrehospitalization. In this paper, we present an overview of the integration of\nInternet of Things, artificial intelligence, and digital assistance\ntechnologies with traditional care pathways to address the challenges and needs\nof healthcare systems in Europe. We identify the current gaps in transitional\ncare and define the technology mapping to enhance the care pathways, aiming to\nimprove patient outcomes, safety, and quality of life avoiding hospital\nreadmissions. Finally, we define the trial setup and evaluation methodology\nneeded to provide clinical evidence that supports the positive impact of\ntechnology integration on patient care and discuss the potential effects on the\nhealthcare system."}
{"id": "2504.14491", "pdf": "https://arxiv.org/pdf/2504.14491", "abs": "https://arxiv.org/abs/2504.14491", "authors": ["Shang Zhang", "Xiaobo Ding", "Huanbin Zhang", "Ruoyan Xiong", "Yue Zhang"], "title": "STARS: Sparse Learning Correlation Filter with Spatio-temporal Regularization and Super-resolution Reconstruction for Thermal Infrared Target Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Thermal infrared (TIR) target tracking methods often adopt the correlation\nfilter (CF) framework due to its computational efficiency. However, the low\nresolution of TIR images, along with tracking interference, significantly\nlimits the perfor-mance of TIR trackers. To address these challenges, we\nintroduce STARS, a novel sparse learning-based CF tracker that incorporates\nspatio-temporal regulari-zation and super-resolution reconstruction. First, we\napply adaptive sparse filter-ing and temporal domain filtering to extract key\nfeatures of the target while reduc-ing interference from background clutter and\nnoise. Next, we introduce an edge-preserving sparse regularization method to\nstabilize target features and prevent excessive blurring. This regularization\nintegrates multiple terms and employs the alternating direction method of\nmultipliers to optimize the solution. Finally, we propose a gradient-enhanced\nsuper-resolution method to extract fine-grained TIR target features and improve\nthe resolution of TIR images, addressing performance degradation in tracking\ncaused by low-resolution sequences. To the best of our knowledge, STARS is the\nfirst to integrate super-resolution methods within a sparse learning-based CF\nframework. Extensive experiments on the LSOTB-TIR, PTB-TIR, VOT-TIR2015, and\nVOT-TIR2017 benchmarks demonstrate that STARS outperforms state-of-the-art\ntrackers in terms of robustness."}
{"id": "2504.15219", "pdf": "https://arxiv.org/pdf/2504.15219", "abs": "https://arxiv.org/abs/2504.15219", "authors": ["Manya Wadhwa", "Zayne Sprague", "Chaitanya Malaviya", "Philippe Laban", "Junyi Jessy Li", "Greg Durrett"], "title": "EvalAgent: Discovering Implicit Evaluation Criteria from the Web", "categories": ["cs.CL"], "comment": null, "summary": "Evaluation of language model outputs on structured writing tasks is typically\nconducted with a number of desirable criteria presented to human evaluators or\nlarge language models (LLMs). For instance, on a prompt like \"Help me draft an\nacademic talk on coffee intake vs research productivity\", a model response may\nbe evaluated for criteria like accuracy and coherence. However, high-quality\nresponses should do more than just satisfy basic task requirements. An\neffective response to this query should include quintessential features of an\nacademic talk, such as a compelling opening, clear research questions, and a\ntakeaway. To help identify these implicit criteria, we introduce EvalAgent, a\nnovel framework designed to automatically uncover nuanced and task-specific\ncriteria. EvalAgent first mines expert-authored online guidance. It then uses\nthis evidence to propose diverse, long-tail evaluation criteria that are\ngrounded in reliable external sources. Our experiments demonstrate that the\ngrounded criteria produced by EvalAgent are often implicit (not directly stated\nin the user's prompt), yet specific (high degree of lexical precision).\nFurther, EvalAgent criteria are often not satisfied by initial responses but\nthey are actionable, such that responses can be refined to satisfy them.\nFinally, we show that combining LLM-generated and EvalAgent criteria uncovers\nmore human-valued criteria than using LLMs alone."}
{"id": "2504.13884", "pdf": "https://arxiv.org/pdf/2504.13884", "abs": "https://arxiv.org/abs/2504.13884", "authors": ["Karan Taneja", "Anjali Singh", "Ashok K. Goel"], "title": "Towards a Multimodal Document-grounded Conversational AI System for Education", "categories": ["cs.HC", "cs.AI", "cs.CV"], "comment": "15 pages, 4 figures, AIED 2025", "summary": "Multimedia learning using text and images has been shown to improve learning\noutcomes compared to text-only instruction. But conversational AI systems in\neducation predominantly rely on text-based interactions while multimodal\nconversations for multimedia learning remain unexplored. Moreover, deploying\nconversational AI in learning contexts requires grounding in reliable sources\nand verifiability to create trust. We present MuDoC, a Multimodal\nDocument-grounded Conversational AI system based on GPT-4o, that leverages both\ntext and visuals from documents to generate responses interleaved with text and\nimages. Its interface allows verification of AI generated content through\nseamless navigation to the source. We compare MuDoC to a text-only system to\nexplore differences in learner engagement, trust in AI system, and their\nperformance on problem-solving tasks. Our findings indicate that both visuals\nand verifiability of content enhance learner engagement and foster trust;\nhowever, no significant impact in performance was observed. We draw upon\ntheories from cognitive and learning sciences to interpret the findings and\nderive implications, and outline future directions for the development of\nmultimodal conversational AI systems in education."}
{"id": "2504.14509", "pdf": "https://arxiv.org/pdf/2504.14509", "abs": "https://arxiv.org/abs/2504.14509", "authors": ["Fulong Ye", "Miao Hua", "Pengze Zhang", "Xinghui Li", "Qichao Sun", "Songtao Zhao", "Qian He", "Xinglong Wu"], "title": "DreamID: High-Fidelity and Fast diffusion-based Face Swapping via Triplet ID Group Learning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In this paper, we introduce DreamID, a diffusion-based face swapping model\nthat achieves high levels of ID similarity, attribute preservation, image\nfidelity, and fast inference speed. Unlike the typical face swapping training\nprocess, which often relies on implicit supervision and struggles to achieve\nsatisfactory results. DreamID establishes explicit supervision for face\nswapping by constructing Triplet ID Group data, significantly enhancing\nidentity similarity and attribute preservation. The iterative nature of\ndiffusion models poses challenges for utilizing efficient image-space loss\nfunctions, as performing time-consuming multi-step sampling to obtain the\ngenerated image during training is impractical. To address this issue, we\nleverage the accelerated diffusion model SD Turbo, reducing the inference steps\nto a single iteration, enabling efficient pixel-level end-to-end training with\nexplicit Triplet ID Group supervision. Additionally, we propose an improved\ndiffusion-based model architecture comprising SwapNet, FaceNet, and ID Adapter.\nThis robust architecture fully unlocks the power of the Triplet ID Group\nexplicit supervision. Finally, to further extend our method, we explicitly\nmodify the Triplet ID Group data during training to fine-tune and preserve\nspecific attributes, such as glasses and face shape. Extensive experiments\ndemonstrate that DreamID outperforms state-of-the-art methods in terms of\nidentity similarity, pose and expression preservation, and image fidelity.\nOverall, DreamID achieves high-quality face swapping results at 512*512\nresolution in just 0.6 seconds and performs exceptionally well in challenging\nscenarios such as complex lighting, large angles, and occlusions."}
{"id": "2504.15220", "pdf": "https://arxiv.org/pdf/2504.15220", "abs": "https://arxiv.org/abs/2504.15220", "authors": ["Julián Cendrero", "Julio Gonzalo", "Ivar Zapata"], "title": "Fully Bayesian Approaches to Topics over Time", "categories": ["cs.CL", "cs.LG"], "comment": "25 pages", "summary": "The Topics over Time (ToT) model captures thematic changes in timestamped\ndatasets by explicitly modeling publication dates jointly with word\nco-occurrence patterns. However, ToT was not approached in a fully Bayesian\nfashion, a flaw that makes it susceptible to stability problems. To address\nthis issue, we propose a fully Bayesian Topics over Time (BToT) model via the\nintroduction of a conjugate prior to the Beta distribution. This prior acts as\na regularization that prevents the online version of the algorithm from\nunstable updates when a topic is poorly represented in a mini-batch. The\ncharacteristics of this prior to the Beta distribution are studied here for the\nfirst time. Still, this model suffers from a difference in scale between the\nsingle-time observations and the multiplicity of words per document. A\nvariation of BToT, Weighted Bayesian Topics over Time (WBToT), is proposed as a\nsolution. In WBToT, publication dates are repeated a certain number of times\nper document, which balances the relative influence of words and timestamps\nalong the inference process. We have tested our models on two datasets: a\ncollection of over 200 years of US state-of-the-union (SOTU) addresses and a\nlarge-scale COVID-19 Twitter corpus of 10 million tweets. The results show that\nWBToT captures events better than Latent Dirichlet Allocation and other SOTA\ntopic models like BERTopic: the median absolute deviation of the topic presence\nover time is reduced by $51\\%$ and $34\\%$, respectively. Our experiments also\ndemonstrate the superior coherence of WBToT over BToT, which highlights the\nimportance of balancing the time and word modalities. Finally, we illustrate\nthe stability of the online optimization algorithm in WBToT, which allows the\napplication of WBToT to problems that are intractable for standard ToT."}
{"id": "2504.13888", "pdf": "https://arxiv.org/pdf/2504.13888", "abs": "https://arxiv.org/abs/2504.13888", "authors": ["Paul Taele", "Jung In Koh", "Tracy Hammond"], "title": "Kanji Workbook: A Writing-Based Intelligent Tutoring System for Learning Proper Japanese Kanji Writing Technique with Instructor-Emulated Assessment", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Kanji script writing is a skill that is often introduced to novice Japanese\nforeign language students for achieving Japanese writing mastery, but often\nposes difficulties to students with primarily English fluency due to their its\nvast differences with written English. Instructors often introduce various\npedagogical methods -- such as visual structure and written techniques -- to\nassist students in kanji study, but may lack availability providing direct\nfeedback on students' writing outside of class. Current educational\napplications are also limited due to lacking richer instructor-emulated\nfeedback. We introduce Kanji Workbook, a writing-based intelligent tutoring\nsystem for students to receive intelligent assessment that emulates human\ninstructor feedback. Our interface not only leverages students' computing\ndevices for allowing them to learn, practice, and review the writing of\nprompted characters from their course's kanji script lessons, but also provides\na diverse set of writing assessment metrics -- derived from instructor\ninterviews and classroom observation insights -- through intelligent scoring\nand visual animations. We deployed our interface onto novice- and\nintermediate-level university courses over an entire academic year, and\nobserved that interface users on average achieved higher course grades than\ntheir peers and also reacted positively to our interface's various features."}
{"id": "2504.14516", "pdf": "https://arxiv.org/pdf/2504.14516", "abs": "https://arxiv.org/abs/2504.14516", "authors": ["Weirong Chen", "Ganlin Zhang", "Felix Wimbauer", "Rui Wang", "Nikita Araslanov", "Andrea Vedaldi", "Daniel Cremers"], "title": "Back on Track: Bundle Adjustment for Dynamic Scene Reconstruction", "categories": ["cs.CV"], "comment": "Project page: https://wrchen530.github.io/projects/batrack/", "summary": "Traditional SLAM systems, which rely on bundle adjustment, struggle with\nhighly dynamic scenes commonly found in casual videos. Such videos entangle the\nmotion of dynamic elements, undermining the assumption of static environments\nrequired by traditional systems. Existing techniques either filter out dynamic\nelements or model their motion independently. However, the former often results\nin incomplete reconstructions, whereas the latter can lead to inconsistent\nmotion estimates. Taking a novel approach, this work leverages a 3D point\ntracker to separate the camera-induced motion from the observed motion of\ndynamic objects. By considering only the camera-induced component, bundle\nadjustment can operate reliably on all scene elements as a result. We further\nensure depth consistency across video frames with lightweight post-processing\nbased on scale maps. Our framework combines the core of traditional SLAM --\nbundle adjustment -- with a robust learning-based 3D tracker front-end.\nIntegrating motion decomposition, bundle adjustment and depth refinement, our\nunified framework, BA-Track, accurately tracks the camera motion and produces\ntemporally coherent and scale-consistent dense reconstructions, accommodating\nboth static and dynamic elements. Our experiments on challenging datasets\nreveal significant improvements in camera pose estimation and 3D reconstruction\naccuracy."}
{"id": "2504.15236", "pdf": "https://arxiv.org/pdf/2504.15236", "abs": "https://arxiv.org/abs/2504.15236", "authors": ["Saffron Huang", "Esin Durmus", "Miles McCain", "Kunal Handa", "Alex Tamkin", "Jerry Hong", "Michael Stern", "Arushi Somani", "Xiuruo Zhang", "Deep Ganguli"], "title": "Values in the Wild: Discovering and Analyzing Values in Real-World Language Model Interactions", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": "44 pages", "summary": "AI assistants can impart value judgments that shape people's decisions and\nworldviews, yet little is known empirically about what values these systems\nrely on in practice. To address this, we develop a bottom-up,\nprivacy-preserving method to extract the values (normative considerations\nstated or demonstrated in model responses) that Claude 3 and 3.5 models exhibit\nin hundreds of thousands of real-world interactions. We empirically discover\nand taxonomize 3,307 AI values and study how they vary by context. We find that\nClaude expresses many practical and epistemic values, and typically supports\nprosocial human values while resisting values like \"moral nihilism\". While some\nvalues appear consistently across contexts (e.g. \"transparency\"), many are more\nspecialized and context-dependent, reflecting the diversity of human\ninterlocutors and their varied contexts. For example, \"harm prevention\" emerges\nwhen Claude resists users, \"historical accuracy\" when responding to queries\nabout controversial events, \"healthy boundaries\" when asked for relationship\nadvice, and \"human agency\" in technology ethics discussions. By providing the\nfirst large-scale empirical mapping of AI values in deployment, our work\ncreates a foundation for more grounded evaluation and design of values in AI\nsystems."}
{"id": "2504.13889", "pdf": "https://arxiv.org/pdf/2504.13889", "abs": "https://arxiv.org/abs/2504.13889", "authors": ["Paul Taele", "Laura Barreto", "Tracy Hammond"], "title": "Maestoso: An Intelligent Educational Sketching Tool for Learning Music Theory", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Learning music theory not only has practical benefits for musicians to write,\nperform, understand, and express music better, but also for both non-musicians\nto improve critical thinking, math analytical skills, and music appreciation.\nHowever, current external tools applicable for learning music theory through\nwriting when human instruction is unavailable are either limited in feedback,\nlacking a written modality, or assuming already strong familiarity of music\ntheory concepts. In this paper, we describe Maestoso, an educational tool for\nnovice learners to learn music theory through sketching practice of quizzed\nmusic structures. Maestoso first automatically recognizes students' sketched\ninput of quizzed concepts, then relies on existing sketch and gesture\nrecognition techniques to automatically recognize the input, and finally\ngenerates instructor-emulated feedback. From our evaluations, we demonstrate\nthat Maestoso performs reasonably well on recognizing music structure elements\nand that novice students can comfortably grasp introductory music theory in a\nsingle session."}
{"id": "2504.14526", "pdf": "https://arxiv.org/pdf/2504.14526", "abs": "https://arxiv.org/abs/2504.14526", "authors": ["Tong Zeng", "Longfeng Wu", "Liang Shi", "Dawei Zhou", "Feng Guo"], "title": "Are Vision LLMs Road-Ready? A Comprehensive Benchmark for Safety-Critical Driving Video Understanding", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Vision Large Language Models (VLLMs) have demonstrated impressive\ncapabilities in general visual tasks such as image captioning and visual\nquestion answering. However, their effectiveness in specialized,\nsafety-critical domains like autonomous driving remains largely unexplored.\nAutonomous driving systems require sophisticated scene understanding in complex\nenvironments, yet existing multimodal benchmarks primarily focus on normal\ndriving conditions, failing to adequately assess VLLMs' performance in\nsafety-critical scenarios. To address this, we introduce DVBench, a pioneering\nbenchmark designed to evaluate the performance of VLLMs in understanding\nsafety-critical driving videos. Built around a hierarchical ability taxonomy\nthat aligns with widely adopted frameworks for describing driving scenarios\nused in assessing highly automated driving systems, DVBench features 10,000\nmultiple-choice questions with human-annotated ground-truth answers, enabling a\ncomprehensive evaluation of VLLMs' capabilities in perception and reasoning.\nExperiments on 14 SOTA VLLMs, ranging from 0.5B to 72B parameters, reveal\nsignificant performance gaps, with no model achieving over 40% accuracy,\nhighlighting critical limitations in understanding complex driving scenarios.\nTo probe adaptability, we fine-tuned selected models using domain-specific data\nfrom DVBench, achieving accuracy gains ranging from 5.24 to 10.94 percentage\npoints, with relative improvements of up to 43.59%. This improvement\nunderscores the necessity of targeted adaptation to bridge the gap between\ngeneral-purpose VLLMs and mission-critical driving applications. DVBench\nestablishes an essential evaluation framework and research roadmap for\ndeveloping VLLMs that meet the safety and robustness requirements for\nreal-world autonomous systems. We released the benchmark toolbox and the\nfine-tuned model at: https://github.com/tong-zeng/DVBench.git."}
{"id": "2504.15241", "pdf": "https://arxiv.org/pdf/2504.15241", "abs": "https://arxiv.org/abs/2504.15241", "authors": ["Yahan Yang", "Soham Dan", "Shuo Li", "Dan Roth", "Insup Lee"], "title": "MR. Guard: Multilingual Reasoning Guardrail using Curriculum Learning", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are susceptible to adversarial attacks such as\njailbreaking, which can elicit harmful or unsafe behaviors. This vulnerability\nis exacerbated in multilingual setting, where multilingual safety-aligned data\nare often limited. Thus, developing a guardrail capable of detecting and\nfiltering unsafe content across diverse languages is critical for deploying\nLLMs in real-world applications. In this work, we propose an approach to build\na multilingual guardrail with reasoning. Our method consists of: (1) synthetic\nmultilingual data generation incorporating culturally and linguistically\nnuanced variants, (2) supervised fine-tuning, and (3) a curriculum-guided Group\nRelative Policy Optimization (GRPO) framework that further improves\nperformance. Experimental results demonstrate that our multilingual guardrail\nconsistently outperforms recent baselines across both in-domain and\nout-of-domain languages. The multilingual reasoning capability of our guardrail\nenables it to generate multilingual explanations, which are particularly useful\nfor understanding language-specific risks and ambiguities in multilingual\ncontent moderation."}
{"id": "2504.13891", "pdf": "https://arxiv.org/pdf/2504.13891", "abs": "https://arxiv.org/abs/2504.13891", "authors": ["Wanfang Xu", "Lixiang Zhao", "Haiwen Song", "Xinheng Song", "Zhaolin Lu", "Yu Liu", "Min Chen", "Eng Gee Lim", "Lingyun Yu"], "title": "Mozualization: Crafting Music and Visual Representation with Multimodal AI", "categories": ["cs.HC", "cs.AI"], "comment": "7 pages, 5 figures, CHI2025", "summary": "In this work, we introduce Mozualization, a music generation and editing tool\nthat creates multi-style embedded music by integrating diverse inputs, such as\nkeywords, images, and sound clips (e.g., segments from various pieces of music\nor even a playful cat's meow). Our work is inspired by the ways people express\ntheir emotions -- writing mood-descriptive poems or articles, creating drawings\nwith warm or cool tones, or listening to sad or uplifting music. Building on\nthis concept, we developed a tool that transforms these emotional expressions\ninto a cohesive and expressive song, allowing users to seamlessly incorporate\ntheir unique preferences and inspirations. To evaluate the tool and, more\nimportantly, gather insights for its improvement, we conducted a user study\ninvolving nine music enthusiasts. The study assessed user experience,\nengagement, and the impact of interacting with and listening to the generated\nmusic."}
{"id": "2504.14534", "pdf": "https://arxiv.org/pdf/2504.14534", "abs": "https://arxiv.org/abs/2504.14534", "authors": ["Liang Peng", "Boxi Wu", "Haoran Cheng", "Yibo Zhao", "Xiaofei He"], "title": "SUDO: Enhancing Text-to-Image Diffusion Models with Self-Supervised Direct Preference Optimization", "categories": ["cs.CV"], "comment": null, "summary": "Previous text-to-image diffusion models typically employ supervised\nfine-tuning (SFT) to enhance pre-trained base models. However, this approach\nprimarily minimizes the loss of mean squared error (MSE) at the pixel level,\nneglecting the need for global optimization at the image level, which is\ncrucial for achieving high perceptual quality and structural coherence. In this\npaper, we introduce Self-sUpervised Direct preference Optimization (SUDO), a\nnovel paradigm that optimizes both fine-grained details at the pixel level and\nglobal image quality. By integrating direct preference optimization into the\nmodel, SUDO generates preference image pairs in a self-supervised manner,\nenabling the model to prioritize global-level learning while complementing the\npixel-level MSE loss. As an effective alternative to supervised fine-tuning,\nSUDO can be seamlessly applied to any text-to-image diffusion model.\nImportantly, it eliminates the need for costly data collection and annotation\nefforts typically associated with traditional direct preference optimization\nmethods. Through extensive experiments on widely-used models, including Stable\nDiffusion 1.5 and XL, we demonstrate that SUDO significantly enhances both\nglobal and local image quality. The codes are provided at\n\\href{https://github.com/SPengLiang/SUDO}{this link}."}
{"id": "2504.15253", "pdf": "https://arxiv.org/pdf/2504.15253", "abs": "https://arxiv.org/abs/2504.15253", "authors": ["Yilun Zhou", "Austin Xu", "Peifeng Wang", "Caiming Xiong", "Shafiq Joty"], "title": "Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as Test-Time Scaling Evaluators", "categories": ["cs.CL", "cs.LG"], "comment": "The first two authors contributed equally. The codebase is at\n  https://github.com/SalesforceAIResearch/jetts-benchmark", "summary": "Scaling test-time computation, or affording a generator large language model\n(LLM) extra compute during inference, typically employs the help of external\nnon-generative evaluators (i.e., reward models). Concurrently, LLM-judges,\nmodels trained to generate evaluations and critiques (explanations) in natural\nlanguage, are becoming increasingly popular in automatic evaluation. Despite\njudge empirical successes, their effectiveness as evaluators in test-time\nscaling settings is largely unknown. In this paper, we introduce the Judge\nEvaluation for Test-Time Scaling (JETTS) benchmark, which evaluates judge\nperformance in three domains (math reasoning, code generation, and instruction\nfollowing) under three task settings: response reranking, step-level beam\nsearch, and critique-based response refinement. We evaluate 10 different judge\nmodels (7B-70B parameters) for 8 different base generator models (6.7B-72B\nparameters). Our benchmark shows that while judges are competitive with outcome\nreward models in reranking, they are consistently worse than process reward\nmodels in beam search procedures. Furthermore, though unique to LLM-judges,\ntheir natural language critiques are currently ineffective in guiding the\ngenerator towards better responses."}
{"id": "2504.13898", "pdf": "https://arxiv.org/pdf/2504.13898", "abs": "https://arxiv.org/abs/2504.13898", "authors": ["Dong Won Lee", "Yubin Kim", "Denison Guvenoz", "Sooyeon Jeong", "Parker Malachowsky", "Louis-Philippe Morency", "Cynthia Breazeal", "Hae Won Park"], "title": "The Human Robot Social Interaction (HSRI) Dataset: Benchmarking Foundational Models' Social Reasoning", "categories": ["cs.HC", "cs.AI"], "comment": "23 pages, 11 figures", "summary": "Our work aims to advance the social reasoning of embodied artificial\nintelligence (AI) agents in real-world social interactions. Recently, language\nmodels (LMs) and foundational models (FMs) are being utilized as automatic\nevaluators of human-AI interactions with the goal of eventually being used to\nimprove the policy of the AI agent. To enable further research in this\ndirection, we introduce a large-scale real-world Human Robot Social Interaction\n(HSRI) Dataset to benchmark the capabilities of LMs and FMs to identify and\nreason about social interactions, specifically with regard to robot social\nerrors and competencies . Our dataset consists of 400 real-world human social\nrobot interaction videos and over 10K annotations, detailing the robot's social\nerrors, competencies, rationale, and corrective actions, capturing unique\naspects of human-AI interaction only present in real-world interactions. To\nfurther assess AI models' ability to reason about social interactions, we\npropose eight new benchmark tasks for evaluating centered around whether AI\nmodels can (1) evaluate social interactions via detecting social errors and\ncompetencies, (2) identify the explanatory factors associated to errors and\ncompetencies, (3) understand the flow of real-world social interactions, and\n(4) provide reasons and corrective actions for social errors. Human studies and\nexperiments with modern LMs and FMs reveal that current models struggle with\nthese tasks, demonstrating that our dataset and benchmark provides a step\nforward towards socially intelligent AI."}
{"id": "2504.14535", "pdf": "https://arxiv.org/pdf/2504.14535", "abs": "https://arxiv.org/abs/2504.14535", "authors": ["Kuanting Wu", "Kei Ota", "Asako Kanezaki"], "title": "FlowLoss: Dynamic Flow-Conditioned Loss Strategy for Video Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Video Diffusion Models (VDMs) can generate high-quality videos, but often\nstruggle with producing temporally coherent motion. Optical flow supervision is\na promising approach to address this, with prior works commonly employing\nwarping-based strategies that avoid explicit flow matching. In this work, we\nexplore an alternative formulation, FlowLoss, which directly compares flow\nfields extracted from generated and ground-truth videos. To account for the\nunreliability of flow estimation under high-noise conditions in diffusion, we\npropose a noise-aware weighting scheme that modulates the flow loss across\ndenoising steps. Experiments on robotic video datasets suggest that FlowLoss\nimproves motion stability and accelerates convergence in early training stages.\nOur findings offer practical insights for incorporating motion-based\nsupervision into noise-conditioned generative models."}
{"id": "2504.13847", "pdf": "https://arxiv.org/pdf/2504.13847", "abs": "https://arxiv.org/abs/2504.13847", "authors": ["Zhe Liu"], "title": "Interview AI-ssistant: Designing for Real-Time Human-AI Collaboration in Interview Preparation and Execution", "categories": ["cs.HC", "cs.CL"], "comment": "4 pages, 2 figures, submitted and accepted by IUI 2025 Doctoral\n  Consortium", "summary": "Recent advances in large language models (LLMs) offer unprecedented\nopportunities to enhance human-AI collaboration in qualitative research\nmethods, including interviews. While interviews are highly valued for gathering\ndeep, contextualized insights, interviewers often face significant cognitive\nchallenges, such as real-time information processing, question adaptation, and\nrapport maintenance. My doctoral research introduces Interview AI-ssistant, a\nsystem designed for real-time interviewer-AI collaboration during both the\npreparation and execution phases. Through four interconnected studies, this\nresearch investigates the design of effective human-AI collaboration in\ninterviewing contexts, beginning with a formative study of interviewers' needs,\nfollowed by a prototype development study focused on AI-assisted interview\npreparation, an experimental evaluation of real-time AI assistance during\ninterviews, and a field study deploying the system in a real-world research\nsetting. Beyond informing practical implementations of intelligent interview\nsupport systems, this work contributes to the Intelligent User Interfaces (IUI)\ncommunity by advancing the understanding of human-AI collaborative interfaces\nin complex social tasks and establishing design guidelines for AI-enhanced\nqualitative research tools."}
{"id": "2504.13899", "pdf": "https://arxiv.org/pdf/2504.13899", "abs": "https://arxiv.org/abs/2504.13899", "authors": ["Marharyta Domnich", "Rasmus Moorits Veski", "Julius Välja", "Kadi Tulver", "Raul Vicente"], "title": "Predicting Satisfaction of Counterfactual Explanations from Human Ratings of Explanatory Qualities", "categories": ["cs.HC", "cs.AI"], "comment": "This work has been accepted to The 3rd World Conference on\n  eXplainable Artificial Intelligence (xAI 2025), July 9-11, 2025 - Istanbul,\n  Turkey", "summary": "Counterfactual explanations are a widely used approach in Explainable AI,\noffering actionable insights into decision-making by illustrating how small\nchanges to input data can lead to different outcomes. Despite their importance,\nevaluating the quality of counterfactual explanations remains an open problem.\nTraditional quantitative metrics, such as sparsity or proximity, fail to fully\naccount for human preferences in explanations, while user studies are\ninsightful but not scalable. Moreover, relying only on a single overall\nsatisfaction rating does not lead to a nuanced understanding of why certain\nexplanations are effective or not. To address this, we analyze a dataset of\ncounterfactual explanations that were evaluated by 206 human participants, who\nrated not only overall satisfaction but also seven explanatory criteria:\nfeasibility, coherence, complexity, understandability, completeness, fairness,\nand trust. Modeling overall satisfaction as a function of these criteria, we\nfind that feasibility (the actionability of suggested changes) and trust (the\nbelief that the changes would lead to the desired outcome) consistently stand\nout as the strongest predictors of user satisfaction, though completeness also\nemerges as a meaningful contributor. Crucially, even excluding feasibility and\ntrust, other metrics explain 58% of the variance, highlighting the importance\nof additional explanatory qualities. Complexity appears independent, suggesting\nmore detailed explanations do not necessarily reduce satisfaction. Strong\nmetric correlations imply a latent structure in how users judge quality, and\ndemographic background significantly shapes ranking patterns. These insights\ninform the design of counterfactual algorithms that adapt explanatory qualities\nto user expertise and domain context."}
{"id": "2504.14548", "pdf": "https://arxiv.org/pdf/2504.14548", "abs": "https://arxiv.org/abs/2504.14548", "authors": ["Lifeng Lin", "Rongfeng Lu", "Quan Chen", "Haofan Ren", "Ming Lu", "Yaoqi Sun", "Chenggang Yan", "Anke Xue"], "title": "VGNC: Reducing the Overfitting of Sparse-view 3DGS via Validation-guided Gaussian Number Control", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages,8 figures", "summary": "Sparse-view 3D reconstruction is a fundamental yet challenging task in\npractical 3D reconstruction applications. Recently, many methods based on the\n3D Gaussian Splatting (3DGS) framework have been proposed to address\nsparse-view 3D reconstruction. Although these methods have made considerable\nadvancements, they still show significant issues with overfitting. To reduce\nthe overfitting, we introduce VGNC, a novel Validation-guided Gaussian Number\nControl (VGNC) approach based on generative novel view synthesis (NVS) models.\nTo the best of our knowledge, this is the first attempt to alleviate the\noverfitting issue of sparse-view 3DGS with generative validation images.\nSpecifically, we first introduce a validation image generation method based on\na generative NVS model. We then propose a Gaussian number control strategy that\nutilizes generated validation images to determine the optimal Gaussian numbers,\nthereby reducing the issue of overfitting. We conducted detailed experiments on\nvarious sparse-view 3DGS baselines and datasets to evaluate the effectiveness\nof VGNC. Extensive experiments show that our approach not only reduces\noverfitting but also improves rendering quality on the test set while\ndecreasing the number of Gaussian points. This reduction lowers storage demands\nand accelerates both training and rendering. The code will be released."}
{"id": "2504.13861", "pdf": "https://arxiv.org/pdf/2504.13861", "abs": "https://arxiv.org/abs/2504.13861", "authors": ["Ivan Sviridov", "Amina Miftakhova", "Artemiy Tereshchenko", "Galina Zubkova", "Pavel Blinov", "Andrey Savchenko"], "title": "3MDBench: Medical Multimodal Multi-agent Dialogue Benchmark", "categories": ["cs.HC", "cs.CL", "cs.MA", "68T42", "I.2.1"], "comment": "26 pages, 8 figures, 7 tables", "summary": "Large Vision-Language Models (LVLMs) are increasingly being explored for\napplications in telemedicine, yet their ability to engage with diverse patient\nbehaviors remains underexplored. We introduce 3MDBench (Medical Multimodal\nMulti-agent Dialogue Benchmark), an open-source evaluation framework designed\nto assess LLM-driven medical consultations. Unlike existing benchmarks,\n3MDBench simulates real-world patient variability by incorporating four\ntemperament-driven Patient Agents and an Assessor Agent that evaluates\ndiagnostic accuracy and dialogue quality. The benchmark integrates textual and\nimage-based patient data across 34 common diagnoses, mirroring real-world\ntelemedicine interactions. Under different diagnostic strategies, we evaluate\nstate-of-the-art LVLMs. Our findings demonstrate that incorporating dialogue\nimproves the F1 score from 50.4 to 54.2 compared to non-dialogue settings,\nunderscoring the value of context-driven, information-seeking questioning.\nAdditionally, we demonstrate that multimodal inputs enhance diagnostic\nefficiency. Image-supported models outperform text-only counterparts by raising\nthe diagnostic F1 score from 52.8 to 54.2 in a similar dialogue setting.\nFinally, we suggest an approach that improves the diagnostic F1-score to 70.3\nby training the CNN model on the diagnosis prediction task and incorporating\nits top-3 predictions into the LVLM context. 3MDBench provides a reproducible\nand extendable evaluation framework for AI-driven medical assistants. It offers\ninsights into how patient temperament, dialogue strategies, and multimodal\nreasoning influence diagnosis quality. By addressing real-world complexities in\ntelemedicine, our benchmark paves the way for more empathetic, reliable, and\ncontext-aware AI-driven healthcare solutions. The source code of our benchmark\nis publicly available: https://github.com/univanxx/3mdbench"}
{"id": "2504.13900", "pdf": "https://arxiv.org/pdf/2504.13900", "abs": "https://arxiv.org/abs/2504.13900", "authors": ["Yue Fu", "Alexis Hiniker"], "title": "Supporting Students' Reading and Cognition with AI", "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": null, "summary": "With the rapid adoption of AI tools in learning contexts, it is vital to\nunderstand how these systems shape users' reading processes and cognitive\nengagement. We collected and analyzed text from 124 sessions with AI tools, in\nwhich students used these tools to support them as they read assigned readings\nfor an undergraduate course. We categorized participants' prompts to AI\naccording to Bloom's Taxonomy of educational objectives -- Remembering,\nUnderstanding, Applying, Analyzing, Evaluating. Our results show that\n``Analyzing'' and ``Evaluating'' are more prevalent in users' second and third\nprompts within a single usage session, suggesting a shift toward higher-order\nthinking. However, in reviewing users' engagement with AI tools over several\nweeks, we found that users converge toward passive reading engagement over\ntime. Based on these results, we propose design implications for future AI\nreading-support systems, including structured scaffolds for lower-level\ncognitive tasks (e.g., recalling terms) and proactive prompts that encourage\nhigher-order thinking (e.g., analyzing, applying, evaluating). Additionally, we\nadvocate for adaptive, human-in-the-loop features that allow students and\ninstructors to tailor their reading experiences with AI, balancing efficiency\nwith enriched cognitive engagement. Our paper expands the dialogue on\nintegrating AI into academic reading, highlighting both its potential benefits\nand challenges."}
{"id": "2504.14553", "pdf": "https://arxiv.org/pdf/2504.14553", "abs": "https://arxiv.org/abs/2504.14553", "authors": ["Weijun Zhuang", "Qizhang Li", "Xin Li", "Ming Liu", "Xiaopeng Hong", "Feng Gao", "Fan Yang", "Wangmeng Zuo"], "title": "Grounding-MD: Grounded Video-language Pre-training for Open-World Moment Detection", "categories": ["cs.CV"], "comment": null, "summary": "Temporal Action Detection and Moment Retrieval constitute two pivotal tasks\nin video understanding, focusing on precisely localizing temporal segments\ncorresponding to specific actions or events. Recent advancements introduced\nMoment Detection to unify these two tasks, yet existing approaches remain\nconfined to closed-set scenarios, limiting their applicability in open-world\ncontexts. To bridge this gap, we present Grounding-MD, an innovative, grounded\nvideo-language pre-training framework tailored for open-world moment detection.\nOur framework incorporates an arbitrary number of open-ended natural language\nqueries through a structured prompt mechanism, enabling flexible and scalable\nmoment detection. Grounding-MD leverages a Cross-Modality Fusion Encoder and a\nText-Guided Fusion Decoder to facilitate comprehensive video-text alignment and\nenable effective cross-task collaboration. Through large-scale pre-training on\ntemporal action detection and moment retrieval datasets, Grounding-MD\ndemonstrates exceptional semantic representation learning capabilities,\neffectively handling diverse and complex query conditions. Comprehensive\nevaluations across four benchmark datasets including ActivityNet, THUMOS14,\nActivityNet-Captions, and Charades-STA demonstrate that Grounding-MD\nestablishes new state-of-the-art performance in zero-shot and supervised\nsettings in open-world moment detection scenarios. All source code and trained\nmodels will be released."}
{"id": "2504.13865", "pdf": "https://arxiv.org/pdf/2504.13865", "abs": "https://arxiv.org/abs/2504.13865", "authors": ["Fei Tang", "Haolei Xu", "Hang Zhang", "Siqi Chen", "Xingyu Wu", "Yongliang Shen", "Wenqi Zhang", "Guiyang Hou", "Zeqi Tan", "Yuchen Yan", "Kaitao Song", "Jian Shao", "Weiming Lu", "Jun Xiao", "Yueting Zhuang"], "title": "A Survey on (M)LLM-Based GUI Agents", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Graphical User Interface (GUI) Agents have emerged as a transformative\nparadigm in human-computer interaction, evolving from rule-based automation\nscripts to sophisticated AI-driven systems capable of understanding and\nexecuting complex interface operations. This survey provides a comprehensive\nexamination of the rapidly advancing field of LLM-based GUI Agents,\nsystematically analyzing their architectural foundations, technical components,\nand evaluation methodologies. We identify and analyze four fundamental\ncomponents that constitute modern GUI Agents: (1) perception systems that\nintegrate text-based parsing with multimodal understanding for comprehensive\ninterface comprehension; (2) exploration mechanisms that construct and maintain\nknowledge bases through internal modeling, historical experience, and external\ninformation retrieval; (3) planning frameworks that leverage advanced reasoning\nmethodologies for task decomposition and execution; and (4) interaction systems\nthat manage action generation with robust safety controls. Through rigorous\nanalysis of these components, we reveal how recent advances in large language\nmodels and multimodal learning have revolutionized GUI automation across\ndesktop, mobile, and web platforms. We critically examine current evaluation\nframeworks, highlighting methodological limitations in existing benchmarks\nwhile proposing directions for standardization. This survey also identifies key\ntechnical challenges, including accurate element localization, effective\nknowledge retrieval, long-horizon planning, and safety-aware execution control,\nwhile outlining promising research directions for enhancing GUI Agents'\ncapabilities. Our systematic review provides researchers and practitioners with\na thorough understanding of the field's current state and offers insights into\nfuture developments in intelligent interface automation."}
{"id": "2504.13904", "pdf": "https://arxiv.org/pdf/2504.13904", "abs": "https://arxiv.org/abs/2504.13904", "authors": ["Donghuo Zeng", "Roberto Legaspi", "Yuewen Sun", "Xinshuai Dong", "Kazushi Ikeda", "Peter Spirtes", "Kun Zhang"], "title": "Generative Framework for Personalized Persuasion: Inferring Causal, Counterfactual, and Latent Knowledge", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "12 pages, 10 figures, 1 table. Accepted by ACM UMAP 2025", "summary": "We hypothesize that optimal system responses emerge from adaptive strategies\ngrounded in causal and counterfactual knowledge. Counterfactual inference\nallows us to create hypothetical scenarios to examine the effects of\nalternative system responses. We enhance this process through causal discovery,\nwhich identifies the strategies informed by the underlying causal structure\nthat govern system behaviors. Moreover, we consider the psychological\nconstructs and unobservable noises that might be influencing user-system\ninteractions as latent factors. We show that these factors can be effectively\nestimated. We employ causal discovery to identify strategy-level causal\nrelationships among user and system utterances, guiding the generation of\npersonalized counterfactual dialogues. We model the user utterance strategies\nas causal factors, enabling system strategies to be treated as counterfactual\nactions. Furthermore, we optimize policies for selecting system responses based\non counterfactual data. Our results using a real-world dataset on social good\ndemonstrate significant improvements in persuasive system outcomes, with\nincreased cumulative rewards validating the efficacy of causal discovery in\nguiding personalized counterfactual inference and optimizing dialogue policies\nfor a persuasive dialogue system."}
{"id": "2504.14566", "pdf": "https://arxiv.org/pdf/2504.14566", "abs": "https://arxiv.org/abs/2504.14566", "authors": ["Shang Zhang", "HuiPan Guan", "XiaoBo Ding", "Ruoyan Xiong", "Yue Zhang"], "title": "SMTT: Novel Structured Multi-task Tracking with Graph-Regularized Sparse Representation for Robust Thermal Infrared Target Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Thermal infrared target tracking is crucial in applications such as\nsurveillance, autonomous driving, and military operations. In this paper, we\npropose a novel tracker, SMTT, which effectively addresses common challenges in\nthermal infrared imagery, such as noise, occlusion, and rapid target motion, by\nleveraging multi-task learning, joint sparse representation, and adaptive graph\nregularization. By reformulating the tracking task as a multi-task learning\nproblem, the SMTT tracker independently optimizes the representation of each\nparticle while dynamically capturing spatial and feature-level similarities\nusing a weighted mixed-norm regularization strategy. To ensure real-time\nperformance, we incorporate the Accelerated Proximal Gradient method for\nefficient optimization. Extensive experiments on benchmark datasets - including\nVOT-TIR, PTB-TIR, and LSOTB-TIR - demonstrate that SMTT achieves superior\naccuracy, robustness, and computational efficiency. These results highlight\nSMTT as a reliable and high-performance solution for thermal infrared target\ntracking in complex environments."}
{"id": "2504.13882", "pdf": "https://arxiv.org/pdf/2504.13882", "abs": "https://arxiv.org/abs/2504.13882", "authors": ["Megan Gu", "Chloe Qianhui Zhao", "Claire Liu", "Nikhil Patel", "Jahnvi Shah", "Jionghao Lin", "Kenneth R. Koedinger"], "title": "Toward Automated Qualitative Analysis: Leveraging Large Language Models for Tutoring Dialogue Evaluation", "categories": ["cs.HC", "cs.CL"], "comment": "Manuscript accepted to the Workshop on \"From Data to Discovery: LLMs\n  for Qualitative Analysis in Education\" at LAK25", "summary": "Our study introduces an automated system leveraging large language models\n(LLMs) to assess the effectiveness of five key tutoring strategies: 1. giving\neffective praise, 2. reacting to errors, 3. determining what students know, 4.\nhelping students manage inequity, and 5. responding to negative self-talk.\nUsing a public dataset from the Teacher-Student Chatroom Corpus, our system\nclassifies each tutoring strategy as either being employed as desired or\nundesired. Our study utilizes GPT-3.5 with few-shot prompting to assess the use\nof these strategies and analyze tutoring dialogues. The results show that for\nthe five tutoring strategies, True Negative Rates (TNR) range from 0.655 to\n0.738, and Recall ranges from 0.327 to 0.432, indicating that the model is\neffective at excluding incorrect classifications but struggles to consistently\nidentify the correct strategy. The strategy \\textit{helping students manage\ninequity} showed the highest performance with a TNR of 0.738 and Recall of\n0.432. The study highlights the potential of LLMs in tutoring strategy analysis\nand outlines directions for future improvements, including incorporating more\nadvanced models for more nuanced feedback."}
{"id": "2504.13908", "pdf": "https://arxiv.org/pdf/2504.13908", "abs": "https://arxiv.org/abs/2504.13908", "authors": ["Soubhik Barari", "Jarret Angbazo", "Natalie Wang", "Leah M. Christian", "Elizabeth Dean", "Zoe Slowinski", "Brandon Sepulvado"], "title": "AI-Assisted Conversational Interviewing: Effects on Data Quality and User Experience", "categories": ["cs.HC", "cs.AI", "stat.AP"], "comment": null, "summary": "Standardized surveys scale efficiently but sacrifice depth, while\nconversational interviews improve response quality at the cost of scalability\nand consistency. This study bridges the gap between these methods by\nintroducing a framework for AI-assisted conversational interviewing. To\nevaluate this framework, we conducted a web survey experiment where 1,800\nparticipants were randomly assigned to text-based conversational AI agents, or\n\"textbots\", to dynamically probe respondents for elaboration and interactively\ncode open-ended responses. We assessed textbot performance in terms of coding\naccuracy, response quality, and respondent experience. Our findings reveal that\ntextbots perform moderately well in live coding even without survey-specific\nfine-tuning, despite slightly inflated false positive errors due to respondent\nacquiescence bias. Open-ended responses were more detailed and informative, but\nthis came at a slight cost to respondent experience. Our findings highlight the\nfeasibility of using AI methods to enhance open-ended data collection in web\nsurveys."}
{"id": "2504.14582", "pdf": "https://arxiv.org/pdf/2504.14582", "abs": "https://arxiv.org/abs/2504.14582", "authors": ["Zheng Chen", "Kai Liu", "Jue Gong", "Jingkai Wang", "Lei Sun", "Zongwei Wu", "Radu Timofte", "Yulun Zhang", "Xiangyu Kong", "Xiaoxuan Yu", "Hyunhee Park", "Suejin Han", "Hakjae Jeon", "Dafeng Zhang", "Hyung-Ju Chun", "Donghun Ryou", "Inju Ha", "Bohyung Han", "Lu Zhao", "Yuyi Zhang", "Pengyu Yan", "Jiawei Hu", "Pengwei Liu", "Fengjun Guo", "Hongyuan Yu", "Pufan Xu", "Zhijuan Huang", "Shuyuan Cui", "Peng Guo", "Jiahui Liu", "Dongkai Zhang", "Heng Zhang", "Huiyuan Fu", "Huadong Ma", "Yanhui Guo", "Sisi Tian", "Xin Liu", "Jinwen Liang", "Jie Liu", "Jie Tang", "Gangshan Wu", "Zeyu Xiao", "Zhuoyuan Li", "Yinxiang Zhang", "Wenxuan Cai", "Vijayalaxmi Ashok Aralikatti", "Nikhil Akalwadi", "G Gyaneshwar Rao", "Chaitra Desai", "Ramesh Ashok Tabib", "Uma Mudenagudi", "Marcos V. Conde", "Alejandro Merino", "Bruno Longarela", "Javier Abad", "Weijun Yuan", "Zhan Li", "Zhanglu Chen", "Boyang Yao", "Aagam Jain", "Milan Kumar Singh", "Ankit Kumar", "Shubh Kawa", "Divyavardhan Singh", "Anjali Sarvaiya", "Kishor Upla", "Raghavendra Ramachandra", "Chia-Ming Lee", "Yu-Fan Lin", "Chih-Chung Hsu", "Risheek V Hiremath", "Yashaswini Palani", "Yuxuan Jiang", "Qiang Zhu", "Siyue Teng", "Fan Zhang", "Shuyuan Zhu", "Bing Zeng", "David Bull", "Jingwei Liao", "Yuqing Yang", "Wenda Shao", "Junyi Zhao", "Qisheng Xu", "Kele Xu", "Sunder Ali Khowaja", "Ik Hyun Lee", "Snehal Singh Tomar", "Rajarshi Ray", "Klaus Mueller", "Sachin Chaudhary", "Surya Vashisth", "Akshay Dudhane", "Praful Hambarde", "Satya Naryan Tazi", "Prashant Patil", "Santosh Kumar Vipparthi", "Subrahmanyam Murala", "Bilel Benjdira", "Anas M. Ali", "Wadii Boulila", "Zahra Moammeri", "Ahmad Mahmoudi-Aznaveh", "Ali Karbasi", "Hossein Motamednia", "Liangyan Li", "Guanhua Zhao", "Kevin Le", "Yimo Ning", "Haoxuan Huang", "Jun Chen"], "title": "NTIRE 2025 Challenge on Image Super-Resolution ($\\times$4): Methods and Results", "categories": ["cs.CV"], "comment": "NTIRE 2025 webpage: https://www.cvlai.net/ntire/2025. Code:\n  https://github.com/zhengchen1999/NTIRE2025_ImageSR_x4", "summary": "This paper presents the NTIRE 2025 image super-resolution ($\\times$4)\nchallenge, one of the associated competitions of the 10th NTIRE Workshop at\nCVPR 2025. The challenge aims to recover high-resolution (HR) images from\nlow-resolution (LR) counterparts generated through bicubic downsampling with a\n$\\times$4 scaling factor. The objective is to develop effective network designs\nor solutions that achieve state-of-the-art SR performance. To reflect the dual\nobjectives of image SR research, the challenge includes two sub-tracks: (1) a\nrestoration track, emphasizes pixel-wise accuracy and ranks submissions based\non PSNR; (2) a perceptual track, focuses on visual realism and ranks results by\na perceptual score. A total of 286 participants registered for the competition,\nwith 25 teams submitting valid entries. This report summarizes the challenge\ndesign, datasets, evaluation protocol, the main results, and methods of each\nteam. The challenge serves as a benchmark to advance the state of the art and\nfoster progress in image SR."}
{"id": "2504.13887", "pdf": "https://arxiv.org/pdf/2504.13887", "abs": "https://arxiv.org/abs/2504.13887", "authors": ["Isabel Villanueva", "Tara Bobinac", "Binwei Yao", "Junjie Hu", "Kaiping Chen"], "title": "AI as a deliberative partner fosters intercultural empathy for Americans but fails for Latin American participants", "categories": ["cs.HC", "cs.CL", "cs.CY"], "comment": null, "summary": "Despite the growing integration of AI chatbots as conversational agents in\npublic discourse, empirical evidence regarding their capacity to foster\nintercultural empathy remains limited. Using a randomized dialogue experiment,\nwe examined how different types of AI chatbot interaction, i.e., deliberative\nversus non-deliberative and culturally aligned versus non-aligned, affect\nintercultural empathy across cultural groups. Results show that deliberative\nconversations increased intercultural empathy among American participants but\nnot Latin American participants, who perceived AI responses as culturally\ninaccurate and failing to represent their cultural contexts and perspectives\nauthentically. Real-time interaction analyses reveal that these differences\nstem from cultural knowledge gaps inherent in Large Language Models. Despite\nexplicit prompting and instruction to represent cultural perspectives in\nparticipants' native languages, AI systems still exhibit significant\ndisparities in cultural representation. This highlights the importance of\ndesigning AI systems capable of culturally authentic engagement in deliberative\nconversations. Our study contributes to deliberation theory and AI alignment\nresearch by underscoring AI's role in intercultural dialogue and the persistent\nchallenge of representational asymmetry in democratic discourse."}
{"id": "2504.13918", "pdf": "https://arxiv.org/pdf/2504.13918", "abs": "https://arxiv.org/abs/2504.13918", "authors": ["Johan van der Meer", "Pamela Hoyte", "Luisa Roeder", "Peter Bruza"], "title": "Modeling the quantum-like dynamics of human reliability ratings in Human-AI interactions by interaction dependent Hamiltonians", "categories": ["cs.HC", "cs.AI"], "comment": "10 pages, 7 figures. Submitted to Phil. Trans. B", "summary": "As our information environments become ever more powered by artificial\nintelligence (AI), the phenomenon of trust in a human's interactions with this\nintelligence is becoming increasingly pertinent. For example, in the not too\ndistant future, there will be teams of humans and intelligent robots involved\nin dealing with the repercussions of high-risk disaster situations such as\nhurricanes, earthquakes, or nuclear accidents. Even in such conditions of high\nuncertainty, humans and intelligent machines will need to engage in shared\ndecision making, and trust is fundamental to the effectiveness of these\ninteractions. A key challenge in modeling the dynamics of this trust is to\nprovide a means to incorporate sensitivity to fluctuations in human trust\njudgments. In this article, we explore the ability of Quantum Random Walk\nmodels to model the dynamics of trust in human-AI interactions, and to\nintegrate a sensitivity to fluctuations in participant trust judgments based on\nthe nature of the interaction with the AI. We found that using empirical\nparameters to inform the use of different Hamiltonians can provide a promising\nmeans to model the evolution of trust in Human-AI interactions."}
{"id": "2504.14583", "pdf": "https://arxiv.org/pdf/2504.14583", "abs": "https://arxiv.org/abs/2504.14583", "authors": ["Akshit Gupta", "Remko Uijlenhoet"], "title": "Using street view imagery and deep generative modeling for estimating the health of urban forests", "categories": ["cs.CV", "cs.CY"], "comment": "Accepted at ICLR 2025 Workshop", "summary": "Healthy urban forests comprising of diverse trees and shrubs play a crucial\nrole in mitigating climate change. They provide several key advantages such as\nproviding shade for energy conservation, and intercepting rainfall to reduce\nflood runoff and soil erosion. Traditional approaches for monitoring the health\nof urban forests require instrumented inspection techniques, often involving a\nhigh amount of human labor and subjective evaluations. As a result, they are\nnot scalable for cities which lack extensive resources. Recent approaches\ninvolving multi-spectral imaging data based on terrestrial sensing and\nsatellites, are constrained respectively with challenges related to dedicated\ndeployments and limited spatial resolutions. In this work, we propose an\nalternative approach for monitoring the urban forests using simplified inputs:\nstreet view imagery, tree inventory data and meteorological conditions. We\npropose to use image-to-image translation networks to estimate two urban forest\nhealth parameters, namely, NDVI and CTD. Finally, we aim to compare the\ngenerated results with ground truth data using an onsite campaign utilizing\nhandheld multi-spectral and thermal imaging sensors. With the advent and\nexpansion of street view imagery platforms such as Google Street View and\nMapillary, this approach should enable effective management of urban forests\nfor the authorities in cities at scale."}
{"id": "2504.13888", "pdf": "https://arxiv.org/pdf/2504.13888", "abs": "https://arxiv.org/abs/2504.13888", "authors": ["Paul Taele", "Jung In Koh", "Tracy Hammond"], "title": "Kanji Workbook: A Writing-Based Intelligent Tutoring System for Learning Proper Japanese Kanji Writing Technique with Instructor-Emulated Assessment", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Kanji script writing is a skill that is often introduced to novice Japanese\nforeign language students for achieving Japanese writing mastery, but often\nposes difficulties to students with primarily English fluency due to their its\nvast differences with written English. Instructors often introduce various\npedagogical methods -- such as visual structure and written techniques -- to\nassist students in kanji study, but may lack availability providing direct\nfeedback on students' writing outside of class. Current educational\napplications are also limited due to lacking richer instructor-emulated\nfeedback. We introduce Kanji Workbook, a writing-based intelligent tutoring\nsystem for students to receive intelligent assessment that emulates human\ninstructor feedback. Our interface not only leverages students' computing\ndevices for allowing them to learn, practice, and review the writing of\nprompted characters from their course's kanji script lessons, but also provides\na diverse set of writing assessment metrics -- derived from instructor\ninterviews and classroom observation insights -- through intelligent scoring\nand visual animations. We deployed our interface onto novice- and\nintermediate-level university courses over an entire academic year, and\nobserved that interface users on average achieved higher course grades than\ntheir peers and also reacted positively to our interface's various features."}
{"id": "2504.13926", "pdf": "https://arxiv.org/pdf/2504.13926", "abs": "https://arxiv.org/abs/2504.13926", "authors": ["Chameera De Silva", "Thilina Halloluwa", "Dhaval Vyas"], "title": "A Multi-Layered Research Framework for Human-Centered AI: Defining the Path to Explainability and Trust", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "The integration of Artificial Intelligence (AI) into high-stakes domains such\nas healthcare, finance, and autonomous systems is often constrained by concerns\nover transparency, interpretability, and trust. While Human-Centered AI (HCAI)\nemphasizes alignment with human values, Explainable AI (XAI) enhances\ntransparency by making AI decisions more understandable. However, the lack of a\nunified approach limits AI's effectiveness in critical decision-making\nscenarios. This paper presents a novel three-layered framework that bridges\nHCAI and XAI to establish a structured explainability paradigm. The framework\ncomprises (1) a foundational AI model with built-in explainability mechanisms,\n(2) a human-centered explanation layer that tailors explanations based on\ncognitive load and user expertise, and (3) a dynamic feedback loop that refines\nexplanations through real-time user interaction. The framework is evaluated\nacross healthcare, finance, and software development, demonstrating its\npotential to enhance decision-making, regulatory compliance, and public trust.\nOur findings advance Human-Centered Explainable AI (HCXAI), fostering AI\nsystems that are transparent, adaptable, and ethically aligned."}
{"id": "2504.14600", "pdf": "https://arxiv.org/pdf/2504.14600", "abs": "https://arxiv.org/abs/2504.14600", "authors": ["Zheng Chen", "Jingkai Wang", "Kai Liu", "Jue Gong", "Lei Sun", "Zongwei Wu", "Radu Timofte", "Yulun Zhang", "Jianxing Zhang", "Jinlong Wu", "Jun Wang", "Zheng Xie", "Hakjae Jeon", "Suejin Han", "Hyung-Ju Chun", "Hyunhee Park", "Zhicun Yin", "Junjie Chen", "Ming Liu", "Xiaoming Li", "Chao Zhou", "Wangmeng Zuo", "Weixia Zhang", "Dingquan Li", "Kede Ma", "Yun Zhang", "Zhuofan Zheng", "Yuyue Liu", "Shizhen Tang", "Zihao Zhang", "Yi Ning", "Hao Jiang", "Wenjie An", "Kangmeng Yu", "Chenyang Wang", "Kui Jiang", "Xianming Liu", "Junjun Jiang", "Yingfu Zhang", "Gang He", "Siqi Wang", "Kepeng Xu", "Zhenyang Liu", "Changxin Zhou", "Shanlan Shen", "Yubo Duan", "Yiang Chen", "Jin Guo", "Mengru Yang", "Jen-Wei Lee", "Chia-Ming Lee", "Chih-Chung Hsu", "Hu Peng", "Chunming He"], "title": "NTIRE 2025 Challenge on Real-World Face Restoration: Methods and Results", "categories": ["cs.CV"], "comment": "NTIRE 2025 webpage: https://www.cvlai.net/ntire/2025. Code:\n  https://github.com/zhengchen1999/NTIRE2025_RealWorld_Face_Restoration", "summary": "This paper provides a review of the NTIRE 2025 challenge on real-world face\nrestoration, highlighting the proposed solutions and the resulting outcomes.\nThe challenge focuses on generating natural, realistic outputs while\nmaintaining identity consistency. Its goal is to advance state-of-the-art\nsolutions for perceptual quality and realism, without imposing constraints on\ncomputational resources or training data. The track of the challenge evaluates\nperformance using a weighted image quality assessment (IQA) score and employs\nthe AdaFace model as an identity checker. The competition attracted 141\nregistrants, with 13 teams submitting valid models, and ultimately, 10 teams\nachieved a valid score in the final ranking. This collaborative effort advances\nthe performance of real-world face restoration while offering an in-depth\noverview of the latest trends in the field."}
{"id": "2504.13890", "pdf": "https://arxiv.org/pdf/2504.13890", "abs": "https://arxiv.org/abs/2504.13890", "authors": ["Chen Shani", "Elizabeth C. Stade"], "title": "Measuring Mental Health Variables in Computational Research: Toward Validated, Dimensional, and Transdiagnostic Approaches", "categories": ["cs.HC", "cs.CL"], "comment": "No figures, 1 Table", "summary": "Computational mental health research develops models to predict and\nunderstand psychological phenomena, but often relies on inappropriate measures\nof psychopathology constructs, undermining validity. We identify three key\nissues: (1) reliance on unvalidated measures (e.g., self-declared diagnosis)\nover validated ones (e.g., diagnosis by clinician); (2) treating mental health\nconstructs as categorical rather than dimensional; and (3) focusing on\ndisorder-specific constructs instead of transdiagnostic ones. We outline the\nbenefits of using validated, dimensional, and transdiagnostic measures and\noffer practical recommendations for practitioners. Using valid measures that\nreflect the nature and structure of psychopathology is essential for\ncomputational mental health research."}
{"id": "2504.13928", "pdf": "https://arxiv.org/pdf/2504.13928", "abs": "https://arxiv.org/abs/2504.13928", "authors": ["Li Song"], "title": "LLM-Driven NPCs: Cross-Platform Dialogue System for Games and Social Platforms", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "NPCs in traditional games are often limited by static dialogue trees and a\nsingle platform for interaction. To overcome these constraints, this study\npresents a prototype system that enables large language model (LLM)-powered\nNPCs to communicate with players both in the game en vironment (Unity) and on a\nsocial platform (Discord). Dialogue logs are stored in a cloud database\n(LeanCloud), allowing the system to synchronize memory between platforms and\nkeep conversa tions coherent. Our initial experiments show that cross-platform\ninteraction is technically feasible and suggest a solid foundation for future\ndevelopments such as emotional modeling and persistent memory support."}
{"id": "2504.14606", "pdf": "https://arxiv.org/pdf/2504.14606", "abs": "https://arxiv.org/abs/2504.14606", "authors": ["Siyi Jiao", "Wenzheng Zeng", "Yerong Li", "Huayu Zhang", "Changxin Gao", "Nong Sang", "Mike Zheng Shou"], "title": "MP-Mat: A 3D-and-Instance-Aware Human Matting and Editing Framework with Multiplane Representation", "categories": ["cs.CV"], "comment": "Accepted by ICLR 2025", "summary": "Human instance matting aims to estimate an alpha matte for each human\ninstance in an image, which is challenging as it easily fails in complex cases\nrequiring disentangling mingled pixels belonging to multiple instances along\nhairy and thin boundary structures. In this work, we address this by\nintroducing MP-Mat, a novel 3D-and-instance-aware matting framework with\nmultiplane representation, where the multiplane concept is designed from two\ndifferent perspectives: scene geometry level and instance level. Specifically,\nwe first build feature-level multiplane representations to split the scene into\nmultiple planes based on depth differences. This approach makes the scene\nrepresentation 3D-aware, and can serve as an effective clue for splitting\ninstances in different 3D positions, thereby improving interpretability and\nboundary handling ability especially in occlusion areas. Then, we introduce\nanother multiplane representation that splits the scene in an instance-level\nperspective, and represents each instance with both matte and color. We also\ntreat background as a special instance, which is often overlooked by existing\nmethods. Such an instance-level representation facilitates both foreground and\nbackground content awareness, and is useful for other down-stream tasks like\nimage editing. Once built, the representation can be reused to realize\ncontrollable instance-level image editing with high efficiency. Extensive\nexperiments validate the clear advantage of MP-Mat in matting task. We also\ndemonstrate its superiority in image editing tasks, an area under-explored by\nexisting matting-focused methods, where our approach under zero-shot inference\neven outperforms trained specialized image editing techniques by large margins.\nCode is open-sourced at https://github.com/JiaoSiyi/MPMat.git}."}
{"id": "2504.13892", "pdf": "https://arxiv.org/pdf/2504.13892", "abs": "https://arxiv.org/abs/2504.13892", "authors": ["Stefano De Paoli", "Alex Fawzi"], "title": "TALLMesh: a simple application for performing Thematic Analysis with Large Language Models", "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "Thematic analysis (TA) is a widely used qualitative research method for\nidentifying and interpreting patterns within textual data, such as qualitative\ninterviews. Recent research has shown that it is possible to satisfactorily\nperform TA using Large Language Models (LLMs). This paper presents a novel\napplication using LLMs to assist researchers in conducting TA. The application\nenables users to upload textual data, generate initial codes and themes. All of\nthis is possible through a simple Graphical User Interface, (GUI) based on the\nstreamlit framework, working with python scripts for the analysis, and using\nApplication Program Interfaces of LLMs. Having a GUI is particularly important\nfor researchers in fields where coding skills may not be prevalent, such as\nsocial sciences or humanities. With the app, users can iteratively refine codes\nand themes adopting a human-in-the-loop process, without the need to work with\nprogramming and scripting. The paper describes the application key features,\nhighlighting its potential for qualitative research while preserving\nmethodological rigor. The paper discusses the design and interface of the app\nand outlines future directions for this work."}
{"id": "2504.13940", "pdf": "https://arxiv.org/pdf/2504.13940", "abs": "https://arxiv.org/abs/2504.13940", "authors": ["Paul Taele", "Tracy Hammond"], "title": "Hashigo: A Next Generation Sketch Interactive System for Japanese Kanji", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Language students can increase their effectiveness in learning written\nJapanese by mastering the visual structure and written technique of Japanese\nkanji. Yet, existing kanji handwriting recognition systems do not assess the\nwritten technique sufficiently enough to discourage students from developing\nbad learning habits. In this paper, we describe our work on Hashigo, a kanji\nsketch interactive system which achieves human instructor-level critique and\nfeedback on both the visual structure and written technique of students'\nsketched kanji. This type of automated critique and feedback allows students to\ntarget and correct specific deficiencies in their sketches that, if left\nuntreated, are detrimental to effective long-term kanji learning."}
{"id": "2504.14618", "pdf": "https://arxiv.org/pdf/2504.14618", "abs": "https://arxiv.org/abs/2504.14618", "authors": ["Han Bi", "Ge Yu", "Yu He", "Wenzhuo Liu", "Zijie Zheng"], "title": "VM-BHINet:Vision Mamba Bimanual Hand Interaction Network for 3D Interacting Hand Mesh Recovery From a Single RGB Image", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Understanding bimanual hand interactions is essential for realistic 3D pose\nand shape reconstruction. However, existing methods struggle with occlusions,\nambiguous appearances, and computational inefficiencies. To address these\nchallenges, we propose Vision Mamba Bimanual Hand Interaction Network\n(VM-BHINet), introducing state space models (SSMs) into hand reconstruction to\nenhance interaction modeling while improving computational efficiency. The core\ncomponent, Vision Mamba Interaction Feature Extraction Block (VM-IFEBlock),\ncombines SSMs with local and global feature operations, enabling deep\nunderstanding of hand interactions. Experiments on the InterHand2.6M dataset\nshow that VM-BHINet reduces Mean per-joint position error (MPJPE) and Mean\nper-vertex position error (MPVPE) by 2-3%, significantly surpassing\nstate-of-the-art methods."}
{"id": "2504.13904", "pdf": "https://arxiv.org/pdf/2504.13904", "abs": "https://arxiv.org/abs/2504.13904", "authors": ["Donghuo Zeng", "Roberto Legaspi", "Yuewen Sun", "Xinshuai Dong", "Kazushi Ikeda", "Peter Spirtes", "Kun Zhang"], "title": "Generative Framework for Personalized Persuasion: Inferring Causal, Counterfactual, and Latent Knowledge", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "12 pages, 10 figures, 1 table. Accepted by ACM UMAP 2025", "summary": "We hypothesize that optimal system responses emerge from adaptive strategies\ngrounded in causal and counterfactual knowledge. Counterfactual inference\nallows us to create hypothetical scenarios to examine the effects of\nalternative system responses. We enhance this process through causal discovery,\nwhich identifies the strategies informed by the underlying causal structure\nthat govern system behaviors. Moreover, we consider the psychological\nconstructs and unobservable noises that might be influencing user-system\ninteractions as latent factors. We show that these factors can be effectively\nestimated. We employ causal discovery to identify strategy-level causal\nrelationships among user and system utterances, guiding the generation of\npersonalized counterfactual dialogues. We model the user utterance strategies\nas causal factors, enabling system strategies to be treated as counterfactual\nactions. Furthermore, we optimize policies for selecting system responses based\non counterfactual data. Our results using a real-world dataset on social good\ndemonstrate significant improvements in persuasive system outcomes, with\nincreased cumulative rewards validating the efficacy of causal discovery in\nguiding personalized counterfactual inference and optimizing dialogue policies\nfor a persuasive dialogue system."}
{"id": "2504.13941", "pdf": "https://arxiv.org/pdf/2504.13941", "abs": "https://arxiv.org/abs/2504.13941", "authors": ["Syeda Nahida Akter", "Shrimai Prabhumoye", "Matvei Novikov", "Seungju Han", "Ying Lin", "Evelina Bakhturi", "Eric Nyberg", "Yejin Choi", "Mostofa Patwary", "Mohammad Shoeybi", "Bryan Catanzaro"], "title": "NEMOTRON-CROSSTHINK: Scaling Self-Learning beyond Math Reasoning", "categories": ["cs.LG", "cs.AI"], "comment": "18 pages, 7 figures", "summary": "Large Language Models (LLMs) have shown strong reasoning capabilities,\nparticularly when enhanced through Reinforcement Learning (RL). While prior\nwork has successfully applied RL to mathematical reasoning -- where rules and\ncorrectness are well-defined -- generalizing these methods to broader reasoning\ndomains remains challenging due to limited data, the lack of verifiable reward\nstructures, and diverse task requirements. In this work, we propose\nNEMOTRON-CROSSTHINK, a framework that systematically incorporates multi-domain\ncorpora, including both synthetic and real-world question-answer pairs, into RL\ntraining to improve generalization across diverse reasoning tasks.\nNEMOTRON-CROSSTHINK addresses key challenges by (1) incorporating data from\nvaried sources spanning STEM, humanities, social sciences, etc.; (2) applying\nstructured templates (e.g., multiple-choice and open-ended) to control\nanswer-space complexity; (3) filtering for verifiable answers; and (4)\noptimizing data blending strategies that utilizes data from multiple sources\neffectively. Our approach enables scalable and verifiable reward modeling\nbeyond mathematics and demonstrates improved accuracies on both math (MATH-500:\n+30.1%, AMC23:+27.5%) and non-math reasoning benchmarks (MMLU-PRO: +12.8%,\nGPQA-DIAMOND: +11.3%, AGIEVAL: +15.1%, SUPERGPQA: +3.8%). Moreover,\nNEMOTRON-CROSSTHINK exhibits significantly improved response efficiency --\nusing 28% fewer tokens for correct answers -- highlighting more focused and\neffective reasoning. Through NEMOTRON-CROSSTHINK, we demonstrate that\nintegrating multi-domain, multi-format data in RL leads to more accurate,\nefficient, and generalizable LLMs."}
{"id": "2504.14621", "pdf": "https://arxiv.org/pdf/2504.14621", "abs": "https://arxiv.org/abs/2504.14621", "authors": ["Zhenkui Yang", "Zeyi Huang", "Ge Wang", "Han Ding", "Tony Xiao Han", "Fei Wang"], "title": "Talk is Not Always Cheap: Promoting Wireless Sensing Models with Text Prompts", "categories": ["cs.CV"], "comment": "10 pages", "summary": "Wireless signal-based human sensing technologies, such as WiFi,\nmillimeter-wave (mmWave) radar, and Radio Frequency Identification (RFID),\nenable the detection and interpretation of human presence, posture, and\nactivities, thereby providing critical support for applications in public\nsecurity, healthcare, and smart environments. These technologies exhibit\nnotable advantages due to their non-contact operation and environmental\nadaptability; however, existing systems often fail to leverage the textual\ninformation inherent in datasets. To address this, we propose an innovative\ntext-enhanced wireless sensing framework, WiTalk, that seamlessly integrates\nsemantic knowledge through three hierarchical prompt strategies-label-only,\nbrief description, and detailed action description-without requiring\narchitectural modifications or incurring additional data costs. We rigorously\nvalidate this framework across three public benchmark datasets: XRF55 for human\naction recognition (HAR), and WiFiTAL and XRFV2 for WiFi temporal action\nlocalization (TAL). Experimental results demonstrate significant performance\nimprovements: on XRF55, accuracy for WiFi, RFID, and mmWave increases by 3.9%,\n2.59%, and 0.46%, respectively; on WiFiTAL, the average performance of WiFiTAD\nimproves by 4.98%; and on XRFV2, the mean average precision gains across\nvarious methods range from 4.02% to 13.68%. Our codes have been included in\nhttps://github.com/yangzhenkui/WiTalk."}
{"id": "2504.13924", "pdf": "https://arxiv.org/pdf/2504.13924", "abs": "https://arxiv.org/abs/2504.13924", "authors": ["Akash V. Maharaj", "David Arbour", "Daniel Lee", "Uttaran Bhattacharya", "Anup Rao", "Austin Zane", "Avi Feller", "Kun Qian", "Yunyao Li"], "title": "Evaluation and Incident Prevention in an Enterprise AI Assistant", "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": "7 pages, 5 figures. Accepted at IAAI-25", "summary": "Enterprise AI Assistants are increasingly deployed in domains where accuracy\nis paramount, making each erroneous output a potentially significant incident.\nThis paper presents a comprehensive framework for monitoring, benchmarking, and\ncontinuously improving such complex, multi-component systems under active\ndevelopment by multiple teams. Our approach encompasses three key elements: (1)\na hierarchical ``severity'' framework for incident detection that identifies\nand categorizes errors while attributing component-specific error rates,\nfacilitating targeted improvements; (2) a scalable and principled methodology\nfor benchmark construction, evaluation, and deployment, designed to accommodate\nmultiple development teams, mitigate overfitting risks, and assess the\ndownstream impact of system modifications; and (3) a continual improvement\nstrategy leveraging multidimensional evaluation, enabling the identification\nand implementation of diverse enhancement opportunities. By adopting this\nholistic framework, organizations can systematically enhance the reliability\nand performance of their AI Assistants, ensuring their efficacy in critical\nenterprise environments. We conclude by discussing how this multifaceted\nevaluation approach opens avenues for various classes of enhancements, paving\nthe way for more robust and trustworthy AI systems."}
{"id": "2504.13942", "pdf": "https://arxiv.org/pdf/2504.13942", "abs": "https://arxiv.org/abs/2504.13942", "authors": ["Sukanth Kalivarathan", "Muhmmad Abrar Raja Mohamed", "Aswathy Ravikumar", "S Harini"], "title": "Intelligence of Things: A Spatial Context-Aware Control System for Smart Devices", "categories": ["cs.HC", "cs.AI", "cs.ET"], "comment": "16 pages, 8 Figures", "summary": "This paper introduces Intelligence of Things (INOT), a novel spatial\ncontext-aware control system that enhances smart home automation through\nintuitive spatial reasoning. Current smart home systems largely rely on\ndevice-specific identifiers, limiting user interaction to explicit naming\nconventions rather than natural spatial references. INOT addresses this\nlimitation through a modular architecture that integrates Vision Language\nModels with IoT control systems to enable natural language commands with\nspatial context (e.g., \"turn on the light near the window\"). The system\ncomprises key components including an Onboarding Inference Engine, Zero-Shot\nDevice Detection, Spatial Topology Inference, and Intent-Based Command\nSynthesis. A comprehensive user study with 15 participants demonstrated INOT's\nsignificant advantages over conventional systems like Google Home Assistant,\nwith users reporting reduced cognitive workload (NASA-TLX scores decreased by\nan average of 13.17 points), higher ease-of-use ratings, and stronger\npreference (14 out of 15 participants). By eliminating the need to memorize\ndevice identifiers and enabling context-aware spatial commands, INOT represents\na significant advancement in creating more intuitive and accessible smart home\ncontrol systems."}
{"id": "2504.14626", "pdf": "https://arxiv.org/pdf/2504.14626", "abs": "https://arxiv.org/abs/2504.14626", "authors": ["Santanu Roy", "Shweta Singh", "Palak Sahu", "Ashvath Suresh", "Debashish Das"], "title": "MSAD-Net: Multiscale and Spatial Attention-based Dense Network for Lung Cancer Classification", "categories": ["cs.CV"], "comment": null, "summary": "Lung cancer, a severe form of malignant tumor that originates in the tissues\nof the lungs, can be fatal if not detected in its early stages. It ranks among\nthe top causes of cancer-related mortality worldwide. Detecting lung cancer\nmanually using chest X-Ray image or Computational Tomography (CT) scans image\nposes significant challenges for radiologists. Hence, there is a need for\nautomatic diagnosis system of lung cancers from radiology images. With the\nrecent emergence of deep learning, particularly through Convolutional Neural\nNetworks (CNNs), the automated detection of lung cancer has become a much\nsimpler task. Nevertheless, numerous researchers have addressed that the\nperformance of conventional CNNs may be hindered due to class imbalance issue,\nwhich is prevalent in medical images. In this research work, we have proposed a\nnovel CNN architecture ``Multi-Scale Dense Network (MSD-Net)''\n(trained-from-scratch). The novelties we bring in the proposed model are (I) We\nintroduce novel dense modules in the 4th block and 5th block of the CNN model.\nWe have leveraged 3 depthwise separable convolutional (DWSC) layers, and one\n1x1 convolutional layer in each dense module, in order to reduce complexity of\nthe model considerably. (II) Additionally, we have incorporated one skip\nconnection from 3rd block to 5th block and one parallel branch connection from\n4th block to Global Average Pooling (GAP) layer. We have utilized dilated\nconvolutional layer (with dilation rate=2) in the last parallel branch in order\nto extract multi-scale features. Extensive experiments reveal that our proposed\nmodel has outperformed latest CNN model ConvNext-Tiny, recent trend Vision\nTransformer (ViT), Pooling-based ViT (PiT), and other existing models by\nsignificant margins."}
{"id": "2504.13932", "pdf": "https://arxiv.org/pdf/2504.13932", "abs": "https://arxiv.org/abs/2504.13932", "authors": ["Deyu Cao", "Samin Aref"], "title": "Enhancing Ultra-Low-Bit Quantization of Large Language Models Through Saliency-Aware Partial Retraining", "categories": ["cs.LG", "cs.CL", "68T50, 68T07, 68T09, 68U15", "I.2.7; I.2.6; I.2.4"], "comment": "28 pages, 5 figures, 11 tables", "summary": "Large language models offer remarkable capabilities, but their size and\ncomputational demands pose practical challenges. Quantization methods compress\ntheir size through replacing their high-precision parameters by quantized\nvalues of lower precision. Post-training quantization reduces model size\nefficiently at the cost of decreased accuracy, while quantization-aware\ntraining better preserves accuracy but is resource-intensive. Among existing\npost-training quantization algorithms, the ApiQ method achieves superior\naccuracy preservation at minimal memory and time overhead. We investigate two\nideas to extend performance in ultra-low-bit quantization beyond ApiQ's level.\nFirst, we look into combining existing quantization-aware training techniques\nwith ApiQ's partial training. We show that this does not outperform the\nbaseline ApiQ method with limited training data and frozen weights. This leads\nto two key insights: (1) The substantial representational capacity that is\ngained through full retraining may not be feasible through partial training.\n(2) This gain seems to depend on using a large and diverse dataset in\nquantization-aware training. Second, through a novel approach informed by the\ntwo insights, we propose an ultra-low-bit quantization method that builds upon\nApiQ and extends its performance without the need for full retraining. It\nrelies on a saliency-aware regularization term that prioritizes preserving the\nmost impactful parameters during quantization. Our experiments on benchmark\nlanguage models from the LLaMA family show that our proposed approach boosts\naccuracy and tightens the gap between the quantized model and the\nfull-precision model, with minimal overhead. Our method will be made publicly\navailable to facilitate future developments in ultra-low-bit quantization of\nlarge language models."}
{"id": "2504.13944", "pdf": "https://arxiv.org/pdf/2504.13944", "abs": "https://arxiv.org/abs/2504.13944", "authors": ["Tace McNamara", "Jon McCormack", "Maria Teresa Llano"], "title": "Mixer Metaphors: audio interfaces for non-musical applications", "categories": ["cs.HC", "cs.AI", "cs.SD", "H.5.2; J.5; I.2.7"], "comment": "9 Pages", "summary": "The NIME conference traditionally focuses on interfaces for music and musical\nexpression. In this paper we reverse this tradition to ask, can interfaces\ndeveloped for music be successfully appropriated to non-musical applications?\nTo help answer this question we designed and developed a new device, which uses\ninterface metaphors borrowed from analogue synthesisers and audio mixing to\nphysically control the intangible aspects of a Large Language Model. We\ncompared two versions of the device, with and without the audio-inspired\naugmentations, with a group of artists who used each version over a one week\nperiod. Our results show that the use of audio-like controls afforded more\nimmediate, direct and embodied control over the LLM, allowing users to\ncreatively experiment and play with the device over its non-mixer counterpart.\nOur project demonstrates how cross-sensory metaphors can support creative\nthinking and embodied practice when designing new technological interfaces."}
{"id": "2504.14638", "pdf": "https://arxiv.org/pdf/2504.14638", "abs": "https://arxiv.org/abs/2504.14638", "authors": ["Junyuan Fang", "Zihan Wang", "Yejun Zhang", "Shuzhe Wang", "Iaroslav Melekhov", "Juho Kannala"], "title": "NVSMask3D: Hard Visual Prompting with Camera Pose Interpolation for 3D Open Vocabulary Instance Segmentation", "categories": ["cs.CV"], "comment": "15 pages, 4 figures, Scandinavian Conference on Image Analysis 2025", "summary": "Vision-language models (VLMs) have demonstrated impressive zero-shot transfer\ncapabilities in image-level visual perception tasks. However, they fall short\nin 3D instance-level segmentation tasks that require accurate localization and\nrecognition of individual objects. To bridge this gap, we introduce a novel 3D\nGaussian Splatting based hard visual prompting approach that leverages camera\ninterpolation to generate diverse viewpoints around target objects without any\n2D-3D optimization or fine-tuning. Our method simulates realistic 3D\nperspectives, effectively augmenting existing hard visual prompts by enforcing\ngeometric consistency across viewpoints. This training-free strategy seamlessly\nintegrates with prior hard visual prompts, enriching object-descriptive\nfeatures and enabling VLMs to achieve more robust and accurate 3D instance\nsegmentation in diverse 3D scenes."}
{"id": "2504.13955", "pdf": "https://arxiv.org/pdf/2504.13955", "abs": "https://arxiv.org/abs/2504.13955", "authors": ["Suhas BN", "Dominik Mattioli", "Saeed Abdullah", "Rosa I. Arriaga", "Chris W. Wiese", "Andrew M. Sherrill"], "title": "Thousand Voices of Trauma: A Large-Scale Synthetic Dataset for Modeling Prolonged Exposure Therapy Conversations", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC", "cs.LG", "68T50", "I.2.7; H.5.2"], "comment": "14 pages, 6 figures", "summary": "The advancement of AI systems for mental health support is hindered by\nlimited access to therapeutic conversation data, particularly for trauma\ntreatment. We present Thousand Voices of Trauma, a synthetic benchmark dataset\nof 3,000 therapy conversations based on Prolonged Exposure therapy protocols\nfor Post-traumatic Stress Disorder (PTSD). The dataset comprises 500 unique\ncases, each explored through six conversational perspectives that mirror the\nprogression of therapy from initial anxiety to peak distress to emotional\nprocessing. We incorporated diverse demographic profiles (ages 18-80, M=49.3,\n49.4% male, 44.4% female, 6.2% non-binary), 20 trauma types, and 10\ntrauma-related behaviors using deterministic and probabilistic generation\nmethods. Analysis reveals realistic distributions of trauma types (witnessing\nviolence 10.6%, bullying 10.2%) and symptoms (nightmares 23.4%, substance abuse\n20.8%). Clinical experts validated the dataset's therapeutic fidelity,\nhighlighting its emotional depth while suggesting refinements for greater\nauthenticity. We also developed an emotional trajectory benchmark with\nstandardized metrics for evaluating model responses. This privacy-preserving\ndataset addresses critical gaps in trauma-focused mental health data, offering\na valuable resource for advancing both patient-facing applications and\nclinician training tools."}
{"id": "2504.13945", "pdf": "https://arxiv.org/pdf/2504.13945", "abs": "https://arxiv.org/abs/2504.13945", "authors": ["Zhanglin Wu", "Tengfei Song", "Ning Xie", "Weidong Zhang", "Mengli Zhu", "Shuang Wu", "Shiliang Sun", "Hao Yang"], "title": "Evaluating Menu OCR and Translation: A Benchmark for Aligning Human and Automated Evaluations in Large Vision-Language Models", "categories": ["cs.LG", "cs.AI"], "comment": "12 pages, 5 figures, 5 Tables", "summary": "The rapid advancement of large vision-language models (LVLMs) has\nsignificantly propelled applications in document understanding, particularly in\noptical character recognition (OCR) and multilingual translation. However,\ncurrent evaluations of LVLMs, like the widely used OCRBench, mainly focus on\nverifying the correctness of their short-text responses and long-text responses\nwith simple layout, while the evaluation of their ability to understand long\ntexts with complex layout design is highly significant but largely overlooked.\nIn this paper, we propose Menu OCR and Translation Benchmark (MOTBench), a\nspecialized evaluation framework emphasizing the pivotal role of menu\ntranslation in cross-cultural communication. MOTBench requires LVLMs to\naccurately recognize and translate each dish, along with its price and unit\nitems on a menu, providing a comprehensive assessment of their visual\nunderstanding and language processing capabilities. Our benchmark is comprised\nof a collection of Chinese and English menus, characterized by intricate\nlayouts, a variety of fonts, and culturally specific elements across different\nlanguages, along with precise human annotations. Experiments show that our\nautomatic evaluation results are highly consistent with professional human\nevaluation. We evaluate a range of publicly available state-of-the-art LVLMs,\nand through analyzing their output to identify the strengths and weaknesses in\ntheir performance, offering valuable insights to guide future advancements in\nLVLM development. MOTBench is available at https://github.com/gitwzl/MOTBench."}
{"id": "2504.14642", "pdf": "https://arxiv.org/pdf/2504.14642", "abs": "https://arxiv.org/abs/2504.14642", "authors": ["Lin Li", "Wei Chen", "Jiahui Li", "Long Chen"], "title": "Relation-R1: Cognitive Chain-of-Thought Guided Reinforcement Learning for Unified Relational Comprehension", "categories": ["cs.CV"], "comment": "Ongoing project", "summary": "Recent advances in multi-modal large language models (MLLMs) have\nsignificantly improved object-level grounding and region captioning, but remain\nlimited in visual relation understanding (\\eg, scene graph generation),\nparticularly in modeling \\textit{N}-ary relationships that identify multiple\nsemantic roles among an action event. Such a lack of \\textit{semantic\ndependencies} modeling among multi-entities leads to unreliable outputs,\nintensifying MLLMs' hallucinations and over-reliance on language priors. To\nthis end, we propose Relation-R1, the first unified relational comprehension\nframework that explicitly integrates cognitive chain-of-thought (CoT)-guided\nSupervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO)\nwithin a reinforcement learning (RL) paradigm. Specifically, we first establish\nfoundational reasoning capabilities via SFT, enforcing structured outputs with\nthinking processes. Then, GRPO is utilized to refine these outputs via\nmulti-reward optimization, prioritizing visual-semantic grounding over\nlanguage-induced biases, thereby improving generalization capability. Extensive\nexperiments on widely-used PSG and SWiG datasets demonstrate that Relation-R1\nachieves state-of-the-art performance in both binary and \\textit{N}-ary\nrelation understanding."}
{"id": "2504.13958", "pdf": "https://arxiv.org/pdf/2504.13958", "abs": "https://arxiv.org/abs/2504.13958", "authors": ["Cheng Qian", "Emre Can Acikgoz", "Qi He", "Hongru Wang", "Xiusi Chen", "Dilek Hakkani-Tür", "Gokhan Tur", "Heng Ji"], "title": "ToolRL: Reward is All Tool Learning Needs", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "19 Pages, 12 Figures, 12 Tables", "summary": "Current Large Language Models (LLMs) often undergo supervised fine-tuning\n(SFT) to acquire tool use capabilities. However, SFT struggles to generalize to\nunfamiliar or complex tool use scenarios. Recent advancements in reinforcement\nlearning (RL), particularly with R1-like models, have demonstrated promising\nreasoning and generalization abilities. Yet, reward design for tool use\npresents unique challenges: multiple tools may be invoked with diverse\nparameters, and coarse-grained reward signals, such as answer matching, fail to\noffer the finegrained feedback required for effective learning. In this work,\nwe present the first comprehensive study on reward design for tool selection\nand application tasks within the RL paradigm. We systematically explore a wide\nrange of reward strategies, analyzing their types, scales, granularity, and\ntemporal dynamics. Building on these insights, we propose a principled reward\ndesign tailored for tool use tasks and apply it to train LLMs using Group\nRelative Policy Optimization (GRPO). Empirical evaluations across diverse\nbenchmarks demonstrate that our approach yields robust, scalable, and stable\ntraining, achieving a 17% improvement over base models and a 15% gain over SFT\nmodels. These results highlight the critical role of thoughtful reward design\nin enhancing the tool use capabilities and generalization performance of LLMs.\nAll the codes are released to facilitate future research."}
{"id": "2504.13947", "pdf": "https://arxiv.org/pdf/2504.13947", "abs": "https://arxiv.org/abs/2504.13947", "authors": ["Shahan Ali Memon", "Soham De", "Sungha Kang", "Riyan Mujtaba", "Bedoor AlShebli", "Katie Davis", "Jaime Snyder", "Jevin D. West"], "title": "From job titles to jawlines: Using context voids to study generative AI systems", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "7 pages, 2 figures", "summary": "In this paper, we introduce a speculative design methodology for studying the\nbehavior of generative AI systems, framing design as a mode of inquiry. We\npropose bridging seemingly unrelated domains to generate intentional context\nvoids, using these tasks as probes to elicit AI model behavior. We demonstrate\nthis through a case study: probing the ChatGPT system (GPT-4 and DALL-E) to\ngenerate headshots from professional Curricula Vitae (CVs). In contrast to\ntraditional ways, our approach assesses system behavior under conditions of\nradical uncertainty -- when forced to invent entire swaths of missing context\n-- revealing subtle stereotypes and value-laden assumptions. We qualitatively\nanalyze how the system interprets identity and competence markers from CVs,\ntranslating them into visual portraits despite the missing context (i.e.\nphysical descriptors). We show that within this context void, the AI system\ngenerates biased representations, potentially relying on stereotypical\nassociations or blatant hallucinations."}
{"id": "2504.14658", "pdf": "https://arxiv.org/pdf/2504.14658", "abs": "https://arxiv.org/abs/2504.14658", "authors": ["Jing Zhang", "Dan Guo", "Zhangbin Li", "Meng Wang"], "title": "EmoSEM: Segment and Explain Emotion Stimuli in Visual Art", "categories": ["cs.CV"], "comment": null, "summary": "This paper focuses on a key challenge in visual art understanding: given an\nart image, the model pinpoints pixel regions that trigger a specific human\nemotion, and generates linguistic explanations for the emotional arousal.\nDespite recent advances in art understanding, pixel-level emotion understanding\nstill faces a dual challenge: first, the subjectivity of emotion makes it\ndifficult for general segmentation models like SAM to adapt to emotion-oriented\nsegmentation tasks; and second, the abstract nature of art expression makes it\ndifficult for captioning models to balance pixel-level semantic understanding\nand emotion reasoning. To solve the above problems, this paper proposes the\nEmotion stimuli Segmentation and Explanation Model (EmoSEM) to endow the\nsegmentation model SAM with emotion comprehension capability. First, to enable\nthe model to perform segmentation under the guidance of emotional intent well,\nwe introduce an emotional prompt with a learnable mask token as the conditional\ninput for segmentation decoding. Then, we design an emotion projector to\nestablish the association between emotion and visual features. Next, more\nimportantly, to address emotion-visual stimuli alignment, we develop a\nlightweight prefix projector, a module that fuses the learned emotional mask\nwith the corresponding emotion into a unified representation compatible with\nthe language model. Finally, we input the joint visual, mask, and emotional\ntokens into the language model and output the emotional explanations. It\nensures that the generated interpretations remain semantically and emotionally\ncoherent with the visual stimuli. The method innovatively realizes end-to-end\nmodeling from low-level pixel features to high-level emotion interpretation,\nproviding the first interpretable fine-grained analysis framework for artistic\nemotion computing. Extensive experiments validate the effectiveness of our\nmodel."}
{"id": "2504.13959", "pdf": "https://arxiv.org/pdf/2504.13959", "abs": "https://arxiv.org/abs/2504.13959", "authors": ["Sanchaita Hazra", "Bodhisattwa Prasad Majumder", "Tuhin Chakrabarty"], "title": "AI Safety Should Prioritize the Future of Work", "categories": ["cs.CY", "cs.AI", "cs.CL", "econ.GN", "q-fin.EC"], "comment": null, "summary": "Current efforts in AI safety prioritize filtering harmful content, preventing\nmanipulation of human behavior, and eliminating existential risks in\ncybersecurity or biosecurity. While pressing, this narrow focus overlooks\ncritical human-centric considerations that shape the long-term trajectory of a\nsociety. In this position paper, we identify the risks of overlooking the\nimpact of AI on the future of work and recommend comprehensive transition\nsupport towards the evolution of meaningful labor with human agency. Through\nthe lens of economic theories, we highlight the intertemporal impacts of AI on\nhuman livelihood and the structural changes in labor markets that exacerbate\nincome inequality. Additionally, the closed-source approach of major\nstakeholders in AI development resembles rent-seeking behavior through\nexploiting resources, breeding mediocrity in creative labor, and monopolizing\ninnovation. To address this, we argue in favor of a robust international\ncopyright anatomy supported by implementing collective licensing that ensures\nfair compensation mechanisms for using data to train AI models. We strongly\nrecommend a pro-worker framework of global AI governance to enhance shared\nprosperity and economic justice while reducing technical debt."}
{"id": "2504.13948", "pdf": "https://arxiv.org/pdf/2504.13948", "abs": "https://arxiv.org/abs/2504.13948", "authors": ["Juan David Salazar Rodriguez", "Sam Conrad Joyce", "Julfendi Julfendi"], "title": "Using customized GPT to develop prompting proficiency in architectural AI-generated images", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "This research investigates the use of customized GPT models to enhance\nprompting proficiency among architecture students when generating AI-driven\nimages. Prompt engineering is increasingly essential in architectural education\ndue to the widespread adoption of generative AI tools. This study utilized a\nmixed-methods experimental design involving architecture students divided into\nthree distinct groups: a control group receiving no structured support, a\nsecond group provided with structured prompting guides, and a third group\nsupported by both structured guides and interactive AI personas. Students\nengaged in reverse engineering tasks, first guessing provided image prompts and\nthen generating their own prompts, aiming to boost critical thinking and\nprompting skills. Variables examined included time spent prompting, word count,\nprompt similarity, and concreteness. Quantitative analysis involved correlation\nassessments between these variables and a one-way ANOVA to evaluate differences\nacross groups. While several correlations showed meaningful relationships, not\nall were statistically significant. ANOVA results indicated statistically\nsignificant improvements in word count, similarity, and concreteness,\nespecially in the group supported by AI personas and structured prompting\nguides. Qualitative feedback complemented these findings, revealing enhanced\nconfidence and critical thinking skills in students. These results suggest\ntailored GPT interactions substantially improve students' ability to\ncommunicate architectural concepts clearly and effectively."}
{"id": "2504.14664", "pdf": "https://arxiv.org/pdf/2504.14664", "abs": "https://arxiv.org/abs/2504.14664", "authors": ["Jixiang Sun", "Fei Lei", "Jiawei Zhang", "Wenxiu Sun", "Yujiu Yang"], "title": "Frequency-domain Learning with Kernel Prior for Blind Image Deblurring", "categories": ["cs.CV"], "comment": null, "summary": "While achieving excellent results on various datasets, many deep learning\nmethods for image deblurring suffer from limited generalization capabilities\nwith out-of-domain data. This limitation is likely caused by their dependence\non certain domain-specific datasets. To address this challenge, we argue that\nit is necessary to introduce the kernel prior into deep learning methods, as\nthe kernel prior remains independent of the image context. For effective fusion\nof kernel prior information, we adopt a rational implementation method inspired\nby traditional deblurring algorithms that perform deconvolution in the\nfrequency domain. We propose a module called Frequency Integration Module (FIM)\nfor fusing the kernel prior and combine it with a frequency-based deblurring\nTransfomer network. Experimental results demonstrate that our method\noutperforms state-of-the-art methods on multiple blind image deblurring tasks,\nshowcasing robust generalization abilities. Source code will be available soon."}
{"id": "2504.13984", "pdf": "https://arxiv.org/pdf/2504.13984", "abs": "https://arxiv.org/abs/2504.13984", "authors": ["Amrit Diggavi Seshadri"], "title": "One Jump Is All You Need: Short-Cutting Transformers for Early Exit Prediction with One Jump to Fit All Exit Levels", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "To reduce the time and computational costs of inference of large language\nmodels, there has been interest in parameter-efficient low-rank early-exit\ncasting of transformer hidden-representations to final-representations. Such\nlow-rank short-cutting has been shown to outperform identity shortcuts at early\nmodel stages while offering parameter-efficiency in shortcut jumps. However,\ncurrent low-rank methods maintain a separate early-exit shortcut jump to\nfinal-representations for each transformer intermediate block-level during\ninference. In this work, we propose selection of a single One-Jump-Fits-All\n(OJFA) low-rank shortcut that offers over a 30x reduction in shortcut parameter\ncosts during inference. We show that despite this extreme reduction, our OJFA\nchoice largely matches the performance of maintaining multiple shortcut jumps\nduring inference and offers stable precision from all transformer block-levels\nfor GPT2-XL, Phi3-Mini and Llama2-7B transformer models."}
{"id": "2504.13949", "pdf": "https://arxiv.org/pdf/2504.13949", "abs": "https://arxiv.org/abs/2504.13949", "authors": ["M. W. Przewozniczek", "F. Chicano", "R. Tinós", "J. Nalepa", "B. Ruszczak", "A. M. Wijata"], "title": "On Revealing the Hidden Problem Structure in Real-World and Theoretical Problems Using Walsh Coefficient Influence", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Gray-box optimization employs Walsh decomposition to obtain non-linear\nvariable dependencies and utilize them to propose masks of variables that have\na joint non-linear influence on fitness value. These masks significantly\nimprove the effectiveness of variation operators. In some problems, all\nvariables are non-linearly dependent, making the aforementioned masks useless.\nWe analyze the features of the real-world instances of such problems and show\nthat many of their dependencies may have noise-like origins. Such noise-caused\ndependencies are irrelevant to the optimization process and can be ignored. To\nidentify them, we propose extending the use of Walsh decomposition by measuring\nvariable dependency strength that allows the construction of the weighted\ndynamic Variable Interaction Graph (wdVIG). wdVIGs adjust the dependency\nstrength to mixed individuals. They allow the filtering of irrelevant\ndependencies and re-enable using dependency-based masks by variation operators.\nWe verify the wdVIG potential on a large benchmark suite. For problems with\nnoise, the wdVIG masks can improve the optimizer's effectiveness. If all\ndependencies are relevant for the optimization, i.e., the problem is not\nnoised, the influence of wdVIG masks is similar to that of state-of-the-art\nstructures of this kind."}
{"id": "2504.14665", "pdf": "https://arxiv.org/pdf/2504.14665", "abs": "https://arxiv.org/abs/2504.14665", "authors": ["A S M Sharifuzzaman Sagar", "Yu Chen", "Jun Hoong Chan"], "title": "DMPCN: Dynamic Modulated Predictive Coding Network with Hybrid Feedback Representations", "categories": ["cs.CV"], "comment": null, "summary": "Traditional predictive coding networks, inspired by theories of brain\nfunction, consistently achieve promising results across various domains,\nextending their influence into the field of computer vision. However, the\nperformance of the predictive coding networks is limited by their error\nfeedback mechanism, which traditionally employs either local or global\nrecurrent updates, leading to suboptimal performance in processing both local\nand broader details simultaneously. In addition, traditional predictive coding\nnetworks face difficulties in dynamically adjusting to the complexity and\ncontext of varying input data, which is crucial for achieving high levels of\nperformance in diverse scenarios. Furthermore, there is a gap in the\ndevelopment and application of specific loss functions that could more\neffectively guide the model towards optimal performance. To deal with these\nissues, this paper introduces a hybrid prediction error feedback mechanism with\ndynamic modulation for deep predictive coding networks by effectively combining\nglobal contexts and local details while adjusting feedback based on input\ncomplexity. Additionally, we present a loss function tailored to this framework\nto improve accuracy by focusing on precise prediction error minimization.\nExperimental results demonstrate the superiority of our model over other\napproaches, showcasing faster convergence and higher predictive accuracy in\nCIFAR-10, CIFAR-100, MNIST, and FashionMNIST datasets."}
{"id": "2504.13989", "pdf": "https://arxiv.org/pdf/2504.13989", "abs": "https://arxiv.org/abs/2504.13989", "authors": ["Lucas Maisonnave", "Cyril Moineau", "Olivier Bichler", "Fabrice Rastello"], "title": "Gradual Binary Search and Dimension Expansion : A general method for activation quantization in LLMs", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have become pivotal in artificial intelligence,\ndemonstrating strong capabilities in reasoning, understanding, and generating\ndata. However, their deployment on edge devices is hindered by their\nsubstantial size, often reaching several billion parameters. Quantization is a\nwidely used method to reduce memory usage and inference time, however LLMs\npresent unique challenges due to the prevalence of outliers in their\nactivations. In this work, we leverage the theoretical advantages of Hadamard\nmatrices over random rotation matrices to push the boundaries of quantization\nin LLMs. We demonstrate that Hadamard matrices are more effective in reducing\noutliers, which are a significant obstacle in achieving low-bit quantization.\nOur method based on a gradual binary search enables 3-bit quantization for\nweights, activations, and key-value (KV) caches, resulting in a 40\\% increase\nin accuracy on common benchmarks compared to SoTA methods. We extend the use of\nrotation matrices to support non-power-of-2 embedding dimensions, similar to\nthe Qwen architecture, by employing the Paley algorithm. We theoretically\ndemonstrates the superiority of Hadamard matrices in reducing outliers.We\nachieved 3-bit quantization for weights, activations, and KV cache,\nsignificantly enhancing model performance. Our experimental results on multiple\nmodels family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of\nour approach, outperforming existing methods and enabling practical 3-bit\nquantization."}
{"id": "2504.13950", "pdf": "https://arxiv.org/pdf/2504.13950", "abs": "https://arxiv.org/abs/2504.13950", "authors": ["Zhongxi Qiu", "Zhang Zhang", "Yan Hu", "Heng Li", "Jiang Liu"], "title": "Open-Medical-R1: How to Choose Data for RLVR Training at Medicine Domain", "categories": ["cs.LG", "cs.AI"], "comment": "15 figures", "summary": "This paper explores optimal data selection strategies for Reinforcement\nLearning with Verified Rewards (RLVR) training in the medical domain. While\nRLVR has shown exceptional potential for enhancing reasoning capabilities in\nlarge language models, most prior implementations have focused on mathematics\nand logical puzzles, with limited exploration of domain-specific applications\nlike medicine. We investigate four distinct data sampling strategies from\nMedQA-USMLE: random sampling (baseline), and filtering using Phi-4,\nGemma-3-27b-it, and Gemma-3-12b-it models. Using Gemma-3-12b-it as our base\nmodel and implementing Group Relative Policy Optimization (GRPO), we evaluate\nperformance across multiple benchmarks including MMLU, GSM8K, MMLU-Pro, and\nCMMLU. Our findings demonstrate that models trained on filtered data generally\noutperform those trained on randomly selected samples. Notably, training on\nself-filtered samples (using Gemma-3-12b-it for filtering) achieved superior\nperformance in medical domains but showed reduced robustness across different\nbenchmarks, while filtering with larger models from the same series yielded\nbetter overall robustness. These results provide valuable insights into\neffective data organization strategies for RLVR in specialized domains and\nhighlight the importance of thoughtful data selection in achieving optimal\nperformance. You can access our repository\n(https://github.com/Qsingle/open-medical-r1) to get the codes."}
{"id": "2504.14666", "pdf": "https://arxiv.org/pdf/2504.14666", "abs": "https://arxiv.org/abs/2504.14666", "authors": ["Kaihang Pan", "Wang Lin", "Zhongqi Yue", "Tenglong Ao", "Liyu Jia", "Wei Zhao", "Juncheng Li", "Siliang Tang", "Hanwang Zhang"], "title": "Generative Multimodal Pretraining with Discrete Diffusion Timestep Tokens", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025 (Oral)", "summary": "Recent endeavors in Multimodal Large Language Models (MLLMs) aim to unify\nvisual comprehension and generation by combining LLM and diffusion models, the\nstate-of-the-art in each task, respectively. Existing approaches rely on\nspatial visual tokens, where image patches are encoded and arranged according\nto a spatial order (e.g., raster scan). However, we show that spatial tokens\nlack the recursive structure inherent to languages, hence form an impossible\nlanguage for LLM to master. In this paper, we build a proper visual language by\nleveraging diffusion timesteps to learn discrete, recursive visual tokens. Our\nproposed tokens recursively compensate for the progressive attribute loss in\nnoisy images as timesteps increase, enabling the diffusion model to reconstruct\nthe original image at any timestep. This approach allows us to effectively\nintegrate the strengths of LLMs in autoregressive reasoning and diffusion\nmodels in precise image generation, achieving seamless multimodal comprehension\nand generation within a unified framework. Extensive experiments show that we\nachieve superior performance for multimodal comprehension and generation\nsimultaneously compared with other MLLMs. Project Page:\nhttps://DDT-LLaMA.github.io/."}
{"id": "2504.14053", "pdf": "https://arxiv.org/pdf/2504.14053", "abs": "https://arxiv.org/abs/2504.14053", "authors": ["Ali Safari"], "title": "Sentiment Analysis of Airbnb Reviews: Exploring Their Impact on Acceptance Rates and Pricing Across Multiple U.S. Regions", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "This research examines whether Airbnb guests' positive and negative comments\ninfluence acceptance rates and rental prices across six U.S. regions: Rhode\nIsland, Broward County, Chicago, Dallas, San Diego, and Boston. Thousands of\nreviews were collected and analyzed using Natural Language Processing (NLP) to\nclassify sentiments as positive or negative, followed by statistical testing\n(t-tests and basic correlations) on the average scores. The findings reveal\nthat over 90 percent of reviews in each region are positive, indicating that\nhaving additional reviews does not significantly enhance prices. However,\nlistings with predominantly positive feedback exhibit slightly higher\nacceptance rates, suggesting that sentiment polarity, rather than the sheer\nvolume of reviews, is a more critical factor for host success. Additionally,\nbudget listings often gather extensive reviews while maintaining competitive\npricing, whereas premium listings sustain higher prices with fewer but highly\npositive reviews. These results underscore the importance of sentiment quality\nover quantity in shaping guest behavior and pricing strategies in an\noverwhelmingly positive review environment."}
{"id": "2504.13951", "pdf": "https://arxiv.org/pdf/2504.13951", "abs": "https://arxiv.org/abs/2504.13951", "authors": ["Michele Casoni", "Tommaso Guidi", "Alessandro Betti", "Stefano Melacci", "Marco Gori"], "title": "Generative System Dynamics in Recurrent Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In this study, we investigate the continuous time dynamics of Recurrent\nNeural Networks (RNNs), focusing on systems with nonlinear activation\nfunctions. The objective of this work is to identify conditions under which\nRNNs exhibit perpetual oscillatory behavior, without converging to static fixed\npoints. We establish that skew-symmetric weight matrices are fundamental to\nenable stable limit cycles in both linear and nonlinear configurations. We\nfurther demonstrate that hyperbolic tangent-like activation functions (odd,\nbounded, and continuous) preserve these oscillatory dynamics by ensuring motion\ninvariants in state space. Numerical simulations showcase how nonlinear\nactivation functions not only maintain limit cycles, but also enhance the\nnumerical stability of the system integration process, mitigating those\ninstabilities that are commonly associated with the forward Euler method. The\nexperimental results of this analysis highlight practical considerations for\ndesigning neural architectures capable of capturing complex temporal\ndependencies, i.e., strategies for enhancing memorization skills in recurrent\nmodels."}
{"id": "2504.14687", "pdf": "https://arxiv.org/pdf/2504.14687", "abs": "https://arxiv.org/abs/2504.14687", "authors": ["Seokju Cho", "Jiahui Huang", "Seungryong Kim", "Joon-Young Lee"], "title": "Seurat: From Moving Points to Depth", "categories": ["cs.CV"], "comment": "CVPR 2025 Highlight. Project page: https://seurat-cvpr.github.io", "summary": "Accurate depth estimation from monocular videos remains challenging due to\nambiguities inherent in single-view geometry, as crucial depth cues like\nstereopsis are absent. However, humans often perceive relative depth\nintuitively by observing variations in the size and spacing of objects as they\nmove. Inspired by this, we propose a novel method that infers relative depth by\nexamining the spatial relationships and temporal evolution of a set of tracked\n2D trajectories. Specifically, we use off-the-shelf point tracking models to\ncapture 2D trajectories. Then, our approach employs spatial and temporal\ntransformers to process these trajectories and directly infer depth changes\nover time. Evaluated on the TAPVid-3D benchmark, our method demonstrates robust\nzero-shot performance, generalizing effectively from synthetic to real-world\ndatasets. Results indicate that our approach achieves temporally smooth,\nhigh-accuracy depth predictions across diverse domains."}
{"id": "2504.14107", "pdf": "https://arxiv.org/pdf/2504.14107", "abs": "https://arxiv.org/abs/2504.14107", "authors": ["Jennifer Hu", "Michael A. Lepori", "Michael Franke"], "title": "Linking forward-pass dynamics in Transformers and real-time human processing", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Modern AI models are increasingly being used as theoretical tools to study\nhuman cognition. One dominant approach is to evaluate whether human-derived\nmeasures (such as offline judgments or real-time processing) are predicted by a\nmodel's output: that is, the end-product of forward pass(es) through the\nnetwork. At the same time, recent advances in mechanistic interpretability have\nbegun to reveal the internal processes that give rise to model outputs, raising\nthe question of whether models and humans might arrive at outputs using similar\n\"processing strategies\". Here, we investigate the link between real-time\nprocessing in humans and \"layer-time\" dynamics in Transformer models. Across\nfive studies spanning domains and modalities, we test whether the dynamics of\ncomputation in a single forward pass of pre-trained Transformers predict\nsignatures of processing in humans, above and beyond properties of the model's\noutput probability distribution. We consistently find that layer-time dynamics\nprovide additional predictive power on top of output measures. Our results\nsuggest that Transformer processing and human processing may be facilitated or\nimpeded by similar properties of an input stimulus, and this similarity has\nemerged through general-purpose objectives such as next-token prediction or\nimage recognition. Our work suggests a new way of using AI models to study\nhuman cognition: not just as a black box mapping stimuli to responses, but\npotentially also as explicit processing models."}
{"id": "2504.13955", "pdf": "https://arxiv.org/pdf/2504.13955", "abs": "https://arxiv.org/abs/2504.13955", "authors": ["Suhas BN", "Dominik Mattioli", "Saeed Abdullah", "Rosa I. Arriaga", "Chris W. Wiese", "Andrew M. Sherrill"], "title": "Thousand Voices of Trauma: A Large-Scale Synthetic Dataset for Modeling Prolonged Exposure Therapy Conversations", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC", "cs.LG", "68T50", "I.2.7; H.5.2"], "comment": "14 pages, 6 figures", "summary": "The advancement of AI systems for mental health support is hindered by\nlimited access to therapeutic conversation data, particularly for trauma\ntreatment. We present Thousand Voices of Trauma, a synthetic benchmark dataset\nof 3,000 therapy conversations based on Prolonged Exposure therapy protocols\nfor Post-traumatic Stress Disorder (PTSD). The dataset comprises 500 unique\ncases, each explored through six conversational perspectives that mirror the\nprogression of therapy from initial anxiety to peak distress to emotional\nprocessing. We incorporated diverse demographic profiles (ages 18-80, M=49.3,\n49.4% male, 44.4% female, 6.2% non-binary), 20 trauma types, and 10\ntrauma-related behaviors using deterministic and probabilistic generation\nmethods. Analysis reveals realistic distributions of trauma types (witnessing\nviolence 10.6%, bullying 10.2%) and symptoms (nightmares 23.4%, substance abuse\n20.8%). Clinical experts validated the dataset's therapeutic fidelity,\nhighlighting its emotional depth while suggesting refinements for greater\nauthenticity. We also developed an emotional trajectory benchmark with\nstandardized metrics for evaluating model responses. This privacy-preserving\ndataset addresses critical gaps in trauma-focused mental health data, offering\na valuable resource for advancing both patient-facing applications and\nclinician training tools."}
{"id": "2504.14693", "pdf": "https://arxiv.org/pdf/2504.14693", "abs": "https://arxiv.org/abs/2504.14693", "authors": ["Enxin Song", "Wenhao Chai", "Weili Xu", "Jianwen Xie", "Yuxuan Liu", "Gaoang Wang"], "title": "Video-MMLU: A Massive Multi-Discipline Lecture Understanding Benchmark", "categories": ["cs.CV", "cs.AI"], "comment": "Code, docs, and benchmark are all avaliable at\n  https://enxinsong.com/Video-MMLU-web/", "summary": "Recent advancements in language multimodal models (LMMs) for video have\ndemonstrated their potential for understanding video content, yet the task of\ncomprehending multi-discipline lectures remains largely unexplored. We\nintroduce Video-MMLU, a massive benchmark designed to evaluate the capabilities\nof LMMs in understanding Multi-Discipline Lectures. We evaluate over 90\nopen-source and proprietary models, ranging from 0.5B to 40B parameters. Our\nresults highlight the limitations of current models in addressing the cognitive\nchallenges presented by these lectures, especially in tasks requiring both\nperception and reasoning. Additionally, we explore how the number of visual\ntokens and the large language models influence performance, offering insights\ninto the interplay between multimodal perception and reasoning in lecture\ncomprehension."}
{"id": "2504.14110", "pdf": "https://arxiv.org/pdf/2504.14110", "abs": "https://arxiv.org/abs/2504.14110", "authors": ["Theo Jaffrelot Inizan", "Sherry Yang", "Aaron Kaplan", "Yen-hsu Lin", "Jian Yin", "Saber Mirzaei", "Mona Abdelgaid", "Ali H. Alawadhi", "KwangHwan Cho", "Zhiling Zheng", "Ekin Dogus Cubuk", "Christian Borgs", "Jennifer T. Chayes", "Kristin A. Persson", "Omar M. Yaghi"], "title": "System of Agentic AI for the Discovery of Metal-Organic Frameworks", "categories": ["cond-mat.mtrl-sci", "cs.AI", "cs.CL", "cs.MA"], "comment": null, "summary": "Generative models and machine learning promise accelerated material discovery\nin MOFs for CO2 capture and water harvesting but face significant challenges\nnavigating vast chemical spaces while ensuring synthetizability. Here, we\npresent MOFGen, a system of Agentic AI comprising interconnected agents: a\nlarge language model that proposes novel MOF compositions, a diffusion model\nthat generates crystal structures, quantum mechanical agents that optimize and\nfilter candidates, and synthetic-feasibility agents guided by expert rules and\nmachine learning. Trained on all experimentally reported MOFs and computational\ndatabases, MOFGen generated hundreds of thousands of novel MOF structures and\nsynthesizable organic linkers. Our methodology was validated through\nhigh-throughput experiments and the successful synthesis of five \"AI-dreamt\"\nMOFs, representing a major step toward automated synthesizable material\ndiscovery."}
{"id": "2504.13957", "pdf": "https://arxiv.org/pdf/2504.13957", "abs": "https://arxiv.org/abs/2504.13957", "authors": ["Lianne Potter"], "title": "Naming is framing: How cybersecurity's language problems are repeating in AI governance", "categories": ["cs.CY", "cs.AI", "cs.CR"], "comment": "20 pages, 2 figures", "summary": "Language is not neutral; it frames understanding, structures power, and\nshapes governance. This paper argues that misnomers like cybersecurity and\nartificial intelligence (AI) are more than semantic quirks; they carry\nsignificant governance risks by obscuring human agency, inflating expectations,\nand distorting accountability. Drawing on lessons from cybersecurity's\nlinguistic pitfalls, such as the 'weakest link' narrative, this paper\nhighlights how AI discourse is falling into similar traps with metaphors like\n'alignment,' 'black box,' and 'hallucination.' These terms embed adversarial,\nmystifying, or overly technical assumptions into governance structures. In\nresponse, the paper advocates for a language-first approach to AI governance:\none that interrogates dominant metaphors, foregrounds human roles, and\nco-develops a lexicon that is precise, inclusive, and reflexive. This paper\ncontends that linguistic reform is not peripheral to governance but central to\nthe construction of transparent, equitable, and anticipatory regulatory\nframeworks."}
{"id": "2504.14699", "pdf": "https://arxiv.org/pdf/2504.14699", "abs": "https://arxiv.org/abs/2504.14699", "authors": ["Sascha Jecklin", "Aidana Massalimova", "Ruyi Zha", "Lilian Calvet", "Christoph J. Laux", "Mazda Farshad", "Philipp Fürnstahl"], "title": "IXGS-Intraoperative 3D Reconstruction from Sparse, Arbitrarily Posed Real X-rays", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Spine surgery is a high-risk intervention demanding precise execution, often\nsupported by image-based navigation systems. Recently, supervised learning\napproaches have gained attention for reconstructing 3D spinal anatomy from\nsparse fluoroscopic data, significantly reducing reliance on\nradiation-intensive 3D imaging systems. However, these methods typically\nrequire large amounts of annotated training data and may struggle to generalize\nacross varying patient anatomies or imaging conditions. Instance-learning\napproaches like Gaussian splatting could offer an alternative by avoiding\nextensive annotation requirements. While Gaussian splatting has shown promise\nfor novel view synthesis, its application to sparse, arbitrarily posed real\nintraoperative X-rays has remained largely unexplored. This work addresses this\nlimitation by extending the $R^2$-Gaussian splatting framework to reconstruct\nanatomically consistent 3D volumes under these challenging conditions. We\nintroduce an anatomy-guided radiographic standardization step using style\ntransfer, improving visual consistency across views, and enhancing\nreconstruction quality. Notably, our framework requires no pretraining, making\nit inherently adaptable to new patients and anatomies. We evaluated our\napproach using an ex-vivo dataset. Expert surgical evaluation confirmed the\nclinical utility of the 3D reconstructions for navigation, especially when\nusing 20 to 30 views, and highlighted the standardization's benefit for\nanatomical clarity. Benchmarking via quantitative 2D metrics (PSNR/SSIM)\nconfirmed performance trade-offs compared to idealized settings, but also\nvalidated the improvement gained from standardization over raw inputs. This\nwork demonstrates the feasibility of instance-based volumetric reconstruction\nfrom arbitrary sparse-view X-rays, advancing intraoperative 3D imaging for\nsurgical navigation."}
{"id": "2504.14123", "pdf": "https://arxiv.org/pdf/2504.14123", "abs": "https://arxiv.org/abs/2504.14123", "authors": ["Mingyu Kim", "Jongwoo Ko", "Mijung Park"], "title": "Bayesian Principles Improve Prompt Learning In Vision-Language Models", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "AISTATS2025", "summary": "Prompt learning is a popular fine-tuning method for vision-language models\ndue to its efficiency. It requires a small number of additional learnable\nparameters while significantly enhancing performance on target tasks. However,\nmost existing methods suffer from overfitting to fine-tuning data, yielding\npoor generalizability. To address this, we propose a new training objective\nfunction based on a Bayesian learning principle to balance adaptability and\ngeneralizability. We derive a prior over the logits, where the mean function is\nparameterized by the pre-trained model, while the posterior corresponds to the\nfine-tuned model. This objective establishes a balance by allowing the\nfine-tuned model to adapt to downstream tasks while remaining close to the\npre-trained model."}
{"id": "2504.13958", "pdf": "https://arxiv.org/pdf/2504.13958", "abs": "https://arxiv.org/abs/2504.13958", "authors": ["Cheng Qian", "Emre Can Acikgoz", "Qi He", "Hongru Wang", "Xiusi Chen", "Dilek Hakkani-Tür", "Gokhan Tur", "Heng Ji"], "title": "ToolRL: Reward is All Tool Learning Needs", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "19 Pages, 12 Figures, 12 Tables", "summary": "Current Large Language Models (LLMs) often undergo supervised fine-tuning\n(SFT) to acquire tool use capabilities. However, SFT struggles to generalize to\nunfamiliar or complex tool use scenarios. Recent advancements in reinforcement\nlearning (RL), particularly with R1-like models, have demonstrated promising\nreasoning and generalization abilities. Yet, reward design for tool use\npresents unique challenges: multiple tools may be invoked with diverse\nparameters, and coarse-grained reward signals, such as answer matching, fail to\noffer the finegrained feedback required for effective learning. In this work,\nwe present the first comprehensive study on reward design for tool selection\nand application tasks within the RL paradigm. We systematically explore a wide\nrange of reward strategies, analyzing their types, scales, granularity, and\ntemporal dynamics. Building on these insights, we propose a principled reward\ndesign tailored for tool use tasks and apply it to train LLMs using Group\nRelative Policy Optimization (GRPO). Empirical evaluations across diverse\nbenchmarks demonstrate that our approach yields robust, scalable, and stable\ntraining, achieving a 17% improvement over base models and a 15% gain over SFT\nmodels. These results highlight the critical role of thoughtful reward design\nin enhancing the tool use capabilities and generalization performance of LLMs.\nAll the codes are released to facilitate future research."}
{"id": "2504.14708", "pdf": "https://arxiv.org/pdf/2504.14708", "abs": "https://arxiv.org/abs/2504.14708", "authors": ["Parshuram N. Aarotale", "Ajita Rattani"], "title": "Time Frequency Analysis of EMG Signal for Gesture Recognition using Fine grained Features", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Electromyography (EMG) based hand gesture recognition converts forearm muscle\nactivity into control commands for prosthetics, rehabilitation, and human\ncomputer interaction. This paper proposes a novel approach to EMG-based hand\ngesture recognition that uses fine-grained classification and presents XMANet,\nwhich unifies low-level local and high level semantic cues through cross layer\nmutual attention among shallow to deep CNN experts. Using stacked spectrograms\nand scalograms derived from the Short Time Fourier Transform (STFT) and Wavelet\nTransform (WT), we benchmark XMANet against ResNet50, DenseNet-121,\nMobileNetV3, and EfficientNetB0. Experimental results on the Grabmyo dataset\nindicate that, using STFT, the proposed XMANet model outperforms the baseline\nResNet50, EfficientNetB0, MobileNetV3, and DenseNet121 models with improvement\nof approximately 1.72%, 4.38%, 5.10%, and 2.53%, respectively. When employing\nthe WT approach, improvements of around 1.57%, 1.88%, 1.46%, and 2.05% are\nobserved over the same baselines. Similarly, on the FORS EMG dataset, the\nXMANet(ResNet50) model using STFT shows an improvement of about 5.04% over the\nbaseline ResNet50. In comparison, the XMANet(DenseNet121) and\nXMANet(MobileNetV3) models yield enhancements of approximately 4.11% and 2.81%,\nrespectively. Moreover, when using WT, the proposed XMANet achieves gains of\naround 4.26%, 9.36%, 5.72%, and 6.09% over the baseline ResNet50, DenseNet121,\nMobileNetV3, and EfficientNetB0 models, respectively. These results confirm\nthat XMANet consistently improves performance across various architectures and\nsignal processing techniques, demonstrating the strong potential of fine\ngrained features for accurate and robust EMG classification."}
{"id": "2504.14126", "pdf": "https://arxiv.org/pdf/2504.14126", "abs": "https://arxiv.org/abs/2504.14126", "authors": ["Saad Hameed", "Basheer Qolomany", "Samir Brahim Belhaouari", "Mohamed Abdallah", "Junaid Qadir", "Ala Al-Fuqaha"], "title": "Large Language Model Enhanced Particle Swarm Optimization for Hyperparameter Tuning for Deep Learning Models", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Determining the ideal architecture for deep learning models, such as the\nnumber of layers and neurons, is a difficult and resource-intensive process\nthat frequently relies on human tuning or computationally costly optimization\napproaches. While Particle Swarm Optimization (PSO) and Large Language Models\n(LLMs) have been individually applied in optimization and deep learning, their\ncombined use for enhancing convergence in numerical optimization tasks remains\nunderexplored. Our work addresses this gap by integrating LLMs into PSO to\nreduce model evaluations and improve convergence for deep learning\nhyperparameter tuning. The proposed LLM-enhanced PSO method addresses the\ndifficulties of efficiency and convergence by using LLMs (particularly\nChatGPT-3.5 and Llama3) to improve PSO performance, allowing for faster\nachievement of target objectives. Our method speeds up search space exploration\nby substituting underperforming particle placements with best suggestions\noffered by LLMs. Comprehensive experiments across three scenarios -- (1)\noptimizing the Rastrigin function, (2) using Long Short-Term Memory (LSTM)\nnetworks for time series regression, and (3) using Convolutional Neural\nNetworks (CNNs) for material classification -- show that the method\nsignificantly improves convergence rates and lowers computational costs.\nDepending on the application, computational complexity is lowered by 20% to 60%\ncompared to traditional PSO methods. Llama3 achieved a 20% to 40% reduction in\nmodel calls for regression tasks, whereas ChatGPT-3.5 reduced model calls by\n60% for both regression and classification tasks, all while preserving accuracy\nand error rates. This groundbreaking methodology offers a very efficient and\neffective solution for optimizing deep learning models, leading to substantial\ncomputational performance improvements across a wide range of applications."}
{"id": "2504.13959", "pdf": "https://arxiv.org/pdf/2504.13959", "abs": "https://arxiv.org/abs/2504.13959", "authors": ["Sanchaita Hazra", "Bodhisattwa Prasad Majumder", "Tuhin Chakrabarty"], "title": "AI Safety Should Prioritize the Future of Work", "categories": ["cs.CY", "cs.AI", "cs.CL", "econ.GN", "q-fin.EC"], "comment": null, "summary": "Current efforts in AI safety prioritize filtering harmful content, preventing\nmanipulation of human behavior, and eliminating existential risks in\ncybersecurity or biosecurity. While pressing, this narrow focus overlooks\ncritical human-centric considerations that shape the long-term trajectory of a\nsociety. In this position paper, we identify the risks of overlooking the\nimpact of AI on the future of work and recommend comprehensive transition\nsupport towards the evolution of meaningful labor with human agency. Through\nthe lens of economic theories, we highlight the intertemporal impacts of AI on\nhuman livelihood and the structural changes in labor markets that exacerbate\nincome inequality. Additionally, the closed-source approach of major\nstakeholders in AI development resembles rent-seeking behavior through\nexploiting resources, breeding mediocrity in creative labor, and monopolizing\ninnovation. To address this, we argue in favor of a robust international\ncopyright anatomy supported by implementing collective licensing that ensures\nfair compensation mechanisms for using data to train AI models. We strongly\nrecommend a pro-worker framework of global AI governance to enhance shared\nprosperity and economic justice while reducing technical debt."}
{"id": "2504.14709", "pdf": "https://arxiv.org/pdf/2504.14709", "abs": "https://arxiv.org/abs/2504.14709", "authors": ["Hui Zhou", "Shaoshuai Shi", "Hongsheng Li"], "title": "Exposing the Copycat Problem of Imitation-based Planner: A Novel Closed-Loop Simulator, Causal Benchmark and Joint IL-RL Baseline", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Machine learning (ML)-based planners have recently gained significant\nattention. They offer advantages over traditional optimization-based planning\nalgorithms. These advantages include fewer manually selected parameters and\nfaster development. Within ML-based planning, imitation learning (IL) is a\ncommon algorithm. It primarily learns driving policies directly from supervised\ntrajectory data. While IL has demonstrated strong performance on many open-loop\nbenchmarks, it remains challenging to determine if the learned policy truly\nunderstands fundamental driving principles, rather than simply extrapolating\nfrom the ego-vehicle's initial state. Several studies have identified this\nlimitation and proposed algorithms to address it. However, these methods often\nuse original datasets for evaluation. In these datasets, future trajectories\nare heavily dependent on initial conditions. Furthermore, IL often overfits to\nthe most common scenarios. It struggles to generalize to rare or unseen\nsituations.\n  To address these challenges, this work proposes: 1) a novel closed-loop\nsimulator supporting both imitation and reinforcement learning, 2) a causal\nbenchmark derived from the Waymo Open Dataset to rigorously assess the impact\nof the copycat problem, and 3) a novel framework integrating imitation learning\nand reinforcement learning to overcome the limitations of purely imitative\napproaches. The code for this work will be released soon."}
{"id": "2504.14128", "pdf": "https://arxiv.org/pdf/2504.14128", "abs": "https://arxiv.org/abs/2504.14128", "authors": ["Christopher Zhang Cui", "Xingdi Yuan", "Ziang Xiao", "Prithviraj Ammanabrolu", "Marc-Alexandre Côté"], "title": "TALES: Text Adventure Learning Environment Suite", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Reasoning is an essential skill to enable Large Language Models (LLMs) to\ninteract with the world. As tasks become more complex, they demand increasingly\nsophisticated and diverse reasoning capabilities for sequential\ndecision-making, requiring structured reasoning over the context history to\ndetermine the next best action. We introduce TALES, a diverse collection of\nsynthetic and human-written text-adventure games designed to challenge and\nevaluate diverse reasoning capabilities. We present results over a range of\nLLMs, open- and closed-weights, performing a qualitative analysis on the top\nperforming models. Despite an impressive showing on synthetic games, even the\ntop LLM-driven agents fail to achieve 15% on games designed for human\nenjoyment. Code and visualization of the experiments can be found at\nhttps://microsoft.github.io/tales."}
{"id": "2504.13961", "pdf": "https://arxiv.org/pdf/2504.13961", "abs": "https://arxiv.org/abs/2504.13961", "authors": ["Chao Yang", "Xiannan Huang", "Shuhan Qiu", "Yan Cheng"], "title": "CONTINA: Confidence Interval for Traffic Demand Prediction with Coverage Guarantee", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Accurate short-term traffic demand prediction is critical for the operation\nof traffic systems. Besides point estimation, the confidence interval of the\nprediction is also of great importance. Many models for traffic operations,\nsuch as shared bike rebalancing and taxi dispatching, take into account the\nuncertainty of future demand and require confidence intervals as the input.\nHowever, existing methods for confidence interval modeling rely on strict\nassumptions, such as unchanging traffic patterns and correct model\nspecifications, to guarantee enough coverage. Therefore, the confidence\nintervals provided could be invalid, especially in a changing traffic\nenvironment. To fill this gap, we propose an efficient method, CONTINA\n(Conformal Traffic Intervals with Adaptation) to provide interval predictions\nthat can adapt to external changes. By collecting the errors of interval during\ndeployment, the method can adjust the interval in the next step by widening it\nif the errors are too large or shortening it otherwise. Furthermore, we\ntheoretically prove that the coverage of the confidence intervals provided by\nour method converges to the target coverage level. Experiments across four\nreal-world datasets and prediction models demonstrate that the proposed method\ncan provide valid confidence intervals with shorter lengths. Our method can\nhelp traffic management personnel develop a more reasonable and robust\noperation plan in practice. And we release the code, model and dataset in\n\\href{ https://github.com/xiannanhuang/CONTINA/}{ Github}."}
{"id": "2504.14715", "pdf": "https://arxiv.org/pdf/2504.14715", "abs": "https://arxiv.org/abs/2504.14715", "authors": ["Md. Sanaullah Chowdhury", "Salauddin Tapu", "Noyon Kumar Sarkar", "Ferdous Bin Ali", "Lameya Sabrin"], "title": "Med-2D SegNet: A Light Weight Deep Neural Network for Medical 2D Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Accurate and efficient medical image segmentation is crucial for advancing\nclinical diagnostics and surgical planning, yet remains a complex challenge due\nto the variability in anatomical structures and the demand for low-complexity\nmodels. In this paper, we introduced Med-2D SegNet, a novel and highly\nefficient segmentation architecture that delivers outstanding accuracy while\nmaintaining a minimal computational footprint. Med-2D SegNet achieves\nstate-of-the-art performance across multiple benchmark datasets, including\nKVASIR-SEG, PH2, EndoVis, and GLAS, with an average Dice similarity coefficient\n(DSC) of 89.77% across 20 diverse datasets. Central to its success is the\ncompact Med Block, a specialized encoder design that incorporates dimension\nexpansion and parameter reduction, enabling precise feature extraction while\nkeeping model parameters to a low count of just 2.07 million. Med-2D SegNet\nexcels in cross-dataset generalization, particularly in polyp segmentation,\nwhere it was trained on KVASIR-SEG and showed strong performance on unseen\ndatasets, demonstrating its robustness in zero-shot learning scenarios, even\nthough we acknowledge that further improvements are possible. With top-tier\nperformance in both binary and multi-class segmentation, Med-2D SegNet\nredefines the balance between accuracy and efficiency, setting a new benchmark\nfor medical image analysis. This work paves the way for developing accessible,\nhigh-performance diagnostic tools suitable for clinical environments and\nresource-constrained settings, making it a step forward in the democratization\nof advanced medical technology."}
{"id": "2504.14147", "pdf": "https://arxiv.org/pdf/2504.14147", "abs": "https://arxiv.org/abs/2504.14147", "authors": ["Jiakai Tang", "Jingsen Zhang", "Zihang Tian", "Xueyang Feng", "Lei Wang", "Xu Chen"], "title": "HF4Rec: Human-Like Feedback-Driven Optimization Framework for Explainable Recommendation", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent advancements in explainable recommendation have greatly bolstered user\nexperience by elucidating the decision-making rationale. However, the existing\nmethods actually fail to provide effective feedback signals for potentially\nbetter or worse generated explanations due to their reliance on traditional\nsupervised learning paradigms in sparse interaction data. To address these\nissues, we propose a novel human-like feedback-driven optimization framework.\nThis framework employs a dynamic interactive optimization mechanism for\nachieving human-centered explainable requirements without incurring high labor\ncosts. Specifically, we propose to utilize large language models (LLMs) as\nhuman simulators to predict human-like feedback for guiding the learning\nprocess. To enable the LLMs to deeply understand the task essence and meet\nuser's diverse personalized requirements, we introduce a human-induced\ncustomized reward scoring method, which helps stimulate the language\nunderstanding and logical reasoning capabilities of LLMs. Furthermore,\nconsidering the potential conflicts between different perspectives of\nexplanation quality, we introduce a principled Pareto optimization that\ntransforms the multi-perspective quality enhancement task into a\nmulti-objective optimization problem for improving explanation performance. At\nlast, to achieve efficient model training, we design an off-policy optimization\npipeline. By incorporating a replay buffer and addressing the data distribution\nbiases, we can effectively improve data utilization and enhance model\ngenerality. Extensive experiments on four datasets demonstrate the superiority\nof our approach."}
{"id": "2504.13969", "pdf": "https://arxiv.org/pdf/2504.13969", "abs": "https://arxiv.org/abs/2504.13969", "authors": ["Nayoung Choi", "Peace Cyebukayire", "Jinho D. Choi"], "title": "Tinker Tales: Interactive Storytelling Framework for Early Childhood Narrative Development and AI Literacy", "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": null, "summary": "This paper presents Tinker Tales, an interactive storytelling framework in\nthe format of a board game, designed to support both narrative development and\nAI literacy in early childhood. The framework integrates tangible and\nspeech-based interactions with AI through NFC chip-attached pawns and tokens,\nalong with a speaker and microphone. Children select and define key story\nelements-such as characters, places, items, and emotions-using the pawns and\ntokens, providing further details to the AI and receiving proper assistance,\nsimilar to how adults prompt AI for specific tasks (e.g., writing). For\nevaluation, several game sessions were simulated with a child AI agent, and the\nquality and safety of the generated stories were assessed from various\nperspectives. This work highlights the potential of combining physical and\ndigital elements in AI literacy, offering a safe and engaging way for children\nto learn how to effectively collaborate with AI."}
{"id": "2504.14717", "pdf": "https://arxiv.org/pdf/2504.14717", "abs": "https://arxiv.org/abs/2504.14717", "authors": ["Bowei Zhang", "Lei Ke", "Adam W. Harley", "Katerina Fragkiadaki"], "title": "TAPIP3D: Tracking Any Point in Persistent 3D Geometry", "categories": ["cs.CV", "cs.LG"], "comment": "Long-term feed-forward 3D point tracking in persistent 3D point maps.\n  Code:https://github.com/zbw001/TAPIP3D", "summary": "We introduce TAPIP3D, a novel approach for long-term 3D point tracking in\nmonocular RGB and RGB-D videos. TAPIP3D represents videos as camera-stabilized\nspatio-temporal feature clouds, leveraging depth and camera motion information\nto lift 2D video features into a 3D world space where camera motion is\neffectively canceled. TAPIP3D iteratively refines multi-frame 3D motion\nestimates within this stabilized representation, enabling robust tracking over\nextended periods. To manage the inherent irregularities of 3D point\ndistributions, we propose a Local Pair Attention mechanism. This 3D\ncontextualization strategy effectively exploits spatial relationships in 3D,\nforming informative feature neighborhoods for precise 3D trajectory estimation.\nOur 3D-centric approach significantly outperforms existing 3D point tracking\nmethods and even enhances 2D tracking accuracy compared to conventional 2D\npixel trackers when accurate depth is available. It supports inference in both\ncamera coordinates (i.e., unstabilized) and world coordinates, and our results\ndemonstrate that compensating for camera motion improves tracking performance.\nOur approach replaces the conventional 2D square correlation neighborhoods used\nin prior 2D and 3D trackers, leading to more robust and accurate results across\nvarious 3D point tracking benchmarks. Project Page: https://tapip3d.github.io"}
{"id": "2504.14177", "pdf": "https://arxiv.org/pdf/2504.14177", "abs": "https://arxiv.org/abs/2504.14177", "authors": ["Li He", "He Zhao", "Stephen Wan", "Dadong Wang", "Lina Yao", "Tongliang Liu"], "title": "Direct Advantage Regression: Aligning LLMs with Online AI Reward", "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Online AI Feedback (OAIF) presents a promising alternative to Reinforcement\nLearning from Human Feedback (RLHF) by utilizing online AI preference in\naligning language models (LLMs). However, the straightforward replacement of\nhumans with AI deprives LLMs from learning more fine-grained AI supervision\nbeyond binary signals. In this paper, we propose Direct Advantage Regression\n(DAR), a simple alignment algorithm using online AI reward to optimize policy\nimprovement through weighted supervised fine-tuning. As an RL-free approach,\nDAR maintains theoretical consistency with online RLHF pipelines while\nsignificantly reducing implementation complexity and improving learning\nefficiency. Our empirical results underscore that AI reward is a better form of\nAI supervision consistently achieving higher human-AI agreement as opposed to\nAI preference. Additionally, evaluations using GPT-4-Turbo and MT-bench show\nthat DAR outperforms both OAIF and online RLHF baselines."}
{"id": "2504.13971", "pdf": "https://arxiv.org/pdf/2504.13971", "abs": "https://arxiv.org/abs/2504.13971", "authors": ["Abdelrahman Soliman"], "title": "The Future of Internet of Things and Multimodal Language Models in 6G Networks: Opportunities and Challenges", "categories": ["cs.CY", "cs.AI", "cs.ET", "cs.NI"], "comment": "11 pages", "summary": "Based on recent trends in artificial intelligence and IoT research. The\ncooperative potential of integrating the Internet of Things (IoT) and\nMultimodal Language Models (MLLMs) is presented in this survey paper for future\n6G systems. It focuses on the applications of this integration in different\nfields, such as healthcare, agriculture, and smart cities, and investigates the\nfour pillars of IoT integration, such as sensors, communication, processing,\nand security. The paper provides a comprehensive description of IoT and MLLM\ntechnologies and applications, addresses the role of multimodality in each\npillar, and concludes with an overview of the most significant challenges and\ndirections for future research. The general survey is a roadmap for researchers\ninterested in tracing the application areas of MLLMs and IoT, highlighting the\npotential and challenges in this rapidly growing field. The survey recognizes\nthe need to deal with data availability, computational expense, privacy, and\nreal-time processing to harness the complete potential of IoT, MLLM, and 6G\ntechnology"}
{"id": "2504.14736", "pdf": "https://arxiv.org/pdf/2504.14736", "abs": "https://arxiv.org/abs/2504.14736", "authors": ["Nicolás Gaggion", "Rodrigo Bonazzola", "María Florencia Legascue", "María Florencia Mammarella", "Florencia Sol Rodriguez", "Federico Emanuel Aballay", "Florencia Belén Catulo", "Andana Barrios", "Franco Accavallo", "Santiago Nahuel Villarreal", "Martin Crespi", "Martiniano María Ricardi", "Ezequiel Petrillo", "Thomas Blein", "Federico Ariel", "Enzo Ferrante"], "title": "ChronoRoot 2.0: An Open AI-Powered Platform for 2D Temporal Plant Phenotyping", "categories": ["cs.CV", "q-bio.QM"], "comment": null, "summary": "The analysis of plant developmental plasticity, including root system\narchitecture, is fundamental to understanding plant adaptability and\ndevelopment, particularly in the context of climate change and agricultural\nsustainability. While significant advances have been made in plant phenotyping\ntechnologies, comprehensive temporal analysis of root development remains\nchallenging, with most existing solutions providing either limited throughput\nor restricted structural analysis capabilities. Here, we present ChronoRoot\n2.0, an integrated open-source platform that combines affordable hardware with\nadvanced artificial intelligence to enable sophisticated temporal plant\nphenotyping. The system introduces several major advances, offering an integral\nperspective of seedling development: (i) simultaneous multi-organ tracking of\nsix distinct plant structures, (ii) quality control through real-time\nvalidation, (iii) comprehensive architectural measurements including novel\ngravitropic response parameters, and (iv) dual specialized user interfaces for\nboth architectural analysis and high-throughput screening. We demonstrate the\nsystem's capabilities through three use cases for Arabidopsis thaliana:\ncharacterization of circadian growth patterns under different light conditions,\ndetailed analysis of gravitropic responses in transgenic plants, and\nhigh-throughput screening of etiolation responses across multiple genotypes.\nChronoRoot 2.0 maintains its predecessor's advantages of low cost and\nmodularity while significantly expanding its capabilities, making sophisticated\ntemporal phenotyping more accessible to the broader plant science community.\nThe system's open-source nature, combined with extensive documentation and\ncontainerized deployment options, ensures reproducibility and enables\ncommunity-driven development of new analytical capabilities."}
{"id": "2504.14183", "pdf": "https://arxiv.org/pdf/2504.14183", "abs": "https://arxiv.org/abs/2504.14183", "authors": ["Natalia Tomashenko", "Xiaoxiao Miao", "Emmanuel Vincent", "Junichi Yamagishi"], "title": "The First VoicePrivacy Attacker Challenge", "categories": ["eess.AS", "cs.CL", "cs.CR"], "comment": "Published in: ICASSP 2025 - 2025 IEEE International Conference on\n  Acoustics, Speech and Signal Processing (ICASSP)", "summary": "The First VoicePrivacy Attacker Challenge is an ICASSP 2025 SP Grand\nChallenge which focuses on evaluating attacker systems against a set of voice\nanonymization systems submitted to the VoicePrivacy 2024 Challenge. Training,\ndevelopment, and evaluation datasets were provided along with a baseline\nattacker. Participants developed their attacker systems in the form of\nautomatic speaker verification systems and submitted their scores on the\ndevelopment and evaluation data. The best attacker systems reduced the equal\nerror rate (EER) by 25-44% relative w.r.t. the baseline."}
{"id": "2504.13972", "pdf": "https://arxiv.org/pdf/2504.13972", "abs": "https://arxiv.org/abs/2504.13972", "authors": ["Dana Alsagheer", "Abdulrahman Kamal", "Mohammad Kamal", "Weidong Shi"], "title": "Governance Challenges in Reinforcement Learning from Human Feedback: Evaluator Rationality and Reinforcement Stability", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Reinforcement Learning from Human Feedback (RLHF) is central in aligning\nlarge language models (LLMs) with human values and expectations. However, the\nprocess remains susceptible to governance challenges, including evaluator bias,\ninconsistency, and the unreliability of feedback. This study examines how the\ncognitive capacity of evaluators, specifically their level of rationality,\naffects the stability of reinforcement signals. A controlled experiment\ncomparing high-rationality and low-rationality participants reveals that\nevaluators with higher rationality scores produce significantly more consistent\nand expert-aligned feedback. In contrast, lower-rationality participants\ndemonstrate considerable variability in their reinforcement decisions ($p <\n0.01$). To address these challenges and improve RLHF governance, we recommend\nimplementing evaluator pre-screening, systematic auditing of feedback\nconsistency, and reliability-weighted reinforcement aggregation. These measures\nenhance the fairness, transparency, and robustness of AI alignment pipelines."}
{"id": "2504.14737", "pdf": "https://arxiv.org/pdf/2504.14737", "abs": "https://arxiv.org/abs/2504.14737", "authors": ["Shuang Zeng", "Lei Zhu", "Xinliang Zhang", "Hangzhou He", "Yanye Lu"], "title": "SuperCL: Superpixel Guided Contrastive Learning for Medical Image Segmentation Pre-training", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Medical image segmentation is a critical yet challenging task, primarily due\nto the difficulty of obtaining extensive datasets of high-quality,\nexpert-annotated images. Contrastive learning presents a potential but still\nproblematic solution to this issue. Because most existing methods focus on\nextracting instance-level or pixel-to-pixel representation, which ignores the\ncharacteristics between intra-image similar pixel groups. Moreover, when\nconsidering contrastive pairs generation, most SOTA methods mainly rely on\nmanually setting thresholds, which requires a large number of gradient\nexperiments and lacks efficiency and generalization. To address these issues,\nwe propose a novel contrastive learning approach named SuperCL for medical\nimage segmentation pre-training. Specifically, our SuperCL exploits the\nstructural prior and pixel correlation of images by introducing two novel\ncontrastive pairs generation strategies: Intra-image Local Contrastive Pairs\n(ILCP) Generation and Inter-image Global Contrastive Pairs (IGCP) Generation.\nConsidering superpixel cluster aligns well with the concept of contrastive\npairs generation, we utilize the superpixel map to generate pseudo masks for\nboth ILCP and IGCP to guide supervised contrastive learning. Moreover, we also\npropose two modules named Average SuperPixel Feature Map Generation (ASP) and\nConnected Components Label Generation (CCL) to better exploit the prior\nstructural information for IGCP. Finally, experiments on 8 medical image\ndatasets indicate our SuperCL outperforms existing 12 methods. i.e. Our SuperCL\nachieves a superior performance with more precise predictions from\nvisualization figures and 3.15%, 5.44%, 7.89% DSC higher than the previous best\nresults on MMWHS, CHAOS, Spleen with 10% annotations. Our code will be released\nafter acceptance."}
{"id": "2504.14191", "pdf": "https://arxiv.org/pdf/2504.14191", "abs": "https://arxiv.org/abs/2504.14191", "authors": ["Yansheng Qiu", "Haoquan Zhang", "Zhaopan Xu", "Ming Li", "Diping Song", "Zheng Wang", "Kaipeng Zhang"], "title": "AI Idea Bench 2025: AI Research Idea Generation Benchmark", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large-scale Language Models (LLMs) have revolutionized human-AI interaction\nand achieved significant success in the generation of novel ideas. However,\ncurrent assessments of idea generation overlook crucial factors such as\nknowledge leakage in LLMs, the absence of open-ended benchmarks with grounded\ntruth, and the limited scope of feasibility analysis constrained by prompt\ndesign. These limitations hinder the potential of uncovering groundbreaking\nresearch ideas. In this paper, we present AI Idea Bench 2025, a framework\ndesigned to quantitatively evaluate and compare the ideas generated by LLMs\nwithin the domain of AI research from diverse perspectives. The framework\ncomprises a comprehensive dataset of 3,495 AI papers and their associated\ninspired works, along with a robust evaluation methodology. This evaluation\nsystem gauges idea quality in two dimensions: alignment with the ground-truth\ncontent of the original papers and judgment based on general reference\nmaterial. AI Idea Bench 2025's benchmarking system stands to be an invaluable\nresource for assessing and comparing idea-generation techniques, thereby\nfacilitating the automation of scientific discovery."}
{"id": "2504.13974", "pdf": "https://arxiv.org/pdf/2504.13974", "abs": "https://arxiv.org/abs/2504.13974", "authors": ["Yao Zhiwan", "Reza Zarrab", "Jean Dubois"], "title": "Enhancing Stroke Diagnosis in the Brain Using a Weighted Deep Learning Approach", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "A brain stroke occurs when blood flow to a part of the brain is disrupted,\nleading to cell death. Traditional stroke diagnosis methods, such as CT scans\nand MRIs, are costly and time-consuming. This study proposes a weighted voting\nensemble (WVE) machine learning model that combines predictions from\nclassifiers like random forest, Deep Learning, and histogram-based gradient\nboosting to predict strokes more effectively. The model achieved 94.91%\naccuracy on a private dataset, enabling early risk assessment and prevention.\nFuture research could explore optimization techniques to further enhance\naccuracy."}
{"id": "2504.14753", "pdf": "https://arxiv.org/pdf/2504.14753", "abs": "https://arxiv.org/abs/2504.14753", "authors": ["Guodong Shen", "Yuqi Ouyang", "Junru Lu", "Yixuan Yang", "Victor Sanchez"], "title": "Advancing Video Anomaly Detection: A Bi-Directional Hybrid Framework for Enhanced Single- and Multi-Task Approaches", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted by IEEE Transactions on Image Processing (TIP)", "summary": "Despite the prevailing transition from single-task to multi-task approaches\nin video anomaly detection, we observe that many adopt sub-optimal frameworks\nfor individual proxy tasks. Motivated by this, we contend that optimizing\nsingle-task frameworks can advance both single- and multi-task approaches.\nAccordingly, we leverage middle-frame prediction as the primary proxy task, and\nintroduce an effective hybrid framework designed to generate accurate\npredictions for normal frames and flawed predictions for abnormal frames. This\nhybrid framework is built upon a bi-directional structure that seamlessly\nintegrates both vision transformers and ConvLSTMs. Specifically, we utilize\nthis bi-directional structure to fully analyze the temporal dimension by\npredicting frames in both forward and backward directions, significantly\nboosting the detection stability. Given the transformer's capacity to model\nlong-range contextual dependencies, we develop a convolutional temporal\ntransformer that efficiently associates feature maps from all context frames to\ngenerate attention-based predictions for target frames. Furthermore, we devise\na layer-interactive ConvLSTM bridge that facilitates the smooth flow of\nlow-level features across layers and time-steps, thereby strengthening\npredictions with fine details. Anomalies are eventually identified by\nscrutinizing the discrepancies between target frames and their corresponding\npredictions. Several experiments conducted on public benchmarks affirm the\nefficacy of our hybrid framework, whether used as a standalone single-task\napproach or integrated as a branch in a multi-task approach. These experiments\nalso underscore the advantages of merging vision transformers and ConvLSTMs for\nvideo anomaly detection."}
{"id": "2504.14232", "pdf": "https://arxiv.org/pdf/2504.14232", "abs": "https://arxiv.org/abs/2504.14232", "authors": ["Antoun Yaacoub", "Jérôme Da-Rugna", "Zainab Assaghir"], "title": "Assessing AI-Generated Questions' Alignment with Cognitive Frameworks in Educational Assessment", "categories": ["cs.AI", "cs.CL"], "comment": "This paper was presented in the 17th Int. Conf. on Computer Science\n  and Information Technology (ICCSIT 2024), Dubai, United Arab Emirates, 2024,\n  Oct. 23-25. IT's now in production to be published in the International\n  Journal of Computer Theory and Engineering", "summary": "This study evaluates the integration of Bloom's Taxonomy into OneClickQuiz,\nan Artificial Intelligence (AI) driven plugin for automating Multiple-Choice\nQuestion (MCQ) generation in Moodle. Bloom's Taxonomy provides a structured\nframework for categorizing educational objectives into hierarchical cognitive\nlevels. Our research investigates whether incorporating this taxonomy can\nimprove the alignment of AI-generated questions with specific cognitive\nobjectives. We developed a dataset of 3691 questions categorized according to\nBloom's levels and employed various classification models-Multinomial Logistic\nRegression, Naive Bayes, Linear Support Vector Classification (SVC), and a\nTransformer-based model (DistilBERT)-to evaluate their effectiveness in\ncategorizing questions. Our results indicate that higher Bloom's levels\ngenerally correlate with increased question length, Flesch-Kincaid Grade Level\n(FKGL), and Lexical Density (LD), reflecting the increased complexity of higher\ncognitive demands. Multinomial Logistic Regression showed varying accuracy\nacross Bloom's levels, performing best for \"Knowledge\" and less accurately for\nhigher-order levels. Merging higher-level categories improved accuracy for\ncomplex cognitive tasks. Naive Bayes and Linear SVC also demonstrated effective\nclassification for lower levels but struggled with higher-order tasks.\nDistilBERT achieved the highest performance, significantly improving\nclassification of both lower and higher-order cognitive levels, achieving an\noverall validation accuracy of 91%. This study highlights the potential of\nintegrating Bloom's Taxonomy into AI-driven assessment tools and underscores\nthe advantages of advanced models like DistilBERT for enhancing educational\ncontent generation."}
{"id": "2504.13975", "pdf": "https://arxiv.org/pdf/2504.13975", "abs": "https://arxiv.org/abs/2504.13975", "authors": ["Mehmet Yamaç", "Muhammad Numan Yousaf", "Serkan Kiranyaz", "Moncef Gabbouj"], "title": "Multiscale Tensor Summation Factorization as a New Neural Network Layer (MTS Layer) for Multidimensional Data Processing", "categories": ["cs.LG", "cs.AI", "cs.CV", "68T07"], "comment": "13 pages", "summary": "Multilayer perceptrons (MLP), or fully connected artificial neural networks,\nare known for performing vector-matrix multiplications using learnable weight\nmatrices; however, their practical application in many machine learning tasks,\nespecially in computer vision, can be limited due to the high dimensionality of\ninput-output pairs at each layer. To improve efficiency, convolutional\noperators have been utilized to facilitate weight sharing and local\nconnections, yet they are constrained by limited receptive fields. In this\npaper, we introduce Multiscale Tensor Summation (MTS) Factorization, a novel\nneural network operator that implements tensor summation at multiple scales,\nwhere each tensor to be summed is obtained through Tucker-decomposition-like\nmode products. Unlike other tensor decomposition methods in the literature, MTS\nis not introduced as a network compression tool; instead, as a new backbone\nneural layer. MTS not only reduces the number of parameters required while\nenhancing the efficiency of weight optimization compared to traditional dense\nlayers (i.e., unfactorized weight matrices in MLP layers), but it also\ndemonstrates clear advantages over convolutional layers. The proof-of-concept\nexperimental comparison of the proposed MTS networks with MLPs and\nConvolutional Neural Networks (CNNs) demonstrates their effectiveness across\nvarious tasks, such as classification, compression, and signal restoration.\nAdditionally, when integrated with modern non-linear units such as the\nmulti-head gate (MHG), also introduced in this study, the corresponding neural\nnetwork, MTSNet, demonstrates a more favorable complexity-performance tradeoff\ncompared to state-of-the-art transformers in various computer vision\napplications. The software implementation of the MTS layer and the\ncorresponding MTS-based networks, MTSNets, is shared at\nhttps://github.com/mehmetyamac/MTSNet."}
{"id": "2504.14783", "pdf": "https://arxiv.org/pdf/2504.14783", "abs": "https://arxiv.org/abs/2504.14783", "authors": ["Wenhui Zhu", "Peijie Qiu", "Xiwen Chen", "Zhangsihao Yang", "Aristeidis Sotiras", "Abolfazl Razi", "Yalin Wang"], "title": "How Effective Can Dropout Be in Multiple Instance Learning ?", "categories": ["cs.CV", "cs.AI", "eess.IV", "stat.ML"], "comment": null, "summary": "Multiple Instance Learning (MIL) is a popular weakly-supervised method for\nvarious applications, with a particular interest in histological whole slide\nimage (WSI) classification. Due to the gigapixel resolution of WSI,\napplications of MIL in WSI typically necessitate a two-stage training scheme:\nfirst, extract features from the pre-trained backbone and then perform MIL\naggregation. However, it is well-known that this suboptimal training scheme\nsuffers from \"noisy\" feature embeddings from the backbone and inherent weak\nsupervision, hindering MIL from learning rich and generalizable features.\nHowever, the most commonly used technique (i.e., dropout) for mitigating this\nissue has yet to be explored in MIL. In this paper, we empirically explore how\neffective the dropout can be in MIL. Interestingly, we observe that dropping\nthe top-k most important instances within a bag leads to better performance and\ngeneralization even under noise attack. Based on this key observation, we\npropose a novel MIL-specific dropout method, termed MIL-Dropout, which\nsystematically determines which instances to drop. Experiments on five MIL\nbenchmark datasets and two WSI datasets demonstrate that MIL-Dropout boosts the\nperformance of current MIL methods with a negligible computational cost. The\ncode is available at https://github.com/ChongQingNoSubway/MILDropout."}
{"id": "2504.14239", "pdf": "https://arxiv.org/pdf/2504.14239", "abs": "https://arxiv.org/abs/2504.14239", "authors": ["Yuhang Liu", "Pengxiang Li", "Congkai Xie", "Xavier Hu", "Xiaotian Han", "Shengyu Zhang", "Hongxia Yang", "Fei Wu"], "title": "InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners", "categories": ["cs.AI", "cs.CL"], "comment": "10 pages, 3 figures, work in progress", "summary": "Multimodal Large Language Models (MLLMs) have powered Graphical User\nInterface (GUI) Agents, showing promise in automating tasks on computing\ndevices. Recent works have begun exploring reasoning in GUI tasks with\nencouraging results. However, many current approaches rely on manually designed\nreasoning templates, which may result in reasoning that is not sufficiently\nrobust and adaptive for complex GUI environments. Meanwhile, some existing\nagents continue to operate as Reactive Actors, relying primarily on implicit\nreasoning that may lack sufficient depth for GUI tasks demanding planning and\nerror recovery. We argue that advancing these agents requires a shift from\nreactive acting towards acting based on deliberate reasoning. To facilitate\nthis transformation, we introduce InfiGUI-R1, an MLLM-based GUI agent developed\nthrough our Actor2Reasoner framework, a reasoning-centric, two-stage training\napproach designed to progressively evolve agents from Reactive Actors to\nDeliberative Reasoners. The first stage, Reasoning Injection, focuses on\nestablishing a basic reasoner. We employ Spatial Reasoning Distillation to\ntransfer cross-modal spatial reasoning capabilities from teacher models to\nMLLMs through trajectories with explicit reasoning steps, enabling models to\nintegrate GUI visual-spatial information with logical reasoning before action\ngeneration. The second stage, Deliberation Enhancement, refines the basic\nreasoner into a deliberative one using Reinforcement Learning. This stage\nintroduces two approaches: Sub-goal Guidance, which rewards models for\ngenerating accurate intermediate sub-goals, and Error Recovery Scenario\nConstruction, which creates failure-and-recovery training scenarios from\nidentified prone-to-error steps. Experimental results show InfiGUI-R1 achieves\nstrong performance in GUI grounding and trajectory tasks. Resources at\nhttps://github.com/Reallm-Labs/InfiGUI-R1."}
{"id": "2504.13976", "pdf": "https://arxiv.org/pdf/2504.13976", "abs": "https://arxiv.org/abs/2504.13976", "authors": ["Wrick Talukdar"], "title": "Gas Station of the Future: A Perspective on AI/ML and IoT in Retail Downstream", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "The gas station of the future is poised to transform from a simple fuel\ndispensing center into an intelligent retail hub, driven by advancements in\nArtificial Intelligence (AI), Machine Learning (ML), and the Internet of Things\n(IoT). This paper explores how technology is reshaping the retail downstream\nsector while briefly addressing the upstream and midstream segments. By\nleveraging AI/ML for predictive analytics, dynamic pricing, personalized\ncustomer engagement, and IoT for real-time monitoring and automation, the\nfuture gas station will redefine the fuel retail experience. Additionally, this\npaper incorporates statistics, AI/ML core technical concepts, mathematical\nformulations, case studies, and a proposed framework for a fully autonomous gas\nstation."}
{"id": "2504.14785", "pdf": "https://arxiv.org/pdf/2504.14785", "abs": "https://arxiv.org/abs/2504.14785", "authors": ["Zhenyu Yu", "Mohd Yamani Idna Idris", "Pei Wang"], "title": "When Cloud Removal Meets Diffusion Model in Remote Sensing", "categories": ["cs.CV"], "comment": null, "summary": "Cloud occlusion significantly hinders remote sensing applications by\nobstructing surface information and complicating analysis. To address this, we\npropose DC4CR (Diffusion Control for Cloud Removal), a novel multimodal\ndiffusion-based framework for cloud removal in remote sensing imagery. Our\nmethod introduces prompt-driven control, allowing selective removal of thin and\nthick clouds without relying on pre-generated cloud masks, thereby enhancing\npreprocessing efficiency and model adaptability. Additionally, we integrate\nlow-rank adaptation for computational efficiency, subject-driven generation for\nimproved generalization, and grouped learning to enhance performance on small\ndatasets. Designed as a plug-and-play module, DC4CR seamlessly integrates into\nexisting cloud removal models, providing a scalable and robust solution.\nExtensive experiments on the RICE and CUHK-CR datasets demonstrate\nstate-of-the-art performance, achieving superior cloud removal across diverse\nconditions. This work presents a practical and efficient approach for remote\nsensing image processing with broad real-world applications."}
{"id": "2504.14245", "pdf": "https://arxiv.org/pdf/2504.14245", "abs": "https://arxiv.org/abs/2504.14245", "authors": ["Yikun Ji", "Yan Hong", "Jiahui Zhan", "Haoxing Chen", "jun lan", "Huijia Zhu", "Weiqiang Wang", "Liqing Zhang", "Jianfu Zhang"], "title": "Towards Explainable Fake Image Detection with Multi-Modal Large Language Models", "categories": ["cs.CV", "cs.CL", "I.2.7; I.2.10"], "comment": null, "summary": "Progress in image generation raises significant public security concerns. We\nargue that fake image detection should not operate as a \"black box\". Instead,\nan ideal approach must ensure both strong generalization and transparency.\nRecent progress in Multi-modal Large Language Models (MLLMs) offers new\nopportunities for reasoning-based AI-generated image detection. In this work,\nwe evaluate the capabilities of MLLMs in comparison to traditional detection\nmethods and human evaluators, highlighting their strengths and limitations.\nFurthermore, we design six distinct prompts and propose a framework that\nintegrates these prompts to develop a more robust, explainable, and\nreasoning-driven detection system. The code is available at\nhttps://github.com/Gennadiyev/mllm-defake."}
{"id": "2504.13979", "pdf": "https://arxiv.org/pdf/2504.13979", "abs": "https://arxiv.org/abs/2504.13979", "authors": ["Thippa Reddy Gadekallu", "Kapal Dev", "Sunder Ali Khowaja", "Weizheng Wang", "Hailin Feng", "Kai Fang", "Sharnil Pandya", "Wei Wang"], "title": "Framework, Standards, Applications and Best practices of Responsible AI : A Comprehensive Survey", "categories": ["cs.CY", "cs.AI"], "comment": "Submitted for peer review", "summary": "Responsible Artificial Intelligence (RAI) is a combination of ethics\nassociated with the usage of artificial intelligence aligned with the common\nand standard frameworks. This survey paper extensively discusses the global and\nnational standards, applications of RAI, current technology and ongoing\nprojects using RAI, and possible challenges in implementing and designing RAI\nin the industries and projects based on AI. Currently, ethical standards and\nimplementation of RAI are decoupled which caters each industry to follow their\nown standards to use AI ethically. Many global firms and government\norganizations are taking necessary initiatives to design a common and standard\nframework. Social pressure and unethical way of using AI forces the RAI design\nrather than implementation."}
{"id": "2504.14807", "pdf": "https://arxiv.org/pdf/2504.14807", "abs": "https://arxiv.org/abs/2504.14807", "authors": ["Deepak Ghimire", "Sunghwan Jeong", "Sunhong Yoon", "Sanghyun Park", "Juhwan Choi"], "title": "Real-Time Sleepiness Detection for Driver State Monitoring System", "categories": ["cs.CV", "cs.HC", "cs.LG"], "comment": "8 pages, published in GST 2015", "summary": "A driver face monitoring system can detect driver fatigue, which is a\nsignificant factor in many accidents, using computer vision techniques. In this\npaper, we present a real-time technique for driver eye state detection. First,\nthe face is detected, and the eyes are located within the face region for\ntracking. A normalized cross-correlation-based online dynamic template matching\ntechnique, combined with Kalman filter tracking, is proposed to track the\ndetected eye positions in subsequent image frames. A support vector machine\nwith histogram of oriented gradients (HOG) features is used to classify the\nstate of the eyes as open or closed. If the eyes remain closed for a specified\nperiod, the driver is considered to be asleep, and an alarm is triggered."}
{"id": "2504.14260", "pdf": "https://arxiv.org/pdf/2504.14260", "abs": "https://arxiv.org/abs/2504.14260", "authors": ["Liu Xiao", "Li Zhiyuan", "Lin Yueyu"], "title": "Cross-attention for State-based model RWKV-7", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "We introduce CrossWKV, a novel cross-attention mechanism for the state-based\nRWKV-7 model, designed to enhance the expressive power of text-to-image\ngeneration. Leveraging RWKV-7's linear-complexity Weighted Key-Value (WKV)\narchitecture, CrossWKV integrates text and image modalities in a single pass,\nutilizing a generalized delta rule with vector-valued gating and low-rank\nadaptations (LoRA) to achieve superior cross-modal alignment. Unlike\nTransformer-based models, CrossWKV's non-diagonal, input-dependent transition\nmatrix enables it to represent complex functions beyond the $\\mathrm{TC}^0$\ncomplexity class, including all regular languages, as demonstrated by its\nability to perform state-tracking tasks like $S_5$ permutation modeling.\nEvaluated within the Diffusion in RWKV-7 (DIR-7) on datasets such as LAION-5B\nand ImageNet, CrossWKV achieves a Frechet Inception Distance (FID) of 2.88 and\na CLIP score of 0.33 on ImageNet 256x256, matching state-of-the-art performance\nwhile offering robust generalization across diverse prompts. The model's\nenhanced expressivity, combined with constant memory usage and linear scaling,\npositions it as a powerful solution for advanced cross-modal tasks, with\npotential applications in high-resolution generation and dynamic state\nmanipulation.Code at https://github.com/TorchRWKV/flash-linear-attention"}
{"id": "2504.13981", "pdf": "https://arxiv.org/pdf/2504.13981", "abs": "https://arxiv.org/abs/2504.13981", "authors": ["Sushant Singh", "Ausif Mahmood"], "title": "CacheFormer: High Attention-Based Segment Caching", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Efficiently handling long contexts in transformer-based language models with\nlow perplexity is an active area of research. Numerous recent approaches like\nLinformer, Longformer, Performer, and Structured state space models (SSMs).,\nhave not fully resolved this problem. All these models strive to reduce the\nquadratic time complexity of the attention mechanism while minimizing the loss\nin quality due to the effective compression of the long context. Inspired by\nthe cache and virtual memory principle in computers, where in case of a cache\nmiss, not only the needed data is retrieved from the memory, but the adjacent\ndata is also obtained, we apply this concept to handling long contexts by\ndividing it into small segments. In our design, we retrieve the nearby segments\nin an uncompressed form when high segment-level attention occurs at the\ncompressed level. Our en-hancements for handling long context include\naggregating four attention mechanisms consisting of short sliding window\nattention, long compressed segmented attention, dynamically retrieving top k\nhigh attention uncompressed segments, and overlapping segments in long segment\nattention to avoid segment fragmentation. These enhancements result in an\narchitecture that outperforms ex-isting SOTA architectures with an average\nperplexity improvement of 8.5% over similar model sizes."}
{"id": "2504.14825", "pdf": "https://arxiv.org/pdf/2504.14825", "abs": "https://arxiv.org/abs/2504.14825", "authors": ["Zhoujie Qian"], "title": "ECViT: Efficient Convolutional Vision Transformer with Local-Attention and Multi-scale Stages", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision Transformers (ViTs) have revolutionized computer vision by leveraging\nself-attention to model long-range dependencies. However, ViTs face challenges\nsuch as high computational costs due to the quadratic scaling of self-attention\nand the requirement of a large amount of training data. To address these\nlimitations, we propose the Efficient Convolutional Vision Transformer (ECViT),\na hybrid architecture that effectively combines the strengths of CNNs and\nTransformers. ECViT introduces inductive biases such as locality and\ntranslation invariance, inherent to Convolutional Neural Networks (CNNs) into\nthe Transformer framework by extracting patches from low-level features and\nenhancing the encoder with convolutional operations. Additionally, it\nincorporates local-attention and a pyramid structure to enable efficient\nmulti-scale feature extraction and representation. Experimental results\ndemonstrate that ECViT achieves an optimal balance between performance and\nefficiency, outperforming state-of-the-art models on various image\nclassification tasks while maintaining low computational and storage\nrequirements. ECViT offers an ideal solution for applications that prioritize\nhigh efficiency without compromising performance."}
{"id": "2504.14359", "pdf": "https://arxiv.org/pdf/2504.14359", "abs": "https://arxiv.org/abs/2504.14359", "authors": ["Kyle Buettner", "Jacob Emmerson", "Adriana Kovashka"], "title": "A Multimodal Recaptioning Framework to Account for Perceptual Diversity in Multilingual Vision-Language Modeling", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "There are many ways to describe, name, and group objects when captioning an\nimage. Differences are evident when speakers come from diverse cultures due to\nthe unique experiences that shape perception. Machine translation of captions\nhas pushed multilingual capabilities in vision-language models (VLMs), but data\ncomes mainly from English speakers, indicating a perceptual bias and lack of\nmodel flexibility. In this work, we address this challenge and outline a\ndata-efficient framework to instill multilingual VLMs with greater\nunderstanding of perceptual diversity. We specifically propose an LLM-based,\nmultimodal recaptioning strategy that alters the object descriptions of English\ncaptions before translation. The greatest benefits are demonstrated in a\ntargeted multimodal mechanism guided by native speaker data. By adding produced\nrewrites as augmentations in training, we improve on German and Japanese\ntext-image retrieval cases studies (up to +3.5 mean recall overall, +4.7 on\nnon-native error cases). We further propose a mechanism to analyze the specific\nobject description differences across datasets, and we offer insights into\ncross-dataset and cross-language generalization."}
{"id": "2504.13984", "pdf": "https://arxiv.org/pdf/2504.13984", "abs": "https://arxiv.org/abs/2504.13984", "authors": ["Amrit Diggavi Seshadri"], "title": "One Jump Is All You Need: Short-Cutting Transformers for Early Exit Prediction with One Jump to Fit All Exit Levels", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "To reduce the time and computational costs of inference of large language\nmodels, there has been interest in parameter-efficient low-rank early-exit\ncasting of transformer hidden-representations to final-representations. Such\nlow-rank short-cutting has been shown to outperform identity shortcuts at early\nmodel stages while offering parameter-efficiency in shortcut jumps. However,\ncurrent low-rank methods maintain a separate early-exit shortcut jump to\nfinal-representations for each transformer intermediate block-level during\ninference. In this work, we propose selection of a single One-Jump-Fits-All\n(OJFA) low-rank shortcut that offers over a 30x reduction in shortcut parameter\ncosts during inference. We show that despite this extreme reduction, our OJFA\nchoice largely matches the performance of maintaining multiple shortcut jumps\nduring inference and offers stable precision from all transformer block-levels\nfor GPT2-XL, Phi3-Mini and Llama2-7B transformer models."}
{"id": "2504.14826", "pdf": "https://arxiv.org/pdf/2504.14826", "abs": "https://arxiv.org/abs/2504.14826", "authors": ["Zhuoran Zheng", "Xin Su", "Chen Wu", "Xiuyi Jia"], "title": "Distribution-aware Dataset Distillation for Efficient Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "With the exponential increase in image data, training an image restoration\nmodel is laborious. Dataset distillation is a potential solution to this\nproblem, yet current distillation techniques are a blank canvas in the field of\nimage restoration. To fill this gap, we propose the Distribution-aware Dataset\nDistillation method (TripleD), a new framework that extends the principles of\ndataset distillation to image restoration. Specifically, TripleD uses a\npre-trained vision Transformer to extract features from images for complexity\nevaluation, and the subset (the number of samples is much smaller than the\noriginal training set) is selected based on complexity. The selected subset is\nthen fed through a lightweight CNN that fine-tunes the image distribution to\nalign with the distribution of the original dataset at the feature level. To\nefficiently condense knowledge, the training is divided into two stages. Early\nstages focus on simpler, low-complexity samples to build foundational\nknowledge, while later stages select more complex and uncertain samples as the\nmodel matures. Our method achieves promising performance on multiple image\nrestoration tasks, including multi-task image restoration, all-in-one image\nrestoration, and ultra-high-definition image restoration tasks. Note that we\ncan train a state-of-the-art image restoration model on an\nultra-high-definition (4K resolution) dataset using only one consumer-grade GPU\nin less than 8 hours (500 savings in computing resources and immeasurable\ntraining time)."}
{"id": "2504.14361", "pdf": "https://arxiv.org/pdf/2504.14361", "abs": "https://arxiv.org/abs/2504.14361", "authors": ["Till Rossner", "Ziteng Li", "Jonas Balke", "Nikoo Salehfard", "Tom Seifert", "Ming Tang"], "title": "Integrating Single-Cell Foundation Models with Graph Neural Networks for Drug Response Prediction", "categories": ["cs.LG", "cs.CL", "q-bio.QM"], "comment": "8 pages, 6 figures", "summary": "In this study, we propose an innovative methodology for predicting Cancer\nDrug Response (CDR) through the integration of the scGPT foundation model\nwithin the DeepCDR model. Our approach utilizes scGPT to generate embeddings\nfrom gene expression data, which are then used as gene expression input data\nfor DeepCDR. The experimental findings demonstrate the efficacy of this\nscGPT-based method in outperforming previous related works, including the\noriginal DeepCDR model and the scFoundation-based model. This study highlights\nthe potential of scGPT embeddings to enhance the accuracy of CDR predictions\nand offers a promising alternative to existing approaches."}
{"id": "2504.13986", "pdf": "https://arxiv.org/pdf/2504.13986", "abs": "https://arxiv.org/abs/2504.13986", "authors": ["Paolo Liberatore"], "title": "On the redundancy of short and heterogeneous sequences of belief revisions", "categories": ["cs.CC", "cs.AI"], "comment": "arXiv admin note: substantial text overlap with arXiv:2402.15445,\n  arXiv:2305.09200", "summary": "Forgetting a specific belief revision episode may not erase information\nbecause the other revisions may provide the same information or allow to deduce\nit. Whether it does was proved coNP-hard for sequence of two arbitrary\nlexicographic revision or arbitrarily long lexicographic Horn revision. A\npolynomial algorithm is presented for the case of two Horn revision.\nHeterogeneous sequences of revisions were proved to belong in Delta2. Their\npreviously proved coNP-hardness is enhanced by a proof of NP-hardness."}
{"id": "2504.14847", "pdf": "https://arxiv.org/pdf/2504.14847", "abs": "https://arxiv.org/abs/2504.14847", "authors": ["Xixi Wan", "Aihua Zheng", "Zi Wang", "Bo Jiang", "Jin Tang", "Jixin Ma"], "title": "Reliable Multi-Modal Object Re-Identification via Modality-Aware Graph Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Multi-modal data provides abundant and diverse object information, crucial\nfor effective modal interactions in Re-Identification (ReID) tasks. However,\nexisting approaches often overlook the quality variations in local features and\nfail to fully leverage the complementary information across modalities,\nparticularly in the case of low-quality features. In this paper, we propose to\naddress this issue by leveraging a novel graph reasoning model, termed the\nModality-aware Graph Reasoning Network (MGRNet). Specifically, we first\nconstruct modality-aware graphs to enhance the extraction of fine-grained local\ndetails by effectively capturing and modeling the relationships between\npatches. Subsequently, the selective graph nodes swap operation is employed to\nalleviate the adverse effects of low-quality local features by considering both\nlocal and global information, enhancing the representation of discriminative\ninformation. Finally, the swapped modality-aware graphs are fed into the\nlocal-aware graph reasoning module, which propagates multi-modal information to\nyield a reliable feature representation. Another advantage of the proposed\ngraph reasoning approach is its ability to reconstruct missing modal\ninformation by exploiting inherent structural relationships, thereby minimizing\ndisparities between different modalities. Experimental results on four\nbenchmarks (RGBNT201, Market1501-MM, RGBNT100, MSVR310) indicate that the\nproposed method achieves state-of-the-art performance in multi-modal object\nReID. The code for our method will be available upon acceptance."}
{"id": "2504.14363", "pdf": "https://arxiv.org/pdf/2504.14363", "abs": "https://arxiv.org/abs/2504.14363", "authors": ["Shihan Dou", "Muling Wu", "Jingwen Xu", "Rui Zheng", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "Improving RL Exploration for LLM Reasoning through Retrospective Replay", "categories": ["cs.LG", "cs.CL"], "comment": "13 pages, 3 figures", "summary": "Reinforcement learning (RL) has increasingly become a pivotal technique in\nthe post-training of large language models (LLMs). The effective exploration of\nthe output space is essential for the success of RL. We observe that for\ncomplex problems, during the early stages of training, the model exhibits\nstrong exploratory capabilities and can identify promising solution ideas.\nHowever, its limited capability at this stage prevents it from successfully\nsolving these problems. The early suppression of these potentially valuable\nsolution ideas by the policy gradient hinders the model's ability to revisit\nand re-explore these ideas later. Consequently, although the LLM's capabilities\nimprove in the later stages of training, it still struggles to effectively\naddress these complex problems. To address this exploration issue, we propose a\nnovel algorithm named Retrospective Replay-based Reinforcement Learning (RRL),\nwhich introduces a dynamic replay mechanism throughout the training process.\nRRL enables the model to revisit promising states identified in the early\nstages, thereby improving its efficiency and effectiveness in exploration. To\nevaluate the effectiveness of RRL, we conduct extensive experiments on complex\nreasoning tasks, including mathematical reasoning and code generation, and\ngeneral dialogue tasks. The results indicate that RRL maintains high\nexploration efficiency throughout the training period, significantly enhancing\nthe effectiveness of RL in optimizing LLMs for complicated reasoning tasks.\nMoreover, it also improves the performance of RLHF, making the model both safer\nand more helpful."}
{"id": "2504.13987", "pdf": "https://arxiv.org/pdf/2504.13987", "abs": "https://arxiv.org/abs/2504.13987", "authors": ["Tariq Berrada Ifriqi", "Adriana Romero-Soriano", "Michal Drozdzal", "Jakob Verbeek", "Karteek Alahari"], "title": "Entropy Rectifying Guidance for Diffusion and Flow Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Guidance techniques are commonly used in diffusion and flow models to improve\nimage quality and consistency for conditional generative tasks such as\nclass-conditional and text-to-image generation. In particular, classifier-free\nguidance (CFG) -- the most widely adopted guidance technique -- contrasts\nconditional and unconditional predictions to improve the generated images. This\nresults, however, in trade-offs across quality, diversity and consistency,\nimproving some at the expense of others. While recent work has shown that it is\npossible to disentangle these factors to some extent, such methods come with an\noverhead of requiring an additional (weaker) model, or require more forward\npasses per sampling step. In this paper, we propose Entropy Rectifying Guidance\n(ERG), a simple and effective guidance mechanism based on inference-time\nchanges in the attention mechanism of state-of-the-art diffusion transformer\narchitectures, which allows for simultaneous improvements over image quality,\ndiversity and prompt consistency. ERG is more general than CFG and similar\nguidance techniques, as it extends to unconditional sampling. ERG results in\nsignificant improvements in various generation tasks such as text-to-image,\nclass-conditional and unconditional image generation. We also show that ERG can\nbe seamlessly combined with other recent guidance methods such as CADS and APG,\nfurther boosting generation performance."}
{"id": "2504.14848", "pdf": "https://arxiv.org/pdf/2504.14848", "abs": "https://arxiv.org/abs/2504.14848", "authors": ["Yunpu Zhao", "Rui Zhang", "Junbin Xiao", "Ruibo Hou", "Jiaming Guo", "Zihao Zhang", "Yifan Hao", "Yunji Chen"], "title": "Object-Level Verbalized Confidence Calibration in Vision-Language Models via Semantic Perturbation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision-language models (VLMs) excel in various multimodal tasks but\nfrequently suffer from poor calibration, resulting in misalignment between\ntheir verbalized confidence and response correctness. This miscalibration\nundermines user trust, especially when models confidently provide incorrect or\nfabricated information. In this work, we propose a novel Confidence Calibration\nthrough Semantic Perturbation (CSP) framework to improve the calibration of\nverbalized confidence for VLMs in response to object-centric queries. We first\nintroduce a perturbed dataset where Gaussian noise is applied to the key object\nregions to simulate visual uncertainty at different confidence levels,\nestablishing an explicit mapping between visual ambiguity and confidence\nlevels. We further enhance calibration through a two-stage training process\ncombining supervised fine-tuning on the perturbed dataset with subsequent\npreference optimization. Extensive experiments on popular benchmarks\ndemonstrate that our method significantly improves the alignment between\nverbalized confidence and response correctness while maintaining or enhancing\noverall task performance. These results highlight the potential of semantic\nperturbation as a practical tool for improving the reliability and\ninterpretability of VLMs."}
{"id": "2504.14370", "pdf": "https://arxiv.org/pdf/2504.14370", "abs": "https://arxiv.org/abs/2504.14370", "authors": ["Jon Kleinberg", "Fan Wei"], "title": "Density Measures for Language Generation", "categories": ["math.CO", "cs.CL", "cs.DM", "cs.LG"], "comment": null, "summary": "The recent successes of large language models (LLMs) have led to a surge of\ntheoretical research into language generation. A recent line of work proposes\nan abstract view, called language generation in the limit, where generation is\nseen as a game between an adversary and an algorithm: the adversary generates\nstrings from an unknown language $K$, chosen from a countable collection of\ncandidate languages, and after seeing a finite set of these strings, the\nalgorithm must generate new strings from $K$ that it has not seen before. This\nformalism highlights a key tension: the trade-off between validity (the\nalgorithm should only produce strings from the language) and breadth (it should\nbe able to produce many strings from the language). This trade-off is central\nin applied language generation as well, where it appears as a balance between\nhallucination (generating invalid utterances) and mode collapse (generating\nonly a restricted set of outputs). Despite its importance, this trade-off has\nbeen challenging to study quantitatively. We develop ways to quantify this\ntrade-off by formalizing breadth using measures of density. Existing algorithms\nfor language generation in the limit produce output sets that can have zero\ndensity in the true language, and this important failure of breadth might seem\nunavoidable. We show, however, that such a failure is not necessary: we provide\nan algorithm for language generation in the limit whose outputs have strictly\npositive density in $K$. We also study the internal representations built by\nthese algorithms, specifically the sequence of hypothesized candidate languages\nthey consider, and show that achieving the strongest form of breadth may\nrequire oscillating indefinitely between high- and low-density representations.\nOur analysis introduces a novel topology on language families, with notions of\nconvergence and limit points playing a key role."}
{"id": "2504.13989", "pdf": "https://arxiv.org/pdf/2504.13989", "abs": "https://arxiv.org/abs/2504.13989", "authors": ["Lucas Maisonnave", "Cyril Moineau", "Olivier Bichler", "Fabrice Rastello"], "title": "Gradual Binary Search and Dimension Expansion : A general method for activation quantization in LLMs", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have become pivotal in artificial intelligence,\ndemonstrating strong capabilities in reasoning, understanding, and generating\ndata. However, their deployment on edge devices is hindered by their\nsubstantial size, often reaching several billion parameters. Quantization is a\nwidely used method to reduce memory usage and inference time, however LLMs\npresent unique challenges due to the prevalence of outliers in their\nactivations. In this work, we leverage the theoretical advantages of Hadamard\nmatrices over random rotation matrices to push the boundaries of quantization\nin LLMs. We demonstrate that Hadamard matrices are more effective in reducing\noutliers, which are a significant obstacle in achieving low-bit quantization.\nOur method based on a gradual binary search enables 3-bit quantization for\nweights, activations, and key-value (KV) caches, resulting in a 40\\% increase\nin accuracy on common benchmarks compared to SoTA methods. We extend the use of\nrotation matrices to support non-power-of-2 embedding dimensions, similar to\nthe Qwen architecture, by employing the Paley algorithm. We theoretically\ndemonstrates the superiority of Hadamard matrices in reducing outliers.We\nachieved 3-bit quantization for weights, activations, and KV cache,\nsignificantly enhancing model performance. Our experimental results on multiple\nmodels family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of\nour approach, outperforming existing methods and enabling practical 3-bit\nquantization."}
{"id": "2504.14860", "pdf": "https://arxiv.org/pdf/2504.14860", "abs": "https://arxiv.org/abs/2504.14860", "authors": ["Ziyi Liu", "Yangcen Liu"], "title": "Bridge the Gap: From Weak to Full Supervision for Temporal Action Localization with PseudoFormer", "categories": ["cs.CV", "cs.AI"], "comment": "CVPR 2025: IEEE Conference on Computer Vision and Pattern Recognition", "summary": "Weakly-supervised Temporal Action Localization (WTAL) has achieved notable\nsuccess but still suffers from a lack of temporal annotations, leading to a\nperformance and framework gap compared with fully-supervised methods. While\nrecent approaches employ pseudo labels for training, three key challenges:\ngenerating high-quality pseudo labels, making full use of different priors, and\noptimizing training methods with noisy labels remain unresolved. Due to these\nperspectives, we propose PseudoFormer, a novel two-branch framework that\nbridges the gap between weakly and fully-supervised Temporal Action\nLocalization (TAL). We first introduce RickerFusion, which maps all predicted\naction proposals to a global shared space to generate pseudo labels with better\nquality. Subsequently, we leverage both snippet-level and proposal-level labels\nwith different priors from the weak branch to train the regression-based model\nin the full branch. Finally, the uncertainty mask and iterative refinement\nmechanism are applied for training with noisy pseudo labels. PseudoFormer\nachieves state-of-the-art WTAL results on the two commonly used benchmarks,\nTHUMOS14 and ActivityNet1.3. Besides, extensive ablation studies demonstrate\nthe contribution of each component of our method."}
{"id": "2504.14439", "pdf": "https://arxiv.org/pdf/2504.14439", "abs": "https://arxiv.org/abs/2504.14439", "authors": ["Avinandan Bose", "Zhihan Xiong", "Yuejie Chi", "Simon Shaolei Du", "Lin Xiao", "Maryam Fazel"], "title": "LoRe: Personalizing LLMs via Low-Rank Reward Modeling", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Personalizing large language models (LLMs) to accommodate diverse user\npreferences is essential for enhancing alignment and user satisfaction.\nTraditional reinforcement learning from human feedback (RLHF) approaches often\nrely on monolithic value representations, limiting their ability to adapt to\nindividual preferences. We introduce a novel framework that leverages low-rank\npreference modeling to efficiently learn and generalize user-specific reward\nfunctions. By representing reward functions in a low-dimensional subspace and\nmodeling individual preferences as weighted combinations of shared basis\nfunctions, our approach avoids rigid user categorization while enabling\nscalability and few-shot adaptation. We validate our method on multiple\npreference datasets, demonstrating superior generalization to unseen users and\nimproved accuracy in preference prediction tasks."}
{"id": "2504.13990", "pdf": "https://arxiv.org/pdf/2504.13990", "abs": "https://arxiv.org/abs/2504.13990", "authors": ["M. Humayun Kabir", "Md. Ali Hasan", "Md. Shafiqul Islam", "Kyeongjun Ko", "Wonjae Shin"], "title": "PC-DeepNet: A GNSS Positioning Error Minimization Framework Using Permutation-Invariant Deep Neural Network", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "comment": "31 pages, 14 figures, 6 tables", "summary": "Global navigation satellite systems (GNSS) face significant challenges in\nurban and sub-urban areas due to non-line-of-sight (NLOS) propagation,\nmultipath effects, and low received power levels, resulting in highly\nnon-linear and non-Gaussian measurement error distributions. In light of this,\nconventional model-based positioning approaches, which rely on Gaussian error\napproximations, struggle to achieve precise localization under these\nconditions. To overcome these challenges, we put forth a novel learning-based\nframework, PC-DeepNet, that employs a permutation-invariant (PI) deep neural\nnetwork (DNN) to estimate position corrections (PC). This approach is designed\nto ensure robustness against changes in the number and/or order of visible\nsatellite measurements, a common issue in GNSS systems, while leveraging NLOS\nand multipath indicators as features to enhance positioning accuracy in\nchallenging urban and sub-urban environments. To validate the performance of\nthe proposed framework, we compare the positioning error with state-of-the-art\nmodel-based and learning-based positioning methods using two publicly available\ndatasets. The results confirm that proposed PC-DeepNet achieves superior\naccuracy than existing model-based and learning-based methods while exhibiting\nlower computational complexity compared to previous learning-based approaches."}
{"id": "2504.14868", "pdf": "https://arxiv.org/pdf/2504.14868", "abs": "https://arxiv.org/abs/2504.14868", "authors": ["Jianhui Wang", "Yangfan He", "Yan Zhong", "Xinyuan Song", "Jiayi Su", "Yuheng Feng", "Hongyang He", "Wenyu Zhu", "Xinhang Yuan", "Kuan Lu", "Menghao Huo", "Miao Zhang", "Keqin Li", "Jiaqi Chen", "Tianyu Shi", "Xueqian Wang"], "title": "Twin Co-Adaptive Dialogue for Progressive Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Modern text-to-image generation systems have enabled the creation of\nremarkably realistic and high-quality visuals, yet they often falter when\nhandling the inherent ambiguities in user prompts. In this work, we present\nTwin-Co, a framework that leverages synchronized, co-adaptive dialogue to\nprogressively refine image generation. Instead of a static generation process,\nTwin-Co employs a dynamic, iterative workflow where an intelligent dialogue\nagent continuously interacts with the user. Initially, a base image is\ngenerated from the user's prompt. Then, through a series of synchronized\ndialogue exchanges, the system adapts and optimizes the image according to\nevolving user feedback. The co-adaptive process allows the system to\nprogressively narrow down ambiguities and better align with user intent.\nExperiments demonstrate that Twin-Co not only enhances user experience by\nreducing trial-and-error iterations but also improves the quality of the\ngenerated images, streamlining the creative process across various\napplications."}
{"id": "2504.14520", "pdf": "https://arxiv.org/pdf/2504.14520", "abs": "https://arxiv.org/abs/2504.14520", "authors": ["Ahsan Bilal", "Muhammad Ahmed Mohsin", "Muhammad Umer", "Muhammad Awais Khan Bangash", "Muhammad Ali Jamshed"], "title": "Meta-Thinking in LLMs via Multi-Agent Reinforcement Learning: A Survey", "categories": ["cs.AI", "cs.CL"], "comment": "Submitted to IEEE Transactions on Artificial Intelligence", "summary": "This survey explores the development of meta-thinking capabilities in Large\nLanguage Models (LLMs) from a Multi-Agent Reinforcement Learning (MARL)\nperspective. Meta-thinking self-reflection, assessment, and control of thinking\nprocesses is an important next step in enhancing LLM reliability, flexibility,\nand performance, particularly for complex or high-stakes tasks. The survey\nbegins by analyzing current LLM limitations, such as hallucinations and the\nlack of internal self-assessment mechanisms. It then talks about newer methods,\nincluding RL from human feedback (RLHF), self-distillation, and\nchain-of-thought prompting, and each of their limitations. The crux of the\nsurvey is to talk about how multi-agent architectures, namely supervisor-agent\nhierarchies, agent debates, and theory of mind frameworks, can emulate\nhuman-like introspective behavior and enhance LLM robustness. By exploring\nreward mechanisms, self-play, and continuous learning methods in MARL, this\nsurvey gives a comprehensive roadmap to building introspective, adaptive, and\ntrustworthy LLMs. Evaluation metrics, datasets, and future research avenues,\nincluding neuroscience-inspired architectures and hybrid symbolic reasoning,\nare also discussed."}
{"id": "2504.13993", "pdf": "https://arxiv.org/pdf/2504.13993", "abs": "https://arxiv.org/abs/2504.13993", "authors": ["Ekta Gujral", "Apurva Sinha", "Lishi Ji", "Bijayani Sanghamitra Mishra"], "title": "CPR: Leveraging LLMs for Topic and Phrase Suggestion to Facilitate Comprehensive Product Reviews", "categories": ["cs.IR", "cs.AI", "cs.LG"], "comment": null, "summary": "Consumers often heavily rely on online product reviews, analyzing both\nquantitative ratings and textual descriptions to assess product quality.\nHowever, existing research hasn't adequately addressed how to systematically\nencourage the creation of comprehensive reviews that capture both customers\nsentiment and detailed product feature analysis. This paper presents CPR, a\nnovel methodology that leverages the power of Large Language Models (LLMs) and\nTopic Modeling to guide users in crafting insightful and well-rounded reviews.\nOur approach employs a three-stage process: first, we present users with\nproduct-specific terms for rating; second, we generate targeted phrase\nsuggestions based on these ratings; and third, we integrate user-written text\nthrough topic modeling, ensuring all key aspects are addressed. We evaluate CPR\nusing text-to-text LLMs, comparing its performance against real-world customer\nreviews from Walmart. Our results demonstrate that CPR effectively identifies\nrelevant product terms, even for new products lacking prior reviews, and\nprovides sentiment-aligned phrase suggestions, saving users time and enhancing\nreviews quality. Quantitative analysis reveals a 12.3% improvement in BLEU\nscore over baseline methods, further supported by manual evaluation of\ngenerated phrases. We conclude by discussing potential extensions and future\nresearch directions."}
{"id": "2504.14875", "pdf": "https://arxiv.org/pdf/2504.14875", "abs": "https://arxiv.org/abs/2504.14875", "authors": ["Chris Dongjoo Kim", "Jihwan Moon", "Sangwoo Moon", "Heeseung Yun", "Sihaeng Lee", "Aniruddha Kembhavi", "Soonyoung Lee", "Gunhee Kim", "Sangho Lee", "Christopher Clark"], "title": "ReSpec: Relevance and Specificity Grounded Online Filtering for Learning on Video-Text Data Streams", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "CVPR 2025 (main conference)", "summary": "The rapid growth of video-text data presents challenges in storage and\ncomputation during training. Online learning, which processes streaming data in\nreal-time, offers a promising solution to these issues while also allowing\nswift adaptations in scenarios demanding real-time responsiveness. One strategy\nto enhance the efficiency and effectiveness of learning involves identifying\nand prioritizing data that enhances performance on target downstream tasks. We\npropose Relevance and Specificity-based online filtering framework (ReSpec)\nthat selects data based on four criteria: (i) modality alignment for clean\ndata, (ii) task relevance for target focused data, (iii) specificity for\ninformative and detailed data, and (iv) efficiency for low-latency processing.\nRelevance is determined by the probabilistic alignment of incoming data with\ndownstream tasks, while specificity employs the distance to a root embedding\nrepresenting the least specific data as an efficient proxy for informativeness.\nBy establishing reference points from target task data, ReSpec filters incoming\ndata in real-time, eliminating the need for extensive storage and compute.\nEvaluating on large-scale datasets WebVid2M and VideoCC3M, ReSpec attains\nstate-of-the-art performance on five zeroshot video retrieval tasks, using as\nlittle as 5% of the data while incurring minimal compute. The source code is\navailable at https://github.com/cdjkim/ReSpec."}
{"id": "2504.14526", "pdf": "https://arxiv.org/pdf/2504.14526", "abs": "https://arxiv.org/abs/2504.14526", "authors": ["Tong Zeng", "Longfeng Wu", "Liang Shi", "Dawei Zhou", "Feng Guo"], "title": "Are Vision LLMs Road-Ready? A Comprehensive Benchmark for Safety-Critical Driving Video Understanding", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Vision Large Language Models (VLLMs) have demonstrated impressive\ncapabilities in general visual tasks such as image captioning and visual\nquestion answering. However, their effectiveness in specialized,\nsafety-critical domains like autonomous driving remains largely unexplored.\nAutonomous driving systems require sophisticated scene understanding in complex\nenvironments, yet existing multimodal benchmarks primarily focus on normal\ndriving conditions, failing to adequately assess VLLMs' performance in\nsafety-critical scenarios. To address this, we introduce DVBench, a pioneering\nbenchmark designed to evaluate the performance of VLLMs in understanding\nsafety-critical driving videos. Built around a hierarchical ability taxonomy\nthat aligns with widely adopted frameworks for describing driving scenarios\nused in assessing highly automated driving systems, DVBench features 10,000\nmultiple-choice questions with human-annotated ground-truth answers, enabling a\ncomprehensive evaluation of VLLMs' capabilities in perception and reasoning.\nExperiments on 14 SOTA VLLMs, ranging from 0.5B to 72B parameters, reveal\nsignificant performance gaps, with no model achieving over 40% accuracy,\nhighlighting critical limitations in understanding complex driving scenarios.\nTo probe adaptability, we fine-tuned selected models using domain-specific data\nfrom DVBench, achieving accuracy gains ranging from 5.24 to 10.94 percentage\npoints, with relative improvements of up to 43.59%. This improvement\nunderscores the necessity of targeted adaptation to bridge the gap between\ngeneral-purpose VLLMs and mission-critical driving applications. DVBench\nestablishes an essential evaluation framework and research roadmap for\ndeveloping VLLMs that meet the safety and robustness requirements for\nreal-world autonomous systems. We released the benchmark toolbox and the\nfine-tuned model at: https://github.com/tong-zeng/DVBench.git."}
{"id": "2504.14011", "pdf": "https://arxiv.org/pdf/2504.14011", "abs": "https://arxiv.org/abs/2504.14011", "authors": ["Fulvio Sanguigni", "Davide Morelli", "Marcella Cornia", "Rita Cucchiara"], "title": "Fashion-RAG: Multimodal Fashion Image Editing via Retrieval-Augmented Generation", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "IJCNN 2025", "summary": "In recent years, the fashion industry has increasingly adopted AI\ntechnologies to enhance customer experience, driven by the proliferation of\ne-commerce platforms and virtual applications. Among the various tasks, virtual\ntry-on and multimodal fashion image editing -- which utilizes diverse input\nmodalities such as text, garment sketches, and body poses -- have become a key\narea of research. Diffusion models have emerged as a leading approach for such\ngenerative tasks, offering superior image quality and diversity. However, most\nexisting virtual try-on methods rely on having a specific garment input, which\nis often impractical in real-world scenarios where users may only provide\ntextual specifications. To address this limitation, in this work we introduce\nFashion Retrieval-Augmented Generation (Fashion-RAG), a novel method that\nenables the customization of fashion items based on user preferences provided\nin textual form. Our approach retrieves multiple garments that match the input\nspecifications and generates a personalized image by incorporating attributes\nfrom the retrieved items. To achieve this, we employ textual inversion\ntechniques, where retrieved garment images are projected into the textual\nembedding space of the Stable Diffusion text encoder, allowing seamless\nintegration of retrieved elements into the generative process. Experimental\nresults on the Dress Code dataset demonstrate that Fashion-RAG outperforms\nexisting methods both qualitatively and quantitatively, effectively capturing\nfine-grained visual details from retrieved garments. To the best of our\nknowledge, this is the first work to introduce a retrieval-augmented generation\napproach specifically tailored for multimodal fashion image editing."}
{"id": "2504.14877", "pdf": "https://arxiv.org/pdf/2504.14877", "abs": "https://arxiv.org/abs/2504.14877", "authors": ["Aihua Zheng", "Yongqi Sun", "Zi Wang", "Chenglong Li", "Jin Tang"], "title": "Collaborative Enhancement Network for Low-quality Multi-spectral Vehicle Re-identification", "categories": ["cs.CV"], "comment": null, "summary": "The performance of multi-spectral vehicle Re-identification (ReID) is\nsignificantly degraded when some important discriminative cues in visible, near\ninfrared and thermal infrared spectra are lost. Existing methods generate or\nenhance missing details in low-quality spectra data using the high-quality one,\ngenerally called the primary spectrum, but how to justify the primary spectrum\nis a challenging problem. In addition, when the quality of the primary spectrum\nis low, the enhancement effect would be greatly degraded, thus limiting the\nperformance of multi-spectral vehicle ReID. To address these problems, we\npropose the Collaborative Enhancement Network (CoEN), which generates a\nhigh-quality proxy from all spectra data and leverages it to supervise the\nselection of primary spectrum and enhance all spectra features in a\ncollaborative manner, for robust multi-spectral vehicle ReID. First, to\nintegrate the rich cues from all spectra data, we design the Proxy Generator\n(PG) to progressively aggregate multi-spectral features. Second, we design the\nDynamic Quality Sort Module (DQSM), which sorts all spectra data by measuring\ntheir correlations with the proxy, to accurately select the primary spectra\nwith the highest correlation. Finally, we design the Collaborative Enhancement\nModule (CEM) to effectively compensate for missing contents of all spectra by\ncollaborating the primary spectra and the proxy, thereby mitigating the impact\nof low-quality primary spectra. Extensive experiments on three benchmark\ndatasets are conducted to validate the efficacy of the proposed approach\nagainst other multi-spectral vehicle ReID methods. The codes will be released\nat https://github.com/yongqisun/CoEN."}
{"id": "2504.14594", "pdf": "https://arxiv.org/pdf/2504.14594", "abs": "https://arxiv.org/abs/2504.14594", "authors": ["Fan Gao", "Xinjie Zhao", "Ding Xia", "Zhongyi Zhou", "Rui Yang", "Jinghui Lu", "Hang Jiang", "Chanjun Park", "Irene Li"], "title": "HealthGenie: Empowering Users with Healthy Dietary Guidance through Knowledge Graph and Large Language Models", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Seeking dietary guidance often requires navigating complex professional\nknowledge while accommodating individual health conditions. Knowledge Graphs\n(KGs) offer structured and interpretable nutritional information, whereas Large\nLanguage Models (LLMs) naturally facilitate conversational recommendation\ndelivery. In this paper, we present HealthGenie, an interactive system that\ncombines the strengths of LLMs and KGs to provide personalized dietary\nrecommendations along with hierarchical information visualization for a quick\nand intuitive overview. Upon receiving a user query, HealthGenie performs query\nrefinement and retrieves relevant information from a pre-built KG. The system\nthen visualizes and highlights pertinent information, organized by defined\ncategories, while offering detailed, explainable recommendation rationales.\nUsers can further tailor these recommendations by adjusting preferences\ninteractively. Our evaluation, comprising a within-subject comparative\nexperiment and an open-ended discussion, demonstrates that HealthGenie\neffectively supports users in obtaining personalized dietary guidance based on\ntheir health conditions while reducing interaction effort and cognitive load.\nThese findings highlight the potential of LLM-KG integration in supporting\ndecision-making through explainable and visualized information. We examine the\nsystem's usefulness and effectiveness with an N=12 within-subject study and\nprovide design considerations for future systems that integrate conversational\nLLM and KG."}
{"id": "2504.14015", "pdf": "https://arxiv.org/pdf/2504.14015", "abs": "https://arxiv.org/abs/2504.14015", "authors": ["Dominik Dold", "Philipp Christian Petersen"], "title": "Causal pieces: analysing and improving spiking neural networks piece by piece", "categories": ["cs.NE", "cs.AI", "cs.LG", "q-bio.NC", "stat.ML"], "comment": null, "summary": "We introduce a novel concept for spiking neural networks (SNNs) derived from\nthe idea of \"linear pieces\" used to analyse the expressiveness and trainability\nof artificial neural networks (ANNs). We prove that the input domain of SNNs\ndecomposes into distinct causal regions where its output spike times are\nlocally Lipschitz continuous with respect to the input spike times and network\nparameters. The number of such regions - which we call \"causal pieces\" - is a\nmeasure of the approximation capabilities of SNNs. In particular, we\ndemonstrate in simulation that parameter initialisations which yield a high\nnumber of causal pieces on the training set strongly correlate with SNN\ntraining success. Moreover, we find that feedforward SNNs with purely positive\nweights exhibit a surprisingly high number of causal pieces, allowing them to\nachieve competitive performance levels on benchmark tasks. We believe that\ncausal pieces are not only a powerful and principled tool for improving SNNs,\nbut might also open up new ways of comparing SNNs and ANNs in the future."}
{"id": "2504.14884", "pdf": "https://arxiv.org/pdf/2504.14884", "abs": "https://arxiv.org/abs/2504.14884", "authors": ["Jingyu Xing", "Chenwei Tang", "Tao Wang", "Rong Xiao", "Wei Ju", "Ji-Zhe Zhou", "Liangli Zhen", "Jiancheng Lv"], "title": "Memory-Augmented Dual-Decoder Networks for Multi-Class Unsupervised Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in unsupervised anomaly detection (UAD) have shifted from\nsingle-class to multi-class scenarios. In such complex contexts, the increasing\npattern diversity has brought two challenges to reconstruction-based\napproaches: (1) over-generalization: anomalies that are subtle or share\ncompositional similarities with normal patterns may be reconstructed with high\nfidelity, making them difficult to distinguish from normal instances; and (2)\ninsufficient normality reconstruction: complex normal features, such as\nintricate textures or fine-grained structures, may not be faithfully\nreconstructed due to the model's limited representational capacity, resulting\nin false positives. Existing methods typically focus on addressing the former,\nwhich unintentionally exacerbate the latter, resulting in inadequate\nrepresentation of intricate normal patterns. To concurrently address these two\nchallenges, we propose a Memory-augmented Dual-Decoder Networks (MDD-Net). This\nnetwork includes two critical components: a Dual-Decoder Reverse Distillation\nNetwork (DRD-Net) and a Class-aware Memory Module (CMM). Specifically, the\nDRD-Net incorporates a restoration decoder designed to recover normal features\nfrom synthetic abnormal inputs and an identity decoder to reconstruct features\nthat maintain the anomalous semantics. By exploiting the discrepancy between\nfeatures produced by two decoders, our approach refines anomaly scores beyond\nthe conventional encoder-decoder comparison paradigm, effectively reducing\nfalse positives and enhancing localization accuracy. Furthermore, the CMM\nexplicitly encodes and preserves class-specific normal prototypes, actively\nsteering the network away from anomaly reconstruction. Comprehensive\nexperimental results across several benchmarks demonstrate the superior\nperformance of our MDD-Net framework over current SoTA approaches in\nmulti-class UAD tasks."}
{"id": "2504.14640", "pdf": "https://arxiv.org/pdf/2504.14640", "abs": "https://arxiv.org/abs/2504.14640", "authors": ["Yuheng Huang", "Lei Ma", "Keizaburo Nishikino", "Takumi Akazaki"], "title": "Risk Assessment Framework for Code LLMs via Leveraging Internal States", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": "To appear in the 33rd ACM International Conference on the Foundations\n  of Software Engineering (FSE Companion'25 Industry Track), June 23-28, 2025,\n  Trondheim, Norway. This work was supported by Fujitsu Limited", "summary": "The pre-training paradigm plays a key role in the success of Large Language\nModels (LLMs), which have been recognized as one of the most significant\nadvancements of AI recently. Building on these breakthroughs, code LLMs with\nadvanced coding capabilities bring huge impacts on software engineering,\nshowing the tendency to become an essential part of developers' daily routines.\nHowever, the current code LLMs still face serious challenges related to\ntrustworthiness, as they can generate incorrect, insecure, or unreliable code.\nRecent exploratory studies find that it can be promising to detect such risky\noutputs by analyzing LLMs' internal states, akin to how the human brain\nunconsciously recognizes its own mistakes. Yet, most of these approaches are\nlimited to narrow sub-domains of LLM operations and fall short of achieving\nindustry-level scalability and practicability. To address these challenges, in\nthis paper, we propose PtTrust, a two-stage risk assessment framework for code\nLLM based on internal state pre-training, designed to integrate seamlessly with\nthe existing infrastructure of software companies. The core idea is that the\nrisk assessment framework could also undergo a pre-training process similar to\nLLMs. Specifically, PtTrust first performs unsupervised pre-training on\nlarge-scale unlabeled source code to learn general representations of LLM\nstates. Then, it uses a small, labeled dataset to train a risk predictor. We\ndemonstrate the effectiveness of PtTrust through fine-grained, code line-level\nrisk assessment and demonstrate that it generalizes across tasks and different\nprogramming languages. Further experiments also reveal that PtTrust provides\nhighly intuitive and interpretable features, fostering greater user trust. We\nbelieve PtTrust makes a promising step toward scalable and trustworthy\nassurance for code LLMs."}
{"id": "2504.14032", "pdf": "https://arxiv.org/pdf/2504.14032", "abs": "https://arxiv.org/abs/2504.14032", "authors": ["Haiwen Huang", "Anpei Chen", "Volodymyr Havrylov", "Andreas Geiger", "Dan Zhang"], "title": "LoftUp: Learning a Coordinate-Based Feature Upsampler for Vision Foundation Models", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "comment": null, "summary": "Vision foundation models (VFMs) such as DINOv2 and CLIP have achieved\nimpressive results on various downstream tasks, but their limited feature\nresolution hampers performance in applications requiring pixel-level\nunderstanding. Feature upsampling offers a promising direction to address this\nchallenge. In this work, we identify two critical factors for enhancing feature\nupsampling: the upsampler architecture and the training objective. For the\nupsampler architecture, we introduce a coordinate-based cross-attention\ntransformer that integrates the high-resolution images with coordinates and\nlow-resolution VFM features to generate sharp, high-quality features. For the\ntraining objective, we propose constructing high-resolution pseudo-groundtruth\nfeatures by leveraging class-agnostic masks and self-distillation. Our approach\neffectively captures fine-grained details and adapts flexibly to various input\nand feature resolutions. Through experiments, we demonstrate that our approach\nsignificantly outperforms existing feature upsampling techniques across various\ndownstream tasks. Our code is released at https://github.com/andrehuang/loftup."}
{"id": "2504.14888", "pdf": "https://arxiv.org/pdf/2504.14888", "abs": "https://arxiv.org/abs/2504.14888", "authors": ["Xinran Xu", "Yuliang Ma", "Sifu Cai"], "title": "WMKA-Net: A Weighted Multi-Kernel Attention NetworkMethod for Retinal Vessel Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "We propose a novel retinal vessel segmentation network, the Weighted\nMulti-Kernel Attention Network (WMKA-Net), which aims to address the issues of\ninsufficient multiscale feature capture, loss of contextual information, and\nnoise sensitivity in retinal vessel segmentation. WMKA-Net significantly\nimproves the segmentation performance of small vessels and low-contrast regions\nby integrating several innovative components, including the MultiKernelFeature\nFusion Module (MKDC), the Progressive Feature Weighting Fusion Strategy (UDFF),\nand the Attention Mechanism Module (AttentionBlock). The MKDC module employs\nmultiscale parallel convolutional kernels to extract vessel characteristics,\nthereby enhancing the ability to capture complex vascular structures. The UDFF\nstrategy optimizes the transmission of feature information by weighted fusion\nof high- and low-level features. The AttentionBlock highlights key regions and\nsuppresses noise interference through the attention mechanism. Experimental\nresults demonstrate that WMKA-Net achieves excellent segmentation performance\nin multiple public datasets, particularly in segmentation of small vessels and\nprocessing of pathological regions. This work provides a robust and efficient\nnew method for segmentation of the retinal vessel."}
{"id": "2504.14655", "pdf": "https://arxiv.org/pdf/2504.14655", "abs": "https://arxiv.org/abs/2504.14655", "authors": ["Yunhui Xia", "Wei Shen", "Yan Wang", "Jason Klein Liu", "Huifeng Sun", "Siyue Wu", "Jian Hu", "Xiaolong Xu"], "title": "LeetCodeDataset: A Temporal Dataset for Robust Evaluation and Efficient Training of Code LLMs", "categories": ["cs.LG", "cs.CL", "cs.SE"], "comment": null, "summary": "We introduce LeetCodeDataset, a high-quality benchmark for evaluating and\ntraining code-generation models, addressing two key challenges in LLM research:\nthe lack of reasoning-focused coding benchmarks and self-contained training\ntestbeds. By curating LeetCode Python problems with rich metadata, broad\ncoverage, 100+ test cases per problem, and temporal splits (pre/post July\n2024), our dataset enables contamination-free evaluation and efficient\nsupervised fine-tuning (SFT). Experiments show reasoning models significantly\noutperform non-reasoning counterparts, while SFT with only 2.6K model-generated\nsolutions achieves performance comparable to 110K-sample counterparts. The\ndataset and evaluation framework are available on Hugging Face and Github."}
{"id": "2504.14038", "pdf": "https://arxiv.org/pdf/2504.14038", "abs": "https://arxiv.org/abs/2504.14038", "authors": ["Stephen N. Freund", "Brooke Simon", "Emery D. Berger", "Eunice Jun"], "title": "Flowco: Rethinking Data Analysis in the Age of LLMs", "categories": ["cs.HC", "cs.AI", "cs.PL", "stat.CO"], "comment": null, "summary": "Conducting data analysis typically involves authoring code to transform,\nvisualize, analyze, and interpret data. Large language models (LLMs) are now\ncapable of generating such code for simple, routine analyses. LLMs promise to\ndemocratize data science by enabling those with limited programming expertise\nto conduct data analyses, including in scientific research, business, and\npolicymaking. However, analysts in many real-world settings must often exercise\nfine-grained control over specific analysis steps, verify intermediate results\nexplicitly, and iteratively refine their analytical approaches. Such tasks\npresent barriers to building robust and reproducible analyses using LLMs alone\nor even in conjunction with existing authoring tools (e.g., computational\nnotebooks). This paper introduces Flowco, a new mixed-initiative system to\naddress these challenges. Flowco leverages a visual dataflow programming model\nand integrates LLMs into every phase of the authoring process. A user study\nsuggests that Flowco supports analysts, particularly those with less\nprogramming experience, in quickly authoring, debugging, and refining data\nanalyses."}
{"id": "2504.14899", "pdf": "https://arxiv.org/pdf/2504.14899", "abs": "https://arxiv.org/abs/2504.14899", "authors": ["Chenjie Cao", "Jingkai Zhou", "Shikai Li", "Jingyun Liang", "Chaohui Yu", "Fan Wang", "Xiangyang Xue", "Yanwei Fu"], "title": "Uni3C: Unifying Precisely 3D-Enhanced Camera and Human Motion Controls for Video Generation", "categories": ["cs.CV"], "comment": "Project page: https://github.com/ewrfcas/Uni3C", "summary": "Camera and human motion controls have been extensively studied for video\ngeneration, but existing approaches typically address them separately,\nsuffering from limited data with high-quality annotations for both aspects. To\novercome this, we present Uni3C, a unified 3D-enhanced framework for precise\ncontrol of both camera and human motion in video generation. Uni3C includes two\nkey contributions. First, we propose a plug-and-play control module trained\nwith a frozen video generative backbone, PCDController, which utilizes\nunprojected point clouds from monocular depth to achieve accurate camera\ncontrol. By leveraging the strong 3D priors of point clouds and the powerful\ncapacities of video foundational models, PCDController shows impressive\ngeneralization, performing well regardless of whether the inference backbone is\nfrozen or fine-tuned. This flexibility enables different modules of Uni3C to be\ntrained in specific domains, i.e., either camera control or human motion\ncontrol, reducing the dependency on jointly annotated data. Second, we propose\na jointly aligned 3D world guidance for the inference phase that seamlessly\nintegrates both scenic point clouds and SMPL-X characters to unify the control\nsignals for camera and human motion, respectively. Extensive experiments\nconfirm that PCDController enjoys strong robustness in driving camera motion\nfor fine-tuned backbones of video generation. Uni3C substantially outperforms\ncompetitors in both camera controllability and human motion quality.\nAdditionally, we collect tailored validation sets featuring challenging camera\nmovements and human actions to validate the effectiveness of our method."}
{"id": "2504.14773", "pdf": "https://arxiv.org/pdf/2504.14773", "abs": "https://arxiv.org/abs/2504.14773", "authors": ["Haoming Li", "Zhaoliang Chen", "Jonathan Zhang", "Fei Liu"], "title": "PLANET: A Collection of Benchmarks for Evaluating LLMs' Planning Capabilities", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "comment": "10 pages", "summary": "Planning is central to agents and agentic AI. The ability to plan, e.g.,\ncreating travel itineraries within a budget, holds immense potential in both\nscientific and commercial contexts. Moreover, optimal plans tend to require\nfewer resources compared to ad-hoc methods. To date, a comprehensive\nunderstanding of existing planning benchmarks appears to be lacking. Without\nit, comparing planning algorithms' performance across domains or selecting\nsuitable algorithms for new scenarios remains challenging. In this paper, we\nexamine a range of planning benchmarks to identify commonly used testbeds for\nalgorithm development and highlight potential gaps. These benchmarks are\ncategorized into embodied environments, web navigation, scheduling, games and\npuzzles, and everyday task automation. Our study recommends the most\nappropriate benchmarks for various algorithms and offers insights to guide\nfuture benchmark development."}
{"id": "2504.14039", "pdf": "https://arxiv.org/pdf/2504.14039", "abs": "https://arxiv.org/abs/2504.14039", "authors": ["Jaime Raldua Veuthey", "Zainab Ali Majid", "Suhas Hariharan", "Jacob Haimes"], "title": "MEQA: A Meta-Evaluation Framework for Question & Answer LLM Benchmarks", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As Large Language Models (LLMs) advance, their potential for widespread\nsocietal impact grows simultaneously. Hence, rigorous LLM evaluations are both\na technical necessity and social imperative. While numerous evaluation\nbenchmarks have been developed, there remains a critical gap in\nmeta-evaluation: effectively assessing benchmarks' quality. We propose MEQA, a\nframework for the meta-evaluation of question and answer (QA) benchmarks, to\nprovide standardized assessments, quantifiable scores, and enable meaningful\nintra-benchmark comparisons. We demonstrate this approach on cybersecurity\nbenchmarks, using human and LLM evaluators, highlighting the benchmarks'\nstrengths and weaknesses. We motivate our choice of test domain by AI models'\ndual nature as powerful defensive tools and security threats."}
{"id": "2504.14913", "pdf": "https://arxiv.org/pdf/2504.14913", "abs": "https://arxiv.org/abs/2504.14913", "authors": ["Kenji Iwata", "Eiki Ishidera", "Toshifumi Yamaai", "Yutaka Satoh", "Hiroshi Tanaka", "Katsuhiko Takahashi", "Akio Furuhata", "Yoshihisa Tanabe", "Hiroshi Matsumura"], "title": "Guidelines for External Disturbance Factors in the Use of OCR in Real-World Environments", "categories": ["cs.CV", "cs.AI", "I.5.2; I.5.m"], "comment": "16 pages, 14 figures", "summary": "The performance of OCR has improved with the evolution of AI technology. As\nOCR continues to broaden its range of applications, the increased likelihood of\ninterference introduced by various usage environments can prevent it from\nachieving its inherent performance. This results in reduced recognition\naccuracy under certain conditions, and makes the quality control of recognition\ndevices more challenging. Therefore, to ensure that users can properly utilize\nOCR, we compiled the real-world external disturbance factors that cause\nperformance degradation, along with the resulting image degradation phenomena,\ninto an external disturbance factor table and, by also indicating how to make\nuse of it, organized them into guidelines."}
{"id": "2504.14822", "pdf": "https://arxiv.org/pdf/2504.14822", "abs": "https://arxiv.org/abs/2504.14822", "authors": ["Rui Qiu", "Shijie Chen", "Yu Su", "Po-Yin Yen", "Han-Wei Shen"], "title": "Completing A Systematic Review in Hours instead of Months with Interactive AI Agents", "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "Systematic reviews (SRs) are vital for evidence-based practice in high stakes\ndisciplines, such as healthcare, but are often impeded by intensive labors and\nlengthy processes that can take months to complete. Due to the high demand for\ndomain expertise, existing automatic summarization methods fail to accurately\nidentify relevant studies and generate high-quality summaries. To that end, we\nintroduce InsightAgent, a human-centered interactive AI agent powered by large\nlanguage models that revolutionize this workflow. InsightAgent partitions a\nlarge literature corpus based on semantics and employs a multi-agent design for\nmore focused processing of literature, leading to significant improvement in\nthe quality of generated SRs. InsightAgent also provides intuitive\nvisualizations of the corpus and agent trajectories, allowing users to\neffortlessly monitor the actions of the agent and provide real-time feedback\nbased on their expertise. Our user studies with 9 medical professionals\ndemonstrate that the visualization and interaction mechanisms can effectively\nimprove the quality of synthesized SRs by 27.2%, reaching 79.7% of\nhuman-written quality. At the same time, user satisfaction is improved by\n34.4%. With InsightAgent, it only takes a clinician about 1.5 hours, rather\nthan months, to complete a high-quality systematic review."}
{"id": "2504.14046", "pdf": "https://arxiv.org/pdf/2504.14046", "abs": "https://arxiv.org/abs/2504.14046", "authors": ["Tahar Nabil", "Ghislain Agoua", "Pierre Cauchois", "Anne De Moliner", "Benoît Grossin"], "title": "A synthetic dataset of French electric load curves with temperature conditioning", "categories": ["cs.LG", "cs.AI"], "comment": "Workshop paper at \"Tackling Climate Change with Machine Learning\",\n  ICLR 2025", "summary": "The undergoing energy transition is causing behavioral changes in electricity\nuse, e.g. with self-consumption of local generation, or flexibility services\nfor demand control. To better understand these changes and the challenges they\ninduce, accessing individual smart meter data is crucial. Yet this is personal\ndata under the European GDPR. A widespread use of such data requires thus to\ncreate synthetic realistic and privacy-preserving samples. This paper\nintroduces a new synthetic load curve dataset generated by conditional latent\ndiffusion. We also provide the contracted power, time-of-use plan and local\ntemperature used for generation. Fidelity, utility and privacy of the dataset\nare thoroughly evaluated, demonstrating its good quality and thereby supporting\nits interest for energy modeling applications."}
{"id": "2504.14919", "pdf": "https://arxiv.org/pdf/2504.14919", "abs": "https://arxiv.org/abs/2504.14919", "authors": ["Donghyeong Kim", "Chaewon Park", "Suhwan Cho", "Hyeonjeong Lim", "Minseok Kang", "Jungho Lee", "Sangyoun Lee"], "title": "GenCLIP: Generalizing CLIP Prompts for Zero-shot Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Zero-shot anomaly detection (ZSAD) aims to identify anomalies in unseen\ncategories by leveraging CLIP's zero-shot capabilities to match text prompts\nwith visual features. A key challenge in ZSAD is learning general prompts\nstably and utilizing them effectively, while maintaining both generalizability\nand category specificity. Although general prompts have been explored in prior\nworks, achieving their stable optimization and effective deployment remains a\nsignificant challenge. In this work, we propose GenCLIP, a novel framework that\nlearns and leverages general prompts more effectively through multi-layer\nprompting and dual-branch inference. Multi-layer prompting integrates\ncategory-specific visual cues from different CLIP layers, enriching general\nprompts with more comprehensive and robust feature representations. By\ncombining general prompts with multi-layer visual features, our method further\nenhances its generalization capability. To balance specificity and\ngeneralization, we introduce a dual-branch inference strategy, where a\nvision-enhanced branch captures fine-grained category-specific features, while\na query-only branch prioritizes generalization. The complementary outputs from\nboth branches improve the stability and reliability of anomaly detection across\nunseen categories. Additionally, we propose an adaptive text prompt filtering\nmechanism, which removes irrelevant or atypical class names not encountered\nduring CLIP's training, ensuring that only meaningful textual inputs contribute\nto the final vision-language alignment."}
{"id": "2504.14858", "pdf": "https://arxiv.org/pdf/2504.14858", "abs": "https://arxiv.org/abs/2504.14858", "authors": ["Jiaqi Wei", "Hao Zhou", "Xiang Zhang", "Di Zhang", "Zijie Qiu", "Wei Wei", "Jinzhe Li", "Wanli Ouyang", "Siqi Sun"], "title": "AlignRAG: An Adaptable Framework for Resolving Misalignments in Retrieval-Aware Reasoning of RAG", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Retrieval-augmented generation (RAG) has emerged as a foundational paradigm\nfor knowledge-grounded text generation. However, existing RAG pipelines often\nfail to ensure that the reasoning trajectories align with the evidential\nconstraints imposed by retrieved content. In this paper, we reframe RAG as a\nproblem of retrieval-aware reasoning and identify a core challenge: reasoning\nmisalignment-the mismatch between a model's reasoning trajectory and the\nretrieved evidence. To address this challenge, we propose AlignRAG, a novel\ntest-time framework that mitigates reasoning misalignment through iterative\nCritique-Driven Alignment (CDA) steps. In contrast to prior approaches that\nrely on static training or post-hoc selection, AlignRAG actively refines\nreasoning trajectories during inference by enforcing fine-grained alignment\nwith evidence. Our framework introduces a new paradigm for retrieval-aware\nreasoning by: (1) constructing context-rich training corpora; (2) generating\ncontrastive critiques from preference-aware reasoning trajectories; (3)\ntraining a dedicated \\textit{Critic Language Model (CLM)} to identify reasoning\nmisalignments; and (4) applying CDA steps to optimize reasoning trajectories\niteratively. Empirical results demonstrate that AlignRAG consistently\noutperforms all baselines and could integrate as a plug-and-play module into\nexisting RAG pipelines without further changes. By reconceptualizing RAG as a\nstructured reasoning trajectory and establishing the test-time framework for\ncorrecting reasoning misalignments in RAG, AlignRAG provides practical\nadvancements for retrieval-aware generation."}
{"id": "2504.14053", "pdf": "https://arxiv.org/pdf/2504.14053", "abs": "https://arxiv.org/abs/2504.14053", "authors": ["Ali Safari"], "title": "Sentiment Analysis of Airbnb Reviews: Exploring Their Impact on Acceptance Rates and Pricing Across Multiple U.S. Regions", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "This research examines whether Airbnb guests' positive and negative comments\ninfluence acceptance rates and rental prices across six U.S. regions: Rhode\nIsland, Broward County, Chicago, Dallas, San Diego, and Boston. Thousands of\nreviews were collected and analyzed using Natural Language Processing (NLP) to\nclassify sentiments as positive or negative, followed by statistical testing\n(t-tests and basic correlations) on the average scores. The findings reveal\nthat over 90 percent of reviews in each region are positive, indicating that\nhaving additional reviews does not significantly enhance prices. However,\nlistings with predominantly positive feedback exhibit slightly higher\nacceptance rates, suggesting that sentiment polarity, rather than the sheer\nvolume of reviews, is a more critical factor for host success. Additionally,\nbudget listings often gather extensive reviews while maintaining competitive\npricing, whereas premium listings sustain higher prices with fewer but highly\npositive reviews. These results underscore the importance of sentiment quality\nover quantity in shaping guest behavior and pricing strategies in an\noverwhelmingly positive review environment."}
{"id": "2504.14920", "pdf": "https://arxiv.org/pdf/2504.14920", "abs": "https://arxiv.org/abs/2504.14920", "authors": ["Geng Li", "Jinglin Xu", "Yunzhen Zhao", "Yuxin Peng"], "title": "DyFo: A Training-Free Dynamic Focus Visual Search for Enhancing LMMs in Fine-Grained Visual Understanding", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025 (Hightlight). Project page with code:\n  https://github.com/PKU-ICST-MIPL/DyFo_CVPR2025", "summary": "Humans can effortlessly locate desired objects in cluttered environments,\nrelying on a cognitive mechanism known as visual search to efficiently filter\nout irrelevant information and focus on task-related regions. Inspired by this\nprocess, we propose Dyfo (Dynamic Focus), a training-free dynamic focusing\nvisual search method that enhances fine-grained visual understanding in large\nmultimodal models (LMMs). Unlike existing approaches which require additional\nmodules or data collection, Dyfo leverages a bidirectional interaction between\nLMMs and visual experts, using a Monte Carlo Tree Search (MCTS) algorithm to\nsimulate human-like focus adjustments. This enables LMMs to focus on key visual\nregions while filtering out irrelevant content, without introducing additional\ntraining caused by vocabulary expansion or the integration of specialized\nlocalization modules. Experimental results demonstrate that Dyfo significantly\nimproves fine-grained visual understanding and reduces hallucination issues in\nLMMs, achieving superior performance across both fixed and dynamic resolution\nmodels. The code is available at https://github.com/PKU-ICST-MIPL/DyFo_CVPR2025"}
{"id": "2504.14870", "pdf": "https://arxiv.org/pdf/2504.14870", "abs": "https://arxiv.org/abs/2504.14870", "authors": ["Hongru Wang", "Cheng Qian", "Wanjun Zhong", "Xiusi Chen", "Jiahao Qiu", "Shijue Huang", "Bowen Jin", "Mengdi Wang", "Kam-Fai Wong", "Heng Ji"], "title": "OTC: Optimal Tool Calls via Reinforcement Learning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Tool-integrated reasoning (TIR) augments large language models (LLMs) with\nthe ability to invoke external tools, such as search engines and code\ninterpreters, to solve tasks beyond the capabilities of language-only\nreasoning. While reinforcement learning (RL) has shown promise in improving TIR\nby optimizing final answer correctness, existing approaches often overlook the\nefficiency and cost associated with tool usage. This can lead to suboptimal\nbehavior, including excessive tool calls that increase computational and\nfinancial overhead, or insufficient tool use that compromises answer quality.\nIn this work, we propose Optimal Tool Call-controlled Policy Optimization\n(OTC-PO), a simple yet effective RL-based framework that encourages models to\nproduce accurate answers with minimal tool calls. Our method introduces a\ntool-integrated reward that jointly considers correctness and tool efficiency,\npromoting high tool productivity. We instantiate this framework within both\nProximal Policy Optimization (PPO) and Group Relative Preference Optimization\n(GRPO), resulting in OTC-PPO and OTC-GRPO. Experiments with Qwen-2.5 and\nQwen-Math across multiple QA benchmarks show that our approach reduces tool\ncalls by up to 73.1\\% and improves tool productivity by up to 229.4\\%, while\nmaintaining comparable answer accuracy. To the best of our knowledge, this is\nthe first RL-based framework that explicitly optimizes tool-use efficiency in\nTIR."}
{"id": "2504.14054", "pdf": "https://arxiv.org/pdf/2504.14054", "abs": "https://arxiv.org/abs/2504.14054", "authors": ["Soroosh Baselizadeh", "Cheuk-To Yu", "Olga Veksler", "Yuri Boykov"], "title": "Occlusion-Ordered Semantic Instance Segmentation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Standard semantic instance segmentation provides useful, but inherently 2D\ninformation from a single image. To enable 3D analysis, one usually integrates\nabsolute monocular depth estimation with instance segmentation. However,\nmonocular depth is a difficult task. Instead, we leverage a simpler\nsingle-image task, occlusion-based relative depth ordering, providing coarser\nbut useful 3D information. We show that relative depth ordering works more\nreliably from occlusions than from absolute depth. We propose to solve the\njoint task of relative depth ordering and segmentation of instances based on\nocclusions. We call this task Occlusion-Ordered Semantic Instance Segmentation\n(OOSIS). We develop an approach to OOSIS that extracts instances and their\nocclusion order simultaneously from oriented occlusion boundaries and semantic\nsegmentation. Unlike popular detect-and-segment framework for instance\nsegmentation, combining occlusion ordering with instance segmentation allows a\nsimple and clean formulation of OOSIS as a labeling problem. As a part of our\nsolution for OOSIS, we develop a novel oriented occlusion boundaries approach\nthat significantly outperforms prior work. We also develop a new joint OOSIS\nmetric based both on instance mask accuracy and correctness of their occlusion\norder. We achieve better performance than strong baselines on KINS and COCOA\ndatasets."}
{"id": "2504.14921", "pdf": "https://arxiv.org/pdf/2504.14921", "abs": "https://arxiv.org/abs/2504.14921", "authors": ["Songping Wang", "Hanqing Liu", "Yueming Lyu", "Xiantao Hu", "Ziwen He", "Wei Wang", "Caifeng Shan", "Liang Wang"], "title": "Fast Adversarial Training with Weak-to-Strong Spatial-Temporal Consistency in the Frequency Domain on Videos", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Adversarial Training (AT) has been shown to significantly enhance adversarial\nrobustness via a min-max optimization approach. However, its effectiveness in\nvideo recognition tasks is hampered by two main challenges. First, fast\nadversarial training for video models remains largely unexplored, which\nseverely impedes its practical applications. Specifically, most video\nadversarial training methods are computationally costly, with long training\ntimes and high expenses. Second, existing methods struggle with the trade-off\nbetween clean accuracy and adversarial robustness. To address these challenges,\nwe introduce Video Fast Adversarial Training with Weak-to-Strong consistency\n(VFAT-WS), the first fast adversarial training method for video data.\nSpecifically, VFAT-WS incorporates the following key designs: First, it\nintegrates a straightforward yet effective temporal frequency augmentation\n(TF-AUG), and its spatial-temporal enhanced form STF-AUG, along with a\nsingle-step PGD attack to boost training efficiency and robustness. Second, it\ndevises a weak-to-strong spatial-temporal consistency regularization, which\nseamlessly integrates the simpler TF-AUG and the more complex STF-AUG.\nLeveraging the consistency regularization, it steers the learning process from\nsimple to complex augmentations. Both of them work together to achieve a better\ntrade-off between clean accuracy and robustness. Extensive experiments on\nUCF-101 and HMDB-51 with both CNN and Transformer-based models demonstrate that\nVFAT-WS achieves great improvements in adversarial robustness and corruption\nrobustness, while accelerating training by nearly 490%."}
{"id": "2504.14904", "pdf": "https://arxiv.org/pdf/2504.14904", "abs": "https://arxiv.org/abs/2504.14904", "authors": ["Xingyu Lu", "Tianke Zhang", "Chang Meng", "Xiaobei Wang", "Jinpeng Wang", "YiFan Zhang", "Shisong Tang", "Changyi Liu", "Haojie Ding", "Kaiyu Jiang", "Kaiyu Tang", "Bin Wen", "Hai-Tao Zheng", "Fan Yang", "Tingting Gao", "Di Zhang", "Kun Gai"], "title": "VLM as Policy: Common-Law Content Moderation Framework for Short Video Platform", "categories": ["cs.SI", "cs.AI", "cs.CL", "cs.MM"], "comment": "20 pages, 6 figures", "summary": "Exponentially growing short video platforms (SVPs) face significant\nchallenges in moderating content detrimental to users' mental health,\nparticularly for minors. The dissemination of such content on SVPs can lead to\ncatastrophic societal consequences. Although substantial efforts have been\ndedicated to moderating such content, existing methods suffer from critical\nlimitations: (1) Manual review is prone to human bias and incurs high\noperational costs. (2) Automated methods, though efficient, lack nuanced\ncontent understanding, resulting in lower accuracy. (3) Industrial moderation\nregulations struggle to adapt to rapidly evolving trends due to long update\ncycles. In this paper, we annotate the first SVP content moderation benchmark\nwith authentic user/reviewer feedback to fill the absence of benchmark in this\nfield. Then we evaluate various methods on the benchmark to verify the\nexistence of the aforementioned limitations. We further propose our common-law\ncontent moderation framework named KuaiMod to address these challenges. KuaiMod\nconsists of three components: training data construction, offline adaptation,\nand online deployment & refinement. Leveraging large vision language model\n(VLM) and Chain-of-Thought (CoT) reasoning, KuaiMod adequately models video\ntoxicity based on sparse user feedback and fosters dynamic moderation policy\nwith rapid update speed and high accuracy. Offline experiments and large-scale\nonline A/B test demonstrates the superiority of KuaiMod: KuaiMod achieves the\nbest moderation performance on our benchmark. The deployment of KuaiMod reduces\nthe user reporting rate by 20% and its application in video recommendation\nincreases both Daily Active User (DAU) and APP Usage Time (AUT) on several\nKuaishou scenarios. We have open-sourced our benchmark at\nhttps://kuaimod.github.io."}
{"id": "2504.14070", "pdf": "https://arxiv.org/pdf/2504.14070", "abs": "https://arxiv.org/abs/2504.14070", "authors": ["Jinesh Jhonsa", "William Whitehead", "David McCarthy", "Shuvro Chowdhury", "Kerem Camsari", "Luke Theogarajan"], "title": "A CMOS Probabilistic Computing Chip With In-situ hardware Aware Learning", "categories": ["cs.AR", "cs.AI"], "comment": "3 pages 12 figuewa", "summary": "This paper demonstrates a probabilistic bit physics inspired solver with 440\nspins configured in a Chimera graph, occupying an area of 0.44 mm^2. Area\nefficiency is maximized through a current-mode implementation of the neuron\nupdate circuit, standard cell design for analog blocks pitch-matched to digital\nblocks, and a shared power supply for both digital and analog components.\nProcess variation related mismatches introduced by this approach are\neffectively mitigated using a hardware aware contrastive divergence algorithm\nduring training. We validate the chip's ability to perform probabilistic\ncomputing tasks such as modeling logic gates and full adders, as well as\noptimization tasks such as MaxCut, demonstrating its potential for AI and\nmachine learning applications."}
{"id": "2504.14933", "pdf": "https://arxiv.org/pdf/2504.14933", "abs": "https://arxiv.org/abs/2504.14933", "authors": ["Mazharul Islam Rakib", "Showrin Rahman", "Joyanta Jyoti Mondal", "Xi Xiao", "David Lewis", "Alessandra Mileo", "Meem Arafat Manab"], "title": "TWIG: Two-Step Image Generation using Segmentation Masks in Diffusion Models", "categories": ["cs.CV", "68T07, 68U10, 68T45"], "comment": "16 pages, 9 figures, preprint", "summary": "In today's age of social media and marketing, copyright issues can be a major\nroadblock to the free sharing of images. Generative AI models have made it\npossible to create high-quality images, but concerns about copyright\ninfringement are a hindrance to their abundant use. As these models use data\nfrom training images to generate new ones, it is often a daunting task to\nensure they do not violate intellectual property rights. Some AI models have\neven been noted to directly copy copyrighted images, a problem often referred\nto as source copying. Traditional copyright protection measures such as\nwatermarks and metadata have also proven to be futile in this regard. To\naddress this issue, we propose a novel two-step image generation model inspired\nby the conditional diffusion model. The first step involves creating an image\nsegmentation mask for some prompt-based generated images. This mask embodies\nthe shape of the image. Thereafter, the diffusion model is asked to generate\nthe image anew while avoiding the shape in question. This approach shows a\ndecrease in structural similarity from the training image, i.e. we are able to\navoid the source copying problem using this approach without expensive\nretraining of the model or user-centered prompt generation techniques. This\nmakes our approach the most computationally inexpensive approach to avoiding\nboth copyright infringement and source copying for diffusion model-based image\ngeneration."}
{"id": "2504.14928", "pdf": "https://arxiv.org/pdf/2504.14928", "abs": "https://arxiv.org/abs/2504.14928", "authors": ["Yao Shi", "Rongkeng Liang", "Yong Xu"], "title": "EducationQ: Evaluating LLMs' Teaching Capabilities Through Multi-Agent Dialogue Framework", "categories": ["cs.AI", "cs.CE", "cs.CL", "cs.CY", "cs.HC"], "comment": null, "summary": "Large language models (LLMs) increasingly serve as educational tools, yet\nevaluating their teaching capabilities remains challenging due to the\nresource-intensive, context-dependent, and methodologically complex nature of\nteacher-student interactions. We introduce EducationQ, a multi-agent dialogue\nframework that efficiently assesses teaching capabilities through simulated\ndynamic educational scenarios, featuring specialized agents for teaching,\nlearning, and evaluation. Testing 14 LLMs across major AI Organizations\n(OpenAI, Meta, Google, Anthropic, and others) on 1,498 questions spanning 13\ndisciplines and 10 difficulty levels reveals that teaching effectiveness does\nnot correlate linearly with model scale or general reasoning capabilities -\nwith some smaller open-source models outperforming larger commercial\ncounterparts in teaching contexts. This finding highlights a critical gap in\ncurrent evaluations that prioritize knowledge recall over interactive pedagogy.\nOur mixed-methods evaluation, combining quantitative metrics with qualitative\nanalysis and expert case studies, identifies distinct pedagogical strengths\nemployed by top-performing models (e.g., sophisticated questioning strategies,\nadaptive feedback mechanisms). Human expert evaluations show 78% agreement with\nour automated qualitative analysis of effective teaching behaviors, validating\nour methodology. EducationQ demonstrates that LLMs-as-teachers require\nspecialized optimization beyond simple scaling, suggesting next-generation\neducational AI prioritize targeted enhancement of specific pedagogical\neffectiveness."}
{"id": "2504.14071", "pdf": "https://arxiv.org/pdf/2504.14071", "abs": "https://arxiv.org/abs/2504.14071", "authors": ["Renaud Bougueng Tchemeube", "Jeff Ens", "Cale Plut", "Philippe Pasquier", "Maryam Safi", "Yvan Grabit", "Jean-Baptiste Rolland"], "title": "Evaluating Human-AI Interaction via Usability, User Experience and Acceptance Measures for MMM-C: A Creative AI System for Music Composition", "categories": ["cs.HC", "cs.AI", "cs.LG", "cs.SD"], "comment": "10 pages, 6 figures, 1 table, first published at the 32nd\n  International Joint Conference on Artificial Intelligence (IJCAI 2023),\n  Macao, China", "summary": "With the rise of artificial intelligence (AI), there has been increasing\ninterest in human-AI co-creation in a variety of artistic domains including\nmusic as AI-driven systems are frequently able to generate human-competitive\nartifacts. Now, the implications of such systems for musical practice are being\ninvestigated. We report on a thorough evaluation of the user adoption of the\nMulti-Track Music Machine (MMM) as a co-creative AI tool for music composers.\nTo do this, we integrate MMM into Cubase, a popular Digital Audio Workstation\n(DAW) by Steinberg, by producing a \"1-parameter\" plugin interface named\nMMM-Cubase (MMM-C), which enables human-AI co-composition. We contribute a\nmethodological assemblage as a 3-part mixed method study measuring usability,\nuser experience and technology acceptance of the system across two groups of\nexpert-level composers: hobbyists and professionals. Results show positive\nusability and acceptance scores. Users report experiences of novelty, surprise\nand ease of use from using the system, and limitations on controllability and\npredictability of the interface when generating music. Findings indicate no\nsignificant difference between the two user groups."}
{"id": "2504.14952", "pdf": "https://arxiv.org/pdf/2504.14952", "abs": "https://arxiv.org/abs/2504.14952", "authors": ["Qianyu Zhu", "Junjie Wang", "Jeremiah Hu", "Jia Ai", "Yong Lee"], "title": "PIV-FlowDiffuser:Transfer-learning-based denoising diffusion models for PIV", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Deep learning algorithms have significantly reduced the computational time\nand improved the spatial resolution of particle image velocimetry~(PIV).\nHowever, the models trained on synthetic datasets might have a degraded\nperformance on practical particle images due to domain gaps. As a result,\nspecial residual patterns are often observed for the vector fields of deep\nlearning-based estimators. To reduce the special noise step-by-step, we employ\na denoising diffusion model~(FlowDiffuser) for PIV analysis. And the\ndata-hungry iterative denoising diffusion model is trained via a transfer\nlearning strategy, resulting in our PIV-FlowDiffuser method. Specifically, (1)\npre-training a FlowDiffuser model with multiple optical flow datasets of the\ncomputer vision community, such as Sintel, KITTI, etc; (2) fine-tuning the\npre-trained model on synthetic PIV datasets. Note that the PIV images are\nupsampled by a factor of two to resolve the small-scale turbulent flow\nstructures. The visualized results indicate that our PIV-FlowDiffuser\neffectively suppresses the noise patterns. Therefore, the denoising diffusion\nmodel reduces the average end-point error~($AEE$) by 59.4% over RAFT256-PIV\nbaseline on the classic Cai's dataset. Besides, PIV-FlowDiffuser exhibits\nenhanced generalization performance on unseen particle images due to transfer\nlearning. Overall, this study highlights the transfer-learning-based denoising\ndiffusion models for PIV. And a detailed implementation is recommended for\ninterested readers in the repository\nhttps://github.com/Zhu-Qianyu/PIV-FlowDiffuser."}
{"id": "2504.14945", "pdf": "https://arxiv.org/pdf/2504.14945", "abs": "https://arxiv.org/abs/2504.14945", "authors": ["Jianhao Yan", "Yafu Li", "Zican Hu", "Zhi Wang", "Ganqu Cui", "Xiaoye Qu", "Yu Cheng", "Yue Zhang"], "title": "Learning to Reason under Off-Policy Guidance", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Work in progress", "summary": "Recent advances in large reasoning models (LRMs) demonstrate that\nsophisticated behaviors such as multi-step reasoning and self-reflection can\nemerge via reinforcement learning (RL) with simple rule-based rewards. However,\nexisting zero-RL approaches are inherently ``on-policy'', limiting learning to\na model's own outputs and failing to acquire reasoning abilities beyond its\ninitial capabilities. We introduce LUFFY (Learning to reason Under oFF-policY\nguidance), a framework that augments zero-RL with off-policy reasoning traces.\nLUFFY dynamically balances imitation and exploration by combining off-policy\ndemonstrations with on-policy rollouts during training. Notably, we propose\npolicy shaping via regularized importance sampling to avoid superficial and\nrigid imitation during mixed-policy training. Remarkably, LUFFY achieves an\nover +7.0 average gain across six math benchmarks and an advantage of over +6.2\npoints in out-of-distribution tasks. It also substantially surpasses\nimitation-based supervised fine-tuning (SFT), particularly in generalization.\nAnalysis shows LUFFY not only imitates effectively but also explores beyond\ndemonstrations, offering a scalable path to train generalizable reasoning\nmodels with off-policy guidance."}
{"id": "2504.14089", "pdf": "https://arxiv.org/pdf/2504.14089", "abs": "https://arxiv.org/abs/2504.14089", "authors": ["Kang He", "Kaushik Roy"], "title": "LogicTree: Structured Proof Exploration for Coherent and Rigorous Logical Reasoning with Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average."}
{"id": "2504.14967", "pdf": "https://arxiv.org/pdf/2504.14967", "abs": "https://arxiv.org/abs/2504.14967", "authors": ["Yating Wang", "Xuan Wang", "Ran Yi", "Yanbo Fan", "Jichen Hu", "Jingcheng Zhu", "Lizhuang Ma"], "title": "3D Gaussian Head Avatars with Expressive Dynamic Appearances by Compact Tensorial Representations", "categories": ["cs.CV"], "comment": null, "summary": "Recent studies have combined 3D Gaussian and 3D Morphable Models (3DMM) to\nconstruct high-quality 3D head avatars. In this line of research, existing\nmethods either fail to capture the dynamic textures or incur significant\noverhead in terms of runtime speed or storage space. To this end, we propose a\nnovel method that addresses all the aforementioned demands. In specific, we\nintroduce an expressive and compact representation that encodes texture-related\nattributes of the 3D Gaussians in the tensorial format. We store appearance of\nneutral expression in static tri-planes, and represents dynamic texture details\nfor different expressions using lightweight 1D feature lines, which are then\ndecoded into opacity offset relative to the neutral face. We further propose\nadaptive truncated opacity penalty and class-balanced sampling to improve\ngeneralization across different expressions. Experiments show this design\nenables accurate face dynamic details capturing while maintains real-time\nrendering and significantly reduces storage costs, thus broadening the\napplicability to more scenarios."}
{"id": "2504.15068", "pdf": "https://arxiv.org/pdf/2504.15068", "abs": "https://arxiv.org/abs/2504.15068", "authors": ["Ronak Pradeep", "Nandan Thakur", "Shivani Upadhyay", "Daniel Campos", "Nick Craswell", "Jimmy Lin"], "title": "The Great Nugget Recall: Automating Fact Extraction and RAG Evaluation with Large Language Models", "categories": ["cs.IR", "cs.CL"], "comment": "To appear in SIGIR 2025. Significant updates and revisions to\n  arXiv:2411.09607", "summary": "Large Language Models (LLMs) have significantly enhanced the capabilities of\ninformation access systems, especially with retrieval-augmented generation\n(RAG). Nevertheless, the evaluation of RAG systems remains a barrier to\ncontinued progress, a challenge we tackle in this work by proposing an\nautomatic evaluation framework that is validated against human annotations. We\nbelieve that the nugget evaluation methodology provides a solid foundation for\nevaluating RAG systems. This approach, originally developed for the TREC\nQuestion Answering (QA) Track in 2003, evaluates systems based on atomic facts\nthat should be present in good answers. Our efforts focus on \"refactoring\" this\nmethodology, where we describe the AutoNuggetizer framework that specifically\napplies LLMs to both automatically create nuggets and automatically assign\nnuggets to system answers. In the context of the TREC 2024 RAG Track, we\ncalibrate a fully automatic approach against strategies where nuggets are\ncreated manually or semi-manually by human assessors and then assigned manually\nto system answers. Based on results from a community-wide evaluation, we\nobserve strong agreement at the run level between scores derived from fully\nautomatic nugget evaluation and human-based variants. The agreement is stronger\nwhen individual framework components such as nugget assignment are automated\nindependently. This suggests that our evaluation framework provides tradeoffs\nbetween effort and quality that can be used to guide the development of future\nRAG systems. However, further research is necessary to refine our approach,\nparticularly in establishing robust per-topic agreement to diagnose system\nfailures effectively."}
{"id": "2504.14094", "pdf": "https://arxiv.org/pdf/2504.14094", "abs": "https://arxiv.org/abs/2504.14094", "authors": ["Enrico Parisini", "Tapabrata Chakraborti", "Chris Harbron", "Ben D. MacArthur", "Christopher R. S. Banerji"], "title": "Leakage and Interpretability in Concept-Based Models", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "38 pages, 27 figures", "summary": "Concept Bottleneck Models aim to improve interpretability by predicting\nhigh-level intermediate concepts, representing a promising approach for\ndeployment in high-risk scenarios. However, they are known to suffer from\ninformation leakage, whereby models exploit unintended information encoded\nwithin the learned concepts. We introduce an information-theoretic framework to\nrigorously characterise and quantify leakage, and define two complementary\nmeasures: the concepts-task leakage (CTL) and interconcept leakage (ICL)\nscores. We show that these measures are strongly predictive of model behaviour\nunder interventions and outperform existing alternatives in robustness and\nreliability. Using this framework, we identify the primary causes of leakage\nand provide strong evidence that Concept Embedding Models exhibit substantial\nleakage regardless of the hyperparameters choice. Finally, we propose practical\nguidelines for designing concept-based models to reduce leakage and ensure\ninterpretability."}
{"id": "2504.14975", "pdf": "https://arxiv.org/pdf/2504.14975", "abs": "https://arxiv.org/abs/2504.14975", "authors": ["Hongbin Xu", "Chaohui Yu", "Feng Xiao", "Jiazheng Xing", "Hai Ci", "Weitao Chen", "Ming Li"], "title": "Cyc3D: Fine-grained Controllable 3D Generation via Cycle Consistency Regularization", "categories": ["cs.CV"], "comment": "Preprint version. The code will be open after finishing the reviewing\n  process", "summary": "Despite the remarkable progress of 3D generation, achieving controllability,\ni.e., ensuring consistency between generated 3D content and input conditions\nlike edge and depth, remains a significant challenge. Existing methods often\nstruggle to maintain accurate alignment, leading to noticeable discrepancies.\nTo address this issue, we propose \\name{}, a new framework that enhances\ncontrollable 3D generation by explicitly encouraging cyclic consistency between\nthe second-order 3D content, generated based on extracted signals from the\nfirst-order generation, and its original input controls. Specifically, we\nemploy an efficient feed-forward backbone that can generate a 3D object from an\ninput condition and a text prompt. Given an initial viewpoint and a control\nsignal, a novel view is rendered from the generated 3D content, from which the\nextracted condition is used to regenerate the 3D content. This re-generated\noutput is then rendered back to the initial viewpoint, followed by another\nround of control signal extraction, forming a cyclic process with two\nconsistency constraints. \\emph{View consistency} ensures coherence between the\ntwo generated 3D objects, measured by semantic similarity to accommodate\ngenerative diversity. \\emph{Condition consistency} aligns the final extracted\nsignal with the original input control, preserving structural or geometric\ndetails throughout the process. Extensive experiments on popular benchmarks\ndemonstrate that \\name{} significantly improves controllability, especially for\nfine-grained details, outperforming existing methods across various conditions\n(e.g., +14.17\\% PSNR for edge, +6.26\\% PSNR for sketch)."}
{"id": "2504.15072", "pdf": "https://arxiv.org/pdf/2504.15072", "abs": "https://arxiv.org/abs/2504.15072", "authors": ["Yulong Li", "Zhixiang Lu", "Feilong Tang", "Simin Lai", "Ming Hu", "Yuxuan Zhang", "Haochen Xue", "Zhaodong Wu", "Imran Razzak", "Qingxia Li", "Jionglong Su"], "title": "Rhythm of Opinion: A Hawkes-Graph Framework for Dynamic Propagation Analysis", "categories": ["cs.SI", "cs.CL"], "comment": null, "summary": "The rapid development of social media has significantly reshaped the dynamics\nof public opinion, resulting in complex interactions that traditional models\nfail to effectively capture. To address this challenge, we propose an\ninnovative approach that integrates multi-dimensional Hawkes processes with\nGraph Neural Network, modeling opinion propagation dynamics among nodes in a\nsocial network while considering the intricate hierarchical relationships\nbetween comments. The extended multi-dimensional Hawkes process captures the\nhierarchical structure, multi-dimensional interactions, and mutual influences\nacross different topics, forming a complex propagation network. Moreover,\nrecognizing the lack of high-quality datasets capable of comprehensively\ncapturing the evolution of public opinion dynamics, we introduce a new dataset,\nVISTA. It includes 159 trending topics, corresponding to 47,207 posts, 327,015\nsecond-level comments, and 29,578 third-level comments, covering diverse\ndomains such as politics, entertainment, sports, health, and medicine. The\ndataset is annotated with detailed sentiment labels across 11 categories and\nclearly defined hierarchical relationships. When combined with our method, it\noffers strong interpretability by linking sentiment propagation to the comment\nhierarchy and temporal evolution. Our approach provides a robust baseline for\nfuture research."}
{"id": "2504.14098", "pdf": "https://arxiv.org/pdf/2504.14098", "abs": "https://arxiv.org/abs/2504.14098", "authors": ["Justus Råmunddal"], "title": "Enhancing Math Learning in an LMS Using AI-Driven Question Recommendations", "categories": ["cs.LG", "cs.AI", "cs.CY", "cs.IR"], "comment": "15 pages, 9 figures, 4 tables", "summary": "This paper presents an AI-driven approach to enhance math learning in a\nmodern Learning Management System (LMS) by recommending similar math questions.\nDeep embeddings for math questions are generated using Meta's\nLlama-3.2-11B-Vision-Instruct model, and three recommendation methods-cosine\nsimilarity, Self-Organizing Maps (SOM), and Gaussian Mixture Models (GMM)-are\napplied to identify similar questions. User interaction data, including session\ndurations, response times, and correctness, are used to evaluate the methods.\nOur findings suggest that while cosine similarity produces nearly identical\nquestion matches, SOM yields higher user satisfaction whereas GMM generally\nunderperforms, indicating that introducing variety to a certain degree may\nenhance engagement and thereby potential learning outcomes until variety is no\nlonger balanced reasonably, which our data about the implementations of all\nthree methods demonstrate."}
{"id": "2504.14977", "pdf": "https://arxiv.org/pdf/2504.14977", "abs": "https://arxiv.org/abs/2504.14977", "authors": ["Jingkai Zhou", "Yifan Wu", "Shikai Li", "Min Wei", "Chao Fan", "Weihua Chen", "Wei Jiang", "Fan Wang"], "title": "RealisDance-DiT: Simple yet Strong Baseline towards Controllable Character Animation in the Wild", "categories": ["cs.CV"], "comment": "Project Page:\n  https://thefoxofsky.github.io/project_pages_new/RealisDance-DiT/index", "summary": "Controllable character animation remains a challenging problem, particularly\nin handling rare poses, stylized characters, character-object interactions,\ncomplex illumination, and dynamic scenes. To tackle these issues, prior work\nhas largely focused on injecting pose and appearance guidance via elaborate\nbypass networks, but often struggles to generalize to open-world scenarios. In\nthis paper, we propose a new perspective that, as long as the foundation model\nis powerful enough, straightforward model modifications with flexible\nfine-tuning strategies can largely address the above challenges, taking a step\ntowards controllable character animation in the wild. Specifically, we\nintroduce RealisDance-DiT, built upon the Wan-2.1 video foundation model. Our\nsufficient analysis reveals that the widely adopted Reference Net design is\nsuboptimal for large-scale DiT models. Instead, we demonstrate that minimal\nmodifications to the foundation model architecture yield a surprisingly strong\nbaseline. We further propose the low-noise warmup and \"large batches and small\niterations\" strategies to accelerate model convergence during fine-tuning while\nmaximally preserving the priors of the foundation model. In addition, we\nintroduce a new test dataset that captures diverse real-world challenges,\ncomplementing existing benchmarks such as TikTok dataset and UBC fashion video\ndataset, to comprehensively evaluate the proposed method. Extensive experiments\nshow that RealisDance-DiT outperforms existing methods by a large margin."}
{"id": "2504.15135", "pdf": "https://arxiv.org/pdf/2504.15135", "abs": "https://arxiv.org/abs/2504.15135", "authors": ["Juyeon Kim", "Geon Lee", "Taeuk Kim", "Kijung Shin"], "title": "KGMEL: Knowledge Graph-Enhanced Multimodal Entity Linking", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "SIGIR 2025 (Short)", "summary": "Entity linking (EL) aligns textual mentions with their corresponding entities\nin a knowledge base, facilitating various applications such as semantic search\nand question answering. Recent advances in multimodal entity linking (MEL) have\nshown that combining text and images can reduce ambiguity and improve alignment\naccuracy. However, most existing MEL methods overlook the rich structural\ninformation available in the form of knowledge-graph (KG) triples. In this\npaper, we propose KGMEL, a novel framework that leverages KG triples to enhance\nMEL. Specifically, it operates in three stages: (1) Generation: Produces\nhigh-quality triples for each mention by employing vision-language models based\non its text and images. (2) Retrieval: Learns joint mention-entity\nrepresentations, via contrastive learning, that integrate text, images, and\n(generated or KG) triples to retrieve candidate entities for each mention. (3)\nReranking: Refines the KG triples of the candidate entities and employs large\nlanguage models to identify the best-matching entity for the mention. Extensive\nexperiments on benchmark datasets demonstrate that KGMEL outperforms existing\nmethods. Our code and datasets are available at:\nhttps://github.com/juyeonnn/KGMEL."}
{"id": "2504.14100", "pdf": "https://arxiv.org/pdf/2504.14100", "abs": "https://arxiv.org/abs/2504.14100", "authors": ["Ahmed Aboulfotouh", "Elsayed Mohammed", "Hatem Abou-Zeid"], "title": "6G WavesFM: A Foundation Model for Sensing, Communication, and Localization", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": null, "summary": "This paper introduces WavesFM, a novel Wireless Foundation Model (WFM)\nframework, capable of supporting a wide array of communication, sensing, and\nlocalization tasks. Our proposed architecture combines a shared Vision\nTransformer (ViT) backbone with task-specific multi-layer perceptron (MLP)\nheads and incorporates Low-Rank Adaptation (LoRA) for parameter-efficient\nfine-tuning. This design promotes full parameter sharing across tasks,\nsignificantly reducing the computational and memory footprint without\nsacrificing performance. The model processes both image-like wireless\nmodalities, such as spectrograms and channel state information (CSI), and\nin-phase and quadrature (IQ) signals arranged as orthogonal frequency-division\nmultiplexing (OFDM) resource grids. We demonstrate the strong generalization\ncapabilities of WavesFM through extensive experiments on four downstream tasks:\nFifth Generation New Radio (5G NR) positioning; multiple-input multiple-output\nOFDM (MIMO-OFDM) channel estimation; human activity sensing; and\nradio-frequency (RF) signal classification. Compared to supervised baselines\ntrained individually, our approach achieves superior performance while sharing\n80% of its parameters across tasks. Furthermore, we show that pretraining on\ndomain-relevant data not only boosts performance but also accelerates\nconvergence, reducing training time by up to 5x. These results demonstrate that\nour unified WFM can support diverse tasks and deliver significant gains in both\nperformance and efficiency, highlighting the transformative potential of\nfoundation models to drive AI-native paradigms in future sixth-generation (6G)\nnetworks."}
{"id": "2504.14988", "pdf": "https://arxiv.org/pdf/2504.14988", "abs": "https://arxiv.org/abs/2504.14988", "authors": ["Hong-Tao Yu", "Xiu-Shen Wei", "Yuxin Peng", "Serge Belongie"], "title": "Benchmarking Large Vision-Language Models on Fine-Grained Image Tasks: A Comprehensive Evaluation", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated\nremarkable multimodal perception capabilities, garnering significant attention.\nWhile numerous evaluation studies have emerged, assessing LVLMs both\nholistically and on specialized tasks, fine-grained image tasks-fundamental to\ncomputer vision-remain largely unexplored. To fill this gap, we introduce a\ncomprehensive fine-grained evaluation benchmark, i.e., FG-BMK, comprising 3.49\nmillion questions and 3.32 million images. Our evaluation systematically\nexamines LVLMs from both human-oriented and machine-oriented perspectives,\nfocusing on their semantic recognition and fine-grained feature representation\ncapabilities. Through extensive experiments on eight representative LVLMs/VLMs,\nwe uncover key findings regarding the influence of training paradigms, modality\nalignment, perturbation susceptibility, and fine-grained category reasoning on\ntask performance. This work provides critical insights into the limitations of\ncurrent LVLMs and offers guidance for future data construction and model design\nin the development of more advanced LVLMs. Our code is open-source and\navailable at https://github.com/SEU-VIPGroup/FG-BMK."}
{"id": "2504.15254", "pdf": "https://arxiv.org/pdf/2504.15254", "abs": "https://arxiv.org/abs/2504.15254", "authors": ["Anirudh Khatry", "Robert Zhang", "Jia Pan", "Ziteng Wang", "Qiaochu Chen", "Greg Durrett", "Isil Dillig"], "title": "CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation", "categories": ["cs.SE", "cs.CL"], "comment": null, "summary": "C-to-Rust transpilation is essential for modernizing legacy C code while\nenhancing safety and interoperability with modern Rust ecosystems. However, no\ndataset currently exists for evaluating whether a system can transpile C into\nsafe Rust that passes a set of test cases. We introduce CRUST-Bench, a dataset\nof 100 C repositories, each paired with manually-written interfaces in safe\nRust as well as test cases that can be used to validate correctness of the\ntranspilation. By considering entire repositories rather than isolated\nfunctions, CRUST-Bench captures the challenges of translating complex projects\nwith dependencies across multiple files. The provided Rust interfaces provide\nexplicit specifications that ensure adherence to idiomatic, memory-safe Rust\npatterns, while the accompanying test cases enforce functional correctness. We\nevaluate state-of-the-art large language models (LLMs) on this task and find\nthat safe and idiomatic Rust generation is still a challenging problem for\nvarious state-of-the-art methods and techniques. We also provide insights into\nthe errors LLMs usually make in transpiling code from C to safe Rust. The best\nperforming model, OpenAI o1, is able to solve only 15 tasks in a single-shot\nsetting. Improvements on CRUST-Bench would lead to improved transpilation\nsystems that can reason about complex scenarios and help in migrating legacy\ncodebases from C into languages like Rust that ensure memory safety. You can\nfind the dataset and code at https://github.com/anirudhkhatry/CRUST-bench."}
{"id": "2504.14103", "pdf": "https://arxiv.org/pdf/2504.14103", "abs": "https://arxiv.org/abs/2504.14103", "authors": ["Merve Atasever", "Ali Okhovat", "Azhang Nazaripouya", "John Nisbet", "Omer Kurkutlu", "Jyotirmoy V. Deshmukh", "Yasemin Ozkan Aydin"], "title": "Coordinating Spinal and Limb Dynamics for Enhanced Sprawling Robot Mobility", "categories": ["cs.RO", "cs.AI"], "comment": "The manuscript has been accepted for presentation at the Mechanical\n  Intelligence in Robotics workshop at ICRA 2025", "summary": "Among vertebrates, salamanders, with their unique ability to transition\nbetween walking and swimming gaits, highlight the role of spinal mobility in\nlocomotion. A flexible spine enables undulation of the body through a wavelike\nmotion along the spine, aiding navigation over uneven terrains and obstacles.\nYet environmental uncertainties, such as surface irregularities and variations\nin friction, can significantly disrupt body-limb coordination and cause\ndiscrepancies between predictions from mathematical models and real-world\noutcomes. Addressing this challenge requires the development of sophisticated\ncontrol strategies capable of dynamically adapting to uncertain conditions\nwhile maintaining efficient locomotion. Deep reinforcement learning (DRL)\noffers a promising framework for handling non-deterministic environments and\nenabling robotic systems to adapt effectively and perform robustly under\nchallenging conditions. In this study, we comparatively examine learning-based\ncontrol strategies and biologically inspired gait design methods on a\nsalamander-like robot."}
{"id": "2504.15003", "pdf": "https://arxiv.org/pdf/2504.15003", "abs": "https://arxiv.org/abs/2504.15003", "authors": ["Xin Li", "Xijun Wang", "Bingchen Li", "Kun Yuan", "Yizhen Shao", "Suhang Yao", "Ming Sun", "Chao Zhou", "Radu Timofte", "Zhibo Chen"], "title": "NTIRE 2025 Challenge on Short-form UGC Video Quality Assessment and Enhancement: KwaiSR Dataset and Study", "categories": ["cs.CV"], "comment": "KwaiSR dataset, a new dataset for image super-resolution, used for\n  CVPR NTIRE 2025 Challenge; CVPR 2025 workshop paper", "summary": "In this work, we build the first benchmark dataset for short-form UGC Image\nSuper-resolution in the wild, termed KwaiSR, intending to advance the research\non developing image super-resolution algorithms for short-form UGC platforms.\nThis dataset is collected from the Kwai Platform, which is composed of two\nparts, i.e., synthetic and wild parts. Among them, the synthetic dataset,\nincluding 1,900 image pairs, is produced by simulating the degradation\nfollowing the distribution of real-world low-quality short-form UGC images,\naiming to provide the ground truth for training and objective comparison in the\nvalidation/testing. The wild dataset contains low-quality images collected\ndirectly from the Kwai Platform, which are filtered using the quality\nassessment method KVQ from the Kwai Platform. As a result, the KwaiSR dataset\ncontains 1800 synthetic image pairs and 1900 wild images, which are divided\ninto training, validation, and testing parts with a ratio of 8:1:1. Based on\nthe KwaiSR dataset, we organize the NTIRE 2025 challenge on a second short-form\nUGC Video quality assessment and enhancement, which attracts lots of\nresearchers to develop the algorithm for it. The results of this competition\nhave revealed that our KwaiSR dataset is pretty challenging for existing Image\nSR methods, which is expected to lead to a new direction in the image\nsuper-resolution field. The dataset can be found from\nhttps://lixinustc.github.io/NTIRE2025-KVQE-KwaSR-KVQ.github.io/."}
{"id": "2504.15266", "pdf": "https://arxiv.org/pdf/2504.15266", "abs": "https://arxiv.org/abs/2504.15266", "authors": ["Vaishnavh Nagarajan", "Chen Henry Wu", "Charles Ding", "Aditi Raghunathan"], "title": "Roll the dice & look before you leap: Going beyond the creative limits of next-token prediction", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "37 pages", "summary": "We design a suite of minimal algorithmic tasks that are a loose abstraction\nof open-ended real-world tasks. This allows us to cleanly and controllably\nquantify the creative limits of the present-day language model. Much like\nreal-world tasks that require a creative, far-sighted leap of thought, our\ntasks require an implicit, open-ended stochastic planning step that either (a)\ndiscovers new connections in an abstract knowledge graph (like in wordplay,\ndrawing analogies, or research) or (b) constructs new patterns (like in\ndesigning math problems or new proteins). In these tasks, we empirically and\nconceptually argue how next-token learning is myopic and memorizes excessively;\ncomparatively, multi-token approaches, namely teacherless training and\ndiffusion models, excel in producing diverse and original output. Secondly, in\nour tasks, we find that to elicit randomness from the Transformer without\nhurting coherence, it is better to inject noise right at the input layer (via a\nmethod we dub hash-conditioning) rather than defer to temperature sampling from\nthe output layer. Thus, our work offers a principled, minimal test-bed for\nanalyzing open-ended creative skills, and offers new arguments for going beyond\nnext-token learning and softmax-based sampling. We make part of the code\navailable under https://github.com/chenwu98/algorithmic-creativity"}
{"id": "2504.14105", "pdf": "https://arxiv.org/pdf/2504.14105", "abs": "https://arxiv.org/abs/2504.14105", "authors": ["Qazi Mamunur Rashid", "Erin van Liemt", "Tiffany Shih", "Amber Ebinama", "Karla Barrios Ramos", "Madhurima Maji", "Aishwarya Verma", "Charu Kalia", "Jamila Smith-Loud", "Joyce Nakatumba-Nabende", "Rehema Baguma", "Andrew Katumba", "Chodrine Mutebi", "Jagen Marvin", "Eric Peter Wairagala", "Mugizi Bruce", "Peter Oketta", "Lawrence Nderu", "Obichi Obiajunwa", "Abigail Oppong", "Michael Zimba", "Data Authors"], "title": "Amplify Initiative: Building A Localized Data Platform for Globalized AI", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Current AI models often fail to account for local context and language, given\nthe predominance of English and Western internet content in their training\ndata. This hinders the global relevance, usefulness, and safety of these models\nas they gain more users around the globe. Amplify Initiative, a data platform\nand methodology, leverages expert communities to collect diverse, high-quality\ndata to address the limitations of these models. The platform is designed to\nenable co-creation of datasets, provide access to high-quality multilingual\ndatasets, and offer recognition to data authors. This paper presents the\napproach to co-creating datasets with domain experts (e.g., health workers,\nteachers) through a pilot conducted in Sub-Saharan Africa (Ghana, Kenya,\nMalawi, Nigeria, and Uganda). In partnership with local researchers situated in\nthese countries, the pilot demonstrated an end-to-end approach to co-creating\ndata with 155 experts in sensitive domains (e.g., physicians, bankers,\nanthropologists, human and civil rights advocates). This approach, implemented\nwith an Android app, resulted in an annotated dataset of 8,091 adversarial\nqueries in seven languages (e.g., Luganda, Swahili, Chichewa), capturing\nnuanced and contextual information related to key themes such as misinformation\nand public interest topics. This dataset in turn can be used to evaluate models\nfor their safety and cultural relevance within the context of these languages."}
{"id": "2504.15007", "pdf": "https://arxiv.org/pdf/2504.15007", "abs": "https://arxiv.org/abs/2504.15007", "authors": ["David C Wong", "Bin Wang", "Gorkem Durak", "Marouane Tliba", "Mohamed Amine Kerkouri", "Aladine Chetouani", "Ahmet Enis Cetin", "Cagdas Topel", "Nicolo Gennaro", "Camila Vendrami", "Tugce Agirlar Trabzonlu", "Amir Ali Rahsepar", "Laetitia Perronne", "Matthew Antalek", "Onural Ozturk", "Gokcan Okur", "Andrew C. Gordon", "Ayis Pyrros", "Frank H Miller", "Amir A Borhani", "Hatice Savas", "Eric M. Hart"], "title": "Shifts in Doctors' Eye Movements Between Real and AI-Generated Medical Images", "categories": ["cs.CV", "cs.HC"], "comment": "This paper was accepted at ETRA 2025 Japan", "summary": "Eye-tracking analysis plays a vital role in medical imaging, providing key\ninsights into how radiologists visually interpret and diagnose clinical cases.\nIn this work, we first analyze radiologists' attention and agreement by\nmeasuring the distribution of various eye-movement patterns, including saccades\ndirection, amplitude, and their joint distribution. These metrics help uncover\npatterns in attention allocation and diagnostic strategies. Furthermore, we\ninvestigate whether and how doctors' gaze behavior shifts when viewing\nauthentic (Real) versus deep-learning-generated (Fake) images. To achieve this,\nwe examine fixation bias maps, focusing on first, last, short, and longest\nfixations independently, along with detailed saccades patterns, to quantify\ndifferences in gaze distribution and visual saliency between authentic and\nsynthetic images."}
{"id": "2504.15270", "pdf": "https://arxiv.org/pdf/2504.15270", "abs": "https://arxiv.org/abs/2504.15270", "authors": ["Ji Qi", "Yuan Yao", "Yushi Bai", "Bin Xu", "Juanzi Li", "Zhiyuan Liu", "Tat-Seng Chua"], "title": "An LMM for Efficient Video Understanding via Reinforced Compression of Video Cubes", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Large Multimodal Models (LMMs) uniformly perceive video frames, creating\ncomputational inefficiency for videos with inherently varying temporal\ninformation density. This paper present \\textbf{Quicksviewer}, an LMM with new\nperceiving paradigm that partitions a video of nonuniform density into varying\ncubes using Gumbel Softmax, followed by a unified resampling for each cube to\nachieve efficient video understanding. This simple and intuitive approach\ndynamically compress video online based on its temporal density, significantly\nreducing spatiotemporal redundancy (overall 45$\\times$ compression rate), while\nenabling efficient training with large receptive field. We train the model from\na language backbone through three progressive stages, each incorporating\nlengthy videos on average of 420s/1fps thanks to the perceiving efficiency.\nWith only 0.8M total video-text samples for training, our model outperforms the\ndirect baseline employing a fixed partitioning strategy by a maximum of 8.72 in\naccuracy, demonstrating the effectiveness in performance. On Video-MME,\nQuicksviewer achieves SOTA under modest sequence lengths using just up to 5\\%\nof tokens per frame required by baselines. With this paradigm, scaling up the\nnumber of input frames reveals a clear power law of the model capabilities. It\nis also empirically verified that the segments generated by the cubing network\ncan help for analyzing continuous events in videos."}
{"id": "2504.14110", "pdf": "https://arxiv.org/pdf/2504.14110", "abs": "https://arxiv.org/abs/2504.14110", "authors": ["Theo Jaffrelot Inizan", "Sherry Yang", "Aaron Kaplan", "Yen-hsu Lin", "Jian Yin", "Saber Mirzaei", "Mona Abdelgaid", "Ali H. Alawadhi", "KwangHwan Cho", "Zhiling Zheng", "Ekin Dogus Cubuk", "Christian Borgs", "Jennifer T. Chayes", "Kristin A. Persson", "Omar M. Yaghi"], "title": "System of Agentic AI for the Discovery of Metal-Organic Frameworks", "categories": ["cond-mat.mtrl-sci", "cs.AI", "cs.CL", "cs.MA"], "comment": null, "summary": "Generative models and machine learning promise accelerated material discovery\nin MOFs for CO2 capture and water harvesting but face significant challenges\nnavigating vast chemical spaces while ensuring synthetizability. Here, we\npresent MOFGen, a system of Agentic AI comprising interconnected agents: a\nlarge language model that proposes novel MOF compositions, a diffusion model\nthat generates crystal structures, quantum mechanical agents that optimize and\nfilter candidates, and synthetic-feasibility agents guided by expert rules and\nmachine learning. Trained on all experimentally reported MOFs and computational\ndatabases, MOFGen generated hundreds of thousands of novel MOF structures and\nsynthesizable organic linkers. Our methodology was validated through\nhigh-throughput experiments and the successful synthesis of five \"AI-dreamt\"\nMOFs, representing a major step toward automated synthesizable material\ndiscovery."}
{"id": "2504.15009", "pdf": "https://arxiv.org/pdf/2504.15009", "abs": "https://arxiv.org/abs/2504.15009", "authors": ["Wensong Song", "Hong Jiang", "Zongxing Yang", "Ruijie Quan", "Yi Yang"], "title": "Insert Anything: Image Insertion via In-Context Editing in DiT", "categories": ["cs.CV"], "comment": null, "summary": "This work presents Insert Anything, a unified framework for reference-based\nimage insertion that seamlessly integrates objects from reference images into\ntarget scenes under flexible, user-specified control guidance. Instead of\ntraining separate models for individual tasks, our approach is trained once on\nour new AnyInsertion dataset--comprising 120K prompt-image pairs covering\ndiverse tasks such as person, object, and garment insertion--and effortlessly\ngeneralizes to a wide range of insertion scenarios. Such a challenging setting\nrequires capturing both identity features and fine-grained details, while\nallowing versatile local adaptations in style, color, and texture. To this end,\nwe propose to leverage the multimodal attention of the Diffusion Transformer\n(DiT) to support both mask- and text-guided editing. Furthermore, we introduce\nan in-context editing mechanism that treats the reference image as contextual\ninformation, employing two prompting strategies to harmonize the inserted\nelements with the target scene while faithfully preserving their distinctive\nfeatures. Extensive experiments on AnyInsertion, DreamBooth, and VTON-HD\nbenchmarks demonstrate that our method consistently outperforms existing\nalternatives, underscoring its great potential in real-world applications such\nas creative content generation, virtual try-on, and scene composition."}
{"id": "2504.15280", "pdf": "https://arxiv.org/pdf/2504.15280", "abs": "https://arxiv.org/abs/2504.15280", "authors": ["Chun-Hsiao Yeh", "Chenyu Wang", "Shengbang Tong", "Ta-Ying Cheng", "Rouyu Wang", "Tianzhe Chu", "Yuexiang Zhai", "Yubei Chen", "Shenghua Gao", "Yi Ma"], "title": "Seeing from Another Perspective: Evaluating Multi-View Understanding in MLLMs", "categories": ["cs.CV", "cs.CL"], "comment": "Project page: https://danielchyeh.github.io/All-Angles-Bench/", "summary": "Multi-view understanding, the ability to reconcile visual information across\ndiverse viewpoints for effective navigation, manipulation, and 3D scene\ncomprehension, is a fundamental challenge in Multi-Modal Large Language Models\n(MLLMs) to be used as embodied agents. While recent MLLMs have shown impressive\nadvances in high-level reasoning and planning, they frequently fall short when\nconfronted with multi-view geometric consistency and cross-view correspondence.\nTo comprehensively evaluate the challenges of MLLMs in multi-view scene\nreasoning, we propose All-Angles Bench, a benchmark of over 2,100 human\ncarefully annotated multi-view question-answer pairs across 90 diverse\nreal-world scenes. Our six tasks (counting, attribute identification, relative\ndistance, relative direction, object manipulation, and camera pose estimation)\nspecifically test model's geometric correspondence and the capacity to align\ninformation consistently across views. Our extensive experiments, benchmark on\n27 representative MLLMs including Gemini-2.0-Flash, Claude-3.7-Sonnet, and\nGPT-4o against human evaluators reveals a substantial performance gap,\nindicating that current MLLMs remain far from human-level proficiency. Through\nin-depth analysis, we show that MLLMs are particularly underperforming under\ntwo aspects: (1) cross-view correspondence for partially occluded views and (2)\nestablishing the coarse camera poses. These findings highlight the necessity of\ndomain-specific refinements or modules that embed stronger multi-view\nawareness. We believe that our All-Angles Bench offers valuable insights and\ncontribute to bridging the gap between MLLMs and human-level multi-view\nunderstanding. The project and benchmark are publicly available at\nhttps://danielchyeh.github.io/All-Angles-Bench/."}
{"id": "2504.14112", "pdf": "https://arxiv.org/pdf/2504.14112", "abs": "https://arxiv.org/abs/2504.14112", "authors": ["Mohit Chandra", "Javier Hernandez", "Gonzalo Ramos", "Mahsa Ershadi", "Ananya Bhattacharjee", "Judith Amores", "Ebele Okoli", "Ann Paradiso", "Shahed Warreth", "Jina Suh"], "title": "Longitudinal Study on Social and Emotional Use of AI Conversational Agent", "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": "16 pages, 5 figures, 2 tables", "summary": "Development in digital technologies has continuously reshaped how individuals\nseek and receive social and emotional support. While online platforms and\ncommunities have long served this need, the increased integration of\ngeneral-purpose conversational AI into daily lives has introduced new dynamics\nin how support is provided and experienced. Existing research has highlighted\nboth benefits (e.g., wider access to well-being resources) and potential risks\n(e.g., over-reliance) of using AI for support seeking. In this five-week,\nexploratory study, we recruited 149 participants divided into two usage groups:\na baseline usage group (BU, n=60) that used the internet and AI as usual, and\nan active usage group (AU, n=89) encouraged to use one of four commercially\navailable AI tools (Microsoft Copilot, Google Gemini, PI AI, ChatGPT) for\nsocial and emotional interactions. Our analysis revealed significant increases\nin perceived attachment towards AI (32.99 percentage points), perceived AI\nempathy (25.8 p.p.), and motivation to use AI for entertainment (22.90 p.p.)\namong the AU group. We also observed that individual differences (e.g., gender\nidentity, prior AI usage) influenced perceptions of AI empathy and attachment.\nLastly, the AU group expressed higher comfort in seeking personal help,\nmanaging stress, obtaining social support, and talking about health with AI,\nindicating potential for broader emotional support while highlighting the need\nfor safeguards against problematic usage. Overall, our exploratory findings\nunderscore the importance of developing consumer-facing AI tools that support\nemotional well-being responsibly, while empowering users to understand the\nlimitations of these tools."}
{"id": "2504.15026", "pdf": "https://arxiv.org/pdf/2504.15026", "abs": "https://arxiv.org/abs/2504.15026", "authors": ["Zijin Yang", "Xin Zhang", "Kejiang Chen", "Kai Zeng", "Qiyi Yao", "Han Fang", "Weiming Zhang", "Nenghai Yu"], "title": "Gaussian Shading++: Rethinking the Realistic Deployment Challenge of Performance-Lossless Image Watermark for Diffusion Models", "categories": ["cs.CV", "cs.CR"], "comment": "18 pages, 8 figures", "summary": "Ethical concerns surrounding copyright protection and inappropriate content\ngeneration pose challenges for the practical implementation of diffusion\nmodels. One effective solution involves watermarking the generated images.\nExisting methods primarily focus on ensuring that watermark embedding does not\ndegrade the model performance. However, they often overlook critical challenges\nin real-world deployment scenarios, such as the complexity of watermark key\nmanagement, user-defined generation parameters, and the difficulty of\nverification by arbitrary third parties. To address this issue, we propose\nGaussian Shading++, a diffusion model watermarking method tailored for\nreal-world deployment. We propose a double-channel design that leverages\npseudorandom error-correcting codes to encode the random seed required for\nwatermark pseudorandomization, achieving performance-lossless watermarking\nunder a fixed watermark key and overcoming key management challenges.\nAdditionally, we model the distortions introduced during generation and\ninversion as an additive white Gaussian noise channel and employ a novel soft\ndecision decoding strategy during extraction, ensuring strong robustness even\nwhen generation parameters vary. To enable third-party verification, we\nincorporate public key signatures, which provide a certain level of resistance\nagainst forgery attacks even when model inversion capabilities are fully\ndisclosed. Extensive experiments demonstrate that Gaussian Shading++ not only\nmaintains performance losslessness but also outperforms existing methods in\nterms of robustness, making it a more practical solution for real-world\ndeployment."}
{"id": "2504.14125", "pdf": "https://arxiv.org/pdf/2504.14125", "abs": "https://arxiv.org/abs/2504.14125", "authors": ["Maria-Teresa De Rosa Palmini", "Eva Cetinic"], "title": "Exploring Language Patterns of Prompts in Text-to-Image Generation and Their Impact on Visual Diversity", "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": null, "summary": "Following the initial excitement, Text-to-Image (TTI) models are now being\nexamined more critically. While much of the discourse has focused on biases and\nstereotypes embedded in large-scale training datasets, the sociotechnical\ndynamics of user interactions with these models remain underexplored. This\nstudy examines the linguistic and semantic choices users make when crafting\nprompts and how these choices influence the diversity of generated outputs.\nAnalyzing over six million prompts from the Civiverse dataset on the CivitAI\nplatform across seven months, we categorize users into three groups based on\ntheir levels of linguistic experimentation: consistent repeaters, occasional\nrepeaters, and non-repeaters. Our findings reveal that as user participation\ngrows over time, prompt language becomes increasingly homogenized through the\nadoption of popular community tags and descriptors, with repeated prompts\ncomprising 40-50% of submissions. At the same time, semantic similarity and\ntopic preferences remain relatively stable, emphasizing common subjects and\nsurface aesthetics. Using Vendi scores to quantify visual diversity, we\ndemonstrate a clear correlation between lexical similarity in prompts and the\nvisual similarity of generated images, showing that linguistic repetition\nreinforces less diverse representations. These findings highlight the\nsignificant role of user-driven factors in shaping AI-generated imagery, beyond\ninherent model biases, and underscore the need for tools and practices that\nencourage greater linguistic and thematic experimentation within TTI systems to\nfoster more inclusive and diverse AI-generated content."}
{"id": "2504.15032", "pdf": "https://arxiv.org/pdf/2504.15032", "abs": "https://arxiv.org/abs/2504.15032", "authors": ["Weijie He", "Mushui Liu", "Yunlong Yu", "Zhao Wang", "Chao Wu"], "title": "DyST-XL: Dynamic Layout Planning and Content Control for Compositional Text-to-Video Generation", "categories": ["cs.CV"], "comment": "9 pages, 6 figures", "summary": "Compositional text-to-video generation, which requires synthesizing dynamic\nscenes with multiple interacting entities and precise spatial-temporal\nrelationships, remains a critical challenge for diffusion-based models.\nExisting methods struggle with layout discontinuity, entity identity drift, and\nimplausible interaction dynamics due to unconstrained cross-attention\nmechanisms and inadequate physics-aware reasoning. To address these\nlimitations, we propose DyST-XL, a \\textbf{training-free} framework that\nenhances off-the-shelf text-to-video models (e.g., CogVideoX-5B) through\nframe-aware control. DyST-XL integrates three key innovations: (1) A Dynamic\nLayout Planner that leverages large language models (LLMs) to parse input\nprompts into entity-attribute graphs and generates physics-aware keyframe\nlayouts, with intermediate frames interpolated via trajectory optimization; (2)\nA Dual-Prompt Controlled Attention Mechanism that enforces localized text-video\nalignment through frame-aware attention masking, achieving the precise control\nover individual entities; and (3) An Entity-Consistency Constraint strategy\nthat propagates first-frame feature embeddings to subsequent frames during\ndenoising, preserving object identity without manual annotation. Experiments\ndemonstrate that DyST-XL excels in compositional text-to-video generation,\nsignificantly improving performance on complex prompts and bridging a crucial\ngap in training-free video synthesis."}
{"id": "2504.14130", "pdf": "https://arxiv.org/pdf/2504.14130", "abs": "https://arxiv.org/abs/2504.14130", "authors": ["Qiang Li", "Xinze Lin", "Shenghao Lv", "Faliang Huang", "Xiangju Li"], "title": "Personalized News Recommendation with Multi-granularity Candidate-aware User Modeling", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Matching candidate news with user interests is crucial for personalized news\nrecommendations. Most existing methods can represent a user's reading interests\nthrough a single profile based on clicked news, which may not fully capture the\ndiversity of user interests. Although some approaches incorporate candidate\nnews or topic information, they remain insufficient because they neglect the\nmulti-granularity relatedness between candidate news and user interests. To\naddress this, this study proposed a multi-granularity candidate-aware user\nmodeling framework that integrated user interest features across various levels\nof granularity. It consisted of two main components: candidate news encoding\nand user modeling. A news textual information extractor and a\nknowledge-enhanced entity information extractor can capture candidate news\nfeatures, and word-level, entity-level, and news-level candidate-aware\nmechanisms can provide a comprehensive representation of user interests.\nExtensive experiments on a real-world dataset demonstrated that the proposed\nmodel could significantly outperform baseline models."}
{"id": "2504.15041", "pdf": "https://arxiv.org/pdf/2504.15041", "abs": "https://arxiv.org/abs/2504.15041", "authors": ["Shiben Liu", "Huijie Fan", "Qiang Wang", "Baojie Fan", "Yandong Tang", "Liangqiong Qu"], "title": "Distribution-aware Forgetting Compensation for Exemplar-Free Lifelong Person Re-identification", "categories": ["cs.CV", "cs.AI"], "comment": "12 pages, 5 figures", "summary": "Lifelong Person Re-identification (LReID) suffers from a key challenge in\npreserving old knowledge while adapting to new information. The existing\nsolutions include rehearsal-based and rehearsal-free methods to address this\nchallenge. Rehearsal-based approaches rely on knowledge distillation,\ncontinuously accumulating forgetting during the distillation process.\nRehearsal-free methods insufficiently learn the distribution of each domain,\nleading to forgetfulness over time. To solve these issues, we propose a novel\nDistribution-aware Forgetting Compensation (DAFC) model that explores\ncross-domain shared representation learning and domain-specific distribution\nintegration without using old exemplars or knowledge distillation. We propose a\nText-driven Prompt Aggregation (TPA) that utilizes text features to enrich\nprompt elements and guide the prompt model to learn fine-grained\nrepresentations for each instance. This can enhance the differentiation of\nidentity information and establish the foundation for domain distribution\nawareness. Then, Distribution-based Awareness and Integration (DAI) is designed\nto capture each domain-specific distribution by a dedicated expert network and\nadaptively consolidate them into a shared region in high-dimensional space. In\nthis manner, DAI can consolidate and enhance cross-domain shared representation\nlearning while alleviating catastrophic forgetting. Furthermore, we develop a\nKnowledge Consolidation Mechanism (KCM) that comprises instance-level\ndiscrimination and cross-domain consistency alignment strategies to facilitate\nmodel adaptive learning of new knowledge from the current domain and promote\nknowledge consolidation learning between acquired domain-specific\ndistributions, respectively. Experimental results show that our DAFC\noutperforms state-of-the-art methods. Our code is available at\nhttps://github.com/LiuShiBen/DAFC."}
{"id": "2504.14139", "pdf": "https://arxiv.org/pdf/2504.14139", "abs": "https://arxiv.org/abs/2504.14139", "authors": ["Hai Pham-Ngoc", "De Nguyen-Van", "Dung Vu-Tien", "Phuong Le-Hong"], "title": "ThyroidEffi 1.0: A Cost-Effective System for High-Performance Multi-Class Thyroid Carcinoma Classification", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Background: Automated classification of thyroid fine needle aspiration biopsy\n(FNAB) images faces challenges in limited data, inter-observer variability, and\ncomputational cost. Efficient, interpretable models are crucial for clinical\nsupport. Objective: To develop and externally validate a deep learning system\nfor the multi-class classification of thyroid FNAB images into three key\ncategories that directly guide post-biopsy treatment decisions in Vietnam:\nbenign (B2), suspicious for malignancy (B5), and malignant (B6), while\nachieving high diagnostic accuracy with low computational overhead. Methods:\nOur framework features: (1) YOLOv10-based cell cluster detection for\ninformative sub-region extraction and noise reduction; (2) a curriculum\nlearning-inspired protocol sequencing localized crops to full images for\nmulti-scale feature capture; (3) adaptive lightweight EfficientNetB0 (4\nmillions parameters) selection balancing performance and efficiency; and (4) a\nTransformer-inspired module for multi-scale, multi-region analysis. External\nvalidation used 1,015 independent FNAB images. Results: ThyroidEffi Basic\nachieved a macro F1 of 89.19\\% and AUCs of 0.98 (B2), 0.95 (B5), and 0.96 (B6)\non the internal test set. External validation yielded AUCs of 0.9495 (B2),\n0.7436 (B5), and 0.8396 (B6). ThyroidEffi Premium improved macro F1 to 89.77\\%.\nGrad-CAM highlighted key diagnostic regions, confirming interpretability. The\nsystem processed 1000 cases in 30 seconds, demonstrating feasibility on widely\naccessible hardware like a 12-core CPU. Conclusions: This work demonstrates\nthat high-accuracy, interpretable thyroid FNAB image classification is\nachievable with minimal computational demands."}
{"id": "2504.15049", "pdf": "https://arxiv.org/pdf/2504.15049", "abs": "https://arxiv.org/abs/2504.15049", "authors": ["Mohamed el amine Boudjoghra", "Ivan Laptev", "Angela Dai"], "title": "ScanEdit: Hierarchically-Guided Functional 3D Scan Editing", "categories": ["cs.CV"], "comment": "Project webpage: https://aminebdj.github.io/scanedit/ Video:\n  https://www.youtube.com/watch?v=Dfmu2g6pVlg", "summary": "With the fast pace of 3D capture technology and resulting abundance of 3D\ndata, effective 3D scene editing becomes essential for a variety of graphics\napplications. In this work we present ScanEdit, an instruction-driven method\nfor functional editing of complex, real-world 3D scans. To model large and\ninterdependent sets of ob- jectswe propose a hierarchically-guided approach.\nGiven a 3D scan decomposed into its object instances, we first construct a\nhierarchical scene graph representation to enable effective, tractable editing.\nWe then leverage reason- ing capabilities of Large Language Models (LLMs) and\ntranslate high-level language instructions into actionable commands applied\nhierarchically to the scene graph. Fi- nally, ScanEdit integrates LLM-based\nguidance with ex- plicit physical constraints and generates realistic scenes\nwhere object arrangements obey both physics and common sense. In our extensive\nexperimental evaluation ScanEdit outperforms state of the art and demonstrates\nexcellent re- sults for a variety of real-world scenes and input instruc-\ntions."}
{"id": "2504.14145", "pdf": "https://arxiv.org/pdf/2504.14145", "abs": "https://arxiv.org/abs/2504.14145", "authors": ["Zhenliang Xue", "Hanpeng Hu", "Xing Chen", "Yimin Jiang", "Yixin Song", "Zeyu Mi", "Yibo Zhu", "Daxin Jiang", "Yubin Xia", "Haibo Chen"], "title": "PipeWeaver: Addressing Data Dynamicity in Large Multimodal Model Training with Dynamic Interleaved Pipeline", "categories": ["cs.DC", "cs.AI"], "comment": null, "summary": "Large multimodal models (LMMs) have demonstrated excellent capabilities in\nboth understanding and generation tasks with various modalities. While these\nmodels can accept flexible combinations of input data, their training\nefficiency suffers from two major issues: pipeline stage imbalance caused by\nheterogeneous model architectures, and training data dynamicity stemming from\nthe diversity of multimodal data.\n  In this paper, we present PipeWeaver, a dynamic pipeline scheduling framework\ndesigned for LMM training. The core of PipeWeaver is dynamic interleaved\npipeline, which searches for pipeline schedules dynamically tailored to current\ntraining batches. PipeWeaver addresses issues of LMM training with two\ntechniques: adaptive modality-aware partitioning and efficient pipeline\nschedule search within a hierarchical schedule space. Meanwhile, PipeWeaver\nutilizes SEMU (Step Emulator), a training simulator for multimodal models, for\naccurate performance estimations, accelerated by spatial-temporal subgraph\nreuse to improve search efficiency. Experiments show that PipeWeaver can\nenhance LMM training efficiency by up to 97.3% compared to state-of-the-art\nsystems, and demonstrate excellent adaptivity to LMM training's data\ndynamicity."}
{"id": "2504.15054", "pdf": "https://arxiv.org/pdf/2504.15054", "abs": "https://arxiv.org/abs/2504.15054", "authors": ["Xiangchen Yin", "Zhenda Yu", "Longtao Jiang", "Xin Gao", "Xiao Sun", "Zhi Liu", "Xun Yang"], "title": "Structure-guided Diffusion Transformer for Low-Light Image Enhancement", "categories": ["cs.CV"], "comment": "Accepted by IEEE Transactions on Multimedia (TMM)", "summary": "While the diffusion transformer (DiT) has become a focal point of interest in\nrecent years, its application in low-light image enhancement remains a blank\narea for exploration. Current methods recover the details from low-light images\nwhile inevitably amplifying the noise in images, resulting in poor visual\nquality. In this paper, we firstly introduce DiT into the low-light enhancement\ntask and design a novel Structure-guided Diffusion Transformer based Low-light\nimage enhancement (SDTL) framework. We compress the feature through wavelet\ntransform to improve the inference efficiency of the model and capture the\nmulti-directional frequency band. Then we propose a Structure Enhancement\nModule (SEM) that uses structural prior to enhance the texture and leverages an\nadaptive fusion strategy to achieve more accurate enhancement effect. In\nAddition, we propose a Structure-guided Attention Block (SAB) to pay more\nattention to texture-riched tokens and avoid interference from noisy areas in\nnoise prediction. Extensive qualitative and quantitative experiments\ndemonstrate that our method achieves SOTA performance on several popular\ndatasets, validating the effectiveness of SDTL in improving image quality and\nthe potential of DiT in low-light enhancement tasks."}
{"id": "2504.14147", "pdf": "https://arxiv.org/pdf/2504.14147", "abs": "https://arxiv.org/abs/2504.14147", "authors": ["Jiakai Tang", "Jingsen Zhang", "Zihang Tian", "Xueyang Feng", "Lei Wang", "Xu Chen"], "title": "HF4Rec: Human-Like Feedback-Driven Optimization Framework for Explainable Recommendation", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent advancements in explainable recommendation have greatly bolstered user\nexperience by elucidating the decision-making rationale. However, the existing\nmethods actually fail to provide effective feedback signals for potentially\nbetter or worse generated explanations due to their reliance on traditional\nsupervised learning paradigms in sparse interaction data. To address these\nissues, we propose a novel human-like feedback-driven optimization framework.\nThis framework employs a dynamic interactive optimization mechanism for\nachieving human-centered explainable requirements without incurring high labor\ncosts. Specifically, we propose to utilize large language models (LLMs) as\nhuman simulators to predict human-like feedback for guiding the learning\nprocess. To enable the LLMs to deeply understand the task essence and meet\nuser's diverse personalized requirements, we introduce a human-induced\ncustomized reward scoring method, which helps stimulate the language\nunderstanding and logical reasoning capabilities of LLMs. Furthermore,\nconsidering the potential conflicts between different perspectives of\nexplanation quality, we introduce a principled Pareto optimization that\ntransforms the multi-perspective quality enhancement task into a\nmulti-objective optimization problem for improving explanation performance. At\nlast, to achieve efficient model training, we design an off-policy optimization\npipeline. By incorporating a replay buffer and addressing the data distribution\nbiases, we can effectively improve data utilization and enhance model\ngenerality. Extensive experiments on four datasets demonstrate the superiority\nof our approach."}
{"id": "2504.15085", "pdf": "https://arxiv.org/pdf/2504.15085", "abs": "https://arxiv.org/abs/2504.15085", "authors": ["Wangyu Wu", "Zhenhong Chen", "Siqi Song", "Xianglin Qiua", "Xiaowei Huang", "Fei Ma", "Jimin Xiao"], "title": "Hierarchical Attention Fusion of Visual and Textual Representations for Cross-Domain Sequential Recommendation", "categories": ["cs.CV"], "comment": "Accepted at CogSCI 2025", "summary": "Cross-Domain Sequential Recommendation (CDSR) predicts user behavior by\nleveraging historical interactions across multiple domains, focusing on\nmodeling cross-domain preferences through intra- and inter-sequence item\nrelationships. Inspired by human cognitive processes, we propose Hierarchical\nAttention Fusion of Visual and Textual Representations (HAF-VT), a novel\napproach integrating visual and textual data to enhance cognitive modeling.\nUsing the frozen CLIP model, we generate image and text embeddings, enriching\nitem representations with multimodal data. A hierarchical attention mechanism\njointly learns single-domain and cross-domain preferences, mimicking human\ninformation integration. Evaluated on four e-commerce datasets, HAF-VT\noutperforms existing methods in capturing cross-domain user interests, bridging\ncognitive principles with computational models and highlighting the role of\nmultimodal data in sequential decision-making."}
{"id": "2504.14150", "pdf": "https://arxiv.org/pdf/2504.14150", "abs": "https://arxiv.org/abs/2504.14150", "authors": ["Katie Matton", "Robert Osazuwa Ness", "John Guttag", "Emre Kıcıman"], "title": "Walk the Talk? Measuring the Faithfulness of Large Language Model Explanations", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "comment": "61 pages, 14 figures, 36 tables", "summary": "Large language models (LLMs) are capable of generating plausible explanations\nof how they arrived at an answer to a question. However, these explanations can\nmisrepresent the model's \"reasoning\" process, i.e., they can be unfaithful.\nThis, in turn, can lead to over-trust and misuse. We introduce a new approach\nfor measuring the faithfulness of LLM explanations. First, we provide a\nrigorous definition of faithfulness. Since LLM explanations mimic human\nexplanations, they often reference high-level concepts in the input question\nthat purportedly influenced the model. We define faithfulness in terms of the\ndifference between the set of concepts that LLM explanations imply are\ninfluential and the set that truly are. Second, we present a novel method for\nestimating faithfulness that is based on: (1) using an auxiliary LLM to modify\nthe values of concepts within model inputs to create realistic counterfactuals,\nand (2) using a Bayesian hierarchical model to quantify the causal effects of\nconcepts at both the example- and dataset-level. Our experiments show that our\nmethod can be used to quantify and discover interpretable patterns of\nunfaithfulness. On a social bias task, we uncover cases where LLM explanations\nhide the influence of social bias. On a medical question answering task, we\nuncover cases where LLM explanations provide misleading claims about which\npieces of evidence influenced the model's decisions."}
{"id": "2504.15095", "pdf": "https://arxiv.org/pdf/2504.15095", "abs": "https://arxiv.org/abs/2504.15095", "authors": ["Mingxia Zhan", "Li Zhang", "Xiaomeng Chu", "Beibei Wang"], "title": "VistaDepth: Frequency Modulation With Bias Reweighting For Enhanced Long-Range Depth Estimation", "categories": ["cs.CV"], "comment": "8 pages, 6 figures, 4 tables", "summary": "Monocular depth estimation (MDE) aims to predict per-pixel depth values from\na single RGB image. Recent advancements have positioned diffusion models as\neffective MDE tools by framing the challenge as a conditional image generation\ntask. Despite their progress, these methods often struggle with accurately\nreconstructing distant depths, due largely to the imbalanced distribution of\ndepth values and an over-reliance on spatial-domain features. To overcome these\nlimitations, we introduce VistaDepth, a novel framework that integrates\nadaptive frequency-domain feature enhancements with an adaptive\nweight-balancing mechanism into the diffusion process. Central to our approach\nis the Latent Frequency Modulation (LFM) module, which dynamically refines\nspectral responses in the latent feature space, thereby improving the\npreservation of structural details and reducing noisy artifacts. Furthermore,\nwe implement an adaptive weighting strategy that modulates the diffusion loss\nin real-time, enhancing the model's sensitivity towards distant depth\nreconstruction. These innovations collectively result in superior depth\nperception performance across both distance and detail. Experimental\nevaluations confirm that VistaDepth achieves state-of-the-art performance among\ndiffusion-based MDE techniques, particularly excelling in the accurate\nreconstruction of distant regions."}
{"id": "2504.14151", "pdf": "https://arxiv.org/pdf/2504.14151", "abs": "https://arxiv.org/abs/2504.14151", "authors": ["Sergio Arnaud", "Paul McVay", "Ada Martin", "Arjun Majumdar", "Krishna Murthy Jatavallabhula", "Phillip Thomas", "Ruslan Partsey", "Daniel Dugas", "Abha Gejji", "Alexander Sax", "Vincent-Pierre Berges", "Mikael Henaff", "Ayush Jain", "Ang Cao", "Ishita Prasad", "Mrinal Kalakrishnan", "Michael Rabbat", "Nicolas Ballas", "Mido Assran", "Oleksandr Maksymets", "Aravind Rajeswaran", "Franziska Meier"], "title": "Locate 3D: Real-World Object Localization via Self-Supervised Learning in 3D", "categories": ["cs.CV", "cs.AI", "cs.RO", "I.2.10; I.2.6; I.2.9; I.3.7; I.4.6; I.4.8"], "comment": null, "summary": "We present LOCATE 3D, a model for localizing objects in 3D scenes from\nreferring expressions like \"the small coffee table between the sofa and the\nlamp.\" LOCATE 3D sets a new state-of-the-art on standard referential grounding\nbenchmarks and showcases robust generalization capabilities. Notably, LOCATE 3D\noperates directly on sensor observation streams (posed RGB-D frames), enabling\nreal-world deployment on robots and AR devices. Key to our approach is 3D-JEPA,\na novel self-supervised learning (SSL) algorithm applicable to sensor point\nclouds. It takes as input a 3D pointcloud featurized using 2D foundation models\n(CLIP, DINO). Subsequently, masked prediction in latent space is employed as a\npretext task to aid the self-supervised learning of contextualized pointcloud\nfeatures. Once trained, the 3D-JEPA encoder is finetuned alongside a\nlanguage-conditioned decoder to jointly predict 3D masks and bounding boxes.\nAdditionally, we introduce LOCATE 3D DATASET, a new dataset for 3D referential\ngrounding, spanning multiple capture setups with over 130K annotations. This\nenables a systematic study of generalization capabilities as well as a stronger\nmodel."}
{"id": "2504.15105", "pdf": "https://arxiv.org/pdf/2504.15105", "abs": "https://arxiv.org/abs/2504.15105", "authors": ["Yurun Wang", "Zerong Qi", "Shujun Fu", "Mingzheng Hu"], "title": "A triple-branch network for latent fingerprint enhancement guided by orientation fields and minutiae", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Latent fingerprint enhancement is a critical step in the process of latent\nfingerprint identification. Existing deep learning-based enhancement methods\nstill fall short of practical application requirements, particularly in\nrestoring low-quality fingerprint regions. Recognizing that different regions\nof latent fingerprints require distinct enhancement strategies, we propose a\nTriple Branch Spatial Fusion Network (TBSFNet), which simultaneously enhances\ndifferent regions of the image using tailored strategies. Furthermore, to\nimprove the generalization capability of the network, we integrate orientation\nfield and minutiae-related modules into TBSFNet and introduce a Multi-Level\nFeature Guidance Network (MLFGNet). Experimental results on the MOLF and MUST\ndatasets demonstrate that MLFGNet outperforms existing enhancement algorithms."}
{"id": "2504.14154", "pdf": "https://arxiv.org/pdf/2504.14154", "abs": "https://arxiv.org/abs/2504.14154", "authors": ["Zhiyuan Wang", "Qingni Wang", "Yue Zhang", "Tianlong Chen", "Xiaofeng Zhu", "Xiaoshuang Shi", "Kaidi Xu"], "title": "SConU: Selective Conformal Uncertainty in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "comment": null, "summary": "As large language models are increasingly utilized in real-world\napplications, guarantees of task-specific metrics are essential for their\nreliable deployment. Previous studies have introduced various criteria of\nconformal uncertainty grounded in split conformal prediction, which offer\nuser-specified correctness coverage. However, existing frameworks often fail to\nidentify uncertainty data outliers that violate the exchangeability assumption,\nleading to unbounded miscoverage rates and unactionable prediction sets. In\nthis paper, we propose a novel approach termed Selective Conformal Uncertainty\n(SConU), which, for the first time, implements significance tests, by\ndeveloping two conformal p-values that are instrumental in determining whether\na given sample deviates from the uncertainty distribution of the calibration\nset at a specific manageable risk level. Our approach not only facilitates\nrigorous management of miscoverage rates across both single-domain and\ninterdisciplinary contexts, but also enhances the efficiency of predictions.\nFurthermore, we comprehensively analyze the components of the conformal\nprocedures, aiming to approximate conditional coverage, particularly in\nhigh-stakes question-answering tasks."}
{"id": "2504.15108", "pdf": "https://arxiv.org/pdf/2504.15108", "abs": "https://arxiv.org/abs/2504.15108", "authors": ["Zhenzhen Xiao", "Heng Liu", "Bingwen Hu"], "title": "Unwarping Screen Content Images via Structure-texture Enhancement Network and Transformation Self-estimation", "categories": ["cs.CV"], "comment": null, "summary": "While existing implicit neural network-based image unwarping methods perform\nwell on natural images, they struggle to handle screen content images (SCIs),\nwhich often contain large geometric distortions, text, symbols, and sharp\nedges. To address this, we propose a structure-texture enhancement network\n(STEN) with transformation self-estimation for SCI warping. STEN integrates a\nB-spline implicit neural representation module and a transformation error\nestimation and self-correction algorithm. It comprises two branches: the\nstructure estimation branch (SEB), which enhances local aggregation and global\ndependency modeling, and the texture estimation branch (TEB), which improves\ntexture detail synthesis using B-spline implicit neural representation.\nAdditionally, the transformation self-estimation module autonomously estimates\nthe transformation error and corrects the coordinate transformation matrix,\neffectively handling real-world image distortions. Extensive experiments on\npublic SCI datasets demonstrate that our approach significantly outperforms\nstate-of-the-art methods. Comparisons on well-known natural image datasets also\nshow the potential of our approach for natural image distortion."}
{"id": "2504.14156", "pdf": "https://arxiv.org/pdf/2504.14156", "abs": "https://arxiv.org/abs/2504.14156", "authors": ["Abdelali Sajia", "Bilal Benzimoun", "Pawan Khatiwada", "Guogan Zhao", "Xiao-Feng Qian"], "title": "Breaking the Diffraction Barrier for Passive Sources: Parameter-Decoupled Superresolution Assisted by Physics-Informed Machine Learning", "categories": ["physics.optics", "cs.AI", "quant-ph"], "comment": "12 pages, 3 figures", "summary": "We present a parameter-decoupled superresolution framework for estimating\nsub-wavelength separations of passive two-point sources without requiring prior\nknowledge or control of the source. Our theoretical foundation circumvents the\nneed to estimate multiple challenging parameters such as partial coherence,\nbrightness imbalance, random relative phase, and photon statistics. A\nphysics-informed machine learning (ML) model (trained with a standard desktop\nworkstation), synergistically integrating this theory, further addresses\npractical imperfections including background noise, photon loss, and\ncentroid/orientation misalignment. The integrated parameter-decoupling\nsuperresolution method achieves resolution 14 and more times below the\ndiffraction limit (corresponding to ~ 13.5 nm in optical microscopy) on\nexperimentally generated realistic images with >82% fidelity, performance\nrivaling state-of-the-art techniques for actively controllable sources.\nCritically, our method's robustness against source parameter variability and\nsource-independent noises enables potential applications in realistic scenarios\nwhere source control is infeasible, such as astrophysical imaging, live-cell\nmicroscopy, and quantum metrology. This work bridges a critical gap between\ntheoretical superresolution limits and practical implementations for passive\nsystems."}
{"id": "2504.15118", "pdf": "https://arxiv.org/pdf/2504.15118", "abs": "https://arxiv.org/abs/2504.15118", "authors": ["Inho Kim", "Youngkil Song", "Jicheol Park", "Won Hwa Kim", "Suha Kwak"], "title": "Improving Sound Source Localization with Joint Slot Attention on Image and Audio", "categories": ["cs.CV", "cs.SD"], "comment": "Accepted to CVPR 2025", "summary": "Sound source localization (SSL) is the task of locating the source of sound\nwithin an image. Due to the lack of localization labels, the de facto standard\nin SSL has been to represent an image and audio as a single embedding vector\neach, and use them to learn SSL via contrastive learning. To this end, previous\nwork samples one of local image features as the image embedding and aggregates\nall local audio features to obtain the audio embedding, which is far from\noptimal due to the presence of noise and background irrelevant to the actual\ntarget in the input. We present a novel SSL method that addresses this chronic\nissue by joint slot attention on image and audio. To be specific, two slots\ncompetitively attend image and audio features to decompose them into target and\noff-target representations, and only target representations of image and audio\nare used for contrastive learning. Also, we introduce cross-modal attention\nmatching to further align local features of image and audio. Our method\nachieved the best in almost all settings on three public benchmarks for SSL,\nand substantially outperformed all the prior work in cross-modal retrieval."}
{"id": "2504.14174", "pdf": "https://arxiv.org/pdf/2504.14174", "abs": "https://arxiv.org/abs/2504.14174", "authors": ["Jing Han", "Hanting Chen", "Kai Han", "Xiaomeng Huang", "Yongyun Hu", "Wenjun Xu", "Dacheng Tao", "Ping Zhang"], "title": "A Physics-guided Multimodal Transformer Path to Weather and Climate Sciences", "categories": ["cs.LG", "cs.AI"], "comment": "Perspective article", "summary": "With the rapid development of machine learning in recent years, many problems\nin meteorology can now be addressed using AI models. In particular, data-driven\nalgorithms have significantly improved accuracy compared to traditional\nmethods. Meteorological data is often transformed into 2D images or 3D videos,\nwhich are then fed into AI models for learning. Additionally, these models\noften incorporate physical signals, such as temperature, pressure, and wind\nspeed, to further enhance accuracy and interpretability. In this paper, we\nreview several representative AI + Weather/Climate algorithms and propose a new\nparadigm where observational data from different perspectives, each with\ndistinct physical meanings, are treated as multimodal data and integrated via\ntransformers. Furthermore, key weather and climate knowledge can be\nincorporated through regularization techniques to further strengthen the\nmodel's capabilities. This new paradigm is versatile and can address a variety\nof tasks, offering strong generalizability. We also discuss future directions\nfor improving model accuracy and interpretability."}
{"id": "2504.15121", "pdf": "https://arxiv.org/pdf/2504.15121", "abs": "https://arxiv.org/abs/2504.15121", "authors": ["Csongor Csanad Kariko", "Muhammad Rafi Faisal", "Levente Hajder"], "title": "Robust and Real-time Surface Normal Estimation from Stereo Disparities using Affine Transformations", "categories": ["cs.CV"], "comment": null, "summary": "This work introduces a novel method for surface normal estimation from\nrectified stereo image pairs, leveraging affine transformations derived from\ndisparity values to achieve fast and accurate results. We demonstrate how the\nrectification of stereo image pairs simplifies the process of surface normal\nestimation by reducing computational complexity. To address noise reduction, we\ndevelop a custom algorithm inspired by convolutional operations, tailored to\nprocess disparity data efficiently. We also introduce adaptive heuristic\ntechniques for efficiently detecting connected surface components within the\nimages, further improving the robustness of the method. By integrating these\nmethods, we construct a surface normal estimator that is both fast and\naccurate, producing a dense, oriented point cloud as the final output. Our\nmethod is validated using both simulated environments and real-world stereo\nimages from the Middlebury and Cityscapes datasets, demonstrating significant\nimprovements in real-time performance and accuracy when implemented on a GPU.\nUpon acceptance, the shader source code will be made publicly available to\nfacilitate further research and reproducibility."}
{"id": "2504.14200", "pdf": "https://arxiv.org/pdf/2504.14200", "abs": "https://arxiv.org/abs/2504.14200", "authors": ["Huiyi Chen", "Jiawei Peng", "Kaihua Tang", "Xin Geng", "Xu Yang"], "title": "Enhancing Multimodal In-Context Learning for Image Classification through Coreset Optimization", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 5 figures", "summary": "In-context learning (ICL) enables Large Vision-Language Models (LVLMs) to\nadapt to new tasks without parameter updates, using a few demonstrations from a\nlarge support set. However, selecting informative demonstrations leads to high\ncomputational and memory costs. While some methods explore selecting a small\nand representative coreset in the text classification, evaluating all support\nset samples remains costly, and discarded samples lead to unnecessary\ninformation loss. These methods may also be less effective for image\nclassification due to differences in feature spaces. Given these limitations,\nwe propose Key-based Coreset Optimization (KeCO), a novel framework that\nleverages untapped data to construct a compact and informative coreset. We\nintroduce visual features as keys within the coreset, which serve as the anchor\nfor identifying samples to be updated through different selection strategies.\nBy leveraging untapped samples from the support set, we update the keys of\nselected coreset samples, enabling the randomly initialized coreset to evolve\ninto a more informative coreset under low computational cost. Through extensive\nexperiments on coarse-grained and fine-grained image classification benchmarks,\nwe demonstrate that KeCO effectively enhances ICL performance for image\nclassification task, achieving an average improvement of more than 20\\%.\nNotably, we evaluate KeCO under a simulated online scenario, and the strong\nperformance in this scenario highlights the practical value of our framework\nfor resource-constrained real-world scenarios."}
{"id": "2504.15122", "pdf": "https://arxiv.org/pdf/2504.15122", "abs": "https://arxiv.org/abs/2504.15122", "authors": ["Minh-Quan Viet Bui", "Jongmin Park", "Juan Luis Gonzalez Bello", "Jaeho Moon", "Jihyong Oh", "Munchurl Kim"], "title": "MoBGS: Motion Deblurring Dynamic 3D Gaussian Splatting for Blurry Monocular Video", "categories": ["cs.CV"], "comment": "The first two authors contributed equally to this work (equal\n  contribution). The last two authors advised equally to this work", "summary": "We present MoBGS, a novel deblurring dynamic 3D Gaussian Splatting (3DGS)\nframework capable of reconstructing sharp and high-quality novel\nspatio-temporal views from blurry monocular videos in an end-to-end manner.\nExisting dynamic novel view synthesis (NVS) methods are highly sensitive to\nmotion blur in casually captured videos, resulting in significant degradation\nof rendering quality. While recent approaches address motion-blurred inputs for\nNVS, they primarily focus on static scene reconstruction and lack dedicated\nmotion modeling for dynamic objects. To overcome these limitations, our MoBGS\nintroduces a novel Blur-adaptive Latent Camera Estimation (BLCE) method for\neffective latent camera trajectory estimation, improving global camera motion\ndeblurring. In addition, we propose a physically-inspired Latent Camera-induced\nExposure Estimation (LCEE) method to ensure consistent deblurring of both\nglobal camera and local object motion. Our MoBGS framework ensures the temporal\nconsistency of unseen latent timestamps and robust motion decomposition of\nstatic and dynamic regions. Extensive experiments on the Stereo Blur dataset\nand real-world blurry videos show that our MoBGS significantly outperforms the\nvery recent advanced methods (DyBluRF and Deblur4DGS), achieving\nstate-of-the-art performance for dynamic NVS under motion blur."}
{"id": "2504.14202", "pdf": "https://arxiv.org/pdf/2504.14202", "abs": "https://arxiv.org/abs/2504.14202", "authors": ["Zichuan Liu", "Liming Jiang", "Qing Yan", "Yumin Jia", "Hao Kang", "Xin Lu"], "title": "Learning Joint ID-Textual Representation for ID-Preserving Image Synthesis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We propose a novel framework for ID-preserving generation using a multi-modal\nencoding strategy rather than injecting identity features via adapters into\npre-trained models. Our method treats identity and text as a unified\nconditioning input. To achieve this, we introduce FaceCLIP, a multi-modal\nencoder that learns a joint embedding space for both identity and textual\nsemantics. Given a reference face and a text prompt, FaceCLIP produces a\nunified representation that encodes both identity and text, which conditions a\nbase diffusion model to generate images that are identity-consistent and\ntext-aligned. We also present a multi-modal alignment algorithm to train\nFaceCLIP, using a loss that aligns its joint representation with face, text,\nand image embedding spaces. We then build FaceCLIP-SDXL, an ID-preserving image\nsynthesis pipeline by integrating FaceCLIP with Stable Diffusion XL (SDXL).\nCompared to prior methods, FaceCLIP-SDXL enables photorealistic portrait\ngeneration with better identity preservation and textual relevance. Extensive\nexperiments demonstrate its quantitative and qualitative superiority."}
{"id": "2504.15134", "pdf": "https://arxiv.org/pdf/2504.15134", "abs": "https://arxiv.org/abs/2504.15134", "authors": ["Xiao Zhang", "Lu Zou", "Tao Lu", "Yuan Yao", "Zhangjin Huang", "Guoping Wang"], "title": "Instance-Adaptive Keypoint Learning with Local-to-Global Geometric Aggregation for Category-Level Object Pose Estimation", "categories": ["cs.CV"], "comment": null, "summary": "Category-level object pose estimation aims to predict the 6D pose and size of\npreviously unseen instances from predefined categories, requiring strong\ngeneralization across diverse object instances. Although many previous methods\nattempt to mitigate intra-class variations, they often struggle with instances\nexhibiting complex geometries or significant deviations from canonical shapes.\nTo address this challenge, we propose INKL-Pose, a novel category-level object\npose estimation framework that enables INstance-adaptive Keypoint Learning with\nlocal-to-global geometric aggregation. Specifically, our approach first\npredicts semantically consistent and geometric informative keypoints through an\nInstance-Adaptive Keypoint Generator, then refines them with: (1) a Local\nKeypoint Feature Aggregator capturing fine-grained geometries, and (2) a Global\nKeypoint Feature Aggregator using bidirectional Mamba for structural\nconsistency. To enable bidirectional modeling in Mamba, we introduce a Feature\nSequence Flipping strategy that preserves spatial coherence while constructing\nbackward feature sequences. Additionally, we design a surface loss and a\nseparation loss to enforce uniform coverage and spatial diversity in keypoint\ndistribution. The generated keypoints are finally mapped to a canonical space\nfor regressing the object's 6D pose and size. Extensive experiments on\nCAMERA25, REAL275, and HouseCat6D demonstrate that INKL-Pose achieves\nstate-of-the-art performance and significantly outperforms existing methods."}
{"id": "2504.14204", "pdf": "https://arxiv.org/pdf/2504.14204", "abs": "https://arxiv.org/abs/2504.14204", "authors": ["Wenxin Zhang", "Xiaojian Lin", "Wenjun Yu", "Guangzhen Yao", "jingxiang Zhong", "Yu Li", "Renda Han", "Songcheng Xu", "Hao Shi", "Cuicui Luo"], "title": "DConAD: A Differencing-based Contrastive Representation Learning Framework for Time Series Anomaly Detection", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Time series anomaly detection holds notable importance for risk\nidentification and fault detection across diverse application domains.\nUnsupervised learning methods have become popular because they have no\nrequirement for labels. However, due to the challenges posed by the\nmultiplicity of abnormal patterns, the sparsity of anomalies, and the growth of\ndata scale and complexity, these methods often fail to capture robust and\nrepresentative dependencies within the time series for identifying anomalies.\nTo enhance the ability of models to capture normal patterns of time series and\navoid the retrogression of modeling ability triggered by the dependencies on\nhigh-quality prior knowledge, we propose a differencing-based contrastive\nrepresentation learning framework for time series anomaly detection (DConAD).\nSpecifically, DConAD generates differential data to provide additional\ninformation about time series and utilizes transformer-based architecture to\ncapture spatiotemporal dependencies, which enhances the robustness of unbiased\nrepresentation learning ability. Furthermore, DConAD implements a novel KL\ndivergence-based contrastive learning paradigm that only uses positive samples\nto avoid deviation from reconstruction and deploys the stop-gradient strategy\nto compel convergence. Extensive experiments on five public datasets show the\nsuperiority and effectiveness of DConAD compared with nine baselines. The code\nis available at https://github.com/shaieesss/DConAD."}
{"id": "2504.15145", "pdf": "https://arxiv.org/pdf/2504.15145", "abs": "https://arxiv.org/abs/2504.15145", "authors": ["Huzheng Yang", "Katherine Xu", "Michael D. Grossberg", "Yutong Bai", "Jianbo Shi"], "title": "\"I Know It When I See It\": Mood Spaces for Connecting and Expressing Visual Concepts", "categories": ["cs.CV"], "comment": "Project page: https://huzeyann.github.io/mspace/", "summary": "Expressing complex concepts is easy when they can be labeled or quantified,\nbut many ideas are hard to define yet instantly recognizable. We propose a Mood\nBoard, where users convey abstract concepts with examples that hint at the\nintended direction of attribute changes. We compute an underlying Mood Space\nthat 1) factors out irrelevant features and 2) finds the connections between\nimages, thus bringing relevant concepts closer. We invent a fibration\ncomputation to compress/decompress pre-trained features into/from a compact\nspace, 50-100x smaller. The main innovation is learning to mimic the pairwise\naffinity relationship of the image tokens across exemplars. To focus on the\ncoarse-to-fine hierarchical structures in the Mood Space, we compute the top\neigenvector structure from the affinity matrix and define a loss in the\neigenvector space. The resulting Mood Space is locally linear and compact,\nallowing image-level operations, such as object averaging, visual analogy, and\npose transfer, to be performed as a simple vector operation in Mood Space. Our\nlearning is efficient in computation without any fine-tuning, needs only a few\n(2-20) exemplars, and takes less than a minute to learn."}
{"id": "2504.14205", "pdf": "https://arxiv.org/pdf/2504.14205", "abs": "https://arxiv.org/abs/2504.14205", "authors": ["Wenxin Zhang", "Jingxing Zhong", "Guangzhen Yao", "Renda Han", "Xiaojian Lin", "Zeyu Zhang", "Cuicui Luo"], "title": "Dual-channel Heterophilic Message Passing for Graph Fraud Detection", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Fraudulent activities have significantly increased across various domains,\nsuch as e-commerce, online review platforms, and social networks, making fraud\ndetection a critical task. Spatial Graph Neural Networks (GNNs) have been\nsuccessfully applied to fraud detection tasks due to their strong inductive\nlearning capabilities. However, existing spatial GNN-based methods often\nenhance the graph structure by excluding heterophilic neighbors during message\npassing to align with the homophilic bias of GNNs. Unfortunately, this approach\ncan disrupt the original graph topology and increase uncertainty in\npredictions. To address these limitations, this paper proposes a novel\nframework, Dual-channel Heterophilic Message Passing (DHMP), for fraud\ndetection. DHMP leverages a heterophily separation module to divide the graph\ninto homophilic and heterophilic subgraphs, mitigating the low-pass inductive\nbias of traditional GNNs. It then applies shared weights to capture signals at\ndifferent frequencies independently and incorporates a customized sampling\nstrategy for training. This allows nodes to adaptively balance the\ncontributions of various signals based on their labels. Extensive experiments\non three real-world datasets demonstrate that DHMP outperforms existing\nmethods, highlighting the importance of separating signals with different\nfrequencies for improved fraud detection. The code is available at\nhttps://github.com/shaieesss/DHMP."}
{"id": "2504.15152", "pdf": "https://arxiv.org/pdf/2504.15152", "abs": "https://arxiv.org/abs/2504.15152", "authors": ["Jun Zhou", "Bingchen Gao", "Kai Wang", "Jialun Pei", "Pheng-Ann Heng", "Jing Qin"], "title": "Landmark-Free Preoperative-to-Intraoperative Registration in Laparoscopic Liver Resection", "categories": ["cs.CV", "cs.AI"], "comment": "TMI under review", "summary": "Liver registration by overlaying preoperative 3D models onto intraoperative\n2D frames can assist surgeons in perceiving the spatial anatomy of the liver\nclearly for a higher surgical success rate. Existing registration methods rely\nheavily on anatomical landmark-based workflows, which encounter two major\nlimitations: 1) ambiguous landmark definitions fail to provide efficient\nmarkers for registration; 2) insufficient integration of intraoperative liver\nvisual information in shape deformation modeling. To address these challenges,\nin this paper, we propose a landmark-free preoperative-to-intraoperative\nregistration framework utilizing effective self-supervised learning, termed\n\\ourmodel. This framework transforms the conventional 3D-2D workflow into a\n3D-3D registration pipeline, which is then decoupled into rigid and non-rigid\nregistration subtasks. \\ourmodel~first introduces a feature-disentangled\ntransformer to learn robust correspondences for recovering rigid\ntransformations. Further, a structure-regularized deformation network is\ndesigned to adjust the preoperative model to align with the intraoperative\nliver surface. This network captures structural correlations through geometry\nsimilarity modeling in a low-rank transformer network. To facilitate the\nvalidation of the registration performance, we also construct an in-vivo\nregistration dataset containing liver resection videos of 21 patients, called\n\\emph{P2I-LReg}, which contains 346 keyframes that provide a global view of the\nliver together with liver mask annotations and calibrated camera intrinsic\nparameters. Extensive experiments and user studies on both synthetic and\nin-vivo datasets demonstrate the superiority and potential clinical\napplicability of our method."}
{"id": "2504.14206", "pdf": "https://arxiv.org/pdf/2504.14206", "abs": "https://arxiv.org/abs/2504.14206", "authors": ["Wenxin Zhang", "Cuicui Luo"], "title": "Decomposition-based multi-scale transformer framework for time series anomaly detection", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Time series anomaly detection is crucial for maintaining stable systems.\nExisting methods face two main challenges. First, it is difficult to directly\nmodel the dependencies of diverse and complex patterns within the sequences.\nSecond, many methods that optimize parameters using mean squared error struggle\nwith noise in the time series, leading to performance deterioration. To address\nthese challenges, we propose a transformer-based framework built on\ndecomposition (TransDe) for multivariate time series anomaly detection. The key\nidea is to combine the strengths of time series decomposition and transformers\nto effectively learn the complex patterns in normal time series data. A\nmulti-scale patch-based transformer architecture is proposed to exploit the\nrepresentative dependencies of each decomposed component of the time series.\nFurthermore, a contrastive learn paradigm based on patch operation is proposed,\nwhich leverages KL divergence to align the positive pairs, namely the pure\nrepresentations of normal patterns between different patch-level views. A novel\nasynchronous loss function with a stop-gradient strategy is further introduced\nto enhance the performance of TransDe effectively. It can avoid time-consuming\nand labor-intensive computation costs in the optimization process. Extensive\nexperiments on five public datasets are conducted and TransDe shows superiority\ncompared with twelve baselines in terms of F1 score. Our code is available at\nhttps://github.com/shaieesss/TransDe."}
{"id": "2504.15155", "pdf": "https://arxiv.org/pdf/2504.15155", "abs": "https://arxiv.org/abs/2504.15155", "authors": ["Guandong Li", "Mengxia Ye"], "title": "Dynamic 3D KAN Convolution with Adaptive Grid Optimization for Hyperspectral Image Classification", "categories": ["cs.CV"], "comment": null, "summary": "Deep neural networks face several challenges in hyperspectral image\nclassification, including high-dimensional data, sparse distribution of ground\nobjects, and spectral redundancy, which often lead to classification\noverfitting and limited generalization capability. To more efficiently adapt to\nground object distributions while extracting image features without introducing\nexcessive parameters and skipping redundant information, this paper proposes\nKANet based on an improved 3D-DenseNet model, consisting of 3D KAN Conv and an\nadaptive grid update mechanism. By introducing learnable univariate B-spline\nfunctions on network edges, specifically by flattening three-dimensional\nneighborhoods into vectors and applying B-spline-parameterized nonlinear\nactivation functions to replace the fixed linear weights of traditional 3D\nconvolutional kernels, we precisely capture complex spectral-spatial nonlinear\nrelationships in hyperspectral data. Simultaneously, through a dynamic grid\nadjustment mechanism, we adaptively update the grid point positions of\nB-splines based on the statistical characteristics of input data, optimizing\nthe resolution of spline functions to match the non-uniform distribution of\nspectral features, significantly improving the model's accuracy in\nhigh-dimensional data modeling and parameter efficiency, effectively\nalleviating the curse of dimensionality. This characteristic demonstrates\nsuperior neural scaling laws compared to traditional convolutional neural\nnetworks and reduces overfitting risks in small-sample and high-noise\nscenarios. KANet enhances model representation capability through a 3D dynamic\nexpert convolution system without increasing network depth or width. The\nproposed method demonstrates superior performance on IN, UP, and KSC datasets,\noutperforming mainstream hyperspectral image classification approaches."}
{"id": "2504.14223", "pdf": "https://arxiv.org/pdf/2504.14223", "abs": "https://arxiv.org/abs/2504.14223", "authors": ["Michael Färber", "Parisa Aghdam", "Kyuri Im", "Mario Tawfelis", "Hardik Ghoshal"], "title": "SimplifyMyText: An LLM-Based System for Inclusive Plain Language Text Simplification", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "accepted at ECIR 2025", "summary": "Text simplification is essential for making complex content accessible to\ndiverse audiences who face comprehension challenges. Yet, the limited\navailability of simplified materials creates significant barriers to personal\nand professional growth and hinders social inclusion. Although researchers have\nexplored various methods for automatic text simplification, none fully leverage\nlarge language models (LLMs) to offer tailored customization for different\ntarget groups and varying levels of simplicity. Moreover, despite its proven\nbenefits for both consumers and organizations, the well-established practice of\nplain language remains underutilized. In this paper, we\nhttps://simplifymytext.org, the first system designed to produce plain language\ncontent from multiple input formats, including typed text and file uploads,\nwith flexible customization options for diverse audiences. We employ GPT-4 and\nLlama-3 and evaluate outputs across multiple metrics. Overall, our work\ncontributes to research on automatic text simplification and highlights the\nimportance of tailored communication in promoting inclusivity."}
{"id": "2504.15159", "pdf": "https://arxiv.org/pdf/2504.15159", "abs": "https://arxiv.org/abs/2504.15159", "authors": ["Junyuan Deng", "Xinyi Wu", "Yongxing Yang", "Congchao Zhu", "Song Wang", "Zhenyao Wu"], "title": "Acquire and then Adapt: Squeezing out Text-to-Image Model for Image Restoration", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Recently, pre-trained text-to-image (T2I) models have been extensively\nadopted for real-world image restoration because of their powerful generative\nprior. However, controlling these large models for image restoration usually\nrequires a large number of high-quality images and immense computational\nresources for training, which is costly and not privacy-friendly. In this\npaper, we find that the well-trained large T2I model (i.e., Flux) is able to\nproduce a variety of high-quality images aligned with real-world distributions,\noffering an unlimited supply of training samples to mitigate the above issue.\nSpecifically, we proposed a training data construction pipeline for image\nrestoration, namely FluxGen, which includes unconditional image generation,\nimage selection, and degraded image simulation. A novel light-weighted adapter\n(FluxIR) with squeeze-and-excitation layers is also carefully designed to\ncontrol the large Diffusion Transformer (DiT)-based T2I model so that\nreasonable details can be restored. Experiments demonstrate that our proposed\nmethod enables the Flux model to adapt effectively to real-world image\nrestoration tasks, achieving superior scores and visual quality on both\nsynthetic and real-world degradation datasets - at only about 8.5\\% of the\ntraining cost compared to current approaches."}
{"id": "2504.14259", "pdf": "https://arxiv.org/pdf/2504.14259", "abs": "https://arxiv.org/abs/2504.14259", "authors": ["Hadeel Jazzaa", "Thomas McCluskey", "David Peebles"], "title": "Experience-based Refinement of Task Planning Knowledge in Autonomous Robots", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "The requirement for autonomous robots to exhibit higher-level cognitive\nskills by planning and adapting in an ever-changing environment is indeed a\ngreat challenge for the AI community. Progress has been made in the automated\nplanning community on refinement and repair of an agent's symbolic knowledge to\ndo task planning in an incomplete or changing environmental model, but these\nadvances up to now have not been transferred to real physical robots. This\npaper demonstrates how a physical robot can be capable of adapting its symbolic\nknowledge of the environment, by using experiences in robot action execution to\ndrive knowledge refinement and hence to improve the success rate of the task\nplans the robot creates. To implement more robust planning systems, we propose\na method for refining domain knowledge to improve the knowledge on which\nintelligent robot behavior is based. This architecture has been implemented and\nevaluated using a NAO robot. The refined knowledge leads to the future\nsynthesis of task plans which demonstrate decreasing rates of failure over time\nas faulty knowledge is removed or adjusted."}
{"id": "2504.15165", "pdf": "https://arxiv.org/pdf/2504.15165", "abs": "https://arxiv.org/abs/2504.15165", "authors": ["Liu Wenbin"], "title": "An Efficient Aerial Image Detection with Variable Receptive Fields", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Aerial object detection using unmanned aerial vehicles (UAVs) faces critical\nchallenges including sub-10px targets, dense occlusions, and stringent\ncomputational constraints. Existing detectors struggle to balance accuracy and\nefficiency due to rigid receptive fields and redundant architectures. To\naddress these limitations, we propose Variable Receptive Field DETR (VRF-DETR),\na transformer-based detector incorporating three key components: 1) Multi-Scale\nContext Fusion (MSCF) module that dynamically recalibrates features through\nadaptive spatial attention and gated multi-scale fusion, 2) Gated Convolution\n(GConv) layer enabling parameter-efficient local-context modeling via depthwise\nseparable operations and dynamic gating, and 3) Gated Multi-scale Fusion (GMCF)\nBottleneck that hierarchically disentangles occluded objects through cascaded\nglobal-local interactions. Experiments on VisDrone2019 demonstrate VRF-DETR\nachieves 51.4\\% mAP\\textsubscript{50} and 31.8\\% mAP\\textsubscript{50:95} with\nonly 13.5M parameters. This work establishes a new efficiency-accuracy Pareto\nfrontier for UAV-based detection tasks."}
{"id": "2504.14300", "pdf": "https://arxiv.org/pdf/2504.14300", "abs": "https://arxiv.org/abs/2504.14300", "authors": ["Xinyu Liang", "Hao Wang"], "title": "Learning and Generating Diverse Residential Load Patterns Using GAN with Weakly-Supervised Training and Weight Selection", "categories": ["cs.LG", "cs.AI"], "comment": "12 pages", "summary": "The scarcity of high-quality residential load data can pose obstacles for\ndecarbonizing the residential sector as well as effective grid planning and\noperation. The above challenges have motivated research into generating\nsynthetic load data, but existing methods faced limitations in terms of\nscalability, diversity, and similarity. This paper proposes a Generative\nAdversarial Network-based Synthetic Residential Load Pattern (RLP-GAN)\ngeneration model, a novel weakly-supervised GAN framework, leveraging an\nover-complete autoencoder to capture dependencies within complex and diverse\nload patterns and learn household-level data distribution at scale. We\nincorporate a model weight selection method to address the mode collapse\nproblem and generate load patterns with high diversity. We develop a holistic\nevaluation method to validate the effectiveness of RLP-GAN using real-world\ndata of 417 households. The results demonstrate that RLP-GAN outperforms\nstate-of-the-art models in capturing temporal dependencies and generating load\npatterns with higher similarity to real data. Furthermore, we have publicly\nreleased the RLP-GAN generated synthetic dataset, which comprises one million\nsynthetic residential load pattern profiles."}
{"id": "2504.15170", "pdf": "https://arxiv.org/pdf/2504.15170", "abs": "https://arxiv.org/abs/2504.15170", "authors": ["Chengxi Han", "Xiaoyu Su", "Zhiqiang Wei", "Meiqi Hu", "Yichu Xu"], "title": "HSANET: A Hybrid Self-Cross Attention Network For Remote Sensing Change Detection", "categories": ["cs.CV"], "comment": null, "summary": "The remote sensing image change detection task is an essential method for\nlarge-scale monitoring. We propose HSANet, a network that uses hierarchical\nconvolution to extract multi-scale features. It incorporates hybrid\nself-attention and cross-attention mechanisms to learn and fuse global and\ncross-scale information. This enables HSANet to capture global context at\ndifferent scales and integrate cross-scale features, refining edge details and\nimproving detection performance. We will also open-source our model code:\nhttps://github.com/ChengxiHAN/HSANet."}
{"id": "2504.14301", "pdf": "https://arxiv.org/pdf/2504.14301", "abs": "https://arxiv.org/abs/2504.14301", "authors": ["Nazia Aslam", "Kamal Nasrollahi"], "title": "Balancing Privacy and Action Performance: A Penalty-Driven Approach to Image Anonymization", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "Accepted to CVPRW 2025", "summary": "The rapid development of video surveillance systems for object detection,\ntracking, activity recognition, and anomaly detection has revolutionized our\nday-to-day lives while setting alarms for privacy concerns. It isn't easy to\nstrike a balance between visual privacy and action recognition performance in\nmost computer vision models. Is it possible to safeguard privacy without\nsacrificing performance? It poses a formidable challenge, as even minor privacy\nenhancements can lead to substantial performance degradation. To address this\nchallenge, we propose a privacy-preserving image anonymization technique that\noptimizes the anonymizer using penalties from the utility branch, ensuring\nimproved action recognition performance while minimally affecting privacy\nleakage. This approach addresses the trade-off between minimizing privacy\nleakage and maintaining high action performance. The proposed approach is\nprimarily designed to align with the regulatory standards of the EU AI Act and\nGDPR, ensuring the protection of personally identifiable information while\nmaintaining action performance. To the best of our knowledge, we are the first\nto introduce a feature-based penalty scheme that exclusively controls the\naction features, allowing freedom to anonymize private attributes. Extensive\nexperiments were conducted to validate the effectiveness of the proposed\nmethod. The results demonstrate that applying a penalty to anonymizer from\nutility branch enhances action performance while maintaining nearly consistent\nprivacy leakage across different penalty settings."}
{"id": "2504.15176", "pdf": "https://arxiv.org/pdf/2504.15176", "abs": "https://arxiv.org/abs/2504.15176", "authors": ["Miaomiao Cai", "Simiao Li", "Wei Li", "Xudong Huang", "Hanting Chen", "Jie Hu", "Yunhe Wang"], "title": "DSPO: Direct Semantic Preference Optimization for Real-World Image Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in diffusion models have improved Real-World Image\nSuper-Resolution (Real-ISR), but existing methods lack human feedback\nintegration, risking misalignment with human preference and may leading to\nartifacts, hallucinations and harmful content generation. To this end, we are\nthe first to introduce human preference alignment into Real-ISR, a technique\nthat has been successfully applied in Large Language Models and Text-to-Image\ntasks to effectively enhance the alignment of generated outputs with human\npreferences. Specifically, we introduce Direct Preference Optimization (DPO)\ninto Real-ISR to achieve alignment, where DPO serves as a general alignment\ntechnique that directly learns from the human preference dataset. Nevertheless,\nunlike high-level tasks, the pixel-level reconstruction objectives of Real-ISR\nare difficult to reconcile with the image-level preferences of DPO, which can\nlead to the DPO being overly sensitive to local anomalies, leading to reduced\ngeneration quality. To resolve this dichotomy, we propose Direct Semantic\nPreference Optimization (DSPO) to align instance-level human preferences by\nincorporating semantic guidance, which is through two strategies: (a) semantic\ninstance alignment strategy, implementing instance-level alignment to ensure\nfine-grained perceptual consistency, and (b) user description feedback\nstrategy, mitigating hallucinations through semantic textual feedback on\ninstance-level images. As a plug-and-play solution, DSPO proves highly\neffective in both one-step and multi-step SR frameworks."}
{"id": "2504.14302", "pdf": "https://arxiv.org/pdf/2504.14302", "abs": "https://arxiv.org/abs/2504.14302", "authors": ["Yogev Kriger", "Shai Fine"], "title": "Learning to Score", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Common machine learning settings range from supervised tasks, where\naccurately labeled data is accessible, through semi-supervised and\nweakly-supervised tasks, where target labels are scant or noisy, to\nunsupervised tasks where labels are unobtainable. In this paper we study a\nscenario where the target labels are not available but additional related\ninformation is at hand. This information, referred to as Side Information, is\neither correlated with the unknown labels or imposes constraints on the feature\nspace. We formulate the problem as an ensemble of three semantic components:\nrepresentation learning, side information and metric learning. The proposed\nscoring model is advantageous for multiple use-cases. For example, in the\nhealthcare domain it can be used to create a severity score for diseases where\nthe symptoms are known but the criteria for the disease progression are not\nwell defined. We demonstrate the utility of the suggested scoring system on\nwell-known benchmark data-sets and bio-medical patient records."}
{"id": "2504.15179", "pdf": "https://arxiv.org/pdf/2504.15179", "abs": "https://arxiv.org/abs/2504.15179", "authors": ["Fei Yin", "Mallikarjun B R", "Chun-Han Yao", "Rafał Mantiuk", "Varun Jampani"], "title": "FaceCraft4D: Animated 3D Facial Avatar Generation from a Single Image", "categories": ["cs.CV"], "comment": null, "summary": "We present a novel framework for generating high-quality, animatable 4D\navatar from a single image. While recent advances have shown promising results\nin 4D avatar creation, existing methods either require extensive multiview data\nor struggle with shape accuracy and identity consistency. To address these\nlimitations, we propose a comprehensive system that leverages shape, image, and\nvideo priors to create full-view, animatable avatars. Our approach first\nobtains initial coarse shape through 3D-GAN inversion. Then, it enhances\nmultiview textures using depth-guided warping signals for cross-view\nconsistency with the help of the image diffusion model. To handle expression\nanimation, we incorporate a video prior with synchronized driving signals\nacross viewpoints. We further introduce a Consistent-Inconsistent training to\neffectively handle data inconsistencies during 4D reconstruction. Experimental\nresults demonstrate that our method achieves superior quality compared to the\nprior art, while maintaining consistency across different viewpoints and\nexpressions."}
{"id": "2504.14320", "pdf": "https://arxiv.org/pdf/2504.14320", "abs": "https://arxiv.org/abs/2504.14320", "authors": ["Nimisha Karnatak", "Adrien Baranes", "Rob Marchant", "Huinan Zeng", "Tríona Butler", "Kristen Olson"], "title": "Expanding the Generative AI Design Space through Structured Prompting and Multimodal Interfaces", "categories": ["cs.HC", "cs.AI"], "comment": "Accepted at CHI'25 Workshop on Designing and Developing User\n  Interfaces with AI", "summary": "Text-based prompting remains the predominant interaction paradigm in\ngenerative AI, yet it often introduces friction for novice users such as small\nbusiness owners (SBOs), who struggle to articulate creative goals in\ndomain-specific contexts like advertising. Through a formative study with six\nSBOs in the United Kingdom, we identify three key challenges: difficulties in\nexpressing brand intuition through prompts, limited opportunities for\nfine-grained adjustment and refinement during and after content generation, and\nthe frequent production of generic content that lacks brand specificity. In\nresponse, we present ACAI (AI Co-Creation for Advertising and Inspiration), a\nmultimodal generative AI tool designed to support novice designers by moving\nbeyond traditional prompt interfaces. ACAI features a structured input system\ncomposed of three panels: Branding, Audience and Goals, and the Inspiration\nBoard. These inputs allow users to convey brand-relevant context and visual\npreferences. This work contributes to HCI research on generative systems by\nshowing how structured interfaces can foreground user-defined context, improve\nalignment, and enhance co-creative control in novice creative workflows."}
{"id": "2504.15182", "pdf": "https://arxiv.org/pdf/2504.15182", "abs": "https://arxiv.org/abs/2504.15182", "authors": ["Xianpan Zhou"], "title": "Tiger200K: Manually Curated High Visual Quality Video Dataset from UGC Platform", "categories": ["cs.CV"], "comment": "Project page: https://tinytigerpan.github.io/tiger200k/", "summary": "The recent surge in open-source text-to-video generation models has\nsignificantly energized the research community, yet their dependence on\nproprietary training datasets remains a key constraint. While existing open\ndatasets like Koala-36M employ algorithmic filtering of web-scraped videos from\nearly platforms, they still lack the quality required for fine-tuning advanced\nvideo generation models. We present Tiger200K, a manually curated high visual\nquality video dataset sourced from User-Generated Content (UGC) platforms. By\nprioritizing visual fidelity and aesthetic quality, Tiger200K underscores the\ncritical role of human expertise in data curation, and providing high-quality,\ntemporally consistent video-text pairs for fine-tuning and optimizing video\ngeneration architectures through a simple but effective pipeline including shot\nboundary detection, OCR, border detecting, motion filter and fine bilingual\ncaption. The dataset will undergo ongoing expansion and be released as an\nopen-source initiative to advance research and applications in video generative\nmodels. Project page: https://tinytigerpan.github.io/tiger200k/"}
{"id": "2504.14335", "pdf": "https://arxiv.org/pdf/2504.14335", "abs": "https://arxiv.org/abs/2504.14335", "authors": ["Zhengbo Zhang", "Yuxi Zhou", "Duo Peng", "Joo-Hwee Lim", "Zhigang Tu", "De Wen Soh", "Lin Geng Foo"], "title": "Visual Prompting for One-shot Controllable Video Editing without Inversion", "categories": ["cs.CV", "cs.AI"], "comment": "accepted by cvpr2025", "summary": "One-shot controllable video editing (OCVE) is an important yet challenging\ntask, aiming to propagate user edits that are made -- using any image editing\ntool -- on the first frame of a video to all subsequent frames, while ensuring\ncontent consistency between edited frames and source frames. To achieve this,\nprior methods employ DDIM inversion to transform source frames into latent\nnoise, which is then fed into a pre-trained diffusion model, conditioned on the\nuser-edited first frame, to generate the edited video. However, the DDIM\ninversion process accumulates errors, which hinder the latent noise from\naccurately reconstructing the source frames, ultimately compromising content\nconsistency in the generated edited frames. To overcome it, our method\neliminates the need for DDIM inversion by performing OCVE through a novel\nperspective based on visual prompting. Furthermore, inspired by consistency\nmodels that can perform multi-step consistency sampling to generate a sequence\nof content-consistent images, we propose a content consistency sampling (CCS)\nto ensure content consistency between the generated edited frames and the\nsource frames. Moreover, we introduce a temporal-content consistency sampling\n(TCS) based on Stein Variational Gradient Descent to ensure temporal\nconsistency across the edited frames. Extensive experiments validate the\neffectiveness of our approach."}
{"id": "2504.15192", "pdf": "https://arxiv.org/pdf/2504.15192", "abs": "https://arxiv.org/abs/2504.15192", "authors": ["Yaqian Chen", "Lin Li", "Hanxue Gu", "Haoyu Dong", "Derek L. Nguyen", "Allan D. Kirk", "Maciej A. Mazurowski", "E. Shelley Hwang"], "title": "Breast density in MRI: an AI-based quantification and relationship to assessment in mammography", "categories": ["cs.CV", "cs.AI"], "comment": "13 pages, 5 figures", "summary": "Mammographic breast density is a well-established risk factor for breast\ncancer. Recently there has been interest in breast MRI as an adjunct to\nmammography, as this modality provides an orthogonal and highly quantitative\nassessment of breast tissue. However, its 3D nature poses analytic challenges\nrelated to delineating and aggregating complex structures across slices. Here,\nwe applied an in-house machine-learning algorithm to assess breast density on\nnormal breasts in three MRI datasets. Breast density was consistent across\ndifferent datasets (0.104 - 0.114). Analysis across different age groups also\ndemonstrated strong consistency across datasets and confirmed a trend of\ndecreasing density with age as reported in previous studies. MR breast density\nwas correlated with mammographic breast density, although some notable\ndifferences suggest that certain breast density components are captured only on\nMRI. Future work will determine how to integrate MR breast density with current\ntools to improve future breast cancer risk prediction."}
{"id": "2504.14345", "pdf": "https://arxiv.org/pdf/2504.14345", "abs": "https://arxiv.org/abs/2504.14345", "authors": ["Youngbin Lee", "Yejin Kim", "Suin Kim", "Yongjae Lee"], "title": "Integrating LLM-Generated Views into Mean-Variance Optimization Using the Black-Litterman Model", "categories": ["q-fin.PM", "cs.AI"], "comment": "Presented at the ICLR 2025 Workshop on Financial AI\n  (https://sites.google.com/view/financialaiiclr25/home)", "summary": "Portfolio optimization faces challenges due to the sensitivity in traditional\nmean-variance models. The Black-Litterman model mitigates this by integrating\ninvestor views, but defining these views remains difficult. This study explores\nthe integration of large language models (LLMs) generated views into portfolio\noptimization using the Black-Litterman framework. Our method leverages LLMs to\nestimate expected stock returns from historical prices and company metadata,\nincorporating uncertainty through the variance in predictions. We conduct a\nbacktest of the LLM-optimized portfolios from June 2024 to February 2025,\nrebalancing biweekly using the previous two weeks of price data. As baselines,\nwe compare against the S&P 500, an equal-weighted portfolio, and a traditional\nmean-variance optimized portfolio constructed using the same set of stocks.\nEmpirical results suggest that different LLMs exhibit varying levels of\npredictive optimism and confidence stability, which impact portfolio\nperformance. The source code and data are available at\nhttps://github.com/youngandbin/LLM-MVO-BLM."}
{"id": "2504.15193", "pdf": "https://arxiv.org/pdf/2504.15193", "abs": "https://arxiv.org/abs/2504.15193", "authors": ["Neelesh Kumar", "Oya Aran"], "title": "Automated Measurement of Eczema Severity with Self-Supervised Learning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Automated diagnosis of eczema using images acquired from digital camera can\nenable individuals to self-monitor their recovery. The process entails first\nsegmenting out the eczema region from the image and then measuring the severity\nof eczema in the segmented region. The state-of-the-art methods for automated\neczema diagnosis rely on deep neural networks such as convolutional neural\nnetwork (CNN) and have shown impressive performance in accurately measuring the\nseverity of eczema. However, these methods require massive volume of annotated\ndata to train which can be hard to obtain. In this paper, we propose a\nself-supervised learning framework for automated eczema diagnosis under limited\ntraining data regime. Our framework consists of two stages: i) Segmentation,\nwhere we use an in-context learning based algorithm called SegGPT for few-shot\nsegmentation of eczema region from the image; ii) Feature extraction and\nclassification, where we extract DINO features from the segmented regions and\nfeed it to a multi-layered perceptron (MLP) for 4-class classification of\neczema severity. When evaluated on a dataset of annotated \"in-the-wild\" eczema\nimages, we show that our method outperforms (Weighted F1: 0.67 $\\pm$ 0.01) the\nstate-of-the-art deep learning methods such as finetuned Resnet-18 (Weighted\nF1: 0.44 $\\pm$ 0.16) and Vision Transformer (Weighted F1: 0.40 $\\pm$ 0.22). Our\nresults show that self-supervised learning can be a viable solution for\nautomated skin diagnosis where labeled data is scarce."}
{"id": "2504.14359", "pdf": "https://arxiv.org/pdf/2504.14359", "abs": "https://arxiv.org/abs/2504.14359", "authors": ["Kyle Buettner", "Jacob Emmerson", "Adriana Kovashka"], "title": "A Multimodal Recaptioning Framework to Account for Perceptual Diversity in Multilingual Vision-Language Modeling", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "There are many ways to describe, name, and group objects when captioning an\nimage. Differences are evident when speakers come from diverse cultures due to\nthe unique experiences that shape perception. Machine translation of captions\nhas pushed multilingual capabilities in vision-language models (VLMs), but data\ncomes mainly from English speakers, indicating a perceptual bias and lack of\nmodel flexibility. In this work, we address this challenge and outline a\ndata-efficient framework to instill multilingual VLMs with greater\nunderstanding of perceptual diversity. We specifically propose an LLM-based,\nmultimodal recaptioning strategy that alters the object descriptions of English\ncaptions before translation. The greatest benefits are demonstrated in a\ntargeted multimodal mechanism guided by native speaker data. By adding produced\nrewrites as augmentations in training, we improve on German and Japanese\ntext-image retrieval cases studies (up to +3.5 mean recall overall, +4.7 on\nnon-native error cases). We further propose a mechanism to analyze the specific\nobject description differences across datasets, and we offer insights into\ncross-dataset and cross-language generalization."}
{"id": "2504.15199", "pdf": "https://arxiv.org/pdf/2504.15199", "abs": "https://arxiv.org/abs/2504.15199", "authors": ["Yassir Benhammou", "Alessandro Tiberio", "Gabriel Trautmann", "Suman Kalyan"], "title": "Zero-Shot, But at What Cost? Unveiling the Hidden Overhead of MILS's LLM-CLIP Framework for Image Captioning", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.PF"], "comment": "9 pages, 2 tables, 1 figure", "summary": "MILS (Multimodal Iterative LLM Solver) is a recently published framework that\nclaims \"LLMs can see and hear without any training\" by leveraging an iterative,\nLLM-CLIP based approach for zero-shot image captioning. While this MILS\napproach demonstrates good performance, our investigation reveals that this\nsuccess comes at a hidden, substantial computational cost due to its expensive\nmulti-step refinement process. In contrast, alternative models such as BLIP-2\nand GPT-4V achieve competitive results through a streamlined, single-pass\napproach. We hypothesize that the significant overhead inherent in MILS's\niterative process may undermine its practical benefits, thereby challenging the\nnarrative that zero-shot performance can be attained without incurring heavy\nresource demands. This work is the first to expose and quantify the trade-offs\nbetween output quality and computational cost in MILS, providing critical\ninsights for the design of more efficient multimodal models."}
{"id": "2504.14365", "pdf": "https://arxiv.org/pdf/2504.14365", "abs": "https://arxiv.org/abs/2504.14365", "authors": ["Akshat Ramachandran", "Souvik Kundu", "Arnab Raha", "Shamik Kundu", "Deepak K. Mathaikutty", "Tushar Krishna"], "title": "Accelerating LLM Inference with Flexible N:M Sparsity via A Fully Digital Compute-in-Memory Accelerator", "categories": ["cs.LG", "cs.AI", "cs.AR"], "comment": null, "summary": "Large language model (LLM) pruning with fixed N:M structured sparsity\nsignificantly limits the expressivity of the sparse model, yielding sub-optimal\nperformance. In contrast, supporting multiple N:M patterns to provide sparse\nrepresentational freedom introduces costly overhead in hardware. To address\nthese challenges for LLMs, we first present a flexible layer-wise\noutlier-density-aware N:M sparsity (FLOW) selection method. FLOW enables the\nidentification of optimal layer-wise N and M values (from a given range) by\nsimultaneously accounting for the presence and distribution of outliers,\nallowing a higher degree of representational freedom. To deploy sparse models\nwith such N:M flexibility, we then introduce a flexible, low-overhead digital\ncompute-in-memory architecture (FlexCiM). FlexCiM supports diverse sparsity\npatterns by partitioning a digital CiM (DCiM) macro into smaller sub-macros,\nwhich are adaptively aggregated and disaggregated through distribution and\nmerging mechanisms for different N and M values. Extensive experiments on both\ntransformer-based and recurrence-based state space foundation models (SSMs)\ndemonstrate that FLOW outperforms existing alternatives with an accuracy\nimprovement of up to 36%, while FlexCiM achieves up to 1.75x lower inference\nlatency and 1.5x lower energy consumption compared to existing sparse\naccelerators. Code is available at: https://github.com/FLOW-open-project/FLOW"}
{"id": "2504.15232", "pdf": "https://arxiv.org/pdf/2504.15232", "abs": "https://arxiv.org/abs/2504.15232", "authors": ["Xiaoyu Han", "Shunyuan Zheng", "Zonglin Li", "Chenyang Wang", "Xin Sun", "Quanling Meng"], "title": "Shape-Guided Clothing Warping for Virtual Try-On", "categories": ["cs.CV"], "comment": "Accepted by ACM MM 2024. The code is available at\n  https://github.com/xyhanHIT/SCW-VTON", "summary": "Image-based virtual try-on aims to seamlessly fit in-shop clothing to a\nperson image while maintaining pose consistency. Existing methods commonly\nemploy the thin plate spline (TPS) transformation or appearance flow to deform\nin-shop clothing for aligning with the person's body. Despite their promising\nperformance, these methods often lack precise control over fine details,\nleading to inconsistencies in shape between clothing and the person's body as\nwell as distortions in exposed limb regions. To tackle these challenges, we\npropose a novel shape-guided clothing warping method for virtual try-on, dubbed\nSCW-VTON, which incorporates global shape constraints and additional limb\ntextures to enhance the realism and consistency of the warped clothing and\ntry-on results. To integrate global shape constraints for clothing warping, we\ndevise a dual-path clothing warping module comprising a shape path and a flow\npath. The former path captures the clothing shape aligned with the person's\nbody, while the latter path leverages the mapping between the pre- and\npost-deformation of the clothing shape to guide the estimation of appearance\nflow. Furthermore, to alleviate distortions in limb regions of try-on results,\nwe integrate detailed limb guidance by developing a limb reconstruction network\nbased on masked image modeling. Through the utilization of SCW-VTON, we are\nable to generate try-on results with enhanced clothing shape consistency and\nprecise control over details. Extensive experiments demonstrate the superiority\nof our approach over state-of-the-art methods both qualitatively and\nquantitatively. The code is available at https://github.com/xyhanHIT/SCW-VTON."}
{"id": "2504.14366", "pdf": "https://arxiv.org/pdf/2504.14366", "abs": "https://arxiv.org/abs/2504.14366", "authors": ["Patrick Haller", "Jonas Golde", "Alan Akbik"], "title": "Empirical Evaluation of Knowledge Distillation from Transformers to Subquadratic Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Knowledge distillation is a widely used technique for compressing large\nlanguage models (LLMs) by training a smaller student model to mimic a larger\nteacher model. Typically, both the teacher and student are Transformer-based\narchitectures, leveraging softmax attention for sequence modeling. However, the\nquadratic complexity of self-attention at inference time remains a significant\nbottleneck, motivating the exploration of subquadratic alternatives such as\nstructured state-space models (SSMs), linear attention, and recurrent\narchitectures. In this work, we systematically evaluate the transferability of\nknowledge distillation from a Transformer teacher to nine subquadratic student\narchitectures. Our study aims to determine which subquadratic model best aligns\nwith the teacher's learned representations and how different architectural\nconstraints influence the distillation process. We also investigate the impact\nof intelligent initialization strategies, including matrix mixing and\nquery-key-value (QKV) copying, on the adaptation process. Our empirical results\non multiple NLP benchmarks provide insights into the trade-offs between\nefficiency and performance, highlighting key factors for successful knowledge\ntransfer to subquadratic architectures."}
{"id": "2504.15259", "pdf": "https://arxiv.org/pdf/2504.15259", "abs": "https://arxiv.org/abs/2504.15259", "authors": ["Yunxuan Cai", "Sitao Xiang", "Zongjian Li", "Haiwei Chen", "Yajie Zhao"], "title": "Bringing Diversity from Diffusion Models to Semantic-Guided Face Asset Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Digital modeling and reconstruction of human faces serve various\napplications. However, its availability is often hindered by the requirements\nof data capturing devices, manual labor, and suitable actors. This situation\nrestricts the diversity, expressiveness, and control over the resulting models.\nThis work aims to demonstrate that a semantically controllable generative\nnetwork can provide enhanced control over the digital face modeling process. To\nenhance diversity beyond the limited human faces scanned in a controlled\nsetting, we introduce a novel data generation pipeline that creates a\nhigh-quality 3D face database using a pre-trained diffusion model. Our proposed\nnormalization module converts synthesized data from the diffusion model into\nhigh-quality scanned data. Using the 44,000 face models we obtained, we further\ndeveloped an efficient GAN-based generator. This generator accepts semantic\nattributes as input, and generates geometry and albedo. It also allows\ncontinuous post-editing of attributes in the latent space. Our asset refinement\ncomponent subsequently creates physically-based facial assets. We introduce a\ncomprehensive system designed for creating and editing high-quality face\nassets. Our proposed model has undergone extensive experiment, comparison and\nevaluation. We also integrate everything into a web-based interactive tool. We\naim to make this tool publicly available with the release of the paper."}
{"id": "2504.14367", "pdf": "https://arxiv.org/pdf/2504.14367", "abs": "https://arxiv.org/abs/2504.14367", "authors": ["Gabriel Machado Santos", "Rita Maria da Silva Julia", "Marcelo Zanchetta do Nascimento"], "title": "Diverse Prompts: Illuminating the Prompt Space of Large Language Models with MAP-Elites", "categories": ["cs.CL", "cs.AI"], "comment": "8 pages Accepted for publication in IEEE CEC 2025", "summary": "Prompt engineering is essential for optimizing large language models (LLMs),\nyet the link between prompt structures and task performance remains\nunderexplored. This work introduces an evolutionary approach that combines\ncontext-free grammar (CFG) with the MAP-Elites algorithm to systematically\nexplore the prompt space. Our method prioritizes quality and diversity,\ngenerating high-performing and structurally varied prompts while analyzing\ntheir alignment with diverse tasks by varying traits such as the number of\nexamples (shots) and reasoning depth. By systematically mapping the phenotypic\nspace, we reveal how structural variations influence LLM performance, offering\nactionable insights for task-specific and adaptable prompt design. Evaluated on\nseven BigBench Lite tasks across multiple LLMs, our results underscore the\ncritical interplay of quality and diversity, advancing the effectiveness and\nversatility of LLMs."}
{"id": "2504.15267", "pdf": "https://arxiv.org/pdf/2504.15267", "abs": "https://arxiv.org/abs/2504.15267", "authors": ["Shaorong Zhang", "Tamoghna Chattopadhyay", "Sophia I. Thomopoulos", "Jose-Luis Ambite", "Paul M. Thompson", "Greg Ver Steeg"], "title": "Diffusion Bridge Models for 3D Medical Image Translation", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion tensor imaging (DTI) provides crucial insights into the\nmicrostructure of the human brain, but it can be time-consuming to acquire\ncompared to more readily available T1-weighted (T1w) magnetic resonance imaging\n(MRI). To address this challenge, we propose a diffusion bridge model for 3D\nbrain image translation between T1w MRI and DTI modalities. Our model learns to\ngenerate high-quality DTI fractional anisotropy (FA) images from T1w images and\nvice versa, enabling cross-modality data augmentation and reducing the need for\nextensive DTI acquisition. We evaluate our approach using perceptual\nsimilarity, pixel-level agreement, and distributional consistency metrics,\ndemonstrating strong performance in capturing anatomical structures and\npreserving information on white matter integrity. The practical utility of the\nsynthetic data is validated through sex classification and Alzheimer's disease\nclassification tasks, where the generated images achieve comparable performance\nto real data. Our diffusion bridge model offers a promising solution for\nimproving neuroimaging datasets and supporting clinical decision-making, with\nthe potential to significantly impact neuroimaging research and clinical\npractice."}
{"id": "2504.14372", "pdf": "https://arxiv.org/pdf/2504.14372", "abs": "https://arxiv.org/abs/2504.14372", "authors": ["Jose Marie Antonio Minoza"], "title": "Learning Enhanced Structural Representations with Block-Based Uncertainties for Ocean Floor Mapping", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Accurate ocean modeling and coastal hazard prediction depend on\nhigh-resolution bathymetric data; yet, current worldwide datasets are too\ncoarse for exact numerical simulations. While recent deep learning advances\nhave improved earth observation data resolution, existing methods struggle with\nthe unique challenges of producing detailed ocean floor maps, especially in\nmaintaining physical structure consistency and quantifying uncertainties. This\nwork presents a novel uncertainty-aware mechanism using spatial blocks to\nefficiently capture local bathymetric complexity based on block-based conformal\nprediction. Using the Vector Quantized Variational Autoencoder (VQ-VAE)\narchitecture, the integration of this uncertainty quantification framework\nyields spatially adaptive confidence estimates while preserving topographical\nfeatures via discrete latent representations. With smaller uncertainty widths\nin well-characterized areas and appropriately larger bounds in areas of complex\nseafloor structures, the block-based design adapts uncertainty estimates to\nlocal bathymetric complexity. Compared to conventional techniques, experimental\nresults over several ocean regions show notable increases in both\nreconstruction quality and uncertainty estimation reliability. This framework\nincreases the reliability of bathymetric reconstructions by preserving\nstructural integrity while offering spatially adaptive uncertainty estimates,\nso opening the path for more solid climate modeling and coastal hazard\nassessment."}
{"id": "2504.15270", "pdf": "https://arxiv.org/pdf/2504.15270", "abs": "https://arxiv.org/abs/2504.15270", "authors": ["Ji Qi", "Yuan Yao", "Yushi Bai", "Bin Xu", "Juanzi Li", "Zhiyuan Liu", "Tat-Seng Chua"], "title": "An LMM for Efficient Video Understanding via Reinforced Compression of Video Cubes", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Large Multimodal Models (LMMs) uniformly perceive video frames, creating\ncomputational inefficiency for videos with inherently varying temporal\ninformation density. This paper present \\textbf{Quicksviewer}, an LMM with new\nperceiving paradigm that partitions a video of nonuniform density into varying\ncubes using Gumbel Softmax, followed by a unified resampling for each cube to\nachieve efficient video understanding. This simple and intuitive approach\ndynamically compress video online based on its temporal density, significantly\nreducing spatiotemporal redundancy (overall 45$\\times$ compression rate), while\nenabling efficient training with large receptive field. We train the model from\na language backbone through three progressive stages, each incorporating\nlengthy videos on average of 420s/1fps thanks to the perceiving efficiency.\nWith only 0.8M total video-text samples for training, our model outperforms the\ndirect baseline employing a fixed partitioning strategy by a maximum of 8.72 in\naccuracy, demonstrating the effectiveness in performance. On Video-MME,\nQuicksviewer achieves SOTA under modest sequence lengths using just up to 5\\%\nof tokens per frame required by baselines. With this paradigm, scaling up the\nnumber of input frames reveals a clear power law of the model capabilities. It\nis also empirically verified that the segments generated by the cubing network\ncan help for analyzing continuous events in videos."}
{"id": "2504.14386", "pdf": "https://arxiv.org/pdf/2504.14386", "abs": "https://arxiv.org/abs/2504.14386", "authors": ["Md Abtahi Majeed Chowdhury", "Md Rifat Ur Rahman", "Akil Ahmad Taki"], "title": "LOOPE: Learnable Optimal Patch Order in Positional Embeddings for Vision Transformers", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Positional embeddings (PE) play a crucial role in Vision Transformers (ViTs)\nby providing spatial information otherwise lost due to the permutation\ninvariant nature of self attention. While absolute positional embeddings (APE)\nhave shown theoretical advantages over relative positional embeddings (RPE),\nparticularly due to the ability of sinusoidal functions to preserve spatial\ninductive biases like monotonicity and shift invariance, a fundamental\nchallenge arises when mapping a 2D grid to a 1D sequence. Existing methods have\nmostly overlooked or never explored the impact of patch ordering in positional\nembeddings. To address this, we propose LOOPE, a learnable patch-ordering\nmethod that optimizes spatial representation for a given set of frequencies,\nproviding a principled approach to patch order optimization. Empirical results\nshow that our PE significantly improves classification accuracy across various\nViT architectures. To rigorously evaluate the effectiveness of positional\nembeddings, we introduce the \"Three Cell Experiment\", a novel benchmarking\nframework that assesses the ability of PEs to retain relative and absolute\npositional information across different ViT architectures. Unlike standard\nevaluations, which typically report a performance gap of 4 to 6% between models\nwith and without PE, our method reveals a striking 30 to 35% difference,\noffering a more sensitive diagnostic tool to measure the efficacy of PEs. Our\nexperimental analysis confirms that the proposed LOOPE demonstrates enhanced\neffectiveness in retaining both relative and absolute positional information."}
{"id": "2504.15271", "pdf": "https://arxiv.org/pdf/2504.15271", "abs": "https://arxiv.org/abs/2504.15271", "authors": ["Guo Chen", "Zhiqi Li", "Shihao Wang", "Jindong Jiang", "Yicheng Liu", "Lidong Lu", "De-An Huang", "Wonmin Byeon", "Matthieu Le", "Tuomas Rintamaki", "Tyler Poon", "Max Ehrlich", "Tuomas Rintamaki", "Tyler Poon", "Tong Lu", "Limin Wang", "Bryan Catanzaro", "Jan Kautz", "Andrew Tao", "Zhiding Yu", "Guilin Liu"], "title": "Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "We introduce Eagle 2.5, a family of frontier vision-language models (VLMs)\nfor long-context multimodal learning. Our work addresses the challenges in long\nvideo comprehension and high-resolution image understanding, introducing a\ngeneralist framework for both tasks. The proposed training framework\nincorporates Automatic Degrade Sampling and Image Area Preservation, two\ntechniques that preserve contextual integrity and visual details. The framework\nalso includes numerous efficiency optimizations in the pipeline for\nlong-context data training. Finally, we propose Eagle-Video-110K, a novel\ndataset that integrates both story-level and clip-level annotations,\nfacilitating long-video understanding. Eagle 2.5 demonstrates substantial\nimprovements on long-context multimodal benchmarks, providing a robust solution\nto the limitations of existing VLMs. Notably, our best model Eagle 2.5-8B\nachieves 72.4% on Video-MME with 512 input frames, matching the results of\ntop-tier commercial model such as GPT-4o and large-scale open-source models\nlike Qwen2.5-VL-72B and InternVL2.5-78B."}
{"id": "2504.14395", "pdf": "https://arxiv.org/pdf/2504.14395", "abs": "https://arxiv.org/abs/2504.14395", "authors": ["Chung-En", "Yu", "Hsuan-Chih", "Chen", "Brian Jalaian", "Nathaniel D. Bastian"], "title": "Hydra: An Agentic Reasoning Approach for Enhancing Adversarial Robustness and Mitigating Hallucinations in Vision-Language Models", "categories": ["cs.CV", "cs.AI", "cs.MA"], "comment": null, "summary": "To develop trustworthy Vision-Language Models (VLMs), it is essential to\naddress adversarial robustness and hallucination mitigation, both of which\nimpact factual accuracy in high-stakes applications such as defense and\nhealthcare. Existing methods primarily focus on either adversarial defense or\nhallucination post-hoc correction, leaving a gap in unified robustness\nstrategies. We introduce \\textbf{Hydra}, an adaptive agentic framework that\nenhances plug-in VLMs through iterative reasoning, structured critiques, and\ncross-model verification, improving both resilience to adversarial\nperturbations and intrinsic model errors. Hydra employs an Action-Critique\nLoop, where it retrieves and critiques visual information, leveraging\nChain-of-Thought (CoT) and In-Context Learning (ICL) techniques to refine\noutputs dynamically. Unlike static post-hoc correction methods, Hydra adapts to\nboth adversarial manipulations and intrinsic model errors, making it robust to\nmalicious perturbations and hallucination-related inaccuracies. We evaluate\nHydra on four VLMs, three hallucination benchmarks, two adversarial attack\nstrategies, and two adversarial defense methods, assessing performance on both\nclean and adversarial inputs. Results show that Hydra surpasses plug-in VLMs\nand state-of-the-art (SOTA) dehallucination methods, even without explicit\nadversarial defenses, demonstrating enhanced robustness and factual\nconsistency. By bridging adversarial resistance and hallucination mitigation,\nHydra provides a scalable, training-free solution for improving the reliability\nof VLMs in real-world applications."}
{"id": "2504.15278", "pdf": "https://arxiv.org/pdf/2504.15278", "abs": "https://arxiv.org/abs/2504.15278", "authors": ["Hongchi Xia", "Entong Su", "Marius Memmel", "Arhan Jain", "Raymond Yu", "Numfor Mbiziwo-Tiapo", "Ali Farhadi", "Abhishek Gupta", "Shenlong Wang", "Wei-Chiu Ma"], "title": "DRAWER: Digital Reconstruction and Articulation With Environment Realism", "categories": ["cs.CV", "cs.RO"], "comment": "Project page: https://drawer-art.github.io/", "summary": "Creating virtual digital replicas from real-world data unlocks significant\npotential across domains like gaming and robotics. In this paper, we present\nDRAWER, a novel framework that converts a video of a static indoor scene into a\nphotorealistic and interactive digital environment. Our approach centers on two\nmain contributions: (i) a reconstruction module based on a dual scene\nrepresentation that reconstructs the scene with fine-grained geometric details,\nand (ii) an articulation module that identifies articulation types and hinge\npositions, reconstructs simulatable shapes and appearances and integrates them\ninto the scene. The resulting virtual environment is photorealistic,\ninteractive, and runs in real time, with compatibility for game engines and\nrobotic simulation platforms. We demonstrate the potential of DRAWER by using\nit to automatically create an interactive game in Unreal Engine and to enable\nreal-to-sim-to-real transfer for robotics applications."}
{"id": "2504.14406", "pdf": "https://arxiv.org/pdf/2504.14406", "abs": "https://arxiv.org/abs/2504.14406", "authors": ["Runlong Ye", "Patrick Yung Kang Lee", "Matthew Varona", "Oliver Huang", "Carolina Nobre"], "title": "ScholarMate: A Mixed-Initiative Tool for Qualitative Knowledge Work and Information Sensemaking", "categories": ["cs.HC", "cs.AI"], "comment": "accepted at CHIWORK 2025", "summary": "Synthesizing knowledge from large document collections is a critical yet\nincreasingly complex aspect of qualitative research and knowledge work. While\nAI offers automation potential, effectively integrating it into human-centric\nsensemaking workflows remains challenging. We present ScholarMate, an\ninteractive system designed to augment qualitative analysis by unifying AI\nassistance with human oversight. ScholarMate enables researchers to dynamically\narrange and interact with text snippets on a non-linear canvas, leveraging AI\nfor theme suggestions, multi-level summarization, and contextual naming, while\nensuring transparency through traceability to source documents. Initial pilot\nstudies indicated that users value this mixed-initiative approach, finding the\nbalance between AI suggestions and direct manipulation crucial for maintaining\ninterpretability and trust. We further demonstrate the system's capability\nthrough a case study analyzing 24 papers. By balancing automation with human\ncontrol, ScholarMate enhances efficiency and supports interpretability,\noffering a valuable approach for productive human-AI collaboration in demanding\nsensemaking tasks common in knowledge work."}
{"id": "2504.15279", "pdf": "https://arxiv.org/pdf/2504.15279", "abs": "https://arxiv.org/abs/2504.15279", "authors": ["Weiye Xu", "Jiahao Wang", "Weiyun Wang", "Zhe Chen", "Wengang Zhou", "Aijun Yang", "Lewei Lu", "Houqiang Li", "Xiaohua Wang", "Xizhou Zhu", "Wenhai Wang", "Jifeng Dai", "Jinguo Zhu"], "title": "VisuLogic: A Benchmark for Evaluating Visual Reasoning in Multi-modal Large Language Models", "categories": ["cs.CV"], "comment": "Code, data, and baselines are available at\n  https://visulogic-benchmark.github.io/VisuLogic", "summary": "Visual reasoning is a core component of human intelligence and a critical\ncapability for advanced multimodal models. Yet current reasoning evaluations of\nmultimodal large language models (MLLMs) often rely on text descriptions and\nallow language-based reasoning shortcuts, failing to measure genuine\nvision-centric reasoning. To address this, we introduce VisuLogic: a benchmark\nof 1,000 human-verified problems across six categories (e.g., quantitative\nshifts, spatial relations, attribute comparisons). These various types of\nquestions can be evaluated to assess the visual reasoning capabilities of MLLMs\nfrom multiple perspectives. We evaluate leading MLLMs on this benchmark and\nanalyze their results to identify common failure modes. Most models score below\n30% accuracy-only slightly above the 25% random baseline and far below the\n51.4% achieved by humans-revealing significant gaps in visual reasoning.\nFurthermore, we provide a supplementary training dataset and a\nreinforcement-learning baseline to support further progress."}
{"id": "2504.14409", "pdf": "https://arxiv.org/pdf/2504.14409", "abs": "https://arxiv.org/abs/2504.14409", "authors": ["Christopher Ick", "Gordon Wichern", "Yoshiki Masuyama", "François G. Germain", "Jonathan Le Roux"], "title": "Data Augmentation Using Neural Acoustic Fields With Retrieval-Augmented Pre-training", "categories": ["eess.AS", "cs.AI", "cs.CV", "cs.LG", "cs.SD"], "comment": "Presented at ICASSP 2025 GenDA Workshop", "summary": "This report details MERL's system for room impulse response (RIR) estimation\nsubmitted to the Generative Data Augmentation Workshop at ICASSP 2025 for\nAugmenting RIR Data (Task 1) and Improving Speaker Distance Estimation (Task\n2). We first pre-train a neural acoustic field conditioned by room geometry on\nan external large-scale dataset in which pairs of RIRs and the geometries are\nprovided. The neural acoustic field is then adapted to each target room by\nusing the enrollment data, where we leverage either the provided room\ngeometries or geometries retrieved from the external dataset, depending on\navailability. Lastly, we predict the RIRs for each pair of source and receiver\nlocations specified by Task 1, and use these RIRs to train the speaker distance\nestimation model in Task 2."}
{"id": "2504.15280", "pdf": "https://arxiv.org/pdf/2504.15280", "abs": "https://arxiv.org/abs/2504.15280", "authors": ["Chun-Hsiao Yeh", "Chenyu Wang", "Shengbang Tong", "Ta-Ying Cheng", "Rouyu Wang", "Tianzhe Chu", "Yuexiang Zhai", "Yubei Chen", "Shenghua Gao", "Yi Ma"], "title": "Seeing from Another Perspective: Evaluating Multi-View Understanding in MLLMs", "categories": ["cs.CV", "cs.CL"], "comment": "Project page: https://danielchyeh.github.io/All-Angles-Bench/", "summary": "Multi-view understanding, the ability to reconcile visual information across\ndiverse viewpoints for effective navigation, manipulation, and 3D scene\ncomprehension, is a fundamental challenge in Multi-Modal Large Language Models\n(MLLMs) to be used as embodied agents. While recent MLLMs have shown impressive\nadvances in high-level reasoning and planning, they frequently fall short when\nconfronted with multi-view geometric consistency and cross-view correspondence.\nTo comprehensively evaluate the challenges of MLLMs in multi-view scene\nreasoning, we propose All-Angles Bench, a benchmark of over 2,100 human\ncarefully annotated multi-view question-answer pairs across 90 diverse\nreal-world scenes. Our six tasks (counting, attribute identification, relative\ndistance, relative direction, object manipulation, and camera pose estimation)\nspecifically test model's geometric correspondence and the capacity to align\ninformation consistently across views. Our extensive experiments, benchmark on\n27 representative MLLMs including Gemini-2.0-Flash, Claude-3.7-Sonnet, and\nGPT-4o against human evaluators reveals a substantial performance gap,\nindicating that current MLLMs remain far from human-level proficiency. Through\nin-depth analysis, we show that MLLMs are particularly underperforming under\ntwo aspects: (1) cross-view correspondence for partially occluded views and (2)\nestablishing the coarse camera poses. These findings highlight the necessity of\ndomain-specific refinements or modules that embed stronger multi-view\nawareness. We believe that our All-Angles Bench offers valuable insights and\ncontribute to bridging the gap between MLLMs and human-level multi-view\nunderstanding. The project and benchmark are publicly available at\nhttps://danielchyeh.github.io/All-Angles-Bench/."}
{"id": "2504.14411", "pdf": "https://arxiv.org/pdf/2504.14411", "abs": "https://arxiv.org/abs/2504.14411", "authors": ["Xiang Zhang", "Yongfeng Zhang"], "title": "Planet as a Brain: Towards Internet of AgentSites based on AIOS Server", "categories": ["cs.NI", "cs.AI"], "comment": null, "summary": "The internet is undergoing a historical transformation from the \"Internet of\nWebsites\" to the \"Internet of AgentSites.\" While traditional Websites served as\nthe foundation for information hosting and dissemination, a new frontier is\nemerging where AgentSites serve as the hubs of the internet, where each\nAgentSite hosts one or more AI agents that receive tasks, address them, and\ndeliver actionable solutions, marking a significant shift in the digital\nlandscape and representing the next generation of online ecosystems. Under this\nvision, AIOS, the AI Agent Operating System, serves as the server for the\ndevelopment, deployment and execution of AI agents, which is a fundamental\ninfrastructure for the Internet of Agentsites.\n  In this paper, we introduce AIOS Server, a runtime framework to host agents\nand enable global-scale collaboration among decentralized agents. AIOS Server\nprovides a communication protocol leveraging the Model Context Protocol (MCP)\nand JSON-RPC to enable agent-agent or human-agent interactions. Each AIOS node\noperates as a server to host and execute agents, while supporting peer-to-peer\ncoordination without reliance on centralized orchestration. Based on AIOS\nServer, we further present the world's first practically deployed Internet of\nAgentsites (AIOS-IoA), including AgentHub for agent registration and discovery\nand AgentChat for interactive communication, at https://planet.aios.foundation.\nThe agent discovery mechanism based on Distributed Hash Tables (DHT) and a\nGossip protocol serves as the search engine for the internet of agentsites.\nThis work provides a practical foundation for building the Internet of\nAgentsites-a new paradigm where autonomous agents become first-class citizens\nof the web. The implementation is available at\nhttps://github.com/agiresearch/AIOS.Server and will be integrated into the AIOS\nmain branch at https://github.com/agiresearch/AIOS."}
{"id": "2504.15281", "pdf": "https://arxiv.org/pdf/2504.15281", "abs": "https://arxiv.org/abs/2504.15281", "authors": ["Cailin Zhuang", "Yaoqi Hu", "Xuanyang Zhang", "Wei Cheng", "Jiacheng Bao", "Shengqi Liu", "Yiying Yang", "Xianfang Zeng", "Gang Yu", "Ming Li"], "title": "StyleMe3D: Stylization with Disentangled Priors by Multiple Encoders on 3D Gaussians", "categories": ["cs.CV"], "comment": "16 pages; Project page: https://styleme3d.github.io/", "summary": "3D Gaussian Splatting (3DGS) excels in photorealistic scene reconstruction\nbut struggles with stylized scenarios (e.g., cartoons, games) due to fragmented\ntextures, semantic misalignment, and limited adaptability to abstract\naesthetics. We propose StyleMe3D, a holistic framework for 3D GS style transfer\nthat integrates multi-modal style conditioning, multi-level semantic alignment,\nand perceptual quality enhancement. Our key insights include: (1) optimizing\nonly RGB attributes preserves geometric integrity during stylization; (2)\ndisentangling low-, medium-, and high-level semantics is critical for coherent\nstyle transfer; (3) scalability across isolated objects and complex scenes is\nessential for practical deployment. StyleMe3D introduces four novel components:\nDynamic Style Score Distillation (DSSD), leveraging Stable Diffusion's latent\nspace for semantic alignment; Contrastive Style Descriptor (CSD) for localized,\ncontent-aware texture transfer; Simultaneously Optimized Scale (SOS) to\ndecouple style details and structural coherence; and 3D Gaussian Quality\nAssessment (3DG-QA), a differentiable aesthetic prior trained on human-rated\ndata to suppress artifacts and enhance visual harmony. Evaluated on NeRF\nsynthetic dataset (objects) and tandt db (scenes) datasets, StyleMe3D\noutperforms state-of-the-art methods in preserving geometric details (e.g.,\ncarvings on sculptures) and ensuring stylistic consistency across scenes (e.g.,\ncoherent lighting in landscapes), while maintaining real-time rendering. This\nwork bridges photorealistic 3D GS and artistic stylization, unlocking\napplications in gaming, virtual worlds, and digital art."}
{"id": "2504.14423", "pdf": "https://arxiv.org/pdf/2504.14423", "abs": "https://arxiv.org/abs/2504.14423", "authors": ["Qiang Chen", "Xiao Wang", "Haowen Wang", "Bo Jiang", "Lin Zhu", "Dawei Zhang", "Yonghong Tian", "Jin Tang"], "title": "Adversarial Attack for RGB-Event based Visual Object Tracking", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Visual object tracking is a crucial research topic in the fields of computer\nvision and multi-modal fusion. Among various approaches, robust visual tracking\nthat combines RGB frames with Event streams has attracted increasing attention\nfrom researchers. While striving for high accuracy and efficiency in tracking,\nit is also important to explore how to effectively conduct adversarial attacks\nand defenses on RGB-Event stream tracking algorithms, yet research in this area\nremains relatively scarce. To bridge this gap, in this paper, we propose a\ncross-modal adversarial attack algorithm for RGB-Event visual tracking. Because\nof the diverse representations of Event streams, and given that Event voxels\nand frames are more commonly used, this paper will focus on these two\nrepresentations for an in-depth study. Specifically, for the RGB-Event voxel,\nwe first optimize the perturbation by adversarial loss to generate RGB frame\nadversarial examples. For discrete Event voxel representations, we propose a\ntwo-step attack strategy, more in detail, we first inject Event voxels into the\ntarget region as initialized adversarial examples, then, conduct a\ngradient-guided optimization by perturbing the spatial location of the Event\nvoxels. For the RGB-Event frame based tracking, we optimize the cross-modal\nuniversal perturbation by integrating the gradient information from multimodal\ndata. We evaluate the proposed approach against attacks on three widely used\nRGB-Event Tracking datasets, i.e., COESOT, FE108, and VisEvent. Extensive\nexperiments show that our method significantly reduces the performance of the\ntracker across numerous datasets in both unimodal and multimodal scenarios. The\nsource code will be released on\nhttps://github.com/Event-AHU/Adversarial_Attack_Defense"}
{"id": "2504.13865", "pdf": "https://arxiv.org/pdf/2504.13865", "abs": "https://arxiv.org/abs/2504.13865", "authors": ["Fei Tang", "Haolei Xu", "Hang Zhang", "Siqi Chen", "Xingyu Wu", "Yongliang Shen", "Wenqi Zhang", "Guiyang Hou", "Zeqi Tan", "Yuchen Yan", "Kaitao Song", "Jian Shao", "Weiming Lu", "Jun Xiao", "Yueting Zhuang"], "title": "A Survey on (M)LLM-Based GUI Agents", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Graphical User Interface (GUI) Agents have emerged as a transformative\nparadigm in human-computer interaction, evolving from rule-based automation\nscripts to sophisticated AI-driven systems capable of understanding and\nexecuting complex interface operations. This survey provides a comprehensive\nexamination of the rapidly advancing field of LLM-based GUI Agents,\nsystematically analyzing their architectural foundations, technical components,\nand evaluation methodologies. We identify and analyze four fundamental\ncomponents that constitute modern GUI Agents: (1) perception systems that\nintegrate text-based parsing with multimodal understanding for comprehensive\ninterface comprehension; (2) exploration mechanisms that construct and maintain\nknowledge bases through internal modeling, historical experience, and external\ninformation retrieval; (3) planning frameworks that leverage advanced reasoning\nmethodologies for task decomposition and execution; and (4) interaction systems\nthat manage action generation with robust safety controls. Through rigorous\nanalysis of these components, we reveal how recent advances in large language\nmodels and multimodal learning have revolutionized GUI automation across\ndesktop, mobile, and web platforms. We critically examine current evaluation\nframeworks, highlighting methodological limitations in existing benchmarks\nwhile proposing directions for standardization. This survey also identifies key\ntechnical challenges, including accurate element localization, effective\nknowledge retrieval, long-horizon planning, and safety-aware execution control,\nwhile outlining promising research directions for enhancing GUI Agents'\ncapabilities. Our systematic review provides researchers and practitioners with\na thorough understanding of the field's current state and offers insights into\nfuture developments in intelligent interface automation."}
{"id": "2504.14427", "pdf": "https://arxiv.org/pdf/2504.14427", "abs": "https://arxiv.org/abs/2504.14427", "authors": ["Spencer Lin", "Miru Jun", "Basem Rizk", "Karen Shieh", "Scott Fisher", "Sharon Mozgai"], "title": "Optimizing SIA Development: A Case Study in User-Centered Design for Estuary, a Multimodal Socially Interactive Agent Framework", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "This case study presents our user-centered design model for Socially\nIntelligent Agent (SIA) development frameworks through our experience\ndeveloping Estuary, an open source multimodal framework for building\nlow-latency real-time socially interactive agents. We leverage the Rapid\nAssessment Process (RAP) to collect the thoughts of leading researchers in the\nfield of SIAs regarding the current state of the art for SIA development as\nwell as their evaluation of how well Estuary may potentially address current\nresearch gaps. We achieve this through a series of end-user interviews\nconducted by a fellow researcher in the community. We hope that the findings of\nour work will not only assist the continued development of Estuary but also\nguide the development of other future frameworks and technologies for SIAs."}
{"id": "2504.13866", "pdf": "https://arxiv.org/pdf/2504.13866", "abs": "https://arxiv.org/abs/2504.13866", "authors": ["Aleksa Marusic", "Sao Mai Nguyen", "Adriana Tapus"], "title": "Skeleton-Based Transformer for Classification of Errors and Better Feedback in Low Back Pain Physical Rehabilitation Exercises", "categories": ["cs.HC", "cs.AI", "cs.CV", "cs.RO"], "comment": "ICORR 2025 - 19th IEEE/RAS-EMBS International Conference on\n  Rehabilitation Robotics, INTERNATIONAL CONSORTIUM FOR REHABILITATION\n  ROBOTICS, May 2025, Michigan, USA, United States", "summary": "Physical rehabilitation exercises suggested by healthcare professionals can\nhelp recovery from various musculoskeletal disorders and prevent re-injury.\nHowever, patients' engagement tends to decrease over time without direct\nsupervision, which is why there is a need for an automated monitoring system.\nIn recent years, there has been great progress in quality assessment of\nphysical rehabilitation exercises. Most of them only provide a binary\nclassification if the performance is correct or incorrect, and a few provide a\ncontinuous score. This information is not sufficient for patients to improve\ntheir performance. In this work, we propose an algorithm for error\nclassification of rehabilitation exercises, thus making the first step toward\nmore detailed feedback to patients. We focus on skeleton-based exercise\nassessment, which utilizes human pose estimation to evaluate motion. Inspired\nby recent algorithms for quality assessment during rehabilitation exercises, we\npropose a Transformer-based model for the described classification. Our model\nis inspired by the HyperFormer method for human action recognition, and adapted\nto our problem and dataset. The evaluation is done on the KERAAL dataset, as it\nis the only medical dataset with clear error labels for the exercises, and our\nmodel significantly surpasses state-of-the-art methods. Furthermore, we bridge\nthe gap towards better feedback to the patients by presenting a way to\ncalculate the importance of joints for each exercise."}
{"id": "2504.14429", "pdf": "https://arxiv.org/pdf/2504.14429", "abs": "https://arxiv.org/abs/2504.14429", "authors": ["Ahmad Khalil", "Mahmoud Khalil", "Alioune Ngom"], "title": "ResNetVLLM-2: Addressing ResNetVLLM's Multi-Modal Hallucinations", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have transformed natural language processing\n(NLP) tasks, but they suffer from hallucination, generating plausible yet\nfactually incorrect content. This issue extends to Video-Language Models\n(VideoLLMs), where textual descriptions may inaccurately represent visual\ncontent, resulting in multi-modal hallucinations. In this paper, we address\nhallucination in ResNetVLLM, a video-language model combining ResNet visual\nencoders with LLMs. We introduce a two-step protocol: (1) a faithfulness\ndetection strategy that uses a modified Lynx model to assess semantic alignment\nbetween generated captions and ground-truth video references, and (2) a\nhallucination mitigation strategy using Retrieval-Augmented Generation (RAG)\nwith an ad-hoc knowledge base dynamically constructed during inference. Our\nenhanced model, ResNetVLLM-2, reduces multi-modal hallucinations by\ncross-verifying generated content against external knowledge, improving factual\nconsistency. Evaluation on the ActivityNet-QA benchmark demonstrates a\nsubstantial accuracy increase from 54.8% to 65.3%, highlighting the\neffectiveness of our hallucination detection and mitigation strategies in\nenhancing video-language model reliability."}
{"id": "2504.13884", "pdf": "https://arxiv.org/pdf/2504.13884", "abs": "https://arxiv.org/abs/2504.13884", "authors": ["Karan Taneja", "Anjali Singh", "Ashok K. Goel"], "title": "Towards a Multimodal Document-grounded Conversational AI System for Education", "categories": ["cs.HC", "cs.AI", "cs.CV"], "comment": "15 pages, 4 figures, AIED 2025", "summary": "Multimedia learning using text and images has been shown to improve learning\noutcomes compared to text-only instruction. But conversational AI systems in\neducation predominantly rely on text-based interactions while multimodal\nconversations for multimedia learning remain unexplored. Moreover, deploying\nconversational AI in learning contexts requires grounding in reliable sources\nand verifiability to create trust. We present MuDoC, a Multimodal\nDocument-grounded Conversational AI system based on GPT-4o, that leverages both\ntext and visuals from documents to generate responses interleaved with text and\nimages. Its interface allows verification of AI generated content through\nseamless navigation to the source. We compare MuDoC to a text-only system to\nexplore differences in learner engagement, trust in AI system, and their\nperformance on problem-solving tasks. Our findings indicate that both visuals\nand verifiability of content enhance learner engagement and foster trust;\nhowever, no significant impact in performance was observed. We draw upon\ntheories from cognitive and learning sciences to interpret the findings and\nderive implications, and outline future directions for the development of\nmultimodal conversational AI systems in education."}
{"id": "2504.14432", "pdf": "https://arxiv.org/pdf/2504.14432", "abs": "https://arxiv.org/abs/2504.14432", "authors": ["Ahmad Khalil", "Mahmoud Khalil", "Alioune Ngom"], "title": "ResNetVLLM -- Multi-modal Vision LLM for the Video Understanding Task", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In this paper, we introduce ResNetVLLM (ResNet Vision LLM), a novel\ncross-modal framework for zero-shot video understanding that integrates a\nResNet-based visual encoder with a Large Language Model (LLM. ResNetVLLM\naddresses the challenges associated with zero-shot video models by avoiding\nreliance on pre-trained video understanding models and instead employing a\nnon-pretrained ResNet to extract visual features. This design ensures the model\nlearns visual and semantic representations within a unified architecture,\nenhancing its ability to generate accurate and contextually relevant textual\ndescriptions from video inputs. Our experimental results demonstrate that\nResNetVLLM achieves state-of-the-art performance in zero-shot video\nunderstanding (ZSVU) on several benchmarks, including MSRVTT-QA, MSVD-QA,\nTGIF-QA FrameQA, and ActivityNet-QA."}
{"id": "2504.13975", "pdf": "https://arxiv.org/pdf/2504.13975", "abs": "https://arxiv.org/abs/2504.13975", "authors": ["Mehmet Yamaç", "Muhammad Numan Yousaf", "Serkan Kiranyaz", "Moncef Gabbouj"], "title": "Multiscale Tensor Summation Factorization as a New Neural Network Layer (MTS Layer) for Multidimensional Data Processing", "categories": ["cs.LG", "cs.AI", "cs.CV", "68T07"], "comment": "13 pages", "summary": "Multilayer perceptrons (MLP), or fully connected artificial neural networks,\nare known for performing vector-matrix multiplications using learnable weight\nmatrices; however, their practical application in many machine learning tasks,\nespecially in computer vision, can be limited due to the high dimensionality of\ninput-output pairs at each layer. To improve efficiency, convolutional\noperators have been utilized to facilitate weight sharing and local\nconnections, yet they are constrained by limited receptive fields. In this\npaper, we introduce Multiscale Tensor Summation (MTS) Factorization, a novel\nneural network operator that implements tensor summation at multiple scales,\nwhere each tensor to be summed is obtained through Tucker-decomposition-like\nmode products. Unlike other tensor decomposition methods in the literature, MTS\nis not introduced as a network compression tool; instead, as a new backbone\nneural layer. MTS not only reduces the number of parameters required while\nenhancing the efficiency of weight optimization compared to traditional dense\nlayers (i.e., unfactorized weight matrices in MLP layers), but it also\ndemonstrates clear advantages over convolutional layers. The proof-of-concept\nexperimental comparison of the proposed MTS networks with MLPs and\nConvolutional Neural Networks (CNNs) demonstrates their effectiveness across\nvarious tasks, such as classification, compression, and signal restoration.\nAdditionally, when integrated with modern non-linear units such as the\nmulti-head gate (MHG), also introduced in this study, the corresponding neural\nnetwork, MTSNet, demonstrates a more favorable complexity-performance tradeoff\ncompared to state-of-the-art transformers in various computer vision\napplications. The software implementation of the MTS layer and the\ncorresponding MTS-based networks, MTSNets, is shared at\nhttps://github.com/mehmetyamac/MTSNet."}
{"id": "2504.14439", "pdf": "https://arxiv.org/pdf/2504.14439", "abs": "https://arxiv.org/abs/2504.14439", "authors": ["Avinandan Bose", "Zhihan Xiong", "Yuejie Chi", "Simon Shaolei Du", "Lin Xiao", "Maryam Fazel"], "title": "LoRe: Personalizing LLMs via Low-Rank Reward Modeling", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Personalizing large language models (LLMs) to accommodate diverse user\npreferences is essential for enhancing alignment and user satisfaction.\nTraditional reinforcement learning from human feedback (RLHF) approaches often\nrely on monolithic value representations, limiting their ability to adapt to\nindividual preferences. We introduce a novel framework that leverages low-rank\npreference modeling to efficiently learn and generalize user-specific reward\nfunctions. By representing reward functions in a low-dimensional subspace and\nmodeling individual preferences as weighted combinations of shared basis\nfunctions, our approach avoids rigid user categorization while enabling\nscalability and few-shot adaptation. We validate our method on multiple\npreference datasets, demonstrating superior generalization to unseen users and\nimproved accuracy in preference prediction tasks."}
{"id": "2504.14078", "pdf": "https://arxiv.org/pdf/2504.14078", "abs": "https://arxiv.org/abs/2504.14078", "authors": ["M-Mahdi Naddaf-Sh", "Andrew Lee", "Kin Yen", "Eemon Amini", "Iman Soltani"], "title": "Infrared Vision Systems for Emergency Vehicle Driver Assistance in Low-Visibility Conditions", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "This study investigates the potential of infrared (IR) camera technology to\nenhance driver safety for emergency vehicles operating in low-visibility\nconditions, particularly at night and in dense fog. Such environments\nsignificantly increase the risk of collisions, especially for tow trucks and\nsnowplows that must remain operational in challenging conditions. Conventional\ndriver assistance systems often struggle under these conditions due to limited\nvisibility. In contrast, IR cameras, which detect the thermal signatures of\nobstacles, offer a promising alternative. The evaluation combines controlled\nlaboratory experiments, real-world field tests, and surveys of emergency\nvehicle operators. In addition to assessing detection performance, the study\nexamines the feasibility of retrofitting existing Department of Transportation\n(DoT) fleets with cost-effective IR-based driver assistance systems. Results\nunderscore the utility of IR technology in enhancing driver awareness and\nprovide data-driven recommendations for scalable deployment across legacy\nemergency vehicle fleets."}
{"id": "2504.14452", "pdf": "https://arxiv.org/pdf/2504.14452", "abs": "https://arxiv.org/abs/2504.14452", "authors": ["Tong Chen", "Faeze Brahman", "Jiacheng Liu", "Niloofar Mireshghallah", "Weijia Shi", "Pang Wei Koh", "Luke Zettlemoyer", "Hannaneh Hajishirzi"], "title": "ParaPO: Aligning Language Models to Reduce Verbatim Reproduction of Pre-training Data", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Language models (LMs) can memorize and reproduce segments from their\npretraining data verbatim even in non-adversarial settings, raising concerns\nabout copyright, plagiarism, privacy, and creativity. We introduce Paraphrase\nPreference Optimization (ParaPO), a post-training method that fine-tunes LMs to\nreduce unintentional regurgitation while preserving their overall utility.\nParaPO trains LMs to prefer paraphrased versions of memorized segments over the\noriginal verbatim content from the pretraining data. To maintain the ability to\nrecall famous quotations when appropriate, we develop a variant of ParaPO that\nuses system prompts to control regurgitation behavior. In our evaluation on\nLlama3.1-8B, ParaPO consistently reduces regurgitation across all tested\ndatasets (e.g., reducing the regurgitation metric from 17.3 to 12.9 in creative\nwriting), whereas unlearning methods used in prior work to mitigate\nregurgitation are less effective outside their targeted unlearned domain (from\n17.3 to 16.9). When applied to the instruction-tuned Tulu3-8B model, ParaPO\nwith system prompting successfully preserves famous quotation recall while\nreducing unintentional regurgitation (from 8.7 to 6.3 in creative writing) when\nprompted not to regurgitate. In contrast, without ParaPO tuning, prompting the\nmodel not to regurgitate produces only a marginal reduction (8.7 to 8.4)."}
{"id": "2504.14117", "pdf": "https://arxiv.org/pdf/2504.14117", "abs": "https://arxiv.org/abs/2504.14117", "authors": ["Nusrat Jahan Prottasha", "Upama Roy Chowdhury", "Shetu Mohanto", "Tasfia Nuzhat", "Abdullah As Sami", "Md Shamol Ali", "Md Shohanur Islam Sobuj", "Hafijur Raman", "Md Kowsher", "Ozlem Ozmen Garibay"], "title": "PEFT A2Z: Parameter-Efficient Fine-Tuning Survey for Large Language and Vision Models", "categories": ["cs.CL", "cs.CV"], "comment": "PEFT Survey paper", "summary": "Large models such as Large Language Models (LLMs) and Vision Language Models\n(VLMs) have transformed artificial intelligence, powering applications in\nnatural language processing, computer vision, and multimodal learning. However,\nfully fine-tuning these models remains expensive, requiring extensive\ncomputational resources, memory, and task-specific data. Parameter-Efficient\nFine-Tuning (PEFT) has emerged as a promising solution that allows adapting\nlarge models to downstream tasks by updating only a small portion of\nparameters. This survey presents a comprehensive overview of PEFT techniques,\nfocusing on their motivations, design principles, and effectiveness. We begin\nby analyzing the resource and accessibility challenges posed by traditional\nfine-tuning and highlight key issues, such as overfitting, catastrophic\nforgetting, and parameter inefficiency. We then introduce a structured taxonomy\nof PEFT methods -- grouped into additive, selective, reparameterized, hybrid,\nand unified frameworks -- and systematically compare their mechanisms and\ntrade-offs. Beyond taxonomy, we explore the impact of PEFT across diverse\ndomains, including language, vision, and generative modeling, showing how these\ntechniques offer strong performance with lower resource costs. We also discuss\nimportant open challenges in scalability, interpretability, and robustness, and\nsuggest future directions such as federated learning, domain adaptation, and\ntheoretical grounding. Our goal is to provide a unified understanding of PEFT\nand its growing role in enabling practical, efficient, and sustainable use of\nlarge models."}
{"id": "2504.14493", "pdf": "https://arxiv.org/pdf/2504.14493", "abs": "https://arxiv.org/abs/2504.14493", "authors": ["Xinyu Wang", "Jijun Chi", "Zhenghan Tai", "Tung Sum Thomas Kwok", "Muzhi Li", "Zhuhong Li", "Hailin He", "Yuchen Hua", "Peng Lu", "Suyuchen Wang", "Yihong Wu", "Jerry Huang", "Ling Zhou"], "title": "FinSage: A Multi-aspect RAG System for Financial Filings Question Answering", "categories": ["cs.IR", "cs.AI", "cs.LG"], "comment": null, "summary": "Leveraging large language models in real-world settings often entails a need\nto utilize domain-specific data and tools in order to follow the complex\nregulations that need to be followed for acceptable use. Within financial\nsectors, modern enterprises increasingly rely on Retrieval-Augmented Generation\n(RAG) systems to address complex compliance requirements in financial document\nworkflows. However, existing solutions struggle to account for the inherent\nheterogeneity of data (e.g., text, tables, diagrams) and evolving nature of\nregulatory standards used in financial filings, leading to compromised accuracy\nin critical information extraction. We propose the FinSage framework as a\nsolution, utilizing a multi-aspect RAG framework tailored for regulatory\ncompliance analysis in multi-modal financial documents. FinSage introduces\nthree innovative components: (1) a multi-modal pre-processing pipeline that\nunifies diverse data formats and generates chunk-level metadata summaries, (2)\na multi-path sparse-dense retrieval system augmented with query expansion\n(HyDE) and metadata-aware semantic search, and (3) a domain-specialized\nre-ranking module fine-tuned via Direct Preference Optimization (DPO) to\nprioritize compliance-critical content. Extensive experiments demonstrate that\nFinSage achieves an impressive recall of 92.51% on 75 expert-curated questions\nderived from surpasses the best baseline method on the FinanceBench question\nanswering datasets by 24.06% in accuracy. Moreover, FinSage has been\nsuccessfully deployed as financial question-answering agent in online meetings,\nwhere it has already served more than 1,200 people."}
{"id": "2504.14123", "pdf": "https://arxiv.org/pdf/2504.14123", "abs": "https://arxiv.org/abs/2504.14123", "authors": ["Mingyu Kim", "Jongwoo Ko", "Mijung Park"], "title": "Bayesian Principles Improve Prompt Learning In Vision-Language Models", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "AISTATS2025", "summary": "Prompt learning is a popular fine-tuning method for vision-language models\ndue to its efficiency. It requires a small number of additional learnable\nparameters while significantly enhancing performance on target tasks. However,\nmost existing methods suffer from overfitting to fine-tuning data, yielding\npoor generalizability. To address this, we propose a new training objective\nfunction based on a Bayesian learning principle to balance adaptability and\ngeneralizability. We derive a prior over the logits, where the mean function is\nparameterized by the pre-trained model, while the posterior corresponds to the\nfine-tuned model. This objective establishes a balance by allowing the\nfine-tuned model to adapt to downstream tasks while remaining close to the\npre-trained model."}
{"id": "2504.14494", "pdf": "https://arxiv.org/pdf/2504.14494", "abs": "https://arxiv.org/abs/2504.14494", "authors": ["Yue Li"], "title": "LBM-GNN: Graph Neural Network Enhanced Lattice Boltzmann Method", "categories": ["physics.flu-dyn", "cs.AI"], "comment": null, "summary": "In this paper, we present LBM-GNN, a novel approach that enhances the\ntraditional Lattice Boltzmann Method (LBM) with Graph Neural Networks (GNNs).\nWe apply this method to fluid dynamics simulations, demonstrating improved\nstability and accuracy compared to standard LBM implementations. The method is\nvalidated using benchmark problems such as the Taylor-Green vortex, focusing on\naccuracy, conservation properties, and performance across different Reynolds\nnumbers and grid resolutions. Our results indicate that GNN-enhanced LBM can\nmaintain better conservation properties while improving numerical stability at\nhigher Reynolds numbers."}
{"id": "2504.14135", "pdf": "https://arxiv.org/pdf/2504.14135", "abs": "https://arxiv.org/abs/2504.14135", "authors": ["Jonathan Embley-Riches", "Jianwei Liu", "Simon Julier", "Dimitrios Kanoulas"], "title": "Unreal Robotics Lab: A High-Fidelity Robotics Simulator with Advanced Physics and Rendering", "categories": ["cs.RO", "cs.CV", "cs.GR", "cs.LG"], "comment": null, "summary": "High-fidelity simulation is essential for robotics research, enabling safe\nand efficient testing of perception, control, and navigation algorithms.\nHowever, achieving both photorealistic rendering and accurate physics modeling\nremains a challenge. This paper presents a novel simulation framework--the\nUnreal Robotics Lab (URL) that integrates the Unreal Engine's advanced\nrendering capabilities with MuJoCo's high-precision physics simulation. Our\napproach enables realistic robotic perception while maintaining accurate\nphysical interactions, facilitating benchmarking and dataset generation for\nvision-based robotics applications. The system supports complex environmental\neffects, such as smoke, fire, and water dynamics, which are critical for\nevaluating robotic performance under adverse conditions. We benchmark visual\nnavigation and SLAM methods within our framework, demonstrating its utility for\ntesting real-world robustness in controlled yet diverse scenarios. By bridging\nthe gap between physics accuracy and photorealistic rendering, our framework\nprovides a powerful tool for advancing robotics research and sim-to-real\ntransfer."}
{"id": "2504.14509", "pdf": "https://arxiv.org/pdf/2504.14509", "abs": "https://arxiv.org/abs/2504.14509", "authors": ["Fulong Ye", "Miao Hua", "Pengze Zhang", "Xinghui Li", "Qichao Sun", "Songtao Zhao", "Qian He", "Xinglong Wu"], "title": "DreamID: High-Fidelity and Fast diffusion-based Face Swapping via Triplet ID Group Learning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In this paper, we introduce DreamID, a diffusion-based face swapping model\nthat achieves high levels of ID similarity, attribute preservation, image\nfidelity, and fast inference speed. Unlike the typical face swapping training\nprocess, which often relies on implicit supervision and struggles to achieve\nsatisfactory results. DreamID establishes explicit supervision for face\nswapping by constructing Triplet ID Group data, significantly enhancing\nidentity similarity and attribute preservation. The iterative nature of\ndiffusion models poses challenges for utilizing efficient image-space loss\nfunctions, as performing time-consuming multi-step sampling to obtain the\ngenerated image during training is impractical. To address this issue, we\nleverage the accelerated diffusion model SD Turbo, reducing the inference steps\nto a single iteration, enabling efficient pixel-level end-to-end training with\nexplicit Triplet ID Group supervision. Additionally, we propose an improved\ndiffusion-based model architecture comprising SwapNet, FaceNet, and ID Adapter.\nThis robust architecture fully unlocks the power of the Triplet ID Group\nexplicit supervision. Finally, to further extend our method, we explicitly\nmodify the Triplet ID Group data during training to fine-tune and preserve\nspecific attributes, such as glasses and face shape. Extensive experiments\ndemonstrate that DreamID outperforms state-of-the-art methods in terms of\nidentity similarity, pose and expression preservation, and image fidelity.\nOverall, DreamID achieves high-quality face swapping results at 512*512\nresolution in just 0.6 seconds and performs exceptionally well in challenging\nscenarios such as complex lighting, large angles, and occlusions."}
{"id": "2504.14219", "pdf": "https://arxiv.org/pdf/2504.14219", "abs": "https://arxiv.org/abs/2504.14219", "authors": ["Alara Dirik", "Tuanfeng Wang", "Duygu Ceylan", "Stefanos Zafeiriou", "Anna Frühstück"], "title": "PRISM: A Unified Framework for Photorealistic Reconstruction and Intrinsic Scene Modeling", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "We present PRISM, a unified framework that enables multiple image generation\nand editing tasks in a single foundational model. Starting from a pre-trained\ntext-to-image diffusion model, PRISM proposes an effective fine-tuning strategy\nto produce RGB images along with intrinsic maps (referred to as X layers)\nsimultaneously. Unlike previous approaches, which infer intrinsic properties\nindividually or require separate models for decomposition and conditional\ngeneration, PRISM maintains consistency across modalities by generating all\nintrinsic layers jointly. It supports diverse tasks, including text-to-RGBX\ngeneration, RGB-to-X decomposition, and X-to-RGBX conditional generation.\nAdditionally, PRISM enables both global and local image editing through\nconditioning on selected intrinsic layers and text prompts. Extensive\nexperiments demonstrate the competitive performance of PRISM both for intrinsic\nimage decomposition and conditional image generation while preserving the base\nmodel's text-to-image generation capability."}
{"id": "2504.14514", "pdf": "https://arxiv.org/pdf/2504.14514", "abs": "https://arxiv.org/abs/2504.14514", "authors": ["Daizhan Cheng"], "title": "On Dimension-Free Transformer: An Application of STP to AI", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "The matrix expressions for every parts of a transformer are firstly\ndescribed. Based on semi-tensor product (STP) of matrices the hypervectors are\nreconsidered and the linear transformation over hypervectors is constructed by\nusing projection. Its properties and calculating formulas are obtained. Using\nprojection-based transformation of hypervector (PBTH), the framework of\ndimension-free transformer (DFT) is proposed by verifying each linear\ntransformation in a transformer and replacing it by a proper PBTH, which allows\nthe inputs and outputs being of arbitrary dimensions. Using balanced\ninformation about all entries, DFT must be more efficient in dealing with\nsignals."}
{"id": "2504.14257", "pdf": "https://arxiv.org/pdf/2504.14257", "abs": "https://arxiv.org/abs/2504.14257", "authors": ["Yilin Liu", "Duoteng Xu", "Xingyao Yu", "Xiang Xu", "Daniel Cohen-Or", "Hao Zhang", "Hui Huang"], "title": "HoLa: B-Rep Generation using a Holistic Latent Representation", "categories": ["cs.GR", "cs.CV"], "comment": "ACM TOG and SIGGRAPH 2025 (Patent Protected); Project page:\n  https://vcc.tech/research/2025/HolaBrep", "summary": "We introduce a novel representation for learning and generating\nComputer-Aided Design (CAD) models in the form of $\\textit{boundary\nrepresentations}$ (B-Reps). Our representation unifies the continuous geometric\nproperties of B-Rep primitives in different orders (e.g., surfaces and curves)\nand their discrete topological relations in a $\\textit{holistic latent}$ (HoLa)\nspace. This is based on the simple observation that the topological connection\nbetween two surfaces is intrinsically tied to the geometry of their\nintersecting curve. Such a prior allows us to reformulate topology learning in\nB-Reps as a geometric reconstruction problem in Euclidean space. Specifically,\nwe eliminate the presence of curves, vertices, and all the topological\nconnections in the latent space by learning to distinguish and derive curve\ngeometries from a pair of surface primitives via a neural intersection network.\nTo this end, our holistic latent space is only defined on surfaces but encodes\na full B-Rep model, including the geometry of surfaces, curves, vertices, and\ntheir topological relations. Our compact and holistic latent space facilitates\nthe design of a first diffusion-based generator to take on a large variety of\ninputs including point clouds, single/multi-view images, 2D sketches, and text\nprompts. Our method significantly reduces ambiguities, redundancies, and\nincoherences among the generated B-Rep primitives, as well as training\ncomplexities inherent in prior multi-step B-Rep learning pipelines, while\nachieving greatly improved validity rate over current state of the art: 82% vs.\n$\\approx$50%."}
{"id": "2504.14519", "pdf": "https://arxiv.org/pdf/2504.14519", "abs": "https://arxiv.org/abs/2504.14519", "authors": ["Zhouyang Li", "Yuliang Liu", "Wei Zhang", "Tailing Yuan", "Bin Chen", "Chengru Song", "Di Zhang"], "title": "SlimPipe: Memory-Thrifty and Efficient Pipeline Parallelism for Long-Context LLM Training", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Pipeline Parallelism (PP) serves as a crucial technique for training Large\nLanguage Models (LLMs), owing to its capability to alleviate memory pressure\nfrom model states with relatively low communication overhead. However, in\nlong-context scenarios, existing pipeline parallelism methods fail to address\nthe substantial activation memory pressure, primarily due to the peak memory\nconsumption resulting from the accumulation of activations across multiple\nmicrobatches. Moreover, these approaches inevitably introduce considerable\npipeline bubbles, further hindering efficiency.\n  To tackle these challenges, we propose SlimPipe, a novel approach to\nfine-grained pipeline parallelism that employs uniform sequence slicing coupled\nwith one-forward-one-backward (1F1B) schedule. It reduces the accumulated\nactivations from several microbatches to just one, which is split into several\nslices. Although the slices are evenly partitioned, the computation cost is not\nequal across slices due to causal attention. We develop a sophisticated\nworkload redistribution technique to address this load imbalance. SlimPipe\nachieves (1) near-zero memory overhead and (2) minimal pipeline bubbles\nsimultaneously. The effectiveness of SlimPipe has been proven by thorough\ntesting with diverse model architectures, context window sizes, and\nSlimPipe-specific configurations. For example, on the Llama 70B model, compared\nto state-of-the-art methods, SlimPipe significantly boosts the Model FLOPs\nUtilization (MFU) to up to $1.57\\times$ for a context length of 512K. More\nnotably, for a context length of 2048K, it maintains over 45% utilization on\n256 NVIDIA Hopper 80GB GPUs, while other approaches either suffer significant\nperformance drops or fail entirely due to memory constraints."}
{"id": "2504.14373", "pdf": "https://arxiv.org/pdf/2504.14373", "abs": "https://arxiv.org/abs/2504.14373", "authors": ["Chen Guo", "Zhuo Su", "Jian Wang", "Shuang Li", "Xu Chang", "Zhaohu Li", "Yang Zhao", "Guidong Wang", "Ruqi Huang"], "title": "SEGA: Drivable 3D Gaussian Head Avatar from a Single Image", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Creating photorealistic 3D head avatars from limited input has become\nincreasingly important for applications in virtual reality, telepresence, and\ndigital entertainment. While recent advances like neural rendering and 3D\nGaussian splatting have enabled high-quality digital human avatar creation and\nanimation, most methods rely on multiple images or multi-view inputs, limiting\ntheir practicality for real-world use. In this paper, we propose SEGA, a novel\napproach for Single-imagE-based 3D drivable Gaussian head Avatar creation that\ncombines generalized prior models with a new hierarchical UV-space Gaussian\nSplatting framework. SEGA seamlessly combines priors derived from large-scale\n2D datasets with 3D priors learned from multi-view, multi-expression, and\nmulti-ID data, achieving robust generalization to unseen identities while\nensuring 3D consistency across novel viewpoints and expressions. We further\npresent a hierarchical UV-space Gaussian Splatting framework that leverages\nFLAME-based structural priors and employs a dual-branch architecture to\ndisentangle dynamic and static facial components effectively. The dynamic\nbranch encodes expression-driven fine details, while the static branch focuses\non expression-invariant regions, enabling efficient parameter inference and\nprecomputation. This design maximizes the utility of limited 3D data and\nachieves real-time performance for animation and rendering. Additionally, SEGA\nperforms person-specific fine-tuning to further enhance the fidelity and\nrealism of the generated avatars. Experiments show our method outperforms\nstate-of-the-art approaches in generalization ability, identity preservation,\nand expression realism, advancing one-shot avatar creation for practical\napplications."}
{"id": "2504.14522", "pdf": "https://arxiv.org/pdf/2504.14522", "abs": "https://arxiv.org/abs/2504.14522", "authors": ["Liudmila Zavolokina", "Kilian Sprenkamp", "Zoya Katashinskaya", "Daniel Gordon Jones"], "title": "Biased by Design: Leveraging AI Biases to Enhance Critical Thinking of News Readers", "categories": ["cs.HC", "cs.AI"], "comment": "European Conference on Information Systems (ECIS)", "summary": "This paper explores the design of a propaganda detection tool using Large\nLanguage Models (LLMs). Acknowledging the inherent biases in AI models,\nespecially in political contexts, we investigate how these biases might be\nleveraged to enhance critical thinking in news consumption. Countering the\ntypical view of AI biases as detrimental, our research proposes strategies of\nuser choice and personalization in response to a user's political stance,\napplying psychological concepts of confirmation bias and cognitive dissonance.\nWe present findings from a qualitative user study, offering insights and design\nrecommendations (bias awareness, personalization and choice, and gradual\nintroduction of diverse perspectives) for AI tools in propaganda detection."}
{"id": "2504.14409", "pdf": "https://arxiv.org/pdf/2504.14409", "abs": "https://arxiv.org/abs/2504.14409", "authors": ["Christopher Ick", "Gordon Wichern", "Yoshiki Masuyama", "François G. Germain", "Jonathan Le Roux"], "title": "Data Augmentation Using Neural Acoustic Fields With Retrieval-Augmented Pre-training", "categories": ["eess.AS", "cs.AI", "cs.CV", "cs.LG", "cs.SD"], "comment": "Presented at ICASSP 2025 GenDA Workshop", "summary": "This report details MERL's system for room impulse response (RIR) estimation\nsubmitted to the Generative Data Augmentation Workshop at ICASSP 2025 for\nAugmenting RIR Data (Task 1) and Improving Speaker Distance Estimation (Task\n2). We first pre-train a neural acoustic field conditioned by room geometry on\nan external large-scale dataset in which pairs of RIRs and the geometries are\nprovided. The neural acoustic field is then adapted to each target room by\nusing the enrollment data, where we leverage either the provided room\ngeometries or geometries retrieved from the external dataset, depending on\navailability. Lastly, we predict the RIRs for each pair of source and receiver\nlocations specified by Task 1, and use these RIRs to train the speaker distance\nestimation model in Task 2."}
{"id": "2504.14530", "pdf": "https://arxiv.org/pdf/2504.14530", "abs": "https://arxiv.org/abs/2504.14530", "authors": ["Zhijing Jin"], "title": "Causality for Natural Language Processing", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": "PhD Thesis 2024", "summary": "Causal reasoning is a cornerstone of human intelligence and a critical\ncapability for artificial systems aiming to achieve advanced understanding and\ndecision-making. This thesis delves into various dimensions of causal reasoning\nand understanding in large language models (LLMs). It encompasses a series of\nstudies that explore the causal inference skills of LLMs, the mechanisms behind\ntheir performance, and the implications of causal and anticausal learning for\nnatural language processing (NLP) tasks. Additionally, it investigates the\napplication of causal reasoning in text-based computational social science,\nspecifically focusing on political decision-making and the evaluation of\nscientific impact through citations. Through novel datasets, benchmark tasks,\nand methodological frameworks, this work identifies key challenges and\nopportunities to improve the causal capabilities of LLMs, providing a\ncomprehensive foundation for future research in this evolving field."}
{"id": "2504.14440", "pdf": "https://arxiv.org/pdf/2504.14440", "abs": "https://arxiv.org/abs/2504.14440", "authors": ["Chuhao Liu", "Zhijian Qiao", "Jieqi Shi", "Ke Wang", "Peize Liu", "Shaojie Shen"], "title": "SG-Reg: Generalizable and Efficient Scene Graph Registration", "categories": ["cs.RO", "cs.CV"], "comment": "IEEE Transactions Robotics Regular Paper", "summary": "This paper addresses the challenges of registering two rigid semantic scene\ngraphs, an essential capability when an autonomous agent needs to register its\nmap against a remote agent, or against a prior map. The hand-crafted\ndescriptors in classical semantic-aided registration, or the ground-truth\nannotation reliance in learning-based scene graph registration, impede their\napplication in practical real-world environments. To address the challenges, we\ndesign a scene graph network to encode multiple modalities of semantic nodes:\nopen-set semantic feature, local topology with spatial awareness, and shape\nfeature. These modalities are fused to create compact semantic node features.\nThe matching layers then search for correspondences in a coarse-to-fine manner.\nIn the back-end, we employ a robust pose estimator to decide transformation\naccording to the correspondences. We manage to maintain a sparse and\nhierarchical scene representation. Our approach demands fewer GPU resources and\nfewer communication bandwidth in multi-agent tasks. Moreover, we design a new\ndata generation approach using vision foundation models and a semantic mapping\nmodule to reconstruct semantic scene graphs. It differs significantly from\nprevious works, which rely on ground-truth semantic annotations to generate\ndata. We validate our method in a two-agent SLAM benchmark. It significantly\noutperforms the hand-crafted baseline in terms of registration success rate.\nCompared to visual loop closure networks, our method achieves a slightly higher\nregistration recall while requiring only 52 KB of communication bandwidth for\neach query frame. Code available at:\n\\href{http://github.com/HKUST-Aerial-Robotics/SG-Reg}{http://github.com/HKUST-Aerial-Robotics/SG-Reg}."}
{"id": "2504.14548", "pdf": "https://arxiv.org/pdf/2504.14548", "abs": "https://arxiv.org/abs/2504.14548", "authors": ["Lifeng Lin", "Rongfeng Lu", "Quan Chen", "Haofan Ren", "Ming Lu", "Yaoqi Sun", "Chenggang Yan", "Anke Xue"], "title": "VGNC: Reducing the Overfitting of Sparse-view 3DGS via Validation-guided Gaussian Number Control", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages,8 figures", "summary": "Sparse-view 3D reconstruction is a fundamental yet challenging task in\npractical 3D reconstruction applications. Recently, many methods based on the\n3D Gaussian Splatting (3DGS) framework have been proposed to address\nsparse-view 3D reconstruction. Although these methods have made considerable\nadvancements, they still show significant issues with overfitting. To reduce\nthe overfitting, we introduce VGNC, a novel Validation-guided Gaussian Number\nControl (VGNC) approach based on generative novel view synthesis (NVS) models.\nTo the best of our knowledge, this is the first attempt to alleviate the\noverfitting issue of sparse-view 3DGS with generative validation images.\nSpecifically, we first introduce a validation image generation method based on\na generative NVS model. We then propose a Gaussian number control strategy that\nutilizes generated validation images to determine the optimal Gaussian numbers,\nthereby reducing the issue of overfitting. We conducted detailed experiments on\nvarious sparse-view 3DGS baselines and datasets to evaluate the effectiveness\nof VGNC. Extensive experiments show that our approach not only reduces\noverfitting but also improves rendering quality on the test set while\ndecreasing the number of Gaussian points. This reduction lowers storage demands\nand accelerates both training and rendering. The code will be released."}
{"id": "2504.14541", "pdf": "https://arxiv.org/pdf/2504.14541", "abs": "https://arxiv.org/abs/2504.14541", "authors": ["Yi Yu", "Song Xia", "Xun Lin", "Chenqi Kong", "Wenhan Yang", "Shijian Lu", "Yap-Peng Tan", "Alex C. Kot"], "title": "Towards Model Resistant to Transferable Adversarial Examples via Trigger Activation", "categories": ["cs.CR", "cs.CV", "cs.LG"], "comment": "Accepted by IEEE TIFS 2025", "summary": "Adversarial examples, characterized by imperceptible perturbations, pose\nsignificant threats to deep neural networks by misleading their predictions. A\ncritical aspect of these examples is their transferability, allowing them to\ndeceive {unseen} models in black-box scenarios. Despite the widespread\nexploration of defense methods, including those on transferability, they show\nlimitations: inefficient deployment, ineffective defense, and degraded\nperformance on clean images. In this work, we introduce a novel training\nparadigm aimed at enhancing robustness against transferable adversarial\nexamples (TAEs) in a more efficient and effective way. We propose a model that\nexhibits random guessing behavior when presented with clean data\n$\\boldsymbol{x}$ as input, and generates accurate predictions when with\ntriggered data $\\boldsymbol{x}+\\boldsymbol{\\tau}$. Importantly, the trigger\n$\\boldsymbol{\\tau}$ remains constant for all data instances. We refer to these\nmodels as \\textbf{models with trigger activation}. We are surprised to find\nthat these models exhibit certain robustness against TAEs. Through the\nconsideration of first-order gradients, we provide a theoretical analysis of\nthis robustness. Moreover, through the joint optimization of the learnable\ntrigger and the model, we achieve improved robustness to transferable attacks.\nExtensive experiments conducted across diverse datasets, evaluating a variety\nof attacking methods, underscore the effectiveness and superiority of our\napproach."}
{"id": "2504.14560", "pdf": "https://arxiv.org/pdf/2504.14560", "abs": "https://arxiv.org/abs/2504.14560", "authors": ["Haiyan Qin", "Zhiwei Xie", "Jingjing Li", "Liangchen Li", "Xiaotong Feng", "Junzhan Liu", "Wang Kang"], "title": "ReasoningV: Efficient Verilog Code Generation with Adaptive Hybrid Reasoning Model", "categories": ["cs.AR", "cs.AI"], "comment": "9 pages, 4 figures", "summary": "Large Language Models (LLMs) have advanced Verilog code generation\nsignificantly, yet face challenges in data quality, reasoning capabilities, and\ncomputational efficiency. This paper presents ReasoningV, a novel model\nemploying a hybrid reasoning strategy that integrates trained intrinsic\ncapabilities with dynamic inference adaptation for Verilog code generation. Our\nframework introduces three complementary innovations: (1) ReasoningV-5K, a\nhigh-quality dataset of 5,000 functionally verified instances with reasoning\npaths created through multi-dimensional filtering of PyraNet samples; (2) a\ntwo-stage training approach combining parameter-efficient fine-tuning for\nfoundational knowledge with full-parameter optimization for enhanced reasoning;\nand (3) an adaptive reasoning mechanism that dynamically adjusts reasoning\ndepth based on problem complexity, reducing token consumption by up to 75\\%\nwhile preserving performance. Experimental results demonstrate ReasoningV's\neffectiveness with a pass@1 accuracy of 57.8\\% on VerilogEval-human, achieving\nperformance competitive with leading commercial models like Gemini-2.0-flash\n(59.5\\%) and exceeding the previous best open-source model by 10.4 percentage\npoints. ReasoningV offers a more reliable and accessible pathway for advancing\nAI-driven hardware design automation, with our model, data, and code available\nat https://github.com/BUAA-CLab/ReasoningV."}
{"id": "2504.14554", "pdf": "https://arxiv.org/pdf/2504.14554", "abs": "https://arxiv.org/abs/2504.14554", "authors": ["Chongye Guo", "Jinhu Fu", "Junfeng Fang", "Kun Wang", "Guorui Feng"], "title": "REDEditing: Relationship-Driven Precise Backdoor Poisoning on Text-to-Image Diffusion Models", "categories": ["cs.CR", "cs.CV"], "comment": "10 pages, 7 figures", "summary": "The rapid advancement of generative AI highlights the importance of\ntext-to-image (T2I) security, particularly with the threat of backdoor\npoisoning. Timely disclosure and mitigation of security vulnerabilities in T2I\nmodels are crucial for ensuring the safe deployment of generative models. We\nexplore a novel training-free backdoor poisoning paradigm through model\nediting, which is recently employed for knowledge updating in large language\nmodels. Nevertheless, we reveal the potential security risks posed by model\nediting techniques to image generation models. In this work, we establish the\nprinciples for backdoor attacks based on model editing, and propose a\nrelationship-driven precise backdoor poisoning method, REDEditing. Drawing on\nthe principles of equivalent-attribute alignment and stealthy poisoning, we\ndevelop an equivalent relationship retrieval and joint-attribute transfer\napproach that ensures consistent backdoor image generation through concept\nrebinding. A knowledge isolation constraint is proposed to preserve benign\ngeneration integrity. Our method achieves an 11\\% higher attack success rate\ncompared to state-of-the-art approaches. Remarkably, adding just one line of\ncode enhances output naturalness while improving backdoor stealthiness by 24\\%.\nThis work aims to heighten awareness regarding this security vulnerability in\neditable image generation models."}
{"id": "2504.14569", "pdf": "https://arxiv.org/pdf/2504.14569", "abs": "https://arxiv.org/abs/2504.14569", "authors": ["Lawrence Liu", "Inesh Chakrabarti", "Yixiao Li", "Mengdi Wang", "Tuo Zhao", "Lin F. Yang"], "title": "NoWag: A Unified Framework for Shape Preserving Compression of Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) exhibit remarkable performance across various\nnatural language processing tasks but suffer from immense computational and\nmemory demands, limiting their deployment in resource-constrained environments.\nTo address this challenge, we propose NoWag: (Normalized Weight and Activation\nGuided Compression), a unified framework for zero-shot shape preserving\ncompression algorithms. We compressed Llama-2 7B/13B/70B and Llama-3 8/70BB\nmodels, using two popular forms of shape-preserving compression, vector\nquantization NoWag-VQ (NoWag for Vector Quantization), and\nunstructured/semi-structured pruning NoWag-P (NoWag for Pruning). We found that\nNoWag-VQ significantly outperforms state-of-the-art zero shot VQ, and that\nNoWag-P performs competitively against state-of-the-art methods. These results\nsuggest commonalities between these compression paradigms that could inspire\nfuture work. Our code is available at https://github.com/LawrenceRLiu/NoWag"}
{"id": "2504.14588", "pdf": "https://arxiv.org/pdf/2504.14588", "abs": "https://arxiv.org/abs/2504.14588", "authors": ["Wenke Xia", "Ruoxuan Feng", "Dong Wang", "Di Hu"], "title": "Phoenix: A Motion-based Self-Reflection Framework for Fine-grained Robotic Action Correction", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Building a generalizable self-correction system is crucial for robots to\nrecover from failures. Despite advancements in Multimodal Large Language Models\n(MLLMs) that empower robots with semantic reflection ability for failure,\ntranslating semantic reflection into how to correct fine-grained robotic\nactions remains a significant challenge. To address this gap, we build the\nPhoenix framework, which leverages motion instruction as a bridge to connect\nhigh-level semantic reflection with low-level robotic action correction. In\nthis motion-based self-reflection framework, we start with a dual-process\nmotion adjustment mechanism with MLLMs to translate the semantic reflection\ninto coarse-grained motion instruction adjustment. To leverage this motion\ninstruction for guiding how to correct fine-grained robotic actions, a\nmulti-task motion-conditioned diffusion policy is proposed to integrate visual\nobservations for high-frequency robotic action correction. By combining these\ntwo models, we could shift the demand for generalization capability from the\nlow-level manipulation policy to the MLLMs-driven motion adjustment model and\nfacilitate precise, fine-grained robotic action correction. Utilizing this\nframework, we further develop a lifelong learning method to automatically\nimprove the model's capability from interactions with dynamic environments. The\nexperiments conducted in both the RoboMimic simulation and real-world scenarios\nprove the superior generalization and robustness of our framework across a\nvariety of manipulation tasks. Our code is released at\n\\href{https://github.com/GeWu-Lab/Motion-based-Self-Reflection-Framework}{https://github.com/GeWu-Lab/Motion-based-Self-Reflection-Framework}."}
{"id": "2504.14573", "pdf": "https://arxiv.org/pdf/2504.14573", "abs": "https://arxiv.org/abs/2504.14573", "authors": ["Jiawei Jiang", "Kei Ota", "Devesh K. Jha", "Asako Kanezaki"], "title": "Modality Selection and Skill Segmentation via Cross-Modality Attention", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Incorporating additional sensory modalities such as tactile and audio into\nfoundational robotic models poses significant challenges due to the curse of\ndimensionality. This work addresses this issue through modality selection. We\npropose a cross-modality attention (CMA) mechanism to identify and selectively\nutilize the modalities that are most informative for action generation at each\ntimestep. Furthermore, we extend the application of CMA to segment primitive\nskills from expert demonstrations and leverage this segmentation to train a\nhierarchical policy capable of solving long-horizon, contact-rich manipulation\ntasks."}
{"id": "2504.14634", "pdf": "https://arxiv.org/pdf/2504.14634", "abs": "https://arxiv.org/abs/2504.14634", "authors": ["Sahara Sheikholeslami", "Ladislau Bölöni"], "title": "Latent Representations for Visual Proprioception in Inexpensive Robots", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Robotic manipulation requires explicit or implicit knowledge of the robot's\njoint positions. Precise proprioception is standard in high-quality industrial\nrobots but is often unavailable in inexpensive robots operating in unstructured\nenvironments. In this paper, we ask: to what extent can a fast, single-pass\nregression architecture perform visual proprioception from a single external\ncamera image, available even in the simplest manipulation settings? We explore\nseveral latent representations, including CNNs, VAEs, ViTs, and bags of\nuncalibrated fiducial markers, using fine-tuning techniques adapted to the\nlimited data available. We evaluate the achievable accuracy through experiments\non an inexpensive 6-DoF robot."}
{"id": "2504.14588", "pdf": "https://arxiv.org/pdf/2504.14588", "abs": "https://arxiv.org/abs/2504.14588", "authors": ["Wenke Xia", "Ruoxuan Feng", "Dong Wang", "Di Hu"], "title": "Phoenix: A Motion-based Self-Reflection Framework for Fine-grained Robotic Action Correction", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Building a generalizable self-correction system is crucial for robots to\nrecover from failures. Despite advancements in Multimodal Large Language Models\n(MLLMs) that empower robots with semantic reflection ability for failure,\ntranslating semantic reflection into how to correct fine-grained robotic\nactions remains a significant challenge. To address this gap, we build the\nPhoenix framework, which leverages motion instruction as a bridge to connect\nhigh-level semantic reflection with low-level robotic action correction. In\nthis motion-based self-reflection framework, we start with a dual-process\nmotion adjustment mechanism with MLLMs to translate the semantic reflection\ninto coarse-grained motion instruction adjustment. To leverage this motion\ninstruction for guiding how to correct fine-grained robotic actions, a\nmulti-task motion-conditioned diffusion policy is proposed to integrate visual\nobservations for high-frequency robotic action correction. By combining these\ntwo models, we could shift the demand for generalization capability from the\nlow-level manipulation policy to the MLLMs-driven motion adjustment model and\nfacilitate precise, fine-grained robotic action correction. Utilizing this\nframework, we further develop a lifelong learning method to automatically\nimprove the model's capability from interactions with dynamic environments. The\nexperiments conducted in both the RoboMimic simulation and real-world scenarios\nprove the superior generalization and robustness of our framework across a\nvariety of manipulation tasks. Our code is released at\n\\href{https://github.com/GeWu-Lab/Motion-based-Self-Reflection-Framework}{https://github.com/GeWu-Lab/Motion-based-Self-Reflection-Framework}."}
{"id": "2504.14662", "pdf": "https://arxiv.org/pdf/2504.14662", "abs": "https://arxiv.org/abs/2504.14662", "authors": ["Yeoreum Lee", "Jinwook Jung", "Sungyong Baik"], "title": "Mitigating Parameter Interference in Model Merging via Sharpness-Aware Fine-Tuning", "categories": ["cs.LG", "cs.CV"], "comment": "ICLR 2025", "summary": "Large-scale deep learning models with a pretraining-finetuning paradigm have\nled to a surge of numerous task-specific models fine-tuned from a common\npre-trained model. Recently, several research efforts have been made on merging\nthese large models into a single multi-task model, particularly with simple\narithmetic on parameters. Such merging methodology faces a central challenge:\ninterference between model parameters fine-tuned on different tasks. Few recent\nworks have focused on designing a new fine-tuning scheme that can lead to small\nparameter interference, however at the cost of the performance of each\ntask-specific fine-tuned model and thereby limiting that of a merged model. To\nimprove the performance of a merged model, we note that a fine-tuning scheme\nshould aim for (1) smaller parameter interference and (2) better performance of\neach fine-tuned model on the corresponding task. In this work, we aim to design\na new fine-tuning objective function to work towards these two goals. In the\ncourse of this process, we find such objective function to be strikingly\nsimilar to sharpness-aware minimization (SAM) objective function, which aims to\nachieve generalization by finding flat minima. Drawing upon our observation, we\npropose to fine-tune pre-trained models via sharpness-aware minimization. The\nexperimental and theoretical results showcase the effectiveness and\northogonality of our proposed approach, improving performance upon various\nmerging and fine-tuning methods. Our code is available at\nhttps://github.com/baiklab/SAFT-Merge."}
{"id": "2504.14594", "pdf": "https://arxiv.org/pdf/2504.14594", "abs": "https://arxiv.org/abs/2504.14594", "authors": ["Fan Gao", "Xinjie Zhao", "Ding Xia", "Zhongyi Zhou", "Rui Yang", "Jinghui Lu", "Hang Jiang", "Chanjun Park", "Irene Li"], "title": "HealthGenie: Empowering Users with Healthy Dietary Guidance through Knowledge Graph and Large Language Models", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Seeking dietary guidance often requires navigating complex professional\nknowledge while accommodating individual health conditions. Knowledge Graphs\n(KGs) offer structured and interpretable nutritional information, whereas Large\nLanguage Models (LLMs) naturally facilitate conversational recommendation\ndelivery. In this paper, we present HealthGenie, an interactive system that\ncombines the strengths of LLMs and KGs to provide personalized dietary\nrecommendations along with hierarchical information visualization for a quick\nand intuitive overview. Upon receiving a user query, HealthGenie performs query\nrefinement and retrieves relevant information from a pre-built KG. The system\nthen visualizes and highlights pertinent information, organized by defined\ncategories, while offering detailed, explainable recommendation rationales.\nUsers can further tailor these recommendations by adjusting preferences\ninteractively. Our evaluation, comprising a within-subject comparative\nexperiment and an open-ended discussion, demonstrates that HealthGenie\neffectively supports users in obtaining personalized dietary guidance based on\ntheir health conditions while reducing interaction effort and cognitive load.\nThese findings highlight the potential of LLM-KG integration in supporting\ndecision-making through explainable and visualized information. We examine the\nsystem's usefulness and effectiveness with an N=12 within-subject study and\nprovide design considerations for future systems that integrate conversational\nLLM and KG."}
{"id": "2504.14727", "pdf": "https://arxiv.org/pdf/2504.14727", "abs": "https://arxiv.org/abs/2504.14727", "authors": ["Geng Liu", "Fei Zhu", "Rong Feng", "Zhiqiang Yi", "Shiqi Wang", "Gaofeng Meng", "Zhaoxiang Zhang"], "title": "Semi-parametric Memory Consolidation: Towards Brain-like Deep Continual Learning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Humans and most animals inherently possess a distinctive capacity to\ncontinually acquire novel experiences and accumulate worldly knowledge over\ntime. This ability, termed continual learning, is also critical for deep neural\nnetworks (DNNs) to adapt to the dynamically evolving world in open\nenvironments. However, DNNs notoriously suffer from catastrophic forgetting of\npreviously learned knowledge when trained on sequential tasks. In this work,\ninspired by the interactive human memory and learning system, we propose a\nnovel biomimetic continual learning framework that integrates semi-parametric\nmemory and the wake-sleep consolidation mechanism. For the first time, our\nmethod enables deep neural networks to retain high performance on novel tasks\nwhile maintaining prior knowledge in real-world challenging continual learning\nscenarios, e.g., class-incremental learning on ImageNet. This study\ndemonstrates that emulating biological intelligence provides a promising path\nto enable deep neural networks with continual learning capabilities."}
{"id": "2504.14602", "pdf": "https://arxiv.org/pdf/2504.14602", "abs": "https://arxiv.org/abs/2504.14602", "authors": ["Jiwei Li", "Bi Zhang", "Xiaowei Tan", "Wanxin Chen", "Zhaoyuan Liu", "Juanjuan Zhang", "Weiguang Huo", "Jian Huang", "Lianqing Liu", "Xingang Zhao"], "title": "K2MUSE: A human lower limb multimodal dataset under diverse conditions for facilitating rehabilitation robotics", "categories": ["cs.RO", "cs.AI", "cs.HC"], "comment": "23 pages, 13 figures,4 tables", "summary": "The natural interaction and control performance of lower limb rehabilitation\nrobots are closely linked to biomechanical information from various human\nlocomotion activities. Multidimensional human motion data significantly deepen\nthe understanding of the complex mechanisms governing neuromuscular\nalterations, thereby facilitating the development and application of\nrehabilitation robots in multifaceted real-world environments. However,\ncurrently available lower limb datasets are inadequate for supplying the\nessential multimodal data and large-scale gait samples necessary for effective\ndata-driven approaches, and they neglect the significant effects of acquisition\ninterference in real applications.To fill this gap, we present the K2MUSE\ndataset, which includes a comprehensive collection of multimodal data,\ncomprising kinematic, kinetic, amplitude-mode ultrasound (AUS), and surface\nelectromyography (sEMG) measurements. The proposed dataset includes lower limb\nmultimodal data from 30 able-bodied participants walking under different\ninclines (0$^\\circ$, $\\pm$5$^\\circ$, and $\\pm$10$^\\circ$), various speeds (0.5\nm/s, 1.0 m/s, and 1.5 m/s), and different nonideal acquisition conditions\n(muscle fatigue, electrode shifts, and inter-day differences). The kinematic\nand ground reaction force data were collected via a Vicon motion capture system\nand an instrumented treadmill with embedded force plates, whereas the sEMG and\nAUS data were synchronously recorded for thirteen muscles on the bilateral\nlower limbs. This dataset offers a new resource for designing control\nframeworks for rehabilitation robots and conducting biomechanical analyses of\nlower limb locomotion. The dataset is available at https://k2muse.github.io/."}
{"id": "2504.14795", "pdf": "https://arxiv.org/pdf/2504.14795", "abs": "https://arxiv.org/abs/2504.14795", "authors": ["Ryu Tadokoro", "Tsukasa Takagi", "Shin-ichi Maeda"], "title": "Segmentation with Noisy Labels via Spatially Correlated Distributions", "categories": ["eess.IV", "cs.CV", "cs.LG", "stat.ML"], "comment": null, "summary": "In semantic segmentation, the accuracy of models heavily depends on the\nhigh-quality annotations. However, in many practical scenarios such as medical\nimaging and remote sensing, obtaining true annotations is not straightforward\nand usually requires significant human labor. Relying on human labor often\nintroduces annotation errors, including mislabeling, omissions, and\ninconsistency between annotators. In the case of remote sensing, differences in\nprocurement time can lead to misaligned ground truth annotations. These label\nerrors are not independently distributed, and instead usually appear in\nspatially connected regions where adjacent pixels are more likely to share the\nsame errors. To address these issues, we propose an approximate Bayesian\nestimation based on a probabilistic model that assumes training data includes\nlabel errors, incorporating the tendency for these errors to occur with spatial\ncorrelations between adjacent pixels. Bayesian inference requires computing the\nposterior distribution of label errors, which becomes intractable when spatial\ncorrelations are present. We represent the correlation of label errors between\nadjacent pixels through a Gaussian distribution whose covariance is structured\nby a Kac-Murdock-Szeg\\\"{o} (KMS) matrix, solving the computational challenges.\nThrough experiments on multiple segmentation tasks, we confirm that leveraging\nthe spatial correlation of label errors significantly improves performance.\nNotably, in specific tasks such as lung segmentation, the proposed method\nachieves performance comparable to training with clean labels under moderate\nnoise levels. Code is available at\nhttps://github.com/pfnet-research/Bayesian_SpatialCorr."}
{"id": "2504.14618", "pdf": "https://arxiv.org/pdf/2504.14618", "abs": "https://arxiv.org/abs/2504.14618", "authors": ["Han Bi", "Ge Yu", "Yu He", "Wenzhuo Liu", "Zijie Zheng"], "title": "VM-BHINet:Vision Mamba Bimanual Hand Interaction Network for 3D Interacting Hand Mesh Recovery From a Single RGB Image", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Understanding bimanual hand interactions is essential for realistic 3D pose\nand shape reconstruction. However, existing methods struggle with occlusions,\nambiguous appearances, and computational inefficiencies. To address these\nchallenges, we propose Vision Mamba Bimanual Hand Interaction Network\n(VM-BHINet), introducing state space models (SSMs) into hand reconstruction to\nenhance interaction modeling while improving computational efficiency. The core\ncomponent, Vision Mamba Interaction Feature Extraction Block (VM-IFEBlock),\ncombines SSMs with local and global feature operations, enabling deep\nunderstanding of hand interactions. Experiments on the InterHand2.6M dataset\nshow that VM-BHINet reduces Mean per-joint position error (MPJPE) and Mean\nper-vertex position error (MPVPE) by 2-3%, significantly surpassing\nstate-of-the-art methods."}
{"id": "2504.14798", "pdf": "https://arxiv.org/pdf/2504.14798", "abs": "https://arxiv.org/abs/2504.14798", "authors": ["Hao Xuan", "Xingyu Li"], "title": "Verifying Robust Unlearning: Probing Residual Knowledge in Unlearned Models", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Machine Unlearning (MUL) is crucial for privacy protection and content\nregulation, yet recent studies reveal that traces of forgotten information\npersist in unlearned models, enabling adversaries to resurface removed\nknowledge. Existing verification methods only confirm whether unlearning was\nexecuted, failing to detect such residual information leaks. To address this,\nwe introduce the concept of Robust Unlearning, ensuring models are\nindistinguishable from retraining and resistant to adversarial recovery. To\nempirically evaluate whether unlearning techniques meet this security standard,\nwe propose the Unlearning Mapping Attack (UMA), a post-unlearning verification\nframework that actively probes models for forgotten traces using adversarial\nqueries. Extensive experiments on discriminative and generative tasks show that\nexisting unlearning techniques remain vulnerable, even when passing existing\nverification metrics. By establishing UMA as a practical verification tool,\nthis study sets a new standard for assessing and enhancing machine unlearning\nsecurity."}
{"id": "2504.14625", "pdf": "https://arxiv.org/pdf/2504.14625", "abs": "https://arxiv.org/abs/2504.14625", "authors": ["Haiyan Qin", "Jiahao Feng", "Xiaotong Feng", "Wei W. Xing", "Wang Kang"], "title": "Towards Optimal Circuit Generation: Multi-Agent Collaboration Meets Collective Intelligence", "categories": ["cs.AR", "cs.AI"], "comment": "9 pages, 6 figures", "summary": "Large language models (LLMs) have transformed code generation, yet their\napplication in hardware design produces gate counts 38\\%--1075\\% higher than\nhuman designs. We present CircuitMind, a multi-agent framework that achieves\nhuman-competitive efficiency through three key innovations: syntax locking\n(constraining generation to basic logic gates), retrieval-augmented generation\n(enabling knowledge-driven design), and dual-reward optimization (balancing\ncorrectness with efficiency). To evaluate our approach, we introduce TC-Bench,\nthe first gate-level benchmark harnessing collective intelligence from the\nTuringComplete ecosystem -- a competitive circuit design platform with hundreds\nof thousands of players. Experiments show CircuitMind enables 55.6\\% of model\nimplementations to match or exceed top-tier human experts in composite\nefficiency metrics. Most remarkably, our framework elevates the 14B Phi-4 model\nto outperform both GPT-4o mini and Gemini 2.0 Flash, achieving efficiency\ncomparable to the top 25\\% of human experts without requiring specialized\ntraining. These innovations establish a new paradigm for hardware optimization\nwhere collaborative AI systems leverage collective human expertise to achieve\noptimal circuit designs. Our model, data, and code are open-source at\nhttps://github.com/BUAA-CLab/CircuitMind."}
{"id": "2504.14800", "pdf": "https://arxiv.org/pdf/2504.14800", "abs": "https://arxiv.org/abs/2504.14800", "authors": ["Shuxian Zhao", "Jie Gui", "Minjing Dong", "Baosheng Yu", "Zhipeng Gui", "Lu Dong", "Yuan Yan Tang", "James Tin-Yau Kwok"], "title": "A Survey on Small Sample Imbalance Problem: Metrics, Feature Analysis, and Solutions", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "The small sample imbalance (S&I) problem is a major challenge in machine\nlearning and data analysis. It is characterized by a small number of samples\nand an imbalanced class distribution, which leads to poor model performance. In\naddition, indistinct inter-class feature distributions further complicate\nclassification tasks. Existing methods often rely on algorithmic heuristics\nwithout sufficiently analyzing the underlying data characteristics. We argue\nthat a detailed analysis from the data perspective is essential before\ndeveloping an appropriate solution. Therefore, this paper proposes a systematic\nanalytical framework for the S\\&I problem. We first summarize imbalance metrics\nand complexity analysis methods, highlighting the need for interpretable\nbenchmarks to characterize S&I problems. Second, we review recent solutions for\nconventional, complexity-based, and extreme S&I problems, revealing\nmethodological differences in handling various data distributions. Our summary\nfinds that resampling remains a widely adopted solution. However, we conduct\nexperiments on binary and multiclass datasets, revealing that classifier\nperformance differences significantly exceed the improvements achieved through\nresampling. Finally, this paper highlights open questions and discusses future\ntrends."}
{"id": "2504.14636", "pdf": "https://arxiv.org/pdf/2504.14636", "abs": "https://arxiv.org/abs/2504.14636", "authors": ["Binjie Guo", "Hanyu Zheng", "Guowei Su", "Ru Zhang", "Haohan Jiang", "Xurong Lin", "Hongyan Wei", "Aisheng Mo", "Jie Li", "Zhiyuan Qian", "Zhuhao Zhang", "Xiaoyuan Cheng"], "title": "AlphaZero-Edu: Making AlphaZero Accessible to Everyone", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recent years have witnessed significant progress in reinforcement learning,\nespecially with Zero-like paradigms, which have greatly boosted the\ngeneralization and reasoning abilities of large-scale language models.\nNevertheless, existing frameworks are often plagued by high implementation\ncomplexity and poor reproducibility. To tackle these challenges, we present\nAlphaZero-Edu, a lightweight, education-focused implementation built upon the\nmathematical framework of AlphaZero. It boasts a modular architecture that\ndisentangles key components, enabling transparent visualization of the\nalgorithmic processes. Additionally, it is optimized for resource-efficient\ntraining on a single NVIDIA RTX 3090 GPU and features highly parallelized\nself-play data generation, achieving a 3.2-fold speedup with 8 processes. In\nGomoku matches, the framework has demonstrated exceptional performance,\nachieving a consistently high win rate against human opponents. AlphaZero-Edu\nhas been open-sourced at https://github.com/StarLight1212/AlphaZero_Edu,\nproviding an accessible and practical benchmark for both academic research and\nindustrial applications."}
{"id": "2504.14815", "pdf": "https://arxiv.org/pdf/2504.14815", "abs": "https://arxiv.org/abs/2504.14815", "authors": ["Xiaoyong Yuan", "Xiaolong Ma", "Linke Guo", "Lan Zhang"], "title": "What Lurks Within? Concept Auditing for Shared Diffusion Models at Scale", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.CV"], "comment": "17 pages, 15 figures", "summary": "Diffusion models (DMs) have revolutionized text-to-image generation, enabling\nthe creation of highly realistic and customized images from text prompts. With\nthe rise of parameter-efficient fine-tuning (PEFT) techniques like LoRA, users\ncan now customize powerful pre-trained models using minimal computational\nresources. However, the widespread sharing of fine-tuned DMs on open platforms\nraises growing ethical and legal concerns, as these models may inadvertently or\ndeliberately generate sensitive or unauthorized content, such as copyrighted\nmaterial, private individuals, or harmful content. Despite the increasing\nregulatory attention on generative AI, there are currently no practical tools\nfor systematically auditing these models before deployment. In this paper, we\naddress the problem of concept auditing: determining whether a fine-tuned DM\nhas learned to generate a specific target concept. Existing approaches\ntypically rely on prompt-based input crafting and output-based image\nclassification but suffer from critical limitations, including prompt\nuncertainty, concept drift, and poor scalability. To overcome these challenges,\nwe introduce Prompt-Agnostic Image-Free Auditing (PAIA), a novel, model-centric\nconcept auditing framework. By treating the DM as the object of inspection,\nPAIA enables direct analysis of internal model behavior, bypassing the need for\noptimized prompts or generated images. We evaluate PAIA on 320 controlled model\nand 690 real-world community models sourced from a public DM sharing platform.\nPAIA achieves over 90% detection accuracy while reducing auditing time by\n18-40x compared to existing baselines. To our knowledge, PAIA is the first\nscalable and practical solution for pre-deployment concept auditing of\ndiffusion models, providing a practical foundation for safer and more\ntransparent diffusion model sharing."}
{"id": "2504.14640", "pdf": "https://arxiv.org/pdf/2504.14640", "abs": "https://arxiv.org/abs/2504.14640", "authors": ["Yuheng Huang", "Lei Ma", "Keizaburo Nishikino", "Takumi Akazaki"], "title": "Risk Assessment Framework for Code LLMs via Leveraging Internal States", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": "To appear in the 33rd ACM International Conference on the Foundations\n  of Software Engineering (FSE Companion'25 Industry Track), June 23-28, 2025,\n  Trondheim, Norway. This work was supported by Fujitsu Limited", "summary": "The pre-training paradigm plays a key role in the success of Large Language\nModels (LLMs), which have been recognized as one of the most significant\nadvancements of AI recently. Building on these breakthroughs, code LLMs with\nadvanced coding capabilities bring huge impacts on software engineering,\nshowing the tendency to become an essential part of developers' daily routines.\nHowever, the current code LLMs still face serious challenges related to\ntrustworthiness, as they can generate incorrect, insecure, or unreliable code.\nRecent exploratory studies find that it can be promising to detect such risky\noutputs by analyzing LLMs' internal states, akin to how the human brain\nunconsciously recognizes its own mistakes. Yet, most of these approaches are\nlimited to narrow sub-domains of LLM operations and fall short of achieving\nindustry-level scalability and practicability. To address these challenges, in\nthis paper, we propose PtTrust, a two-stage risk assessment framework for code\nLLM based on internal state pre-training, designed to integrate seamlessly with\nthe existing infrastructure of software companies. The core idea is that the\nrisk assessment framework could also undergo a pre-training process similar to\nLLMs. Specifically, PtTrust first performs unsupervised pre-training on\nlarge-scale unlabeled source code to learn general representations of LLM\nstates. Then, it uses a small, labeled dataset to train a risk predictor. We\ndemonstrate the effectiveness of PtTrust through fine-grained, code line-level\nrisk assessment and demonstrate that it generalizes across tasks and different\nprogramming languages. Further experiments also reveal that PtTrust provides\nhighly intuitive and interpretable features, fostering greater user trust. We\nbelieve PtTrust makes a promising step toward scalable and trustworthy\nassurance for code LLMs."}
{"id": "2504.14882", "pdf": "https://arxiv.org/pdf/2504.14882", "abs": "https://arxiv.org/abs/2504.14882", "authors": ["Mojtaba Kolahdouzi", "Hatice Gunes", "Ali Etemad"], "title": "Some Optimizers are More Equal: Understanding the Role of Optimizers in Group Fairness", "categories": ["cs.LG", "cs.CV", "stat.ML"], "comment": null, "summary": "We study whether and how the choice of optimization algorithm can impact\ngroup fairness in deep neural networks. Through stochastic differential\nequation analysis of optimization dynamics in an analytically tractable setup,\nwe demonstrate that the choice of optimization algorithm indeed influences\nfairness outcomes, particularly under severe imbalance. Furthermore, we show\nthat when comparing two categories of optimizers, adaptive methods and\nstochastic methods, RMSProp (from the adaptive category) has a higher\nlikelihood of converging to fairer minima than SGD (from the stochastic\ncategory). Building on this insight, we derive two new theoretical guarantees\nshowing that, under appropriate conditions, RMSProp exhibits fairer parameter\nupdates and improved fairness in a single optimization step compared to SGD. We\nthen validate these findings through extensive experiments on three publicly\navailable datasets, namely CelebA, FairFace, and MS-COCO, across different\ntasks as facial expression recognition, gender classification, and multi-label\nclassification, using various backbones. Considering multiple fairness\ndefinitions including equalized odds, equal opportunity, and demographic\nparity, adaptive optimizers like RMSProp and Adam consistently outperform SGD\nin terms of group fairness, while maintaining comparable predictive accuracy.\nOur results highlight the role of adaptive updates as a crucial yet overlooked\nmechanism for promoting fair outcomes."}
{"id": "2504.14645", "pdf": "https://arxiv.org/pdf/2504.14645", "abs": "https://arxiv.org/abs/2504.14645", "authors": ["Philipp Altmann", "Céline Davignon", "Maximilian Zorn", "Fabian Ritz", "Claudia Linnhoff-Popien", "Thomas Gabor"], "title": "Surrogate Fitness Metrics for Interpretable Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": "30 pages, 7 figures, under review", "summary": "We employ an evolutionary optimization framework that perturbs initial states\nto generate informative and diverse policy demonstrations. A joint surrogate\nfitness function guides the optimization by combining local diversity,\nbehavioral certainty, and global population diversity. To assess demonstration\nquality, we apply a set of evaluation metrics, including the reward-based\noptimality gap, fidelity interquartile means (IQMs), fitness composition\nanalysis, and trajectory visualizations. Hyperparameter sensitivity is also\nexamined to better understand the dynamics of trajectory optimization. Our\nfindings demonstrate that optimizing trajectory selection via surrogate fitness\nmetrics significantly improves interpretability of RL policies in both discrete\nand continuous environments. In gridworld domains, evaluations reveal\nsignificantly enhanced demonstration fidelities compared to random and ablated\nbaselines. In continuous control, the proposed framework offers valuable\ninsights, particularly for early-stage policies, while fidelity-based\noptimization proves more effective for mature policies. By refining and\nsystematically analyzing surrogate fitness functions, this study advances the\ninterpretability of RL models. The proposed improvements provide deeper\ninsights into RL decision-making, benefiting applications in safety-critical\nand explainability-focused domains."}
{"id": "2504.14906", "pdf": "https://arxiv.org/pdf/2504.14906", "abs": "https://arxiv.org/abs/2504.14906", "authors": ["Huadai Liu", "Tianyi Luo", "Qikai Jiang", "Kaicheng Luo", "Peiwen Sun", "Jialei Wan", "Rongjie Huang", "Qian Chen", "Wen Wang", "Xiangtai Li", "Shiliang Zhang", "Zhijie Yan", "Zhou Zhao", "Wei Xue"], "title": "OmniAudio: Generating Spatial Audio from 360-Degree Video", "categories": ["eess.AS", "cs.CV", "cs.SD"], "comment": "Work in Progress", "summary": "Traditional video-to-audio generation techniques primarily focus on\nfield-of-view (FoV) video and non-spatial audio, often missing the spatial cues\nnecessary for accurately representing sound sources in 3D environments. To\naddress this limitation, we introduce a novel task, 360V2SA, to generate\nspatial audio from 360-degree videos, specifically producing First-order\nAmbisonics (FOA) audio - a standard format for representing 3D spatial audio\nthat captures sound directionality and enables realistic 3D audio reproduction.\nWe first create Sphere360, a novel dataset tailored for this task that is\ncurated from real-world data. We also design an efficient semi-automated\npipeline for collecting and cleaning paired video-audio data. To generate\nspatial audio from 360-degree video, we propose a novel framework OmniAudio,\nwhich leverages self-supervised pre-training using both spatial audio data (in\nFOA format) and large-scale non-spatial data. Furthermore, OmniAudio features a\ndual-branch framework that utilizes both panoramic and FoV video inputs to\ncapture comprehensive local and global information from 360-degree videos.\nExperimental results demonstrate that OmniAudio achieves state-of-the-art\nperformance across both objective and subjective metrics on Sphere360. Code and\ndatasets will be released at https://github.com/liuhuadai/OmniAudio. The demo\npage is available at https://OmniAudio-360V2SA.github.io."}
{"id": "2504.14657", "pdf": "https://arxiv.org/pdf/2504.14657", "abs": "https://arxiv.org/abs/2504.14657", "authors": ["Yihan Lin", "Zhirong Bella Yu", "Simon Lee"], "title": "A Case Study Exploring the Current Landscape of Synthetic Medical Record Generation with Commercial LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at the Conference of Health, Inference, Learning (CHIL 2025)\n  in Berkeley, CA. To appear in PMLR later in 2025", "summary": "Synthetic Electronic Health Records (EHRs) offer a valuable opportunity to\ncreate privacy preserving and harmonized structured data, supporting numerous\napplications in healthcare. Key benefits of synthetic data include precise\ncontrol over the data schema, improved fairness and representation of patient\npopulations, and the ability to share datasets without concerns about\ncompromising real individuals privacy. Consequently, the AI community has\nincreasingly turned to Large Language Models (LLMs) to generate synthetic data\nacross various domains. However, a significant challenge in healthcare is\nensuring that synthetic health records reliably generalize across different\nhospitals, a long standing issue in the field. In this work, we evaluate the\ncurrent state of commercial LLMs for generating synthetic data and investigate\nmultiple aspects of the generation process to identify areas where these models\nexcel and where they fall short. Our main finding from this work is that while\nLLMs can reliably generate synthetic health records for smaller subsets of\nfeatures, they struggle to preserve realistic distributions and correlations as\nthe dimensionality of the data increases, ultimately limiting their ability to\ngeneralize across diverse hospital settings."}
{"id": "2504.15028", "pdf": "https://arxiv.org/pdf/2504.15028", "abs": "https://arxiv.org/abs/2504.15028", "authors": ["Santiago Jimenez-Navarro", "Julia Guerrero-Viu", "Belen Masia"], "title": "A Controllable Appearance Representation for Flexible Transfer and Editing", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "We present a method that computes an interpretable representation of material\nappearance within a highly compact, disentangled latent space. This\nrepresentation is learned in a self-supervised fashion using an adapted\nFactorVAE. We train our model with a carefully designed unlabeled dataset,\navoiding possible biases induced by human-generated labels. Our model\ndemonstrates strong disentanglement and interpretability by effectively\nencoding material appearance and illumination, despite the absence of explicit\nsupervision. Then, we use our representation as guidance for training a\nlightweight IP-Adapter to condition a diffusion pipeline that transfers the\nappearance of one or more images onto a target geometry, and allows the user to\nfurther edit the resulting appearance. Our approach offers fine-grained control\nover the generated results: thanks to the well-structured compact latent space,\nusers can intuitively manipulate attributes such as hue or glossiness in image\nspace to achieve the desired final appearance."}
{"id": "2504.14677", "pdf": "https://arxiv.org/pdf/2504.14677", "abs": "https://arxiv.org/abs/2504.14677", "authors": ["Jia Liu", "Cheng Jinguo", "Xia Fang", "Zhenyuan Ma", "Yuankai Wu"], "title": "Evaluating Temporal Plasticity in Foundation Time Series Models for Incremental Fine-tuning", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at IJCNN 2025", "summary": "Time series foundation models excel at diverse time series forecasting tasks,\nbut their capacity for continuous improvement through incremental learning\nremains unexplored. We present the first comprehensive study investigating\nthese models' temporal plasticity - their ability to progressively enhance\nperformance through continual learning while maintaining existing capabilities.\nThrough experiments on real-world datasets exhibiting distribution shifts, we\nevaluate both conventional deep learning models and foundation models using a\nnovel continual learning framework. Our findings reveal that while traditional\nmodels struggle with performance deterioration during incremental fine-tuning,\nfoundation models like Time-MoE and Chronos demonstrate sustained improvement\nin predictive accuracy. This suggests that optimizing foundation model\nfine-tuning strategies may be more valuable than developing domain-specific\nsmall models. Our research introduces new evaluation methodologies and insights\nfor developing foundation time series models with robust continuous learning\ncapabilities."}
{"id": "2504.15051", "pdf": "https://arxiv.org/pdf/2504.15051", "abs": "https://arxiv.org/abs/2504.15051", "authors": ["Ashkan Shakarami", "Yousef Yeganeh", "Azade Farshad", "Lorenzo Nicolè", "Stefano Ghidoni", "Nassir Navab"], "title": "VeLU: Variance-enhanced Learning Unit for Deep Neural Networks", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Activation functions are fundamental in deep neural networks and directly\nimpact gradient flow, optimization stability, and generalization. Although ReLU\nremains standard because of its simplicity, it suffers from vanishing gradients\nand lacks adaptability. Alternatives like Swish and GELU introduce smooth\ntransitions, but fail to dynamically adjust to input statistics. We propose\nVeLU, a Variance-enhanced Learning Unit as an activation function that\ndynamically scales based on input variance by integrating ArcTan-Sin\ntransformations and Wasserstein-2 regularization, effectively mitigating\ncovariate shifts and stabilizing optimization. Extensive experiments on\nViT_B16, VGG19, ResNet50, DenseNet121, MobileNetV2, and EfficientNetB3 confirm\nVeLU's superiority over ReLU, ReLU6, Swish, and GELU on six vision benchmarks.\nThe codes of VeLU are publicly available on GitHub."}
{"id": "2504.14681", "pdf": "https://arxiv.org/pdf/2504.14681", "abs": "https://arxiv.org/abs/2504.14681", "authors": ["Zeyu Wang", "Frank P. -W. Lo", "Qian Chen", "Yongqi Zhang", "Chen Lin", "Xu Chen", "Zhenhua Yu", "Alexander J. Thompson", "Eric M. Yeatman", "Benny P. L. Lo"], "title": "An LLM-enabled Multi-Agent Autonomous Mechatronics Design Framework", "categories": ["cs.RO", "cs.AI"], "comment": "Accepted by CVPR 2025 Workshop", "summary": "Existing LLM-enabled multi-agent frameworks are predominantly limited to\ndigital or simulated environments and confined to narrowly focused knowledge\ndomain, constraining their applicability to complex engineering tasks that\nrequire the design of physical embodiment, cross-disciplinary integration, and\nconstraint-aware reasoning. This work proposes a multi-agent autonomous\nmechatronics design framework, integrating expertise across mechanical design,\noptimization, electronics, and software engineering to autonomously generate\nfunctional prototypes with minimal direct human design input. Operating\nprimarily through a language-driven workflow, the framework incorporates\nstructured human feedback to ensure robust performance under real-world\nconstraints. To validate its capabilities, the framework is applied to a\nreal-world challenge involving autonomous water-quality monitoring and\nsampling, where traditional methods are labor-intensive and ecologically\ndisruptive. Leveraging the proposed system, a fully functional autonomous\nvessel was developed with optimized propulsion, cost-effective electronics, and\nadvanced control. The design process was carried out by specialized agents,\nincluding a high-level planning agent responsible for problem abstraction and\ndedicated agents for structural, electronics, control, and software\ndevelopment. This approach demonstrates the potential of LLM-based multi-agent\nsystems to automate real-world engineering workflows and reduce reliance on\nextensive domain expertise."}
{"id": "2504.15129", "pdf": "https://arxiv.org/pdf/2504.15129", "abs": "https://arxiv.org/abs/2504.15129", "authors": ["Kangyao Huang", "Hao Wang", "Yu Luo", "Jingyu Chen", "Jintao Chen", "Xiangkui Zhang", "Xiangyang Ji", "Huaping Liu"], "title": "A General Infrastructure and Workflow for Quadrotor Deep Reinforcement Learning and Reality Deployment", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Deploying robot learning methods to a quadrotor in unstructured outdoor\nenvironments is an exciting task. Quadrotors operating in real-world\nenvironments by learning-based methods encounter several challenges: a large\namount of simulator generated data required for training, strict demands for\nreal-time processing onboard, and the sim-to-real gap caused by dynamic and\nnoisy conditions. Current works have made a great breakthrough in applying\nlearning-based methods to end-to-end control of quadrotors, but rarely mention\nthe infrastructure system training from scratch and deploying to reality, which\nmakes it difficult to reproduce methods and applications. To bridge this gap,\nwe propose a platform that enables the seamless transfer of end-to-end deep\nreinforcement learning (DRL) policies. We integrate the training environment,\nflight dynamics control, DRL algorithms, the MAVROS middleware stack, and\nhardware into a comprehensive workflow and architecture that enables\nquadrotors' policies to be trained from scratch to real-world deployment in\nseveral minutes. Our platform provides rich types of environments including\nhovering, dynamic obstacle avoidance, trajectory tracking, balloon hitting, and\nplanning in unknown environments, as a physical experiment benchmark. Through\nextensive empirical validation, we demonstrate the efficiency of proposed\nsim-to-real platform, and robust outdoor flight performance under real-world\nperturbations. Details can be found from our website\nhttps://emnavi.tech/AirGym/."}
{"id": "2504.14686", "pdf": "https://arxiv.org/pdf/2504.14686", "abs": "https://arxiv.org/abs/2504.14686", "authors": ["José Suárez-Varela", "Andra Lutu"], "title": "Uncovering Issues in the Radio Access Network by Looking at the Neighbors", "categories": ["cs.NI", "cs.AI", "cs.LG"], "comment": "7 pages", "summary": "Mobile network operators (MNOs) manage Radio Access Networks (RANs) with\nmassive amounts of cells over multiple radio generations (2G-5G). To handle\nsuch complexity, operations teams rely on monitoring systems, including anomaly\ndetection tools that identify unexpected behaviors. In this paper, we present\nc-ANEMON, a Contextual ANomaly dEtection MONitor for the RAN based on Graph\nNeural Networks (GNNs). Our solution captures spatio-temporal variations by\nanalyzing the behavior of individual cells in relation to their local\nneighborhoods, enabling the detection of anomalies that are independent of\nexternal mobility factors. This, in turn, allows focusing on anomalies\nassociated with network issues (e.g., misconfigurations, equipment failures).\nWe evaluate c-ANEMON using real-world data from a large European metropolitan\narea (7,890 cells; 3 months). First, we show that the GNN model within our\nsolution generalizes effectively to cells from previously unseen areas,\nsuggesting the possibility of using a single model across extensive deployment\nregions. Then, we analyze the anomalies detected by c-ANEMON through manual\ninspection and define several categories of long-lasting anomalies (6+ hours).\nNotably, 45.95% of these anomalies fall into a category that is more likely to\nrequire intervention by operations teams."}
{"id": "2504.15133", "pdf": "https://arxiv.org/pdf/2504.15133", "abs": "https://arxiv.org/abs/2504.15133", "authors": ["Ziwen Xu", "Shuxun Wang", "Kewei Xu", "Haoming Xu", "Mengru Wang", "Xinle Deng", "Yunzhi Yao", "Guozhou Zheng", "Huajun Chen", "Ningyu Zhang"], "title": "EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "comment": "Work in progress. Demo:\n  https://zjunlp.github.io/project/EasyEdit2/video; code:\n  https://github.com/zjunlp/EasyEdit", "summary": "In this paper, we introduce EasyEdit2, a framework designed to enable\nplug-and-play adjustability for controlling Large Language Model (LLM)\nbehaviors. EasyEdit2 supports a wide range of test-time interventions,\nincluding safety, sentiment, personality, reasoning patterns, factuality, and\nlanguage features. Unlike its predecessor, EasyEdit2 features a new\narchitecture specifically designed for seamless model steering. It comprises\nkey modules such as the steering vector generator and the steering vector\napplier, which enable automatic generation and application of steering vectors\nto influence the model's behavior without modifying its parameters. One of the\nmain advantages of EasyEdit2 is its ease of use-users do not need extensive\ntechnical knowledge. With just a single example, they can effectively guide and\nadjust the model's responses, making precise control both accessible and\nefficient. Empirically, we report model steering performance across different\nLLMs, demonstrating the effectiveness of these techniques. We have released the\nsource code on GitHub at https://github.com/zjunlp/EasyEdit along with a\ndemonstration notebook. In addition, we provide a demo video at\nhttps://zjunlp.github.io/project/EasyEdit2/video for a quick introduction."}
{"id": "2504.14690", "pdf": "https://arxiv.org/pdf/2504.14690", "abs": "https://arxiv.org/abs/2504.14690", "authors": ["Mehrnoush Shamsfard", "Zahra Saaberi", "Mostafa Karimi manesh", "Seyed Mohammad Hossein Hashemi", "Zahra Vatankhah", "Motahareh Ramezani", "Niki Pourazin", "Tara Zare", "Maryam Azimi", "Sarina Chitsaz", "Sama Khoraminejad", "Morteza Mahdavi Mortazavi", "Mohammad Mahdi Chizari", "Sahar Maleki", "Seyed Soroush Majd", "Mostafa Masumi", "Sayed Ali Musavi Khoeini", "Amir Mohseni", "Sogol Alipour"], "title": "FarsEval-PKBETS: A new diverse benchmark for evaluating Persian large language models", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7; E.0"], "comment": "24 pages, 3 figures, 3 tables", "summary": "Research on evaluating and analyzing large language models (LLMs) has been\nextensive for resource-rich languages such as English, yet their performance in\nlanguages such as Persian has received considerably less attention. This paper\nintroduces FarsEval-PKBETS benchmark, a subset of FarsEval project for\nevaluating large language models in Persian. This benchmark consists of 4000\nquestions and answers in various formats, including multiple choice, short\nanswer and descriptive responses. It covers a wide range of domains and\ntasks,including medicine, law, religion, Persian language, encyclopedic\nknowledge, human preferences, social knowledge, ethics and bias, text\ngeneration, and respecting others' rights. This bechmark incorporates\nlinguistics, cultural, and local considerations relevant to the Persian\nlanguage and Iran. To ensure the questions are challenging for current LLMs,\nthree models -- Llama3-70B, PersianMind, and Dorna -- were evaluated using this\nbenchmark. Their average accuracy was below 50%, meaning they provided fully\ncorrect answers to fewer than half of the questions. These results indicate\nthat current language models are still far from being able to solve this\nbenchmark"}
{"id": "2504.15252", "pdf": "https://arxiv.org/pdf/2504.15252", "abs": "https://arxiv.org/abs/2504.15252", "authors": ["Tue Vo", "Lakshay Sharma", "Tuan Dinh", "Khuong Dinh", "Trang Nguyen", "Trung Phan", "Minh Do", "Duong Vu"], "title": "SuoiAI: Building a Dataset for Aquatic Invertebrates in Vietnam", "categories": ["cs.AI", "cs.CV", "cs.LG"], "comment": "Published as a workshop paper at \"Tackling Climate Change with\n  Machine Learning\", ICLR 2025", "summary": "Understanding and monitoring aquatic biodiversity is critical for ecological\nhealth and conservation efforts. This paper proposes SuoiAI, an end-to-end\npipeline for building a dataset of aquatic invertebrates in Vietnam and\nemploying machine learning (ML) techniques for species classification. We\noutline the methods for data collection, annotation, and model training,\nfocusing on reducing annotation effort through semi-supervised learning and\nleveraging state-of-the-art object detection and classification models. Our\napproach aims to overcome challenges such as data scarcity, fine-grained\nclassification, and deployment in diverse environmental conditions."}
{"id": "2504.14693", "pdf": "https://arxiv.org/pdf/2504.14693", "abs": "https://arxiv.org/abs/2504.14693", "authors": ["Enxin Song", "Wenhao Chai", "Weili Xu", "Jianwen Xie", "Yuxuan Liu", "Gaoang Wang"], "title": "Video-MMLU: A Massive Multi-Discipline Lecture Understanding Benchmark", "categories": ["cs.CV", "cs.AI"], "comment": "Code, docs, and benchmark are all avaliable at\n  https://enxinsong.com/Video-MMLU-web/", "summary": "Recent advancements in language multimodal models (LMMs) for video have\ndemonstrated their potential for understanding video content, yet the task of\ncomprehending multi-discipline lectures remains largely unexplored. We\nintroduce Video-MMLU, a massive benchmark designed to evaluate the capabilities\nof LMMs in understanding Multi-Discipline Lectures. We evaluate over 90\nopen-source and proprietary models, ranging from 0.5B to 40B parameters. Our\nresults highlight the limitations of current models in addressing the cognitive\nchallenges presented by these lectures, especially in tasks requiring both\nperception and reasoning. Additionally, we explore how the number of visual\ntokens and the large language models influence performance, offering insights\ninto the interplay between multimodal perception and reasoning in lecture\ncomprehension."}
{"id": "2504.15262", "pdf": "https://arxiv.org/pdf/2504.15262", "abs": "https://arxiv.org/abs/2504.15262", "authors": ["Brandon Zhao", "Aviad Levis", "Liam Connor", "Pratul P. Srinivasan", "Katherine L. Bouman"], "title": "Revealing the 3D Cosmic Web through Gravitationally Constrained Neural Fields", "categories": ["astro-ph.CO", "cs.CV"], "comment": null, "summary": "Weak gravitational lensing is the slight distortion of galaxy shapes caused\nprimarily by the gravitational effects of dark matter in the universe. In our\nwork, we seek to invert the weak lensing signal from 2D telescope images to\nreconstruct a 3D map of the universe's dark matter field. While inversion\ntypically yields a 2D projection of the dark matter field, accurate 3D maps of\nthe dark matter distribution are essential for localizing structures of\ninterest and testing theories of our universe. However, 3D inversion poses\nsignificant challenges. First, unlike standard 3D reconstruction that relies on\nmultiple viewpoints, in this case, images are only observed from a single\nviewpoint. This challenge can be partially addressed by observing how galaxy\nemitters throughout the volume are lensed. However, this leads to the second\nchallenge: the shapes and exact locations of unlensed galaxies are unknown, and\ncan only be estimated with a very large degree of uncertainty. This introduces\nan overwhelming amount of noise which nearly drowns out the lensing signal\ncompletely. Previous approaches tackle this by imposing strong assumptions\nabout the structures in the volume. We instead propose a methodology using a\ngravitationally-constrained neural field to flexibly model the continuous\nmatter distribution. We take an analysis-by-synthesis approach, optimizing the\nweights of the neural network through a fully differentiable physical forward\nmodel to reproduce the lensing signal present in image measurements. We\nshowcase our method on simulations, including realistic simulated measurements\nof dark matter distributions that mimic data from upcoming telescope surveys.\nOur results show that our method can not only outperform previous methods, but\nimportantly is also able to recover potentially surprising dark matter\nstructures."}
{"id": "2504.14694", "pdf": "https://arxiv.org/pdf/2504.14694", "abs": "https://arxiv.org/abs/2504.14694", "authors": ["Yuting He", "Yiqiang Chen", "XiaoDong Yang", "Hanchao Yu", "Yi-Hua Huang", "Yang Gu"], "title": "Learning Critically: Selective Self Distillation in Federated Learning on Non-IID Data", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Federated learning (FL) enables multiple clients to collaboratively train a\nglobal model while keeping local data decentralized. Data heterogeneity\n(non-IID) across clients has imposed significant challenges to FL, which makes\nlocal models re-optimize towards their own local optima and forget the global\nknowledge, resulting in performance degradation and convergence slowdown. Many\nexisting works have attempted to address the non-IID issue by adding an extra\nglobal-model-based regularizing item to the local training but without an\nadaption scheme, which is not efficient enough to achieve high performance with\ndeep learning models. In this paper, we propose a Selective Self-Distillation\nmethod for Federated learning (FedSSD), which imposes adaptive constraints on\nthe local updates by self-distilling the global model's knowledge and\nselectively weighting it by evaluating the credibility at both the class and\nsample level. The convergence guarantee of FedSSD is theoretically analyzed and\nextensive experiments are conducted on three public benchmark datasets, which\ndemonstrates that FedSSD achieves better generalization and robustness in fewer\ncommunication rounds, compared with other state-of-the-art FL methods."}
{"id": "2504.14699", "pdf": "https://arxiv.org/pdf/2504.14699", "abs": "https://arxiv.org/abs/2504.14699", "authors": ["Sascha Jecklin", "Aidana Massalimova", "Ruyi Zha", "Lilian Calvet", "Christoph J. Laux", "Mazda Farshad", "Philipp Fürnstahl"], "title": "IXGS-Intraoperative 3D Reconstruction from Sparse, Arbitrarily Posed Real X-rays", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Spine surgery is a high-risk intervention demanding precise execution, often\nsupported by image-based navigation systems. Recently, supervised learning\napproaches have gained attention for reconstructing 3D spinal anatomy from\nsparse fluoroscopic data, significantly reducing reliance on\nradiation-intensive 3D imaging systems. However, these methods typically\nrequire large amounts of annotated training data and may struggle to generalize\nacross varying patient anatomies or imaging conditions. Instance-learning\napproaches like Gaussian splatting could offer an alternative by avoiding\nextensive annotation requirements. While Gaussian splatting has shown promise\nfor novel view synthesis, its application to sparse, arbitrarily posed real\nintraoperative X-rays has remained largely unexplored. This work addresses this\nlimitation by extending the $R^2$-Gaussian splatting framework to reconstruct\nanatomically consistent 3D volumes under these challenging conditions. We\nintroduce an anatomy-guided radiographic standardization step using style\ntransfer, improving visual consistency across views, and enhancing\nreconstruction quality. Notably, our framework requires no pretraining, making\nit inherently adaptable to new patients and anatomies. We evaluated our\napproach using an ex-vivo dataset. Expert surgical evaluation confirmed the\nclinical utility of the 3D reconstructions for navigation, especially when\nusing 20 to 30 views, and highlighted the standardization's benefit for\nanatomical clarity. Benchmarking via quantitative 2D metrics (PSNR/SSIM)\nconfirmed performance trade-offs compared to idealized settings, but also\nvalidated the improvement gained from standardization over raw inputs. This\nwork demonstrates the feasibility of instance-based volumetric reconstruction\nfrom arbitrary sparse-view X-rays, advancing intraoperative 3D imaging for\nsurgical navigation."}
{"id": "2504.14704", "pdf": "https://arxiv.org/pdf/2504.14704", "abs": "https://arxiv.org/abs/2504.14704", "authors": ["Hong Yang", "Qi Yu", "Travis Desel"], "title": "Can We Ignore Labels In Out of Distribution Detection?", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "Out-of-distribution (OOD) detection methods have recently become more\nprominent, serving as a core element in safety-critical autonomous systems. One\nmajor purpose of OOD detection is to reject invalid inputs that could lead to\nunpredictable errors and compromise safety. Due to the cost of labeled data,\nrecent works have investigated the feasibility of self-supervised learning\n(SSL) OOD detection, unlabeled OOD detection, and zero shot OOD detection. In\nthis work, we identify a set of conditions for a theoretical guarantee of\nfailure in unlabeled OOD detection algorithms from an information-theoretic\nperspective. These conditions are present in all OOD tasks dealing with\nreal-world data: I) we provide theoretical proof of unlabeled OOD detection\nfailure when there exists zero mutual information between the learning\nobjective and the in-distribution labels, a.k.a. 'label blindness', II) we\ndefine a new OOD task - Adjacent OOD detection - that tests for label blindness\nand accounts for a previously ignored safety gap in all OOD detection\nbenchmarks, and III) we perform experiments demonstrating that existing\nunlabeled OOD methods fail under conditions suggested by our label blindness\ntheory and analyze the implications for future research in unlabeled OOD\nmethods."}
{"id": "2504.14708", "pdf": "https://arxiv.org/pdf/2504.14708", "abs": "https://arxiv.org/abs/2504.14708", "authors": ["Parshuram N. Aarotale", "Ajita Rattani"], "title": "Time Frequency Analysis of EMG Signal for Gesture Recognition using Fine grained Features", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Electromyography (EMG) based hand gesture recognition converts forearm muscle\nactivity into control commands for prosthetics, rehabilitation, and human\ncomputer interaction. This paper proposes a novel approach to EMG-based hand\ngesture recognition that uses fine-grained classification and presents XMANet,\nwhich unifies low-level local and high level semantic cues through cross layer\nmutual attention among shallow to deep CNN experts. Using stacked spectrograms\nand scalograms derived from the Short Time Fourier Transform (STFT) and Wavelet\nTransform (WT), we benchmark XMANet against ResNet50, DenseNet-121,\nMobileNetV3, and EfficientNetB0. Experimental results on the Grabmyo dataset\nindicate that, using STFT, the proposed XMANet model outperforms the baseline\nResNet50, EfficientNetB0, MobileNetV3, and DenseNet121 models with improvement\nof approximately 1.72%, 4.38%, 5.10%, and 2.53%, respectively. When employing\nthe WT approach, improvements of around 1.57%, 1.88%, 1.46%, and 2.05% are\nobserved over the same baselines. Similarly, on the FORS EMG dataset, the\nXMANet(ResNet50) model using STFT shows an improvement of about 5.04% over the\nbaseline ResNet50. In comparison, the XMANet(DenseNet121) and\nXMANet(MobileNetV3) models yield enhancements of approximately 4.11% and 2.81%,\nrespectively. Moreover, when using WT, the proposed XMANet achieves gains of\naround 4.26%, 9.36%, 5.72%, and 6.09% over the baseline ResNet50, DenseNet121,\nMobileNetV3, and EfficientNetB0 models, respectively. These results confirm\nthat XMANet consistently improves performance across various architectures and\nsignal processing techniques, demonstrating the strong potential of fine\ngrained features for accurate and robust EMG classification."}
{"id": "2504.14709", "pdf": "https://arxiv.org/pdf/2504.14709", "abs": "https://arxiv.org/abs/2504.14709", "authors": ["Hui Zhou", "Shaoshuai Shi", "Hongsheng Li"], "title": "Exposing the Copycat Problem of Imitation-based Planner: A Novel Closed-Loop Simulator, Causal Benchmark and Joint IL-RL Baseline", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Machine learning (ML)-based planners have recently gained significant\nattention. They offer advantages over traditional optimization-based planning\nalgorithms. These advantages include fewer manually selected parameters and\nfaster development. Within ML-based planning, imitation learning (IL) is a\ncommon algorithm. It primarily learns driving policies directly from supervised\ntrajectory data. While IL has demonstrated strong performance on many open-loop\nbenchmarks, it remains challenging to determine if the learned policy truly\nunderstands fundamental driving principles, rather than simply extrapolating\nfrom the ego-vehicle's initial state. Several studies have identified this\nlimitation and proposed algorithms to address it. However, these methods often\nuse original datasets for evaluation. In these datasets, future trajectories\nare heavily dependent on initial conditions. Furthermore, IL often overfits to\nthe most common scenarios. It struggles to generalize to rare or unseen\nsituations.\n  To address these challenges, this work proposes: 1) a novel closed-loop\nsimulator supporting both imitation and reinforcement learning, 2) a causal\nbenchmark derived from the Waymo Open Dataset to rigorously assess the impact\nof the copycat problem, and 3) a novel framework integrating imitation learning\nand reinforcement learning to overcome the limitations of purely imitative\napproaches. The code for this work will be released soon."}
{"id": "2504.14727", "pdf": "https://arxiv.org/pdf/2504.14727", "abs": "https://arxiv.org/abs/2504.14727", "authors": ["Geng Liu", "Fei Zhu", "Rong Feng", "Zhiqiang Yi", "Shiqi Wang", "Gaofeng Meng", "Zhaoxiang Zhang"], "title": "Semi-parametric Memory Consolidation: Towards Brain-like Deep Continual Learning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Humans and most animals inherently possess a distinctive capacity to\ncontinually acquire novel experiences and accumulate worldly knowledge over\ntime. This ability, termed continual learning, is also critical for deep neural\nnetworks (DNNs) to adapt to the dynamically evolving world in open\nenvironments. However, DNNs notoriously suffer from catastrophic forgetting of\npreviously learned knowledge when trained on sequential tasks. In this work,\ninspired by the interactive human memory and learning system, we propose a\nnovel biomimetic continual learning framework that integrates semi-parametric\nmemory and the wake-sleep consolidation mechanism. For the first time, our\nmethod enables deep neural networks to retain high performance on novel tasks\nwhile maintaining prior knowledge in real-world challenging continual learning\nscenarios, e.g., class-incremental learning on ImageNet. This study\ndemonstrates that emulating biological intelligence provides a promising path\nto enable deep neural networks with continual learning capabilities."}
{"id": "2504.14737", "pdf": "https://arxiv.org/pdf/2504.14737", "abs": "https://arxiv.org/abs/2504.14737", "authors": ["Shuang Zeng", "Lei Zhu", "Xinliang Zhang", "Hangzhou He", "Yanye Lu"], "title": "SuperCL: Superpixel Guided Contrastive Learning for Medical Image Segmentation Pre-training", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Medical image segmentation is a critical yet challenging task, primarily due\nto the difficulty of obtaining extensive datasets of high-quality,\nexpert-annotated images. Contrastive learning presents a potential but still\nproblematic solution to this issue. Because most existing methods focus on\nextracting instance-level or pixel-to-pixel representation, which ignores the\ncharacteristics between intra-image similar pixel groups. Moreover, when\nconsidering contrastive pairs generation, most SOTA methods mainly rely on\nmanually setting thresholds, which requires a large number of gradient\nexperiments and lacks efficiency and generalization. To address these issues,\nwe propose a novel contrastive learning approach named SuperCL for medical\nimage segmentation pre-training. Specifically, our SuperCL exploits the\nstructural prior and pixel correlation of images by introducing two novel\ncontrastive pairs generation strategies: Intra-image Local Contrastive Pairs\n(ILCP) Generation and Inter-image Global Contrastive Pairs (IGCP) Generation.\nConsidering superpixel cluster aligns well with the concept of contrastive\npairs generation, we utilize the superpixel map to generate pseudo masks for\nboth ILCP and IGCP to guide supervised contrastive learning. Moreover, we also\npropose two modules named Average SuperPixel Feature Map Generation (ASP) and\nConnected Components Label Generation (CCL) to better exploit the prior\nstructural information for IGCP. Finally, experiments on 8 medical image\ndatasets indicate our SuperCL outperforms existing 12 methods. i.e. Our SuperCL\nachieves a superior performance with more precise predictions from\nvisualization figures and 3.15%, 5.44%, 7.89% DSC higher than the previous best\nresults on MMWHS, CHAOS, Spleen with 10% annotations. Our code will be released\nafter acceptance."}
{"id": "2504.14739", "pdf": "https://arxiv.org/pdf/2504.14739", "abs": "https://arxiv.org/abs/2504.14739", "authors": ["Arpit Agarwal", "Mohammad Amin Mirzaee", "Xiping Sun", "Wenzhen Yuan"], "title": "A Modularized Design Approach for GelSight Family of Vision-based Tactile Sensors", "categories": ["cs.RO", "cs.AI"], "comment": "The paper is accepted to International Journal of Robotics Research\n  with DOI 10.1177/02783649251339680", "summary": "GelSight family of vision-based tactile sensors has proven to be effective\nfor multiple robot perception and manipulation tasks. These sensors are based\non an internal optical system and an embedded camera to capture the deformation\nof the soft sensor surface, inferring the high-resolution geometry of the\nobjects in contact. However, customizing the sensors for different robot hands\nrequires a tedious trial-and-error process to re-design the optical system. In\nthis paper, we formulate the GelSight sensor design process as a systematic and\nobjective-driven design problem and perform the design optimization with a\nphysically accurate optical simulation. The method is based on modularizing and\nparameterizing the sensor's optical components and designing four generalizable\nobjective functions to evaluate the sensor. We implement the method with an\ninteractive and easy-to-use toolbox called OptiSense Studio. With the toolbox,\nnon-sensor experts can quickly optimize their sensor design in both forward and\ninverse ways following our predefined modules and steps. We demonstrate our\nsystem with four different GelSight sensors by quickly optimizing their initial\ndesign in simulation and transferring it to the real sensors."}
{"id": "2504.14751", "pdf": "https://arxiv.org/pdf/2504.14751", "abs": "https://arxiv.org/abs/2504.14751", "authors": ["Jianyu Zhang"], "title": "AI for the Open-World: the Learning Principles", "categories": ["cs.LG", "cs.AI"], "comment": "PhD thesis. This is not a compilation of published papers, but a new\n  one", "summary": "During the past decades, numerous successes of AI has been made on \"specific\ncapabilities\", named closed-world, such as artificial environments or specific\nreal-world tasks. This well-defined narrow capability brings two nice benefits,\na clear criterion of success and the opportunity to collect a lot of examples.\nThe criteria not only reveal whether a machine has achieved a goal, but reveal\nhow the machine falls short of the goal. As a result, human designers can fix\nthe problems one after the other until the machine is deemed good enough for\nthe task. Furthermore, the large set of collected examples reduces the\ndifficulty of this problem-fixing process (by the central limit theorem).\n  Do the success in closed-world translate into broad open-world, where a\nmachine is required to perform any task that a human could possibly undertake\nwith fewer examples and less priori knowledge from human designers? No. Because\ncompetence in a specific task provides little insight in handling other tasks,\nthe valuable criteria for specific tasks become helpless when handling broader\nunseen tasks. Furthermore, due to the shortage of examples in unseen tasks,\ncentral limit theorem does not stand on our side. At the end, human designers\nlose the oscilloscope to \"hack\" an AI system for the open-world.\n  Achieving AI for the open-world requires unique learning principles and\ninnovated techniques, which are different from the ones in building AI for the\nclosed-world. This thesis explores necessary learning principles required to\nconstruct AI for the open-world, including rich features (analogy a large tool\nbox), disentangled representation (an organized tool box), and inference-time\nlearning (a tool-savvy hand). Driven by the learning principles, this thesis\nfurther proposes techniques to use the learning principles, conducts enormous\nlarge-scale experiments to verify the learning principles."}
{"id": "2504.14757", "pdf": "https://arxiv.org/pdf/2504.14757", "abs": "https://arxiv.org/abs/2504.14757", "authors": ["Minh V. T. Pham", "Huy N. Phan", "Hoang N. Phan", "Cuong Le Chi", "Tien N. Nguyen", "Nghi D. Q. Bui"], "title": "SWE-Synth: Synthesizing Verifiable Bug-Fix Data to Enable Large Language Models in Resolving Real-World Bugs", "categories": ["cs.SE", "cs.AI"], "comment": "Work in progress", "summary": "Large language models (LLMs) are transforming automated program repair (APR)\nthrough agent-based approaches that localize bugs, generate patches, and verify\nfixes. However, the lack of high-quality, scalable training datasets,\nespecially those with verifiable outputs and intermediate reasoning\ntraces-limits progress, particularly for open-source models. In this work, we\npresent SWE-Synth, a framework for synthesizing realistic, verifiable, and\nprocess-aware bug-fix datasets at the repository level. SWE-Synth leverages LLM\nagents to simulate debugging workflows, producing not only bug-fix pairs but\nalso test cases and structured repair trajectories. Compared to manually\ncurated datasets, our method scales with minimal human effort while preserving\ncontextual richness and correctness. Experiments show that models trained on\nSWE-Synth outperform those trained on real-world datasets by 2.3% on SWE-Bench\nLite. Our results highlight the potential of synthetic, agent-generated data to\nadvance the state of the art in APR and software engineering automation."}
{"id": "2504.14762", "pdf": "https://arxiv.org/pdf/2504.14762", "abs": "https://arxiv.org/abs/2504.14762", "authors": ["Sahil Rajesh Dhayalkar"], "title": "A Combinatorial Theory of Dropout: Subnetworks, Graph Geometry, and Generalization", "categories": ["cs.LG", "cs.AI"], "comment": "17 pages (9 pages main content and remaining pages are references,\n  appendix which includes 7 figures, proofs and derivations)", "summary": "We propose a combinatorial and graph-theoretic theory of dropout by modeling\ntraining as a random walk over a high-dimensional graph of binary subnetworks.\nEach node represents a masked version of the network, and dropout induces\nstochastic traversal across this space. We define a subnetwork contribution\nscore that quantifies generalization and show that it varies smoothly over the\ngraph. Using tools from spectral graph theory, PAC-Bayes analysis, and\ncombinatorics, we prove that generalizing subnetworks form large, connected,\nlow-resistance clusters, and that their number grows exponentially with network\nwidth. This reveals dropout as a mechanism for sampling from a robust,\nstructured ensemble of well-generalizing subnetworks with built-in redundancy.\nExtensive experiments validate every theoretical claim across diverse\narchitectures. Together, our results offer a unified foundation for\nunderstanding dropout and suggest new directions for mask-guided regularization\nand subnetwork optimization."}
{"id": "2504.14779", "pdf": "https://arxiv.org/pdf/2504.14779", "abs": "https://arxiv.org/abs/2504.14779", "authors": ["Janet G. Johnson", "Macarena Peralta", "Mansanjam Kaur", "Ruijie Sophia Huang", "Sheng Zhao", "Ruijia Guan", "Shwetha Rajaram", "Michael Nebeling"], "title": "Exploring Collaborative GenAI Agents in Synchronous Group Settings: Eliciting Team Perceptions and Design Considerations for the Future of Work", "categories": ["cs.HC", "cs.AI"], "comment": "To be published in ACM Conference on Computer-Supported Cooperative\n  Work and Social Computing (CSCW 2025). 33 pages, 11 figures, 1 table", "summary": "While generative artificial intelligence (GenAI) is finding increased\nadoption in workplaces, current tools are primarily designed for individual\nuse. Prior work established the potential for these tools to enhance personal\ncreativity and productivity towards shared goals; however, we don't know yet\nhow to best take into account the nuances of group work and team dynamics when\ndeploying GenAI in work settings. In this paper, we investigate the potential\nof collaborative GenAI agents to augment teamwork in synchronous group settings\nthrough an exploratory study that engaged 25 professionals across 6 teams in\nspeculative design workshops and individual follow-up interviews. Our workshops\nincluded a mixed reality provotype to simulate embodied collaborative GenAI\nagents capable of actively participating in group discussions. Our findings\nsuggest that, if designed well, collaborative GenAI agents offer valuable\nopportunities to enhance team problem-solving by challenging groupthink,\nbridging communication gaps, and reducing social friction. However, teams'\nwillingness to integrate GenAI agents depended on its perceived fit across a\nnumber of individual, team, and organizational factors. We outline the key\ndesign tensions around agent representation, social prominence, and engagement\nand highlight the opportunities spatial and immersive technologies could offer\nto modulate GenAI influence on team outcomes and strike a balance between\naugmentation and agency."}
{"id": "2504.14783", "pdf": "https://arxiv.org/pdf/2504.14783", "abs": "https://arxiv.org/abs/2504.14783", "authors": ["Wenhui Zhu", "Peijie Qiu", "Xiwen Chen", "Zhangsihao Yang", "Aristeidis Sotiras", "Abolfazl Razi", "Yalin Wang"], "title": "How Effective Can Dropout Be in Multiple Instance Learning ?", "categories": ["cs.CV", "cs.AI", "eess.IV", "stat.ML"], "comment": null, "summary": "Multiple Instance Learning (MIL) is a popular weakly-supervised method for\nvarious applications, with a particular interest in histological whole slide\nimage (WSI) classification. Due to the gigapixel resolution of WSI,\napplications of MIL in WSI typically necessitate a two-stage training scheme:\nfirst, extract features from the pre-trained backbone and then perform MIL\naggregation. However, it is well-known that this suboptimal training scheme\nsuffers from \"noisy\" feature embeddings from the backbone and inherent weak\nsupervision, hindering MIL from learning rich and generalizable features.\nHowever, the most commonly used technique (i.e., dropout) for mitigating this\nissue has yet to be explored in MIL. In this paper, we empirically explore how\neffective the dropout can be in MIL. Interestingly, we observe that dropping\nthe top-k most important instances within a bag leads to better performance and\ngeneralization even under noise attack. Based on this key observation, we\npropose a novel MIL-specific dropout method, termed MIL-Dropout, which\nsystematically determines which instances to drop. Experiments on five MIL\nbenchmark datasets and two WSI datasets demonstrate that MIL-Dropout boosts the\nperformance of current MIL methods with a negligible computational cost. The\ncode is available at https://github.com/ChongQingNoSubway/MILDropout."}
{"id": "2504.14797", "pdf": "https://arxiv.org/pdf/2504.14797", "abs": "https://arxiv.org/abs/2504.14797", "authors": ["Clare E. Laney", "Andrew Barovic", "Armin Moin"], "title": "Automated Duplicate Bug Report Detection in Large Open Bug Repositories", "categories": ["cs.SE", "cs.AI"], "comment": "IEEE COMPSAC 2025", "summary": "Many users and contributors of large open-source projects report software\ndefects or enhancement requests (known as bug reports) to the issue-tracking\nsystems. However, they sometimes report issues that have already been reported.\nFirst, they may not have time to do sufficient research on existing bug\nreports. Second, they may not possess the right expertise in that specific area\nto realize that an existing bug report is essentially elaborating on the same\nmatter, perhaps with a different wording. In this paper, we propose a novel\napproach based on machine learning methods that can automatically detect\nduplicate bug reports in an open bug repository based on the textual data in\nthe reports. We present six alternative methods: Topic modeling, Gaussian Naive\nBayes, deep learning, time-based organization, clustering, and summarization\nusing a generative pre-trained transformer large language model. Additionally,\nwe introduce a novel threshold-based approach for duplicate identification, in\ncontrast to the conventional top-k selection method that has been widely used\nin the literature. Our approach demonstrates promising results across all the\nproposed methods, achieving accuracy rates ranging from the high 70%'s to the\nlow 90%'s. We evaluated our methods on a public dataset of issues belonging to\nan Eclipse open-source project."}
{"id": "2504.14804", "pdf": "https://arxiv.org/pdf/2504.14804", "abs": "https://arxiv.org/abs/2504.14804", "authors": ["Jiaxin GUO", "Xiaoyu Chen", "Zhiqiang Rao", "Jinlong Yang", "Zongyao Li", "Hengchao Shang", "Daimeng Wei", "Hao Yang"], "title": "Automatic Evaluation Metrics for Document-level Translation: Overview, Challenges and Trends", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "With the rapid development of deep learning technologies, the field of\nmachine translation has witnessed significant progress, especially with the\nadvent of large language models (LLMs) that have greatly propelled the\nadvancement of document-level translation. However, accurately evaluating the\nquality of document-level translation remains an urgent issue. This paper first\nintroduces the development status of document-level translation and the\nimportance of evaluation, highlighting the crucial role of automatic evaluation\nmetrics in reflecting translation quality and guiding the improvement of\ntranslation systems. It then provides a detailed analysis of the current state\nof automatic evaluation schemes and metrics, including evaluation methods with\nand without reference texts, as well as traditional metrics, Model-based\nmetrics and LLM-based metrics. Subsequently, the paper explores the challenges\nfaced by current evaluation methods, such as the lack of reference diversity,\ndependence on sentence-level alignment information, and the bias, inaccuracy,\nand lack of interpretability of the LLM-as-a-judge method. Finally, the paper\nlooks ahead to the future trends in evaluation methods, including the\ndevelopment of more user-friendly document-level evaluation methods and more\nrobust LLM-as-a-judge methods, and proposes possible research directions, such\nas reducing the dependency on sentence-level information, introducing\nmulti-level and multi-granular evaluation approaches, and training models\nspecifically for machine translation evaluation. This study aims to provide a\ncomprehensive analysis of automatic evaluation for document-level translation\nand offer insights into future developments."}
{"id": "2504.14805", "pdf": "https://arxiv.org/pdf/2504.14805", "abs": "https://arxiv.org/abs/2504.14805", "authors": ["Jinwoo Choi", "Seung-Woo Seo"], "title": "Dynamic Contrastive Skill Learning with State-Transition Based Skill Clustering and Dynamic Length Adjustment", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "ICLR 2025; 23 pages, 12 figures", "summary": "Reinforcement learning (RL) has made significant progress in various domains,\nbut scaling it to long-horizon tasks with complex decision-making remains\nchallenging. Skill learning attempts to address this by abstracting actions\ninto higher-level behaviors. However, current approaches often fail to\nrecognize semantically similar behaviors as the same skill and use fixed skill\nlengths, limiting flexibility and generalization. To address this, we propose\nDynamic Contrastive Skill Learning (DCSL), a novel framework that redefines\nskill representation and learning. DCSL introduces three key ideas:\nstate-transition based skill representation, skill similarity function\nlearning, and dynamic skill length adjustment. By focusing on state transitions\nand leveraging contrastive learning, DCSL effectively captures the semantic\ncontext of behaviors and adapts skill lengths to match the appropriate temporal\nextent of behaviors. Our approach enables more flexible and adaptive skill\nextraction, particularly in complex or noisy datasets, and demonstrates\ncompetitive performance compared to existing methods in task completion and\nefficiency."}
{"id": "2504.14808", "pdf": "https://arxiv.org/pdf/2504.14808", "abs": "https://arxiv.org/abs/2504.14808", "authors": ["Mario M. Kubek", "Shiraj Pokharel", "Thomas Böhme", "Emma L. McDaniel", "Herwig Unger", "Armin R. Mikler"], "title": "On Self-improving Token Embeddings", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "68T50, 68T07", "I.2.6; I.2.7; H.3.3"], "comment": "18 pages, 4 figures, 3 tables, accepted at the 2025 25th\n  International Conference on Innovations for Community Services (I4CS), June\n  11 - 13, Munich, Germany, 2025", "summary": "This article introduces a novel and fast method for refining pre-trained\nstatic word or, more generally, token embeddings. By incorporating the\nembeddings of neighboring tokens in text corpora, it continuously updates the\nrepresentation of each token, including those without pre-assigned embeddings.\nThis approach effectively addresses the out-of-vocabulary problem, too.\nOperating independently of large language models and shallow neural networks,\nit enables versatile applications such as corpus exploration, conceptual\nsearch, and word sense disambiguation. The method is designed to enhance token\nrepresentations within topically homogeneous corpora, where the vocabulary is\nrestricted to a specific domain, resulting in more meaningful embeddings\ncompared to general-purpose pre-trained vectors. As an example, the methodology\nis applied to explore storm events and their impacts on infrastructure and\ncommunities using narratives from a subset of the NOAA Storm Events database.\nThe article also demonstrates how the approach improves the representation of\nstorm-related terms over time, providing valuable insights into the evolving\nnature of disaster narratives."}
{"id": "2504.14815", "pdf": "https://arxiv.org/pdf/2504.14815", "abs": "https://arxiv.org/abs/2504.14815", "authors": ["Xiaoyong Yuan", "Xiaolong Ma", "Linke Guo", "Lan Zhang"], "title": "What Lurks Within? Concept Auditing for Shared Diffusion Models at Scale", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.CV"], "comment": "17 pages, 15 figures", "summary": "Diffusion models (DMs) have revolutionized text-to-image generation, enabling\nthe creation of highly realistic and customized images from text prompts. With\nthe rise of parameter-efficient fine-tuning (PEFT) techniques like LoRA, users\ncan now customize powerful pre-trained models using minimal computational\nresources. However, the widespread sharing of fine-tuned DMs on open platforms\nraises growing ethical and legal concerns, as these models may inadvertently or\ndeliberately generate sensitive or unauthorized content, such as copyrighted\nmaterial, private individuals, or harmful content. Despite the increasing\nregulatory attention on generative AI, there are currently no practical tools\nfor systematically auditing these models before deployment. In this paper, we\naddress the problem of concept auditing: determining whether a fine-tuned DM\nhas learned to generate a specific target concept. Existing approaches\ntypically rely on prompt-based input crafting and output-based image\nclassification but suffer from critical limitations, including prompt\nuncertainty, concept drift, and poor scalability. To overcome these challenges,\nwe introduce Prompt-Agnostic Image-Free Auditing (PAIA), a novel, model-centric\nconcept auditing framework. By treating the DM as the object of inspection,\nPAIA enables direct analysis of internal model behavior, bypassing the need for\noptimized prompts or generated images. We evaluate PAIA on 320 controlled model\nand 690 real-world community models sourced from a public DM sharing platform.\nPAIA achieves over 90% detection accuracy while reducing auditing time by\n18-40x compared to existing baselines. To our knowledge, PAIA is the first\nscalable and practical solution for pre-deployment concept auditing of\ndiffusion models, providing a practical foundation for safer and more\ntransparent diffusion model sharing."}
{"id": "2504.14825", "pdf": "https://arxiv.org/pdf/2504.14825", "abs": "https://arxiv.org/abs/2504.14825", "authors": ["Zhoujie Qian"], "title": "ECViT: Efficient Convolutional Vision Transformer with Local-Attention and Multi-scale Stages", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision Transformers (ViTs) have revolutionized computer vision by leveraging\nself-attention to model long-range dependencies. However, ViTs face challenges\nsuch as high computational costs due to the quadratic scaling of self-attention\nand the requirement of a large amount of training data. To address these\nlimitations, we propose the Efficient Convolutional Vision Transformer (ECViT),\na hybrid architecture that effectively combines the strengths of CNNs and\nTransformers. ECViT introduces inductive biases such as locality and\ntranslation invariance, inherent to Convolutional Neural Networks (CNNs) into\nthe Transformer framework by extracting patches from low-level features and\nenhancing the encoder with convolutional operations. Additionally, it\nincorporates local-attention and a pyramid structure to enable efficient\nmulti-scale feature extraction and representation. Experimental results\ndemonstrate that ECViT achieves an optimal balance between performance and\nefficiency, outperforming state-of-the-art models on various image\nclassification tasks while maintaining low computational and storage\nrequirements. ECViT offers an ideal solution for applications that prioritize\nhigh efficiency without compromising performance."}
{"id": "2504.14832", "pdf": "https://arxiv.org/pdf/2504.14832", "abs": "https://arxiv.org/abs/2504.14832", "authors": ["Yue Li", "Weizhi Liu", "Dongdong Lin"], "title": "Protecting Your Voice: Temporal-aware Robust Watermarking", "categories": ["cs.CR", "cs.AI", "cs.SD"], "comment": null, "summary": "The rapid advancement of generative models has led to the synthesis of\nreal-fake ambiguous voices. To erase the ambiguity, embedding watermarks into\nthe frequency-domain features of synthesized voices has become a common\nroutine. However, the robustness achieved by choosing the frequency domain\noften comes at the expense of fine-grained voice features, leading to a loss of\nfidelity. Maximizing the comprehensive learning of time-domain features to\nenhance fidelity while maintaining robustness, we pioneer a\n\\textbf{\\underline{t}}emporal-aware\n\\textbf{\\underline{r}}ob\\textbf{\\underline{u}}st\nwat\\textbf{\\underline{e}}rmarking (\\emph{True}) method for protecting the\nspeech and singing voice."}
{"id": "2504.14839", "pdf": "https://arxiv.org/pdf/2504.14839", "abs": "https://arxiv.org/abs/2504.14839", "authors": ["Xinjie Shen", "Zhichao Geng", "Yang Yang"], "title": "Exploring $\\ell_0$ Sparsification for Inference-free Sparse Retrievers", "categories": ["cs.IR", "cs.AI"], "comment": "Accepted by SIGIR 2025", "summary": "With increasing demands for efficiency, information retrieval has developed a\nbranch of sparse retrieval, further advancing towards inference-free retrieval\nwhere the documents are encoded during indexing time and there is no\nmodel-inference for queries. Existing sparse retrieval models rely on FLOPS\nregularization for sparsification, while this mechanism was originally designed\nfor Siamese encoders, it is considered to be suboptimal in inference-free\nscenarios which is asymmetric. Previous attempts to adapt FLOPS for\ninference-free scenarios have been limited to rule-based methods, leaving the\npotential of sparsification approaches for inference-free retrieval models\nlargely unexplored. In this paper, we explore $\\ell_0$ inspired sparsification\nmanner for inference-free retrievers. Through comprehensive out-of-domain\nevaluation on the BEIR benchmark, our method achieves state-of-the-art\nperformance among inference-free sparse retrieval models and is comparable to\nleading Siamese sparse retrieval models. Furthermore, we provide insights into\nthe trade-off between retrieval effectiveness and computational efficiency,\ndemonstrating practical value for real-world applications."}
{"id": "2504.14848", "pdf": "https://arxiv.org/pdf/2504.14848", "abs": "https://arxiv.org/abs/2504.14848", "authors": ["Yunpu Zhao", "Rui Zhang", "Junbin Xiao", "Ruibo Hou", "Jiaming Guo", "Zihao Zhang", "Yifan Hao", "Yunji Chen"], "title": "Object-Level Verbalized Confidence Calibration in Vision-Language Models via Semantic Perturbation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision-language models (VLMs) excel in various multimodal tasks but\nfrequently suffer from poor calibration, resulting in misalignment between\ntheir verbalized confidence and response correctness. This miscalibration\nundermines user trust, especially when models confidently provide incorrect or\nfabricated information. In this work, we propose a novel Confidence Calibration\nthrough Semantic Perturbation (CSP) framework to improve the calibration of\nverbalized confidence for VLMs in response to object-centric queries. We first\nintroduce a perturbed dataset where Gaussian noise is applied to the key object\nregions to simulate visual uncertainty at different confidence levels,\nestablishing an explicit mapping between visual ambiguity and confidence\nlevels. We further enhance calibration through a two-stage training process\ncombining supervised fine-tuning on the perturbed dataset with subsequent\npreference optimization. Extensive experiments on popular benchmarks\ndemonstrate that our method significantly improves the alignment between\nverbalized confidence and response correctness while maintaining or enhancing\noverall task performance. These results highlight the potential of semantic\nperturbation as a practical tool for improving the reliability and\ninterpretability of VLMs."}
{"id": "2504.14860", "pdf": "https://arxiv.org/pdf/2504.14860", "abs": "https://arxiv.org/abs/2504.14860", "authors": ["Ziyi Liu", "Yangcen Liu"], "title": "Bridge the Gap: From Weak to Full Supervision for Temporal Action Localization with PseudoFormer", "categories": ["cs.CV", "cs.AI"], "comment": "CVPR 2025: IEEE Conference on Computer Vision and Pattern Recognition", "summary": "Weakly-supervised Temporal Action Localization (WTAL) has achieved notable\nsuccess but still suffers from a lack of temporal annotations, leading to a\nperformance and framework gap compared with fully-supervised methods. While\nrecent approaches employ pseudo labels for training, three key challenges:\ngenerating high-quality pseudo labels, making full use of different priors, and\noptimizing training methods with noisy labels remain unresolved. Due to these\nperspectives, we propose PseudoFormer, a novel two-branch framework that\nbridges the gap between weakly and fully-supervised Temporal Action\nLocalization (TAL). We first introduce RickerFusion, which maps all predicted\naction proposals to a global shared space to generate pseudo labels with better\nquality. Subsequently, we leverage both snippet-level and proposal-level labels\nwith different priors from the weak branch to train the regression-based model\nin the full branch. Finally, the uncertainty mask and iterative refinement\nmechanism are applied for training with noisy pseudo labels. PseudoFormer\nachieves state-of-the-art WTAL results on the two commonly used benchmarks,\nTHUMOS14 and ActivityNet1.3. Besides, extensive ablation studies demonstrate\nthe contribution of each component of our method."}
{"id": "2504.14875", "pdf": "https://arxiv.org/pdf/2504.14875", "abs": "https://arxiv.org/abs/2504.14875", "authors": ["Chris Dongjoo Kim", "Jihwan Moon", "Sangwoo Moon", "Heeseung Yun", "Sihaeng Lee", "Aniruddha Kembhavi", "Soonyoung Lee", "Gunhee Kim", "Sangho Lee", "Christopher Clark"], "title": "ReSpec: Relevance and Specificity Grounded Online Filtering for Learning on Video-Text Data Streams", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "CVPR 2025 (main conference)", "summary": "The rapid growth of video-text data presents challenges in storage and\ncomputation during training. Online learning, which processes streaming data in\nreal-time, offers a promising solution to these issues while also allowing\nswift adaptations in scenarios demanding real-time responsiveness. One strategy\nto enhance the efficiency and effectiveness of learning involves identifying\nand prioritizing data that enhances performance on target downstream tasks. We\npropose Relevance and Specificity-based online filtering framework (ReSpec)\nthat selects data based on four criteria: (i) modality alignment for clean\ndata, (ii) task relevance for target focused data, (iii) specificity for\ninformative and detailed data, and (iv) efficiency for low-latency processing.\nRelevance is determined by the probabilistic alignment of incoming data with\ndownstream tasks, while specificity employs the distance to a root embedding\nrepresenting the least specific data as an efficient proxy for informativeness.\nBy establishing reference points from target task data, ReSpec filters incoming\ndata in real-time, eliminating the need for extensive storage and compute.\nEvaluating on large-scale datasets WebVid2M and VideoCC3M, ReSpec attains\nstate-of-the-art performance on five zeroshot video retrieval tasks, using as\nlittle as 5% of the data while incurring minimal compute. The source code is\navailable at https://github.com/cdjkim/ReSpec."}
{"id": "2504.14879", "pdf": "https://arxiv.org/pdf/2504.14879", "abs": "https://arxiv.org/abs/2504.14879", "authors": ["Hassan Wasswa", "Aziida Nanyonga", "Timothy Lynar"], "title": "Impact of Latent Space Dimension on IoT Botnet Detection Performance: VAE-Encoder Versus ViT-Encoder", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The rapid evolution of Internet of Things (IoT) technology has led to a\nsignificant increase in the number of IoT devices, applications, and services.\nThis surge in IoT devices, along with their widespread presence, has made them\na prime target for various cyber-attacks, particularly through IoT botnets. As\na result, security has become a major concern within the IoT ecosystem. This\nstudy focuses on investigating how the latent dimension impacts the performance\nof different deep learning classifiers when trained on latent vector\nrepresentations of the train dataset. The primary objective is to compare the\noutcomes of these models when encoder components from two cutting-edge\narchitectures: the Vision Transformer (ViT) and the Variational Auto-Encoder\n(VAE) are utilized to project the high dimensional train dataset to the learned\nlow dimensional latent space. The encoder components are employed to project\nhigh-dimensional structured .csv IoT botnet traffic datasets to various latent\nsizes. Evaluated on N-BaIoT and CICIoT2022 datasets, findings reveal that\nVAE-encoder based dimension reduction outperforms ViT-encoder based dimension\nreduction for both datasets in terms of four performance metrics including\naccuracy, precision, recall, and F1-score for all models which can be\nattributed to absence of spatial patterns in the datasets the ViT model\nattempts to learn and extract from image instances."}
{"id": "2504.14889", "pdf": "https://arxiv.org/pdf/2504.14889", "abs": "https://arxiv.org/abs/2504.14889", "authors": ["Seunghun Lee", "Jinyoung Park", "Jaewon Chu", "Minseo Yoon", "Hyunwoo J. Kim"], "title": "Latent Bayesian Optimization via Autoregressive Normalizing Flows", "categories": ["cs.LG", "cs.AI"], "comment": "ICLR 2025", "summary": "Bayesian Optimization (BO) has been recognized for its effectiveness in\noptimizing expensive and complex objective functions. Recent advancements in\nLatent Bayesian Optimization (LBO) have shown promise by integrating generative\nmodels such as variational autoencoders (VAEs) to manage the complexity of\nhigh-dimensional and structured data spaces. However, existing LBO approaches\noften suffer from the value discrepancy problem, which arises from the\nreconstruction gap between input and latent spaces. This value discrepancy\nproblem propagates errors throughout the optimization process, leading to\nsuboptimal outcomes. To address this issue, we propose a Normalizing Flow-based\nBayesian Optimization (NF-BO), which utilizes normalizing flow as a generative\nmodel to establish one-to-one encoding function from the input space to the\nlatent space, along with its left-inverse decoding function, eliminating the\nreconstruction gap. Specifically, we introduce SeqFlow, an autoregressive\nnormalizing flow for sequence data. In addition, we develop a new candidate\nsampling strategy that dynamically adjusts the exploration probability for each\ntoken based on its importance. Through extensive experiments, our NF-BO method\ndemonstrates superior performance in molecule generation tasks, significantly\noutperforming both traditional and recent LBO approaches."}
{"id": "2504.14904", "pdf": "https://arxiv.org/pdf/2504.14904", "abs": "https://arxiv.org/abs/2504.14904", "authors": ["Xingyu Lu", "Tianke Zhang", "Chang Meng", "Xiaobei Wang", "Jinpeng Wang", "YiFan Zhang", "Shisong Tang", "Changyi Liu", "Haojie Ding", "Kaiyu Jiang", "Kaiyu Tang", "Bin Wen", "Hai-Tao Zheng", "Fan Yang", "Tingting Gao", "Di Zhang", "Kun Gai"], "title": "VLM as Policy: Common-Law Content Moderation Framework for Short Video Platform", "categories": ["cs.SI", "cs.AI", "cs.CL", "cs.MM"], "comment": "20 pages, 6 figures", "summary": "Exponentially growing short video platforms (SVPs) face significant\nchallenges in moderating content detrimental to users' mental health,\nparticularly for minors. The dissemination of such content on SVPs can lead to\ncatastrophic societal consequences. Although substantial efforts have been\ndedicated to moderating such content, existing methods suffer from critical\nlimitations: (1) Manual review is prone to human bias and incurs high\noperational costs. (2) Automated methods, though efficient, lack nuanced\ncontent understanding, resulting in lower accuracy. (3) Industrial moderation\nregulations struggle to adapt to rapidly evolving trends due to long update\ncycles. In this paper, we annotate the first SVP content moderation benchmark\nwith authentic user/reviewer feedback to fill the absence of benchmark in this\nfield. Then we evaluate various methods on the benchmark to verify the\nexistence of the aforementioned limitations. We further propose our common-law\ncontent moderation framework named KuaiMod to address these challenges. KuaiMod\nconsists of three components: training data construction, offline adaptation,\nand online deployment & refinement. Leveraging large vision language model\n(VLM) and Chain-of-Thought (CoT) reasoning, KuaiMod adequately models video\ntoxicity based on sparse user feedback and fosters dynamic moderation policy\nwith rapid update speed and high accuracy. Offline experiments and large-scale\nonline A/B test demonstrates the superiority of KuaiMod: KuaiMod achieves the\nbest moderation performance on our benchmark. The deployment of KuaiMod reduces\nthe user reporting rate by 20% and its application in video recommendation\nincreases both Daily Active User (DAU) and APP Usage Time (AUT) on several\nKuaishou scenarios. We have open-sourced our benchmark at\nhttps://kuaimod.github.io."}
{"id": "2504.14913", "pdf": "https://arxiv.org/pdf/2504.14913", "abs": "https://arxiv.org/abs/2504.14913", "authors": ["Kenji Iwata", "Eiki Ishidera", "Toshifumi Yamaai", "Yutaka Satoh", "Hiroshi Tanaka", "Katsuhiko Takahashi", "Akio Furuhata", "Yoshihisa Tanabe", "Hiroshi Matsumura"], "title": "Guidelines for External Disturbance Factors in the Use of OCR in Real-World Environments", "categories": ["cs.CV", "cs.AI", "I.5.2; I.5.m"], "comment": "16 pages, 14 figures", "summary": "The performance of OCR has improved with the evolution of AI technology. As\nOCR continues to broaden its range of applications, the increased likelihood of\ninterference introduced by various usage environments can prevent it from\nachieving its inherent performance. This results in reduced recognition\naccuracy under certain conditions, and makes the quality control of recognition\ndevices more challenging. Therefore, to ensure that users can properly utilize\nOCR, we compiled the real-world external disturbance factors that cause\nperformance degradation, along with the resulting image degradation phenomena,\ninto an external disturbance factor table and, by also indicating how to make\nuse of it, organized them into guidelines."}
{"id": "2504.14915", "pdf": "https://arxiv.org/pdf/2504.14915", "abs": "https://arxiv.org/abs/2504.14915", "authors": ["Yeona Hong", "Hyewon Han", "Woo-jin Chung", "Hong-Goo Kang"], "title": "StableQuant: Layer Adaptive Post-Training Quantization for Speech Foundation Models", "categories": ["eess.AS", "cs.AI"], "comment": "Accepted at ICASSP 2025", "summary": "In this paper, we propose StableQuant, a novel adaptive post-training\nquantization (PTQ) algorithm for widely used speech foundation models (SFMs).\nWhile PTQ has been successfully employed for compressing large language models\n(LLMs) due to its ability to bypass additional fine-tuning, directly applying\nthese techniques to SFMs may not yield optimal results, as SFMs utilize\ndistinct network architecture for feature extraction. StableQuant demonstrates\noptimal quantization performance regardless of the network architecture type,\nas it adaptively determines the quantization range for each layer by analyzing\nboth the scale distributions and overall performance. We evaluate our algorithm\non two SFMs, HuBERT and wav2vec2.0, for an automatic speech recognition (ASR)\ntask, and achieve superior performance compared to traditional PTQ methods.\nStableQuant successfully reduces the sizes of SFM models to a quarter and\ndoubles the inference speed while limiting the word error rate (WER)\nperformance drop to less than 0.3% with 8-bit quantization."}
{"id": "2504.14921", "pdf": "https://arxiv.org/pdf/2504.14921", "abs": "https://arxiv.org/abs/2504.14921", "authors": ["Songping Wang", "Hanqing Liu", "Yueming Lyu", "Xiantao Hu", "Ziwen He", "Wei Wang", "Caifeng Shan", "Liang Wang"], "title": "Fast Adversarial Training with Weak-to-Strong Spatial-Temporal Consistency in the Frequency Domain on Videos", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Adversarial Training (AT) has been shown to significantly enhance adversarial\nrobustness via a min-max optimization approach. However, its effectiveness in\nvideo recognition tasks is hampered by two main challenges. First, fast\nadversarial training for video models remains largely unexplored, which\nseverely impedes its practical applications. Specifically, most video\nadversarial training methods are computationally costly, with long training\ntimes and high expenses. Second, existing methods struggle with the trade-off\nbetween clean accuracy and adversarial robustness. To address these challenges,\nwe introduce Video Fast Adversarial Training with Weak-to-Strong consistency\n(VFAT-WS), the first fast adversarial training method for video data.\nSpecifically, VFAT-WS incorporates the following key designs: First, it\nintegrates a straightforward yet effective temporal frequency augmentation\n(TF-AUG), and its spatial-temporal enhanced form STF-AUG, along with a\nsingle-step PGD attack to boost training efficiency and robustness. Second, it\ndevises a weak-to-strong spatial-temporal consistency regularization, which\nseamlessly integrates the simpler TF-AUG and the more complex STF-AUG.\nLeveraging the consistency regularization, it steers the learning process from\nsimple to complex augmentations. Both of them work together to achieve a better\ntrade-off between clean accuracy and robustness. Extensive experiments on\nUCF-101 and HMDB-51 with both CNN and Transformer-based models demonstrate that\nVFAT-WS achieves great improvements in adversarial robustness and corruption\nrobustness, while accelerating training by nearly 490%."}
{"id": "2504.14936", "pdf": "https://arxiv.org/pdf/2504.14936", "abs": "https://arxiv.org/abs/2504.14936", "authors": ["Maria Fay", "Frederik F. Flöther"], "title": "Giving AI a voice: how does AI think it should be treated?", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "With the astounding progress in (generative) artificial intelligence (AI),\nthere has been significant public discourse regarding regulation and ethics of\nthe technology. Is it sufficient when humans discuss this with other humans?\nOr, given that AI is increasingly becoming a viable source of inspiration for\npeople (and let alone the hypothetical possibility that the technology may at\nsome point become \"artificial general intelligence\" and/or develop\nconsciousness), should AI not join the discourse? There are new questions and\nangles that AI brings to the table that we might not have considered before -\nso let us make the key subject of this book an active participant. This chapter\ntherefore includes a brief human-AI conversation on the topic of AI rights and\nethics."}
{"id": "2504.14945", "pdf": "https://arxiv.org/pdf/2504.14945", "abs": "https://arxiv.org/abs/2504.14945", "authors": ["Jianhao Yan", "Yafu Li", "Zican Hu", "Zhi Wang", "Ganqu Cui", "Xiaoye Qu", "Yu Cheng", "Yue Zhang"], "title": "Learning to Reason under Off-Policy Guidance", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Work in progress", "summary": "Recent advances in large reasoning models (LRMs) demonstrate that\nsophisticated behaviors such as multi-step reasoning and self-reflection can\nemerge via reinforcement learning (RL) with simple rule-based rewards. However,\nexisting zero-RL approaches are inherently ``on-policy'', limiting learning to\na model's own outputs and failing to acquire reasoning abilities beyond its\ninitial capabilities. We introduce LUFFY (Learning to reason Under oFF-policY\nguidance), a framework that augments zero-RL with off-policy reasoning traces.\nLUFFY dynamically balances imitation and exploration by combining off-policy\ndemonstrations with on-policy rollouts during training. Notably, we propose\npolicy shaping via regularized importance sampling to avoid superficial and\nrigid imitation during mixed-policy training. Remarkably, LUFFY achieves an\nover +7.0 average gain across six math benchmarks and an advantage of over +6.2\npoints in out-of-distribution tasks. It also substantially surpasses\nimitation-based supervised fine-tuning (SFT), particularly in generalization.\nAnalysis shows LUFFY not only imitates effectively but also explores beyond\ndemonstrations, offering a scalable path to train generalizable reasoning\nmodels with off-policy guidance."}
{"id": "2504.14963", "pdf": "https://arxiv.org/pdf/2504.14963", "abs": "https://arxiv.org/abs/2504.14963", "authors": ["Rui Ribeiro", "Luísa Coheur", "Joao P. Carvalho"], "title": "Speaker Fuzzy Fingerprints: Benchmarking Text-Based Identification in Multiparty Dialogues", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.LG", "cs.NE"], "comment": "Paper accepted at the FUZZY IEEE 2025 conference", "summary": "Speaker identification using voice recordings leverages unique acoustic\nfeatures, but this approach fails when only textual data is available. Few\napproaches have attempted to tackle the problem of identifying speakers solely\nfrom text, and the existing ones have primarily relied on traditional methods.\nIn this work, we explore the use of fuzzy fingerprints from large pre-trained\nmodels to improve text-based speaker identification. We integrate\nspeaker-specific tokens and context-aware modeling, demonstrating that\nconversational context significantly boosts accuracy, reaching 70.6% on the\nFriends dataset and 67.7% on the Big Bang Theory dataset. Additionally, we show\nthat fuzzy fingerprints can approximate full fine-tuning performance with fewer\nhidden units, offering improved interpretability. Finally, we analyze ambiguous\nutterances and propose a mechanism to detect speaker-agnostic lines. Our\nfindings highlight key challenges and provide insights for future improvements\nin text-based speaker identification."}
{"id": "2504.14985", "pdf": "https://arxiv.org/pdf/2504.14985", "abs": "https://arxiv.org/abs/2504.14985", "authors": ["Fatih Deniz", "Dorde Popovic", "Yazan Boshmaf", "Euisuh Jeong", "Minhaj Ahmad", "Sanjay Chawla", "Issa Khalil"], "title": "aiXamine: LLM Safety and Security Simplified", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Evaluating Large Language Models (LLMs) for safety and security remains a\ncomplex task, often requiring users to navigate a fragmented landscape of ad\nhoc benchmarks, datasets, metrics, and reporting formats. To address this\nchallenge, we present aiXamine, a comprehensive black-box evaluation platform\nfor LLM safety and security. aiXamine integrates over 40 tests (i.e.,\nbenchmarks) organized into eight key services targeting specific dimensions of\nsafety and security: adversarial robustness, code security, fairness and bias,\nhallucination, model and data privacy, out-of-distribution (OOD) robustness,\nover-refusal, and safety alignment. The platform aggregates the evaluation\nresults into a single detailed report per model, providing a detailed breakdown\nof model performance, test examples, and rich visualizations. We used aiXamine\nto assess over 50 publicly available and proprietary LLMs, conducting over 2K\nexaminations. Our findings reveal notable vulnerabilities in leading models,\nincluding susceptibility to adversarial attacks in OpenAI's GPT-4o, biased\noutputs in xAI's Grok-3, and privacy weaknesses in Google's Gemini 2.0.\nAdditionally, we observe that open-source models can match or exceed\nproprietary models in specific services such as safety alignment, fairness and\nbias, and OOD robustness. Finally, we identify trade-offs between distillation\nstrategies, model size, training methods, and architectural choices."}
{"id": "2504.14995", "pdf": "https://arxiv.org/pdf/2504.14995", "abs": "https://arxiv.org/abs/2504.14995", "authors": ["Keisuke Murota", "Takumi Kobori"], "title": "Trainable Quantum Neural Network for Multiclass Image Classification with the Power of Pre-trained Tree Tensor Networks", "categories": ["quant-ph", "cs.AI", "cs.LG"], "comment": "11 pages, 12 figures, 2 tables. This work has been submitted to the\n  IEEE for possible publication", "summary": "Tree tensor networks (TTNs) offer powerful models for image classification.\nWhile these TTN image classifiers already show excellent performance on\nclassical hardware, embedding them into quantum neural networks (QNNs) may\nfurther improve the performance by leveraging quantum resources. However,\nembedding TTN classifiers into QNNs for multiclass classification remains\nchallenging. Key obstacles are the highorder gate operations required for large\nbond dimensions and the mid-circuit postselection with exponentially low\nsuccess rates necessary for the exact embedding. In this work, to address these\nchallenges, we propose forest tensor network (FTN)-classifiers, which aggregate\nmultiple small-bond-dimension TTNs. This allows us to handle multiclass\nclassification without requiring large gates in the embedded circuits. We then\nremove the overhead of mid-circuit postselection by extending the adiabatic\nencoding framework to our setting and smoothly encode the FTN-classifiers into\na quantum forest tensor network (qFTN)- classifiers. Numerical experiments on\nMNIST and CIFAR-10 demonstrate that we can successfully train FTN-classifiers\nand encode them into qFTN-classifiers, while maintaining or even improving the\nperformance of the pre-trained FTN-classifiers. These results suggest that\nsynergy between TTN classification models and QNNs can provide a robust and\nscalable framework for multiclass quantum-enhanced image classification."}
{"id": "2504.15035", "pdf": "https://arxiv.org/pdf/2504.15035", "abs": "https://arxiv.org/abs/2504.15035", "authors": ["Yue Li", "Weizhi Liu", "Dongdong Lin"], "title": "SOLIDO: A Robust Watermarking Method for Speech Synthesis via Low-Rank Adaptation", "categories": ["cs.CR", "cs.AI", "cs.SD"], "comment": null, "summary": "The accelerated advancement of speech generative models has given rise to\nsecurity issues, including model infringement and unauthorized abuse of\ncontent. Although existing generative watermarking techniques have proposed\ncorresponding solutions, most methods require substantial computational\noverhead and training costs. In addition, some methods have limitations in\nrobustness when handling variable-length inputs. To tackle these challenges, we\npropose \\textsc{SOLIDO}, a novel generative watermarking method that integrates\nparameter-efficient fine-tuning with speech watermarking through low-rank\nadaptation (LoRA) for speech diffusion models. Concretely, the watermark\nencoder converts the watermark to align with the input of diffusion models. To\nachieve precise watermark extraction from variable-length inputs, the watermark\ndecoder based on depthwise separable convolution is designed for watermark\nrecovery. To further enhance speech generation performance and watermark\nextraction capability, we propose a speech-driven lightweight fine-tuning\nstrategy, which reduces computational overhead through LoRA. Comprehensive\nexperiments demonstrate that the proposed method ensures high-fidelity\nwatermarked speech even at a large capacity of 2000 bps. Furthermore, against\ncommon individual and compound speech attacks, our SOLIDO achieves a maximum\naverage extraction accuracy of 99.20\\% and 98.43\\%, respectively. It surpasses\nother state-of-the-art methods by nearly 23\\% in resisting time-stretching\nattacks."}
{"id": "2504.15041", "pdf": "https://arxiv.org/pdf/2504.15041", "abs": "https://arxiv.org/abs/2504.15041", "authors": ["Shiben Liu", "Huijie Fan", "Qiang Wang", "Baojie Fan", "Yandong Tang", "Liangqiong Qu"], "title": "Distribution-aware Forgetting Compensation for Exemplar-Free Lifelong Person Re-identification", "categories": ["cs.CV", "cs.AI"], "comment": "12 pages, 5 figures", "summary": "Lifelong Person Re-identification (LReID) suffers from a key challenge in\npreserving old knowledge while adapting to new information. The existing\nsolutions include rehearsal-based and rehearsal-free methods to address this\nchallenge. Rehearsal-based approaches rely on knowledge distillation,\ncontinuously accumulating forgetting during the distillation process.\nRehearsal-free methods insufficiently learn the distribution of each domain,\nleading to forgetfulness over time. To solve these issues, we propose a novel\nDistribution-aware Forgetting Compensation (DAFC) model that explores\ncross-domain shared representation learning and domain-specific distribution\nintegration without using old exemplars or knowledge distillation. We propose a\nText-driven Prompt Aggregation (TPA) that utilizes text features to enrich\nprompt elements and guide the prompt model to learn fine-grained\nrepresentations for each instance. This can enhance the differentiation of\nidentity information and establish the foundation for domain distribution\nawareness. Then, Distribution-based Awareness and Integration (DAI) is designed\nto capture each domain-specific distribution by a dedicated expert network and\nadaptively consolidate them into a shared region in high-dimensional space. In\nthis manner, DAI can consolidate and enhance cross-domain shared representation\nlearning while alleviating catastrophic forgetting. Furthermore, we develop a\nKnowledge Consolidation Mechanism (KCM) that comprises instance-level\ndiscrimination and cross-domain consistency alignment strategies to facilitate\nmodel adaptive learning of new knowledge from the current domain and promote\nknowledge consolidation learning between acquired domain-specific\ndistributions, respectively. Experimental results show that our DAFC\noutperforms state-of-the-art methods. Our code is available at\nhttps://github.com/LiuShiBen/DAFC."}
{"id": "2504.15044", "pdf": "https://arxiv.org/pdf/2504.15044", "abs": "https://arxiv.org/abs/2504.15044", "authors": ["Benshan Wang", "Qiarong Xiao", "Tengji Xu", "Li Fan", "Shaojie Liu", "Jianji Dong", "Junwen Zhang", "Chaoran Huang"], "title": "Beyond Terabit/s Integrated Neuromorphic Photonic Processor for DSP-Free Optical Interconnects", "categories": ["physics.optics", "cs.AI", "cs.ET"], "comment": "22 pages, 6 figures", "summary": "The rapid expansion of generative AI drives unprecedented demands for\nhigh-performance computing. Training large-scale AI models now requires vast\ninterconnected GPU clusters across multiple data centers. Multi-scale AI\ntraining and inference demand uniform, ultra-low latency, and energy-efficient\nlinks to enable massive GPUs to function as a single cohesive unit. However,\ntraditional electrical and optical interconnects, relying on conventional\ndigital signal processors (DSPs) for signal distortion compensation,\nincreasingly fail to meet these stringent requirements. To overcome these\nlimitations, we present an integrated neuromorphic optical signal processor\n(OSP) that leverages deep reservoir computing and achieves DSP-free,\nall-optical, real-time processing. Experimentally, our OSP achieves a 100 Gbaud\nPAM4 per lane, 1.6 Tbit/s data center interconnect over a 5 km optical fiber in\nthe C-band (equivalent to over 80 km in the O-band), far exceeding the reach of\nstate-of-the-art DSP solutions, which are fundamentally constrained by\nchromatic dispersion in IMDD systems. Simultaneously, it reduces processing\nlatency by four orders of magnitude and energy consumption by three orders of\nmagnitude. Unlike DSPs, which introduce increased latency at high data rates,\nour OSP maintains consistent, ultra-low latency regardless of data rate\nscaling, making it ideal for future optical interconnects. Moreover, the OSP\nretains full optical field information for better impairment compensation and\nadapts to various modulation formats, data rates, and wavelengths. Fabricated\nusing a mature silicon photonic process, the OSP can be monolithically\nintegrated with silicon photonic transceivers, enhancing the compactness and\nreliability of all-optical interconnects. This research provides a highly\nscalable, energy-efficient, and high-speed solution, paving the way for\nnext-generation AI infrastructure."}
{"id": "2504.15051", "pdf": "https://arxiv.org/pdf/2504.15051", "abs": "https://arxiv.org/abs/2504.15051", "authors": ["Ashkan Shakarami", "Yousef Yeganeh", "Azade Farshad", "Lorenzo Nicolè", "Stefano Ghidoni", "Nassir Navab"], "title": "VeLU: Variance-enhanced Learning Unit for Deep Neural Networks", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Activation functions are fundamental in deep neural networks and directly\nimpact gradient flow, optimization stability, and generalization. Although ReLU\nremains standard because of its simplicity, it suffers from vanishing gradients\nand lacks adaptability. Alternatives like Swish and GELU introduce smooth\ntransitions, but fail to dynamically adjust to input statistics. We propose\nVeLU, a Variance-enhanced Learning Unit as an activation function that\ndynamically scales based on input variance by integrating ArcTan-Sin\ntransformations and Wasserstein-2 regularization, effectively mitigating\ncovariate shifts and stabilizing optimization. Extensive experiments on\nViT_B16, VGG19, ResNet50, DenseNet121, MobileNetV2, and EfficientNetB3 confirm\nVeLU's superiority over ReLU, ReLU6, Swish, and GELU on six vision benchmarks.\nThe codes of VeLU are publicly available on GitHub."}
{"id": "2504.15062", "pdf": "https://arxiv.org/pdf/2504.15062", "abs": "https://arxiv.org/abs/2504.15062", "authors": ["Egon Peršak", "Miguel F. Anjos"], "title": "OPO: Making Decision-Focused Data Acquisition Decisions", "categories": ["math.OC", "cs.AI"], "comment": null, "summary": "We propose a model for making data acquisition decisions for variables in\ncontextual stochastic optimisation problems. Data acquisition decisions are\ntypically treated as separate and fixed. We explore problem settings in which\nthe acquisition of contextual variables is costly and consequently constrained.\nThe data acquisition problem is often solved heuristically for proxy objectives\nsuch as coverage. The more intuitive objective is the downstream decision\nquality as a result of data acquisition decisions. The whole pipeline can be\ncharacterised as an optimise-then-predict-then-optimise (OPO) problem.\nAnalogously, much recent research has focused on how to integrate prediction\nand optimisation (PO) in the form of decision-focused learning. We propose\nleveraging differentiable optimisation to extend the integration to data\nacquisition. We solve the data acquisition problem with well-defined\nconstraints by learning a surrogate linear objective function. We demonstrate\nan application of this model on a shortest path problem for which we first have\nto set a drone reconnaissance strategy to capture image segments serving as\ninputs to a model that predicts travel costs. We ablate the problem with a\nnumber of training modalities and demonstrate that the differentiable\noptimisation approach outperforms random search strategies."}
{"id": "2504.15063", "pdf": "https://arxiv.org/pdf/2504.15063", "abs": "https://arxiv.org/abs/2504.15063", "authors": ["Hongli Peng", "Xiaoqi Li", "Wenkai Li"], "title": "Mining Characteristics of Vulnerable Smart Contracts Across Lifecycle Stages", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Smart contracts are the cornerstone of decentralized applications and\nfinancial protocols, which extend the application of digital currency\ntransactions. The applications and financial protocols introduce significant\nsecurity challenges, resulting in substantial economic losses. Existing\nsolutions predominantly focus on code vulnerabilities within smart contracts,\naccounting for only 50% of security incidents. Therefore, a more comprehensive\nstudy of security issues related to smart contracts is imperative. The existing\nempirical research realizes the static analysis of smart contracts from the\nperspective of the lifecycle and gives the corresponding measures for each\nstage. However, they lack the characteristic analysis of vulnerabilities in\neach stage and the distinction between the vulnerabilities. In this paper, we\npresent the first empirical study on the security of smart contracts throughout\ntheir lifecycle, including deployment and execution, upgrade, and destruction\nstages. It delves into the security issues at each stage and provides at least\nseven feature descriptions. Finally, utilizing these seven features, five\nmachine-learning classification models are used to identify vulnerabilities at\ndifferent stages. The classification results reveal that vulnerable contracts\nexhibit distinct transaction features and ego network properties at various\nstages."}
{"id": "2504.15066", "pdf": "https://arxiv.org/pdf/2504.15066", "abs": "https://arxiv.org/abs/2504.15066", "authors": ["Jinghua Zhao", "Yuhang Jia", "Shiyao Wang", "Jiaming Zhou", "Hui Wang", "Yong Qin"], "title": "Chinese-LiPS: A Chinese audio-visual speech recognition dataset with Lip-reading and Presentation Slides", "categories": ["cs.MM", "cs.AI"], "comment": "6 pages, 7 figures", "summary": "Incorporating visual modalities to assist Automatic Speech Recognition (ASR)\ntasks has led to significant improvements. However, existing Audio-Visual\nSpeech Recognition (AVSR) datasets and methods typically rely solely on\nlip-reading information or speaking contextual video, neglecting the potential\nof combining these different valuable visual cues within the speaking context.\nIn this paper, we release a multimodal Chinese AVSR dataset, Chinese-LiPS,\ncomprising 100 hours of speech, video, and corresponding manual transcription,\nwith the visual modality encompassing both lip-reading information and the\npresentation slides used by the speaker. Based on Chinese-LiPS, we develop a\nsimple yet effective pipeline, LiPS-AVSR, which leverages both lip-reading and\npresentation slide information as visual modalities for AVSR tasks. Experiments\nshow that lip-reading and presentation slide information improve ASR\nperformance by approximately 8\\% and 25\\%, respectively, with a combined\nperformance improvement of about 35\\%. The dataset is available at\nhttps://kiri0824.github.io/Chinese-LiPS/"}
{"id": "2504.15080", "pdf": "https://arxiv.org/pdf/2504.15080", "abs": "https://arxiv.org/abs/2504.15080", "authors": ["Chen Xie", "Mingsheng Jiao", "Xiaodong Gu", "Beijun Shen"], "title": "Empowering AI to Generate Better AI Code: Guided Generation of Deep Learning Projects with LLMs", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "While large language models (LLMs) have been widely applied to code\ngeneration, they struggle with generating entire deep learning projects, which\nare characterized by complex structures, longer functions, and stronger\nreliance on domain knowledge than general-purpose code. An open-domain LLM\noften lacks coherent contextual guidance and domain expertise for specific\nprojects, making it challenging to produce complete code that fully meets user\nrequirements.\n  In this paper, we propose a novel planning-guided code generation method,\nDLCodeGen, tailored for generating deep learning projects. DLCodeGen predicts a\nstructured solution plan, offering global guidance for LLMs to generate the\nproject. The generated plan is then leveraged to retrieve semantically\nanalogous code samples and subsequently abstract a code template. To\neffectively integrate these multiple retrieval-augmented techniques, a\ncomparative learning mechanism is designed to generate the final code. We\nvalidate the effectiveness of our approach on a dataset we build for deep\nlearning code generation. Experimental results demonstrate that DLCodeGen\noutperforms other baselines, achieving improvements of 9.7% in CodeBLEU and\n3.6% in human evaluation metrics."}
{"id": "2504.15090", "pdf": "https://arxiv.org/pdf/2504.15090", "abs": "https://arxiv.org/abs/2504.15090", "authors": ["Junxiang Gao", "Yixin Ran", "Jia Chen"], "title": "Federated Latent Factor Model for Bias-Aware Recommendation with Privacy-Preserving", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "A recommender system (RS) aims to provide users with personalized item\nrecommendations, enhancing their overall experience. Traditional RSs collect\nand process all user data on a central server. However, this centralized\napproach raises significant privacy concerns, as it increases the risk of data\nbreaches and privacy leakages, which are becoming increasingly unacceptable to\nprivacy-sensitive users. To address these privacy challenges, federated\nlearning has been integrated into RSs, ensuring that user data remains secure.\nIn centralized RSs, the issue of rating bias is effectively addressed by\njointly analyzing all users' raw interaction data. However, this becomes a\nsignificant challenge in federated RSs, as raw data is no longer accessible due\nto privacy-preserving constraints. To overcome this problem, we propose a\nFederated Bias-Aware Latent Factor (FBALF) model. In FBALF, training bias is\nexplicitly incorporated into every local model's loss function, allowing for\nthe effective elimination of rating bias without compromising data privacy.\nExtensive experiments conducted on three real-world datasets demonstrate that\nFBALF achieves significantly higher recommendation accuracy compared to other\nstate-of-the-art federated RSs."}
{"id": "2504.15093", "pdf": "https://arxiv.org/pdf/2504.15093", "abs": "https://arxiv.org/abs/2504.15093", "authors": ["K. Wong", "B. Wu", "S. Bulathwela", "M. Cukurova"], "title": "Rethinking the Potential of Multimodality in Collaborative Problem Solving Diagnosis with Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted for 26th International Conference on Artificial Intelligence\n  in Education (AIED 2025), 22 - 26 July 2025, Palermo, Italy. 17 pages, 1\n  figure", "summary": "Detecting collaborative and problem-solving behaviours from digital traces to\ninterpret students' collaborative problem solving (CPS) competency is a\nlong-term goal in the Artificial Intelligence in Education (AIEd) field.\nAlthough multimodal data and advanced models are argued to have the potential\nto detect complex CPS behaviours, empirical evidence on their value remains\nlimited with some contrasting evidence. In this study, we investigated the\npotential of multimodal data to improve model performance in diagnosing 78\nsecondary school students' CPS subskills and indicators in authentic\neducational settings. In particular, text embeddings from verbal data and\nacoustic embeddings from audio data were used in a multimodal classification\nmodel for CPS diagnosis. Both unimodal and multimodal transformer-based models\noutperformed traditional models in detecting CPS classes. Although the\ninclusion of multimodality did not improve the performance of traditional\nunimodal models, its integration into transformer-based models demonstrated\nimproved performance for diagnosing social-cognitive CPS classes compared to\nunimodal transformer-based models. Based on the results, the paper argues that\nmultimodality and the selection of a particular modelling technique should not\nbe taken for granted to achieve the best performance in the automated detection\nof every CPS subskill and indicator. Rather, their value is limited to certain\ntypes of CPS indicators, affected by the complexity of the labels, and\ndependent on the composition of indicators in the dataset. We conclude the\npaper by discussing the required nuance when considering the value of LLMs and\nmultimodality in automated CPS diagnosis, highlighting the need for human-AI\ncomplementarity, and proposing the exploration of relevant model architectures\nand techniques to improve CPS diagnosis in authentic educational contexts."}
{"id": "2504.15099", "pdf": "https://arxiv.org/pdf/2504.15099", "abs": "https://arxiv.org/abs/2504.15099", "authors": ["Lin Wang", "Xiancheng Wang", "Rui Wang", "Zhibo Zhang", "Minghang Zhao"], "title": "Fast-Slow Co-advancing Optimizer: Toward Harmonious Adversarial Training of GAN", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Up to now, the training processes of typical Generative Adversarial Networks\n(GANs) are still particularly sensitive to data properties and hyperparameters,\nwhich may lead to severe oscillations, difficulties in convergence, or even\nfailures to converge, especially when the overall variances of the training\nsets are large. These phenomena are often attributed to the training\ncharacteristics of such networks. Aiming at the problem, this paper develops a\nnew intelligent optimizer, Fast-Slow Co-advancing Optimizer (FSCO), which\nemploys reinforcement learning in the training process of GANs to make training\neasier. Specifically, this paper allows the training step size to be controlled\nby an agent to improve training stability, and makes the training process more\nintelligent with variable learning rates, making GANs less sensitive to step\nsize. Experiments have been conducted on three benchmark datasets to verify the\neffectiveness of the developed FSCO."}
{"id": "2504.15101", "pdf": "https://arxiv.org/pdf/2504.15101", "abs": "https://arxiv.org/abs/2504.15101", "authors": ["Yiqian Yang"], "title": "NeuGaze: Reshaping the future BCI", "categories": ["cs.HC", "cs.AI", "cs.ET"], "comment": null, "summary": "Traditional brain-computer interfaces (BCIs), reliant on costly\nelectroencephalography or invasive implants, struggle with complex\nhuman-computer interactions due to setup complexity and limited precision. We\npresent NeuGaze, a novel webcam-based system that leverages eye gaze, head\nmovements, and facial expressions to enable intuitive, real-time control using\nonly a standard 30 Hz webcam, often pre-installed in laptops. Requiring minimal\ncalibration, NeuGaze achieves performance comparable to conventional inputs,\nsupporting precise cursor navigation, key triggering via an efficient skill\nwheel, and dynamic gaming interactions, such as defeating formidable opponents\nin first-person games. By harnessing preserved neck-up functionalities in\nmotor-impaired individuals, NeuGaze eliminates the need for specialized\nhardware, offering a low-cost, accessible alternative to BCIs. This paradigm\nempowers diverse applications, from assistive technology to entertainment,\nredefining human-computer interaction for motor-impaired users. Project is at\n\\href{https://github.com/NeuSpeech/NeuGaze}{github.com/NeuSpeech/NeuGaze}."}
{"id": "2504.15105", "pdf": "https://arxiv.org/pdf/2504.15105", "abs": "https://arxiv.org/abs/2504.15105", "authors": ["Yurun Wang", "Zerong Qi", "Shujun Fu", "Mingzheng Hu"], "title": "A triple-branch network for latent fingerprint enhancement guided by orientation fields and minutiae", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Latent fingerprint enhancement is a critical step in the process of latent\nfingerprint identification. Existing deep learning-based enhancement methods\nstill fall short of practical application requirements, particularly in\nrestoring low-quality fingerprint regions. Recognizing that different regions\nof latent fingerprints require distinct enhancement strategies, we propose a\nTriple Branch Spatial Fusion Network (TBSFNet), which simultaneously enhances\ndifferent regions of the image using tailored strategies. Furthermore, to\nimprove the generalization capability of the network, we integrate orientation\nfield and minutiae-related modules into TBSFNet and introduce a Multi-Level\nFeature Guidance Network (MLFGNet). Experimental results on the MOLF and MUST\ndatasets demonstrate that MLFGNet outperforms existing enhancement algorithms."}
{"id": "2504.15120", "pdf": "https://arxiv.org/pdf/2504.15120", "abs": "https://arxiv.org/abs/2504.15120", "authors": ["Khalil Hennara", "Sara Chrouf", "Mohamed Motaism Hamed", "Zeina Aldallal", "Omar Hadid", "Safwan AlModhayan"], "title": "Kuwain 1.5B: An Arabic SLM via Language Injection", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Enhancing existing models with new knowledge is a crucial aspect of AI\ndevelopment. This paper introduces a novel method for integrating a new\nlanguage into a large language model (LLM). Our approach successfully\nincorporates a previously unseen target language into an existing LLM without\ncompromising its prior knowledge. We trained a tiny model with 1.5 billion\nparameters named Kuwain by injecting the Arabic language into a small\nopen-source model mainly trained in English. Our method demonstrates\nsignificant improvements in Arabic language performance, with an average 8%\nimprovement across various benchmarks, while retaining the model's existing\nknowledge with a minimum amount of the original model's data. This offers a\ncost-effective alternative to training a comprehensive model in both English\nand Arabic. The results highlight the potential for efficient, targeted\nlanguage model expansion without extensive retraining or resource-intensive\nprocesses."}
{"id": "2504.15129", "pdf": "https://arxiv.org/pdf/2504.15129", "abs": "https://arxiv.org/abs/2504.15129", "authors": ["Kangyao Huang", "Hao Wang", "Yu Luo", "Jingyu Chen", "Jintao Chen", "Xiangkui Zhang", "Xiangyang Ji", "Huaping Liu"], "title": "A General Infrastructure and Workflow for Quadrotor Deep Reinforcement Learning and Reality Deployment", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Deploying robot learning methods to a quadrotor in unstructured outdoor\nenvironments is an exciting task. Quadrotors operating in real-world\nenvironments by learning-based methods encounter several challenges: a large\namount of simulator generated data required for training, strict demands for\nreal-time processing onboard, and the sim-to-real gap caused by dynamic and\nnoisy conditions. Current works have made a great breakthrough in applying\nlearning-based methods to end-to-end control of quadrotors, but rarely mention\nthe infrastructure system training from scratch and deploying to reality, which\nmakes it difficult to reproduce methods and applications. To bridge this gap,\nwe propose a platform that enables the seamless transfer of end-to-end deep\nreinforcement learning (DRL) policies. We integrate the training environment,\nflight dynamics control, DRL algorithms, the MAVROS middleware stack, and\nhardware into a comprehensive workflow and architecture that enables\nquadrotors' policies to be trained from scratch to real-world deployment in\nseveral minutes. Our platform provides rich types of environments including\nhovering, dynamic obstacle avoidance, trajectory tracking, balloon hitting, and\nplanning in unknown environments, as a physical experiment benchmark. Through\nextensive empirical validation, we demonstrate the efficiency of proposed\nsim-to-real platform, and robust outdoor flight performance under real-world\nperturbations. Details can be found from our website\nhttps://emnavi.tech/AirGym/."}
{"id": "2504.15130", "pdf": "https://arxiv.org/pdf/2504.15130", "abs": "https://arxiv.org/abs/2504.15130", "authors": ["Kushal Shah", "Jihyun Park", "Seung-Kyum Choi"], "title": "Neural ATTF: A Scalable Solution to Lifelong Multi-Agent Path Planning", "categories": ["cs.RO", "cs.AI"], "comment": "13 Pages, 5 Figures, 5 Tables", "summary": "Multi-Agent Pickup and Delivery (MAPD) is a fundamental problem in robotics,\nparticularly in applications such as warehouse automation and logistics.\nExisting solutions often face challenges in scalability, adaptability, and\nefficiency, limiting their applicability in dynamic environments with real-time\nplanning requirements. This paper presents Neural ATTF (Adaptive Task Token\nFramework), a new algorithm that combines a Priority Guided Task Matching\n(PGTM) Module with Neural STA* (Space-Time A*), a data-driven path planning\nmethod. Neural STA* enhances path planning by enabling rapid exploration of the\nsearch space through guided learned heuristics and ensures collision avoidance\nunder dynamic constraints. PGTM prioritizes delayed agents and dynamically\nassigns tasks by prioritizing agents nearest to these tasks, optimizing both\ncontinuity and system throughput. Experimental evaluations against\nstate-of-the-art MAPD algorithms, including TPTS, CENTRAL, RMCA, LNS-PBS, and\nLNS-wPBS, demonstrate the superior scalability, solution quality, and\ncomputational efficiency of Neural ATTF. These results highlight the\nframework's potential for addressing the critical demands of complex,\nreal-world multi-agent systems operating in high-demand, unpredictable\nsettings."}
{"id": "2504.15133", "pdf": "https://arxiv.org/pdf/2504.15133", "abs": "https://arxiv.org/abs/2504.15133", "authors": ["Ziwen Xu", "Shuxun Wang", "Kewei Xu", "Haoming Xu", "Mengru Wang", "Xinle Deng", "Yunzhi Yao", "Guozhou Zheng", "Huajun Chen", "Ningyu Zhang"], "title": "EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "comment": "Work in progress. Demo:\n  https://zjunlp.github.io/project/EasyEdit2/video; code:\n  https://github.com/zjunlp/EasyEdit", "summary": "In this paper, we introduce EasyEdit2, a framework designed to enable\nplug-and-play adjustability for controlling Large Language Model (LLM)\nbehaviors. EasyEdit2 supports a wide range of test-time interventions,\nincluding safety, sentiment, personality, reasoning patterns, factuality, and\nlanguage features. Unlike its predecessor, EasyEdit2 features a new\narchitecture specifically designed for seamless model steering. It comprises\nkey modules such as the steering vector generator and the steering vector\napplier, which enable automatic generation and application of steering vectors\nto influence the model's behavior without modifying its parameters. One of the\nmain advantages of EasyEdit2 is its ease of use-users do not need extensive\ntechnical knowledge. With just a single example, they can effectively guide and\nadjust the model's responses, making precise control both accessible and\nefficient. Empirically, we report model steering performance across different\nLLMs, demonstrating the effectiveness of these techniques. We have released the\nsource code on GitHub at https://github.com/zjunlp/EasyEdit along with a\ndemonstration notebook. In addition, we provide a demo video at\nhttps://zjunlp.github.io/project/EasyEdit2/video for a quick introduction."}
{"id": "2504.15135", "pdf": "https://arxiv.org/pdf/2504.15135", "abs": "https://arxiv.org/abs/2504.15135", "authors": ["Juyeon Kim", "Geon Lee", "Taeuk Kim", "Kijung Shin"], "title": "KGMEL: Knowledge Graph-Enhanced Multimodal Entity Linking", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "SIGIR 2025 (Short)", "summary": "Entity linking (EL) aligns textual mentions with their corresponding entities\nin a knowledge base, facilitating various applications such as semantic search\nand question answering. Recent advances in multimodal entity linking (MEL) have\nshown that combining text and images can reduce ambiguity and improve alignment\naccuracy. However, most existing MEL methods overlook the rich structural\ninformation available in the form of knowledge-graph (KG) triples. In this\npaper, we propose KGMEL, a novel framework that leverages KG triples to enhance\nMEL. Specifically, it operates in three stages: (1) Generation: Produces\nhigh-quality triples for each mention by employing vision-language models based\non its text and images. (2) Retrieval: Learns joint mention-entity\nrepresentations, via contrastive learning, that integrate text, images, and\n(generated or KG) triples to retrieve candidate entities for each mention. (3)\nReranking: Refines the KG triples of the candidate entities and employs large\nlanguage models to identify the best-matching entity for the mention. Extensive\nexperiments on benchmark datasets demonstrate that KGMEL outperforms existing\nmethods. Our code and datasets are available at:\nhttps://github.com/juyeonnn/KGMEL."}
{"id": "2504.15144", "pdf": "https://arxiv.org/pdf/2504.15144", "abs": "https://arxiv.org/abs/2504.15144", "authors": ["Melih Sirlanci", "Carter Yagemann", "Zhiqiang Lin"], "title": "C2RUST-BENCH: A Minimized, Representative Dataset for C-to-Rust Transpilation Evaluation", "categories": ["cs.CR", "cs.AI", "cs.PL"], "comment": null, "summary": "Despite the effort in vulnerability detection over the last two decades,\nmemory safety vulnerabilities continue to be a critical problem. Recent reports\nsuggest that the key solution is to migrate to memory-safe languages. To this\nend, C-to-Rust transpilation becomes popular to resolve memory-safety issues in\nC programs. Recent works propose C-to-Rust transpilation frameworks; however, a\ncomprehensive evaluation dataset is missing. Although one solution is to put\ntogether a large enough dataset, this increases the analysis time in automated\nframeworks as well as in manual efforts for some cases. In this work, we build\na method to select functions from a large set to construct a minimized yet\nrepresentative dataset to evaluate the C-to-Rust transpilation. We propose\nC2RUST-BENCH that contains 2,905 functions, which are representative of\nC-to-Rust transpilation, selected from 15,503 functions of real-world programs."}
{"id": "2504.15152", "pdf": "https://arxiv.org/pdf/2504.15152", "abs": "https://arxiv.org/abs/2504.15152", "authors": ["Jun Zhou", "Bingchen Gao", "Kai Wang", "Jialun Pei", "Pheng-Ann Heng", "Jing Qin"], "title": "Landmark-Free Preoperative-to-Intraoperative Registration in Laparoscopic Liver Resection", "categories": ["cs.CV", "cs.AI"], "comment": "TMI under review", "summary": "Liver registration by overlaying preoperative 3D models onto intraoperative\n2D frames can assist surgeons in perceiving the spatial anatomy of the liver\nclearly for a higher surgical success rate. Existing registration methods rely\nheavily on anatomical landmark-based workflows, which encounter two major\nlimitations: 1) ambiguous landmark definitions fail to provide efficient\nmarkers for registration; 2) insufficient integration of intraoperative liver\nvisual information in shape deformation modeling. To address these challenges,\nin this paper, we propose a landmark-free preoperative-to-intraoperative\nregistration framework utilizing effective self-supervised learning, termed\n\\ourmodel. This framework transforms the conventional 3D-2D workflow into a\n3D-3D registration pipeline, which is then decoupled into rigid and non-rigid\nregistration subtasks. \\ourmodel~first introduces a feature-disentangled\ntransformer to learn robust correspondences for recovering rigid\ntransformations. Further, a structure-regularized deformation network is\ndesigned to adjust the preoperative model to align with the intraoperative\nliver surface. This network captures structural correlations through geometry\nsimilarity modeling in a low-rank transformer network. To facilitate the\nvalidation of the registration performance, we also construct an in-vivo\nregistration dataset containing liver resection videos of 21 patients, called\n\\emph{P2I-LReg}, which contains 346 keyframes that provide a global view of the\nliver together with liver mask annotations and calibrated camera intrinsic\nparameters. Extensive experiments and user studies on both synthetic and\nin-vivo datasets demonstrate the superiority and potential clinical\napplicability of our method."}
{"id": "2504.15165", "pdf": "https://arxiv.org/pdf/2504.15165", "abs": "https://arxiv.org/abs/2504.15165", "authors": ["Liu Wenbin"], "title": "An Efficient Aerial Image Detection with Variable Receptive Fields", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Aerial object detection using unmanned aerial vehicles (UAVs) faces critical\nchallenges including sub-10px targets, dense occlusions, and stringent\ncomputational constraints. Existing detectors struggle to balance accuracy and\nefficiency due to rigid receptive fields and redundant architectures. To\naddress these limitations, we propose Variable Receptive Field DETR (VRF-DETR),\na transformer-based detector incorporating three key components: 1) Multi-Scale\nContext Fusion (MSCF) module that dynamically recalibrates features through\nadaptive spatial attention and gated multi-scale fusion, 2) Gated Convolution\n(GConv) layer enabling parameter-efficient local-context modeling via depthwise\nseparable operations and dynamic gating, and 3) Gated Multi-scale Fusion (GMCF)\nBottleneck that hierarchically disentangles occluded objects through cascaded\nglobal-local interactions. Experiments on VisDrone2019 demonstrate VRF-DETR\nachieves 51.4\\% mAP\\textsubscript{50} and 31.8\\% mAP\\textsubscript{50:95} with\nonly 13.5M parameters. This work establishes a new efficiency-accuracy Pareto\nfrontier for UAV-based detection tasks."}
{"id": "2504.15181", "pdf": "https://arxiv.org/pdf/2504.15181", "abs": "https://arxiv.org/abs/2504.15181", "authors": ["Lily Stelling", "Mick Yang", "Rokas Gipiškis", "Leon Staufer", "Ze Shen Chin", "Siméon Campos", "Michael Chen"], "title": "Existing Industry Practice for the EU AI Act's General-Purpose AI Code of Practice Safety and Security Measures", "categories": ["cs.CY", "cs.AI"], "comment": "158 pages", "summary": "This report provides a detailed comparison between the measures proposed in\nthe EU AI Act's General-Purpose AI (GPAI) Code of Practice (Third Draft) and\ncurrent practices adopted by leading AI companies. As the EU moves toward\nenforcing binding obligations for GPAI model providers, the Code of Practice\nwill be key to bridging legal requirements with concrete technical commitments.\nOur analysis focuses on the draft's Safety and Security section which is only\nrelevant for the providers of the most advanced models (Commitments II.1-II.16)\nand excerpts from current public-facing documents quotes that are relevant to\neach individual measure.\n  We systematically reviewed different document types - including companies'\nfrontier safety frameworks and model cards - from over a dozen companies,\nincluding OpenAI, Anthropic, Google DeepMind, Microsoft, Meta, Amazon, and\nothers. This report is not meant to be an indication of legal compliance nor\ndoes it take any prescriptive viewpoint about the Code of Practice or\ncompanies' policies. Instead, it aims to inform the ongoing dialogue between\nregulators and GPAI model providers by surfacing evidence of precedent."}
{"id": "2504.15192", "pdf": "https://arxiv.org/pdf/2504.15192", "abs": "https://arxiv.org/abs/2504.15192", "authors": ["Yaqian Chen", "Lin Li", "Hanxue Gu", "Haoyu Dong", "Derek L. Nguyen", "Allan D. Kirk", "Maciej A. Mazurowski", "E. Shelley Hwang"], "title": "Breast density in MRI: an AI-based quantification and relationship to assessment in mammography", "categories": ["cs.CV", "cs.AI"], "comment": "13 pages, 5 figures", "summary": "Mammographic breast density is a well-established risk factor for breast\ncancer. Recently there has been interest in breast MRI as an adjunct to\nmammography, as this modality provides an orthogonal and highly quantitative\nassessment of breast tissue. However, its 3D nature poses analytic challenges\nrelated to delineating and aggregating complex structures across slices. Here,\nwe applied an in-house machine-learning algorithm to assess breast density on\nnormal breasts in three MRI datasets. Breast density was consistent across\ndifferent datasets (0.104 - 0.114). Analysis across different age groups also\ndemonstrated strong consistency across datasets and confirmed a trend of\ndecreasing density with age as reported in previous studies. MR breast density\nwas correlated with mammographic breast density, although some notable\ndifferences suggest that certain breast density components are captured only on\nMRI. Future work will determine how to integrate MR breast density with current\ntools to improve future breast cancer risk prediction."}
{"id": "2504.15199", "pdf": "https://arxiv.org/pdf/2504.15199", "abs": "https://arxiv.org/abs/2504.15199", "authors": ["Yassir Benhammou", "Alessandro Tiberio", "Gabriel Trautmann", "Suman Kalyan"], "title": "Zero-Shot, But at What Cost? Unveiling the Hidden Overhead of MILS's LLM-CLIP Framework for Image Captioning", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.PF"], "comment": "9 pages, 2 tables, 1 figure", "summary": "MILS (Multimodal Iterative LLM Solver) is a recently published framework that\nclaims \"LLMs can see and hear without any training\" by leveraging an iterative,\nLLM-CLIP based approach for zero-shot image captioning. While this MILS\napproach demonstrates good performance, our investigation reveals that this\nsuccess comes at a hidden, substantial computational cost due to its expensive\nmulti-step refinement process. In contrast, alternative models such as BLIP-2\nand GPT-4V achieve competitive results through a streamlined, single-pass\napproach. We hypothesize that the significant overhead inherent in MILS's\niterative process may undermine its practical benefits, thereby challenging the\nnarrative that zero-shot performance can be attained without incurring heavy\nresource demands. This work is the first to expose and quantify the trade-offs\nbetween output quality and computational cost in MILS, providing critical\ninsights for the design of more efficient multimodal models."}
{"id": "2504.15205", "pdf": "https://arxiv.org/pdf/2504.15205", "abs": "https://arxiv.org/abs/2504.15205", "authors": ["Nandan Thakur", "Ronak Pradeep", "Shivani Upadhyay", "Daniel Campos", "Nick Craswell", "Jimmy Lin"], "title": "Support Evaluation for the TREC 2024 RAG Track: Comparing Human versus LLM Judges", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Accepted at SIGIR 2025 (short)", "summary": "Retrieval-augmented generation (RAG) enables large language models (LLMs) to\ngenerate answers with citations from source documents containing \"ground\ntruth\", thereby reducing system hallucinations. A crucial factor in RAG\nevaluation is \"support\", whether the information in the cited documents\nsupports the answer. To this end, we conducted a large-scale comparative study\nof 45 participant submissions on 36 topics to the TREC 2024 RAG Track,\ncomparing an automatic LLM judge (GPT-4o) against human judges for support\nassessment. We considered two conditions: (1) fully manual assessments from\nscratch and (2) manual assessments with post-editing of LLM predictions. Our\nresults indicate that for 56% of the manual from-scratch assessments, human and\nGPT-4o predictions match perfectly (on a three-level scale), increasing to 72%\nin the manual with post-editing condition. Furthermore, by carefully analyzing\nthe disagreements in an unbiased study, we found that an independent human\njudge correlates better with GPT-4o than a human judge, suggesting that LLM\njudges can be a reliable alternative for support assessment. To conclude, we\nprovide a qualitative analysis of human and GPT-4o errors to help guide future\niterations of support assessment."}
{"id": "2504.15208", "pdf": "https://arxiv.org/pdf/2504.15208", "abs": "https://arxiv.org/abs/2504.15208", "authors": ["Marc Finzi", "Sanyam Kapoor", "Diego Granziol", "Anming Gu", "Christopher De Sa", "J. Zico Kolter", "Andrew Gordon Wilson"], "title": "Compute-Optimal LLMs Provably Generalize Better With Scale", "categories": ["cs.LG", "cs.AI"], "comment": "ICLR 2025", "summary": "Why do larger language models generalize better? To investigate this\nquestion, we develop generalization bounds on the pretraining objective of\nlarge language models (LLMs) in the compute-optimal regime, as described by the\nChinchilla scaling laws. We introduce a novel, fully empirical Freedman-type\nmartingale concentration inequality that tightens existing bounds by accounting\nfor the variance of the loss function. This generalization bound can be\ndecomposed into three interpretable components: the number of parameters per\ntoken, the loss variance, and the quantization error at a fixed bitrate. As\ncompute-optimal language models are scaled up, the number of parameters per\ndata point remains constant; however, both the loss variance and the\nquantization error decrease, implying that larger models should have smaller\ngeneralization gaps. We examine why larger models tend to be more quantizable\nfrom an information theoretic perspective, showing that the rate at which they\ncan integrate new information grows more slowly than their capacity on the\ncompute-optimal frontier. From these findings we produce a scaling law for the\ngeneralization gap, with bounds that become predictably stronger with scale."}
{"id": "2504.15209", "pdf": "https://arxiv.org/pdf/2504.15209", "abs": "https://arxiv.org/abs/2504.15209", "authors": ["Xin Liao", "Bing Yang", "Tan Dongli", "Cai Yu"], "title": "A Causal Convolutional Low-rank Representation Model for Imputation of Water Quality Data", "categories": ["cs.LG", "cs.AI", "68T07 (Primary) 62M10, 65C60 (Secondary)", "I.2.7"], "comment": "9 pages, 3 figures", "summary": "The monitoring of water quality is a crucial part of environmental\nprotection, and a large number of monitors are widely deployed to monitor water\nquality. Due to unavoidable factors such as data acquisition breakdowns,\nsensors and communication failures, water quality monitoring data suffers from\nmissing values over time, resulting in High-Dimensional and Sparse (HDS) Water\nQuality Data (WQD). The simple and rough filling of the missing values leads to\ninaccurate results and affects the implementation of relevant measures.\nTherefore, this paper proposes a Causal convolutional Low-rank Representation\n(CLR) model for imputing missing WQD to improve the completeness of the WQD,\nwhich employs a two-fold idea: a) applying causal convolutional operation to\nconsider the temporal dependence of the low-rank representation, thus\nincorporating temporal information to improve the imputation accuracy; and b)\nimplementing a hyperparameters adaptation scheme to automatically adjust the\nbest hyperparameters during model training, thereby reducing the tedious manual\nadjustment of hyper-parameters. Experimental studies on three real-world water\nquality datasets demonstrate that the proposed CLR model is superior to some of\nthe existing state-of-the-art imputation models in terms of imputation accuracy\nand time cost, as well as indicating that the proposed model provides more\nreliable decision support for environmental monitoring."}
{"id": "2504.15210", "pdf": "https://arxiv.org/pdf/2504.15210", "abs": "https://arxiv.org/abs/2504.15210", "authors": ["Marina Sakharova", "Abhinav Anand", "Mira Mezini"], "title": "Integrating Symbolic Execution into the Fine-Tuning of Code-Generating LLMs", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Code-generating Large Language Models (LLMs) have become essential tools in\nmodern software development, enhancing productivity and accelerating\ndevelopment. This paper aims to investigate the fine-tuning of code-generating\nLLMs using Reinforcement Learning and Direct Preference Optimization, further\nimproving their performance. To achieve this, we enhance the training data for\nthe reward model with the help of symbolic execution techniques, ensuring more\ncomprehensive and objective data. With symbolic execution, we create a custom\ndataset that better captures the nuances in code evaluation. Our reward models,\nfine-tuned on this dataset, demonstrate significant improvements over the\nbaseline, CodeRL, in estimating the quality of generated code. Our\ncode-generating LLMs, trained with the help of reward model feedback, achieve\nsimilar results compared to the CodeRL benchmark."}
{"id": "2504.15225", "pdf": "https://arxiv.org/pdf/2504.15225", "abs": "https://arxiv.org/abs/2504.15225", "authors": ["Sarah Alnegheimish", "Zelin He", "Matthew Reimherr", "Akash Chandrayan", "Abhinav Pradhan", "Luca D'Angelo"], "title": "M$^2$AD: Multi-Sensor Multi-System Anomaly Detection through Global Scoring and Calibrated Thresholding", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at AISTATS 2025", "summary": "With the widespread availability of sensor data across industrial and\noperational systems, we frequently encounter heterogeneous time series from\nmultiple systems. Anomaly detection is crucial for such systems to facilitate\npredictive maintenance. However, most existing anomaly detection methods are\ndesigned for either univariate or single-system multivariate data, making them\ninsufficient for these complex scenarios. To address this, we introduce\nM$^2$AD, a framework for unsupervised anomaly detection in multivariate time\nseries data from multiple systems. M$^2$AD employs deep models to capture\nexpected behavior under normal conditions, using the residuals as indicators of\npotential anomalies. These residuals are then aggregated into a global anomaly\nscore through a Gaussian Mixture Model and Gamma calibration. We theoretically\ndemonstrate that this framework can effectively address heterogeneity and\ndependencies across sensors and systems. Empirically, M$^2$AD outperforms\nexisting methods in extensive evaluations by 21% on average, and its\neffectiveness is demonstrated on a large-scale real-world case study on 130\nassets in Amazon Fulfillment Centers. Our code and results are available at\nhttps://github.com/sarahmish/M2AD."}
{"id": "2504.15226", "pdf": "https://arxiv.org/pdf/2504.15226", "abs": "https://arxiv.org/abs/2504.15226", "authors": ["Nathan Steffen", "Wilhelm Louw", "Nicholas Ernest", "Timothy Arnett", "Kelly Cohen"], "title": "A Genetic Fuzzy-Enabled Framework on Robotic Manipulation for In-Space Servicing", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "Automation of robotic systems for servicing in cislunar space is becoming\nextremely important as the number of satellites in orbit increases. Safety is\ncritical in performing satellite maintenance, so the control techniques\nutilized must be trusted in addition to being highly efficient. In this work,\nGenetic Fuzzy Trees are combined with the widely used LQR control scheme via\nThales' TrUE AI Toolkit to create a trusted and efficient controller for a\ntwo-degree-of-freedom planar robotic manipulator that would theoretically be\nused to perform satellite maintenance. It was found that Genetic Fuzzy-LQR is\n18.5% more performant than optimal LQR on average, and that it is incredibly\nrobust to uncertainty."}
{"id": "2504.15236", "pdf": "https://arxiv.org/pdf/2504.15236", "abs": "https://arxiv.org/abs/2504.15236", "authors": ["Saffron Huang", "Esin Durmus", "Miles McCain", "Kunal Handa", "Alex Tamkin", "Jerry Hong", "Michael Stern", "Arushi Somani", "Xiuruo Zhang", "Deep Ganguli"], "title": "Values in the Wild: Discovering and Analyzing Values in Real-World Language Model Interactions", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": "44 pages", "summary": "AI assistants can impart value judgments that shape people's decisions and\nworldviews, yet little is known empirically about what values these systems\nrely on in practice. To address this, we develop a bottom-up,\nprivacy-preserving method to extract the values (normative considerations\nstated or demonstrated in model responses) that Claude 3 and 3.5 models exhibit\nin hundreds of thousands of real-world interactions. We empirically discover\nand taxonomize 3,307 AI values and study how they vary by context. We find that\nClaude expresses many practical and epistemic values, and typically supports\nprosocial human values while resisting values like \"moral nihilism\". While some\nvalues appear consistently across contexts (e.g. \"transparency\"), many are more\nspecialized and context-dependent, reflecting the diversity of human\ninterlocutors and their varied contexts. For example, \"harm prevention\" emerges\nwhen Claude resists users, \"historical accuracy\" when responding to queries\nabout controversial events, \"healthy boundaries\" when asked for relationship\nadvice, and \"human agency\" in technology ethics discussions. By providing the\nfirst large-scale empirical mapping of AI values in deployment, our work\ncreates a foundation for more grounded evaluation and design of values in AI\nsystems."}
{"id": "2504.15259", "pdf": "https://arxiv.org/pdf/2504.15259", "abs": "https://arxiv.org/abs/2504.15259", "authors": ["Yunxuan Cai", "Sitao Xiang", "Zongjian Li", "Haiwei Chen", "Yajie Zhao"], "title": "Bringing Diversity from Diffusion Models to Semantic-Guided Face Asset Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Digital modeling and reconstruction of human faces serve various\napplications. However, its availability is often hindered by the requirements\nof data capturing devices, manual labor, and suitable actors. This situation\nrestricts the diversity, expressiveness, and control over the resulting models.\nThis work aims to demonstrate that a semantically controllable generative\nnetwork can provide enhanced control over the digital face modeling process. To\nenhance diversity beyond the limited human faces scanned in a controlled\nsetting, we introduce a novel data generation pipeline that creates a\nhigh-quality 3D face database using a pre-trained diffusion model. Our proposed\nnormalization module converts synthesized data from the diffusion model into\nhigh-quality scanned data. Using the 44,000 face models we obtained, we further\ndeveloped an efficient GAN-based generator. This generator accepts semantic\nattributes as input, and generates geometry and albedo. It also allows\ncontinuous post-editing of attributes in the latent space. Our asset refinement\ncomponent subsequently creates physically-based facial assets. We introduce a\ncomprehensive system designed for creating and editing high-quality face\nassets. Our proposed model has undergone extensive experiment, comparison and\nevaluation. We also integrate everything into a web-based interactive tool. We\naim to make this tool publicly available with the release of the paper."}
{"id": "2504.15266", "pdf": "https://arxiv.org/pdf/2504.15266", "abs": "https://arxiv.org/abs/2504.15266", "authors": ["Vaishnavh Nagarajan", "Chen Henry Wu", "Charles Ding", "Aditi Raghunathan"], "title": "Roll the dice & look before you leap: Going beyond the creative limits of next-token prediction", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "37 pages", "summary": "We design a suite of minimal algorithmic tasks that are a loose abstraction\nof open-ended real-world tasks. This allows us to cleanly and controllably\nquantify the creative limits of the present-day language model. Much like\nreal-world tasks that require a creative, far-sighted leap of thought, our\ntasks require an implicit, open-ended stochastic planning step that either (a)\ndiscovers new connections in an abstract knowledge graph (like in wordplay,\ndrawing analogies, or research) or (b) constructs new patterns (like in\ndesigning math problems or new proteins). In these tasks, we empirically and\nconceptually argue how next-token learning is myopic and memorizes excessively;\ncomparatively, multi-token approaches, namely teacherless training and\ndiffusion models, excel in producing diverse and original output. Secondly, in\nour tasks, we find that to elicit randomness from the Transformer without\nhurting coherence, it is better to inject noise right at the input layer (via a\nmethod we dub hash-conditioning) rather than defer to temperature sampling from\nthe output layer. Thus, our work offers a principled, minimal test-bed for\nanalyzing open-ended creative skills, and offers new arguments for going beyond\nnext-token learning and softmax-based sampling. We make part of the code\navailable under https://github.com/chenwu98/algorithmic-creativity"}
{"id": "2504.15304", "pdf": "https://arxiv.org/pdf/2504.15304", "abs": "https://arxiv.org/abs/2504.15304", "authors": ["Kangyu Wang"], "title": "Can Machine Learning Agents Deal with Hard Choices?", "categories": ["cs.AI"], "comment": "22 pages excluding bibliography, 27 pagas including bibliography, 3\n  figures", "summary": "Machine Learning ML agents have been increasingly used in decision-making\nacross a wide range of tasks and environments. These ML agents are typically\ndesigned to balance multiple objectives when making choices. Understanding how\ntheir decision-making processes align with or diverge from human reasoning is\nessential. Human agents often encounter hard choices, that is, situations where\noptions are incommensurable; neither option is preferred, yet the agent is not\nindifferent between them. In such cases, human agents can identify hard choices\nand resolve them through deliberation. In contrast, current ML agents, due to\nfundamental limitations in Multi-Objective Optimisation or MOO methods, cannot\nidentify hard choices, let alone resolve them. Neither Scalarised Optimisation\nnor Pareto Optimisation, the two principal MOO approaches, can capture\nincommensurability. This limitation generates three distinct alignment\nproblems: the alienness of ML decision-making behaviour from a human\nperspective; the unreliability of preference-based alignment strategies for\nhard choices; and the blockage of alignment strategies pursuing multiple\nobjectives. Evaluating two potential technical solutions, I recommend an\nensemble solution that appears most promising for enabling ML agents to\nidentify hard choices and mitigate alignment problems. However, no known\ntechnique allows ML agents to resolve hard choices through deliberation, as\nthey cannot autonomously change their goals. This underscores the\ndistinctiveness of human agency and urges ML researchers to reconceptualise\nmachine autonomy and develop frameworks and methods that can better address\nthis fundamental gap."}
{"id": "2504.15313", "pdf": "https://arxiv.org/pdf/2504.15313", "abs": "https://arxiv.org/abs/2504.15313", "authors": ["Yajie Yu", "Yue Feng"], "title": "PolicyEvol-Agent: Evolving Policy via Environment Perception and Self-Awareness with Theory of Mind", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Multi-agents has exhibited significant intelligence in real-word simulations\nwith Large language models (LLMs) due to the capabilities of social cognition\nand knowledge retrieval. However, existing research on agents equipped with\neffective cognition chains including reasoning, planning, decision-making and\nreflecting remains limited, especially in the dynamically interactive\nscenarios. In addition, unlike human, prompt-based responses face challenges in\npsychological state perception and empirical calibration during uncertain\ngaming process, which can inevitably lead to cognition bias. In light of above,\nwe introduce PolicyEvol-Agent, a comprehensive LLM-empowered framework\ncharacterized by systematically acquiring intentions of others and adaptively\noptimizing irrational strategies for continual enhancement. Specifically,\nPolicyEvol-Agent first obtains reflective expertise patterns and then\nintegrates a range of cognitive operations with Theory of Mind alongside\ninternal and external perspectives. Simulation results, outperforming RL-based\nmodels and agent-based methods, demonstrate the superiority of PolicyEvol-Agent\nfor final gaming victory. Moreover, the policy evolution mechanism reveals the\neffectiveness of dynamic guideline adjustments in both automatic and human\nevaluation."}
{"id": "2504.15360", "pdf": "https://arxiv.org/pdf/2504.15360", "abs": "https://arxiv.org/abs/2504.15360", "authors": ["Javier Fumanal-Idocin", "Javier Andreu-Perez"], "title": "Reliable Classification with Conformal Learning and Interval-Type 2 Fuzzy Sets", "categories": ["cs.AI"], "comment": null, "summary": "Classical machine learning classifiers tend to be overconfident can be\nunreliable outside of the laboratory benchmarks. Properly assessing the\nreliability of the output of the model per sample is instrumental for real-life\nscenarios where these systems are deployed. Because of this, different\ntechniques have been employed to properly quantify the quality of prediction\nfor a given model. These are most commonly Bayesian statistics and, more\nrecently, conformal learning. Given a calibration set, conformal learning can\nproduce outputs that are guaranteed to cover the target class with a desired\nsignificance level, and are more reliable than the standard confidence\nintervals used by Bayesian methods. In this work, we propose to use conformal\nlearning with fuzzy rule-based systems in classification and show some metrics\nof their performance. Then, we discuss how the use of type 2 fuzzy sets can\nimprove the quality of the output of the system compared to both fuzzy and\ncrisp rules. Finally, we also discuss how the fine-tuning of the system can be\nadapted to improve the quality of the conformal prediction."}
{"id": "2504.15364", "pdf": "https://arxiv.org/pdf/2504.15364", "abs": "https://arxiv.org/abs/2504.15364", "authors": ["Junyoung Park", "Dalton Jones", "Matt Morse", "Raghavv Goel", "Mingu Lee", "Chris Lott"], "title": "KeDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM Inference in Resource-Constrained Environments", "categories": ["cs.AI"], "comment": "8 pages, 14 figures", "summary": "In this work, we demonstrate that distinctive keys during LLM inference tend\nto have high attention scores. We explore this phenomenon and propose KeyDiff,\na training-free KV cache eviction method based on key similarity. This method\nfacilitates the deployment of LLM-based application requiring long input\nprompts in resource-constrained environments with limited memory and compute\nbudgets. Unlike other KV cache eviction methods, KeyDiff can process\narbitrarily long prompts within strict resource constraints and efficiently\ngenerate responses. We demonstrate that KeyDiff computes the optimal solution\nto a KV cache selection problem that maximizes key diversity, providing a\ntheoretical understanding of KeyDiff. Notably,KeyDiff does not rely on\nattention scores, allowing the use of optimized attention mechanisms like\nFlashAttention. We demonstrate the effectiveness of KeyDiff across diverse\ntasks and models, illustrating a performance gap of less than 0.04\\% with 8K\ncache budget ($\\sim$ 23\\% KV cache reduction) from the non-evicting baseline on\nthe LongBench benchmark for Llama 3.1-8B and Llama 3.2-3B."}
{"id": "2504.15309", "pdf": "https://arxiv.org/pdf/2504.15309", "abs": "https://arxiv.org/abs/2504.15309", "authors": ["Anran Yu", "Wei Feng", "Yaochen Zhang", "Xiang Li", "Lei Meng", "Lei Wu", "Xiangxu Meng"], "title": "LLM-Enabled Style and Content Regularization for Personalized Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "The personalized text-to-image generation has rapidly advanced with the\nemergence of Stable Diffusion. Existing methods, which typically fine-tune\nmodels using embedded identifiers, often struggle with insufficient stylization\nand inaccurate image content due to reduced textual controllability. In this\npaper, we propose style refinement and content preservation strategies. The\nstyle refinement strategy leverages the semantic information of visual\nreasoning prompts and reference images to optimize style embeddings, allowing a\nmore precise and consistent representation of style information. The content\npreservation strategy addresses the content bias problem by preserving the\nmodel's generalization capabilities, ensuring enhanced textual controllability\nwithout compromising stylization. Experimental results verify that our approach\nachieves superior performance in generating consistent and personalized\ntext-to-image outputs."}
{"id": "2504.15349", "pdf": "https://arxiv.org/pdf/2504.15349", "abs": "https://arxiv.org/abs/2504.15349", "authors": ["William Bruns"], "title": "Exploring Compositional Generalization (in ReCOGS_pos) by Transformers using Restricted Access Sequence Processing (RASP)", "categories": ["cs.CL"], "comment": "8 pages main text with 3 figures and 1 table; limitations page and\n  references separate; 4 more figures, 1 image, and 1 more table in the\n  appendices supplement the work. 29 pages of appendix content", "summary": "Humans understand new combinations of words encountered if they are\ncombinations of words recognized from different contexts, an ability called\nCompositional Generalization. The COGS benchmark (Kim and Linzen, 2020)\narXiv:2010.05465 reports 0% accuracy for Transformer models on some structural\ngeneralizations. We use (Weiss et al., 2021) arXiv:2106.06981's Restricted\nAccess Sequence Processing (RASP), a Transformer-equivalent programming\nlanguage, to prove by construction that a Transformer encoder-decoder can\nperform the semantically equivalent ReCOGS_pos (Wu et al., 2024)\narXiv:2303.13716 variant of COGS systematically and compositionally: Our RASP\nmodel attains 100% semantic exact match on the ReCOGS test set and 100% SEM on\nall generalization splits except obj_pp_to_subj_pp which gets 92%. Furthermore,\nour RASP model shows the ReCOGS_pos task does not require a hierarchical or\ntree-structured solution: we use word-level tokens with an \"embedding\" layer\nthat tags with possible parts of speech, applying just once per encoder pass 19\nattention-head compatible flat pattern-matching rules, shown using grammar\ncoverage (Zeller et al., 2023) to be learnable from the training data, plus\ngeneral prepositional phrase (pp) handling and sentential complement (cp)\nhandling logic, and output the next logical form (LF) token (repeating until\nthe LF is complete). The model does not apply recursive, tree-structured rules\nlike 'np_det pp np -> np_pp -> np', but scores 100% semantic and string exact\nmatch on pp recursion, cp recursion using the decoder loop."}
{"id": "2504.15434", "pdf": "https://arxiv.org/pdf/2504.15434", "abs": "https://arxiv.org/abs/2504.15434", "authors": ["Sarath Shekkizhar", "Romain Cosentino"], "title": "AGI Is Coming... Right After AI Learns to Play Wordle", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "This paper investigates multimodal agents, in particular, OpenAI's\nComputer-User Agent (CUA), trained to control and complete tasks through a\nstandard computer interface, similar to humans. We evaluated the agent's\nperformance on the New York Times Wordle game to elicit model behaviors and\nidentify shortcomings. Our findings revealed a significant discrepancy in the\nmodel's ability to recognize colors correctly depending on the context. The\nmodel had a $5.36\\%$ success rate over several hundred runs across a week of\nWordle. Despite the immense enthusiasm surrounding AI agents and their\npotential to usher in Artificial General Intelligence (AGI), our findings\nreinforce the fact that even simple tasks present substantial challenges for\ntoday's frontier AI models. We conclude with a discussion of the potential\nunderlying causes, implications for future development, and research directions\nto improve these AI systems."}
{"id": "2504.15362", "pdf": "https://arxiv.org/pdf/2504.15362", "abs": "https://arxiv.org/abs/2504.15362", "authors": ["Yuan-Hong Liao", "Sven Elflein", "Liu He", "Laura Leal-Taixé", "Yejin Choi", "Sanja Fidler", "David Acuna"], "title": "LongPerceptualThoughts: Distilling System-2 Reasoning for System-1 Perception", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "24 pages, 10 figures, in submission. Project page:\n  https://andrewliao11.github.io/LongPerceptualThoughts", "summary": "Recent reasoning models through test-time scaling have demonstrated that long\nchain-of-thoughts can unlock substantial performance boosts in hard reasoning\ntasks such as math and code. However, the benefit of such long thoughts for\nsystem-2 reasoning is relatively less explored in other domains such as\nperceptual tasks where shallower, system-1 reasoning seems sufficient. In this\npaper, we introduce LongPerceptualThoughts, a new synthetic dataset with 30K\nlong-thought traces for perceptual tasks. The key challenges in synthesizing\nelaborate reasoning thoughts for perceptual tasks are that off-the-shelf models\nare not yet equipped with such thinking behavior and that it is not\nstraightforward to build a reliable process verifier for perceptual tasks.\nThus, we propose a novel three-stage data synthesis framework that first\nsynthesizes verifiable multiple-choice questions from dense image descriptions,\nthen extracts simple CoTs from VLMs for those verifiable problems, and finally\nexpands those simple thoughts to elaborate long thoughts via frontier reasoning\nmodels. In controlled experiments with a strong instruction-tuned 7B model, we\ndemonstrate notable improvements over existing visual reasoning data-generation\nmethods. Our model, trained on the generated dataset, achieves an average +3.4\npoints improvement over 5 vision-centric benchmarks, including +11.8 points on\nV$^*$ Bench. Notably, despite being tuned for vision tasks, it also improves\nperformance on the text reasoning benchmark, MMLU-Pro, by +2 points."}
{"id": "2504.15392", "pdf": "https://arxiv.org/pdf/2504.15392", "abs": "https://arxiv.org/abs/2504.15392", "authors": ["Myrthe Reuver", "Indira Sen", "Matteo Melis", "Gabriella Lapesa"], "title": "Tell Me What You Know About Sexism: Expert-LLM Interaction Strategies and Co-Created Definitions for Zero-Shot Sexism Detection", "categories": ["cs.CL", "cs.CY"], "comment": "Accepted and published at Findings of NAACL 2025: cite published\n  version whenever possible", "summary": "This paper investigates hybrid intelligence and collaboration between\nresearchers of sexism and Large Language Models (LLMs), with a four-component\npipeline. First, nine sexism researchers answer questions about their knowledge\nof sexism and of LLMs. They then participate in two interactive experiments\ninvolving an LLM (GPT3.5). The first experiment has experts assessing the\nmodel's knowledge about sexism and suitability for use in research. The second\nexperiment tasks them with creating three different definitions of sexism: an\nexpert-written definition, an LLM-written one, and a co-created definition.\nLastly, zero-shot classification experiments use the three definitions from\neach expert in a prompt template for sexism detection, evaluating GPT4o on\n2.500 texts sampled from five sexism benchmarks. We then analyze the resulting\n67.500 classification decisions. The LLM interactions lead to longer and more\ncomplex definitions of sexism. Expert-written definitions on average perform\npoorly compared to LLM-generated definitions. However, some experts do improve\nclassification performance with their co-created definitions of sexism, also\nexperts who are inexperienced in using LLMs."}
{"id": "2504.15457", "pdf": "https://arxiv.org/pdf/2504.15457", "abs": "https://arxiv.org/abs/2504.15457", "authors": ["Paresh Chaudhary", "Yancheng Liang", "Daphne Chen", "Simon S. Du", "Natasha Jaques"], "title": "Improving Human-AI Coordination through Adversarial Training and Generative Models", "categories": ["cs.AI"], "comment": null, "summary": "Being able to cooperate with new people is an important component of many\neconomically valuable AI tasks, from household robotics to autonomous driving.\nHowever, generalizing to novel humans requires training on data that captures\nthe diversity of human behaviors. Adversarial training is one avenue for\nsearching for such data and ensuring that agents are robust. However, it is\ndifficult to apply in the cooperative setting because adversarial policies\nintentionally learn to sabotage the task instead of simulating valid\ncooperation partners. To address this challenge, we propose a novel strategy\nfor overcoming self-sabotage that combines a pre-trained generative model to\nsimulate valid cooperative agent policies with adversarial training to maximize\nregret. We call our method GOAT: Generative Online Adversarial Training. In\nthis framework, the GOAT dynamically searches for and generates coordination\nstrategies where the learning policy -- the Cooperator agent -- underperforms.\nGOAT enables better generalization by exposing the Cooperator to various\nchallenging interaction scenarios. We maintain realistic coordination\nstrategies by updating only the generative model's embedding while keeping its\nparameters frozen, thus avoiding adversarial exploitation. We evaluate GOAT\nwith real human partners, and the results demonstrate state-of-the-art\nperformance on the Overcooked benchmark, highlighting its effectiveness in\ngeneralizing to diverse human behaviors."}
{"id": "2504.15371", "pdf": "https://arxiv.org/pdf/2504.15371", "abs": "https://arxiv.org/abs/2504.15371", "authors": ["Wei Fang", "Priyadarshini Panda"], "title": "Event2Vec: Processing neuromorphic events directly by representations in vector space", "categories": ["cs.CV", "cs.NE"], "comment": null, "summary": "The neuromorphic event cameras have overwhelming advantages in temporal\nresolution, power efficiency, and dynamic range compared to traditional\ncameras. However, the event cameras output asynchronous, sparse, and irregular\nevents, which are not compatible with mainstream computer vision and deep\nlearning methods. Various methods have been proposed to solve this issue but at\nthe cost of long preprocessing procedures, losing temporal resolutions, or\nbeing incompatible with massively parallel computation. Inspired by the great\nsuccess of the word to vector, we summarize the similarities between words and\nevents, then propose the first event to vector (event2vec) representation. We\nvalidate event2vec on classifying the ASL-DVS dataset, showing impressive\nparameter efficiency, accuracy, and speed than previous graph/image/voxel-based\nrepresentations. Beyond task performance, the most attractive advantage of\nevent2vec is that it aligns events to the domain of natural language\nprocessing, showing the promising prospect of integrating events into large\nlanguage and multimodal models. Our codes, models, and training logs are\navailable at https://github.com/fangwei123456/event2vec."}
{"id": "2504.15431", "pdf": "https://arxiv.org/pdf/2504.15431", "abs": "https://arxiv.org/abs/2504.15431", "authors": ["Sungjun Han", "Juyoung Suk", "Suyeong An", "Hyungguk Kim", "Kyuseok Kim", "Wonsuk Yang", "Seungtaek Choi", "Jamin Shin"], "title": "Trillion 7B Technical Report", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Preview version", "summary": "We introduce Trillion-7B, the most token-efficient Korean-centric\nmultilingual LLM available. Our novel Cross-lingual Document Attention (XLDA)\nmechanism enables highly efficient and effective knowledge transfer from\nEnglish to target languages like Korean and Japanese. Combined with optimized\ndata mixtures, language-specific filtering, and tailored tokenizer\nconstruction, Trillion-7B achieves competitive performance while dedicating\nonly 10\\% of its 2T training tokens to multilingual data and requiring just\n59.4K H100 GPU hours (\\$148K) for full training. Comprehensive evaluations\nacross 27 benchmarks in four languages demonstrate Trillion-7B's robust\nmultilingual performance and exceptional cross-lingual consistency."}
{"id": "2504.15466", "pdf": "https://arxiv.org/pdf/2504.15466", "abs": "https://arxiv.org/abs/2504.15466", "authors": ["Jiayi Pan", "Xiuyu Li", "Long Lian", "Charlie Snell", "Yifei Zhou", "Adam Yala", "Trevor Darrell", "Kurt Keutzer", "Alane Suhr"], "title": "Learning Adaptive Parallel Reasoning with Language Models", "categories": ["cs.AI", "cs.CL"], "comment": "Code, model, and data are available at\n  https://github.com/Parallel-Reasoning/APR. The first three authors\n  contributed equally to this work", "summary": "Scaling inference-time computation has substantially improved the reasoning\ncapabilities of language models. However, existing methods have significant\nlimitations: serialized chain-of-thought approaches generate overly long\noutputs, leading to increased latency and exhausted context windows, while\nparallel methods such as self-consistency suffer from insufficient\ncoordination, resulting in redundant computations and limited performance\ngains. To address these shortcomings, we propose Adaptive Parallel Reasoning\n(APR), a novel reasoning framework that enables language models to orchestrate\nboth serialized and parallel computations end-to-end. APR generalizes existing\nreasoning methods by enabling adaptive multi-threaded inference using spawn()\nand join() operations. A key innovation is our end-to-end reinforcement\nlearning strategy, optimizing both parent and child inference threads to\nenhance task success rate without requiring predefined reasoning structures.\nExperiments on the Countdown reasoning task demonstrate significant benefits of\nAPR: (1) higher performance within the same context window (83.4% vs. 60.0% at\n4k context); (2) superior scalability with increased computation (80.1% vs.\n66.6% at 20k total tokens); (3) improved accuracy at equivalent latency (75.2%\nvs. 57.3% at approximately 5,000ms). APR represents a step towards enabling\nlanguage models to autonomously optimize their reasoning processes through\nadaptive allocation of computation."}
{"id": "2504.15376", "pdf": "https://arxiv.org/pdf/2504.15376", "abs": "https://arxiv.org/abs/2504.15376", "authors": ["Zhiqiu Lin", "Siyuan Cen", "Daniel Jiang", "Jay Karhade", "Hewei Wang", "Chancharik Mitra", "Tiffany Ling", "Yuhan Huang", "Sifan Liu", "Mingyu Chen", "Rushikesh Zawar", "Xue Bai", "Yilun Du", "Chuang Gan", "Deva Ramanan"], "title": "Towards Understanding Camera Motions in Any Video", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "Project site: https://linzhiqiu.github.io/papers/camerabench/", "summary": "We introduce CameraBench, a large-scale dataset and benchmark designed to\nassess and improve camera motion understanding. CameraBench consists of ~3,000\ndiverse internet videos, annotated by experts through a rigorous multi-stage\nquality control process. One of our contributions is a taxonomy of camera\nmotion primitives, designed in collaboration with cinematographers. We find,\nfor example, that some motions like \"follow\" (or tracking) require\nunderstanding scene content like moving subjects. We conduct a large-scale\nhuman study to quantify human annotation performance, revealing that domain\nexpertise and tutorial-based training can significantly enhance accuracy. For\nexample, a novice may confuse zoom-in (a change of intrinsics) with translating\nforward (a change of extrinsics), but can be trained to differentiate the two.\nUsing CameraBench, we evaluate Structure-from-Motion (SfM) and Video-Language\nModels (VLMs), finding that SfM models struggle to capture semantic primitives\nthat depend on scene content, while VLMs struggle to capture geometric\nprimitives that require precise estimation of trajectories. We then fine-tune a\ngenerative VLM on CameraBench to achieve the best of both worlds and showcase\nits applications, including motion-augmented captioning, video question\nanswering, and video-text retrieval. We hope our taxonomy, benchmark, and\ntutorials will drive future efforts towards the ultimate goal of understanding\ncamera motions in any video."}
{"id": "2504.15432", "pdf": "https://arxiv.org/pdf/2504.15432", "abs": "https://arxiv.org/abs/2504.15432", "authors": ["Yucheng Lu", "Kazimier Smith"], "title": "Feeding LLM Annotations to BERT Classifiers at Your Own Risk", "categories": ["cs.CL"], "comment": null, "summary": "Using LLM-generated labels to fine-tune smaller encoder-only models for text\nclassification has gained popularity in various settings. While this approach\nmay be justified in simple and low-stakes applications, we conduct empirical\nanalysis to demonstrate how the perennial curse of training on synthetic data\nmanifests itself in this specific setup. Compared to models trained on gold\nlabels, we observe not only the expected performance degradation in accuracy\nand F1 score, but also increased instability across training runs and premature\nperformance plateaus. These findings cast doubts on the reliability of such\napproaches in real-world applications. We contextualize the observed phenomena\nthrough the lens of error propagation and offer several practical mitigation\nstrategies, including entropy-based filtering and ensemble techniques. Although\nthese heuristics offer partial relief, they do not fully resolve the inherent\nrisks of propagating non-random errors from LLM annotations to smaller\nclassifiers, underscoring the need for caution when applying this workflow in\nhigh-stakes text classification tasks."}
{"id": "2504.15552", "pdf": "https://arxiv.org/pdf/2504.15552", "abs": "https://arxiv.org/abs/2504.15552", "authors": ["Gengxian Cao", "Fengyuan Li", "Hong Duan", "Ye Yang", "Bofeng Wang", "Donghe Li"], "title": "A Multi-Agent Framework for Automated Qinqiang Opera Script Generation Using Large Language Models", "categories": ["cs.AI"], "comment": "17 pages,7 figures,1 tables", "summary": "This paper introduces a novel multi-Agent framework that automates the end to\nend production of Qinqiang opera by integrating Large Language Models , visual\ngeneration, and Text to Speech synthesis. Three specialized agents collaborate\nin sequence: Agent1 uses an LLM to craft coherent, culturally grounded\nscripts;Agent2 employs visual generation models to render contextually accurate\nstage scenes; and Agent3 leverages TTS to produce synchronized, emotionally\nexpressive vocal performances. In a case study on Dou E Yuan, the system\nachieved expert ratings of 3.8 for script fidelity, 3.5 for visual coherence,\nand 3.8 for speech accuracy-culminating in an overall score of 3.6, a 0.3 point\nimprovement over a Single Agent baseline. Ablation experiments demonstrate that\nremoving Agent2 or Agent3 leads to drops of 0.4 and 0.5 points, respectively,\nunderscoring the value of modular collaboration. This work showcases how AI\ndriven pipelines can streamline and scale the preservation of traditional\nperforming arts, and points toward future enhancements in cross modal\nalignment, richer emotional nuance, and support for additional opera genres."}
{"id": "2504.15378", "pdf": "https://arxiv.org/pdf/2504.15378", "abs": "https://arxiv.org/abs/2504.15378", "authors": ["Scott Sorensen", "Wayne Treible", "Robert Wagner", "Andrew D. Gilliam", "Todd Rovito", "Joseph L. Mundy"], "title": "Physics Driven Image Simulation from Commercial Satellite Imagery", "categories": ["cs.CV"], "comment": "15 pages, 9 figures", "summary": "Physics driven image simulation allows for the modeling and creation of\nrealistic imagery beyond what is afforded by typical rendering pipelines. We\naim to automatically generate a physically realistic scene for simulation of a\ngiven region using satellite imagery to model the scene geometry, drive\nmaterial estimates, and populate the scene with dynamic elements. We present\nautomated techniques to utilize satellite imagery throughout the simulated\nscene to expedite scene construction and decrease manual overhead. Our\ntechnique does not use lidar, enabling simulations that could not be\nconstructed previously. To develop a 3D scene, we model the various components\nof the real location, addressing the terrain, modelling man-made structures,\nand populating the scene with smaller elements such as vegetation and vehicles.\nTo create the scene we begin with a Digital Surface Model, which serves as the\nbasis for scene geometry, and allows us to reason about the real location in a\ncommon 3D frame of reference. These simulated scenes can provide increased\nfidelity with less manual intervention for novel locations on earth, and can\nfacilitate algorithm development, and processing pipelines for imagery ranging\nfrom UV to LWIR $(200nm-20\\mu m)$."}
{"id": "2504.15471", "pdf": "https://arxiv.org/pdf/2504.15471", "abs": "https://arxiv.org/abs/2504.15471", "authors": ["Tyler A. Chang", "Benjamin K. Bergen"], "title": "Bigram Subnetworks: Mapping to Next Tokens in Transformer Language Models", "categories": ["cs.CL"], "comment": null, "summary": "In Transformer language models, activation vectors transform from current\ntoken embeddings to next token predictions as they pass through the model. To\nisolate a minimal form of this transformation, we identify language model\nsubnetworks that make bigram predictions, naive next token predictions based\nonly on the current token. We find that bigram subnetworks can be found in\nfully trained language models up to 1B parameters, and these subnetworks are\ncritical for model performance even when they consist of less than 0.2% of\nmodel parameters. Bigram subnetworks are concentrated in the first Transformer\nMLP layer, and they overlap significantly with subnetworks trained to optimally\nprune a given model. Mechanistically, the bigram subnetworks often recreate a\npattern from the full models where the first layer induces a sharp change that\naligns activations with next token predictions rather than current token\nrepresentations. Our results demonstrate that bigram subnetworks comprise a\nminimal subset of parameters that are both necessary and sufficient for basic\nnext token predictions in language models, and they help drive the\ntransformation from current to next token activations in the residual stream.\nThese subnetworks can lay a foundation for studying language model circuits by\nbuilding up from a minimal circuit rather than the traditional approach of\nablating circuits from a full model."}
{"id": "2504.15610", "pdf": "https://arxiv.org/pdf/2504.15610", "abs": "https://arxiv.org/abs/2504.15610", "authors": ["Md Millat", "Md Motiur"], "title": "A LoRA-Based Approach to Fine-Tuning LLMs for Educational Guidance in Resource-Constrained Settings", "categories": ["cs.AI", "68T05 (Learning and adaptive systems), 68T07 (Artificial\n  intelligence and education)"], "comment": "18 pages, 6 figures (3 graphs + 3 flowchart/architecture diagrams),\n  submitted as a preprint for review consideration in AI for Education or\n  Machine Learning applications in low-resource settings. Includes detailed\n  experiments with LoRA and quantization methods for efficient LLM fine-tuning", "summary": "The current study describes a cost-effective method for adapting large\nlanguage models (LLMs) for academic advising with study-abroad contexts in mind\nand for application in low-resource methods for acculturation. With the\nMistral-7B-Instruct model applied with a Low-Rank Adaptation (LoRA) method and\na 4-bit quantization method, the model underwent training in two distinct\nstages related to this study's purpose to enhance domain specificity while\nmaintaining computational efficiency. In Phase 1, the model was conditioned\nwith a synthetic dataset via the Gemini Pro API, and in Phase 2, it was trained\nwith manually curated datasets from the StudyAbroadGPT project to achieve\nenhanced, contextualized responses. Technical innovations entailed\nmemory-efficient quantization, parameter-efficient adaptation, and continuous\ntraining analytics via Weights & Biases. After training, this study\ndemonstrated a reduction in training loss by 52.7%, 92% accuracy in\ndomain-specific recommendations, achieved 95% markdown-based formatting\nsupport, and a median run-rate of 100 samples per second on off-the-shelf GPU\nequipment. These findings support the effective application of\ninstruction-tuned LLMs within educational advisers, especially in low-resource\ninstitutional scenarios. Limitations included decreased generalizability and\nthe application of a synthetically generated dataset, but this framework is\nscalable for adding new multilingual-augmented and real-time academic advising\nprocesses. Future directions may include plans for the integration of\nretrieval-augmented generation, applying dynamic quantization routines, and\nconnecting to real-time academic databases to increase adaptability and\naccuracy."}
{"id": "2504.15380", "pdf": "https://arxiv.org/pdf/2504.15380", "abs": "https://arxiv.org/abs/2504.15380", "authors": ["Huimin Zeng", "Jiacheng Li", "Zhiwei Xiong"], "title": "Plug-and-Play Versatile Compressed Video Enhancement", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "As a widely adopted technique in data transmission, video compression\neffectively reduces the size of files, making it possible for real-time cloud\ncomputing. However, it comes at the cost of visual quality, posing challenges\nto the robustness of downstream vision models. In this work, we present a\nversatile codec-aware enhancement framework that reuses codec information to\nadaptively enhance videos under different compression settings, assisting\nvarious downstream vision tasks without introducing computation bottleneck.\nSpecifically, the proposed codec-aware framework consists of a\ncompression-aware adaptation (CAA) network that employs a hierarchical\nadaptation mechanism to estimate parameters of the frame-wise enhancement\nnetwork, namely the bitstream-aware enhancement (BAE) network. The BAE network\nfurther leverages temporal and spatial priors embedded in the bitstream to\neffectively improve the quality of compressed input frames. Extensive\nexperimental results demonstrate the superior quality enhancement performance\nof our framework over existing enhancement methods, as well as its versatility\nin assisting multiple downstream tasks on compressed videos as a plug-and-play\nmodule. Code and models are available at\nhttps://huimin-zeng.github.io/PnP-VCVE/."}
{"id": "2504.15475", "pdf": "https://arxiv.org/pdf/2504.15475", "abs": "https://arxiv.org/abs/2504.15475", "authors": ["Szymon Kobus", "Deniz Gündüz"], "title": "Speculative Sampling via Exponential Races", "categories": ["cs.CL", "cs.IT", "math.IT"], "comment": null, "summary": "Speculative decoding accelerates large language model inference using a\nsmaller draft model. In this paper, we establish a surprising connection\nbetween speculative decoding and channel simulation, which aims at simulating a\nnoisy channel using as few bits as possible. This connection allows us to\nprovide an information-theoretic analysis of the speed up that can be achieved\nby speculative decoding. Leveraging this link, we derive an explicit relation\nbetween generation speed-up and the number of tokens $k$ generated by the draft\nmodel for large $k$, which serves as an upper bound for all $k$. We also\npropose a novel speculative decoding method via exponential race ERSD that\nmatches state-of-the-art performance."}
{"id": "2504.15668", "pdf": "https://arxiv.org/pdf/2504.15668", "abs": "https://arxiv.org/abs/2504.15668", "authors": ["Mir Md Sajid Sarwar", "Rajarshi Ray"], "title": "Exploring Inevitable Waypoints for Unsolvability Explanation in Hybrid Planning Problems", "categories": ["cs.AI", "cs.FL", "I.2.0; F.4.3"], "comment": null, "summary": "Explaining unsolvability of planning problems is of significant research\ninterest in Explainable AI Planning. AI planning literature has reported\nseveral research efforts on generating explanations of solutions to planning\nproblems. However, explaining the unsolvability of planning problems remains a\nlargely open and understudied problem. A widely practiced approach to plan\ngeneration and automated problem solving, in general, is to decompose tasks\ninto sub-problems that help progressively converge towards the goal. In this\npaper, we propose to adopt the same philosophy of sub-problem identification as\na mechanism for analyzing and explaining unsolvability of planning problems in\nhybrid systems. In particular, for a given unsolvable planning problem, we\npropose to identify common waypoints, which are universal obstacles to plan\nexistence; in other words, they appear on every plan from the source to the\nplanning goal. This work envisions such waypoints as sub-problems of the\nplanning problem and the unreachability of any of these waypoints as an\nexplanation for the unsolvability of the original planning problem. We propose\na novel method of waypoint identification by casting the problem as an instance\nof the longest common subsequence problem, a widely popular problem in computer\nscience, typically considered as an illustrative example for the dynamic\nprogramming paradigm. Once the waypoints are identified, we perform symbolic\nreachability analysis on them to identify the earliest unreachable waypoint and\nreport it as the explanation of unsolvability. We present experimental results\non unsolvable planning problems in hybrid domains."}
{"id": "2504.15384", "pdf": "https://arxiv.org/pdf/2504.15384", "abs": "https://arxiv.org/abs/2504.15384", "authors": ["Chen Zhao", "Anjum Shaik", "Joyce H. Keyak", "Nancy E. Lane", "Jeffrey D. Deng", "Kuan-Jui Su", "Qiuying Sha", "Hui Shen", "Hong-Wen Deng", "Weihua Zhou"], "title": "ICGM-FRAX: Iterative Cross Graph Matching for Hip Fracture Risk Assessment using Dual-energy X-ray Absorptiometry Images", "categories": ["cs.CV"], "comment": "23 pages, 4 figures", "summary": "Hip fractures represent a major health concern, particularly among the\nelderly, often leading decreased mobility and increased mortality. Early and\naccurate detection of at risk individuals is crucial for effective\nintervention. In this study, we propose Iterative Cross Graph Matching for Hip\nFracture Risk Assessment (ICGM-FRAX), a novel approach for predicting hip\nfractures using Dual-energy X-ray Absorptiometry (DXA) images. ICGM-FRAX\ninvolves iteratively comparing a test (subject) graph with multiple template\ngraphs representing the characteristics of hip fracture subjects to assess the\nsimilarity and accurately to predict hip fracture risk. These graphs are\nobtained as follows. The DXA images are separated into multiple regions of\ninterest (RoIs), such as the femoral head, shaft, and lesser trochanter.\nRadiomic features are then calculated for each RoI, with the central\ncoordinates used as nodes in a graph. The connectivity between nodes is\nestablished according to the Euclidean distance between these coordinates. This\nprocess transforms each DXA image into a graph, where each node represents a\nRoI, and edges derived by the centroids of RoIs capture the spatial\nrelationships between them. If the test graph closely matches a set of template\ngraphs representing subjects with incident hip fractures, it is classified as\nindicating high hip fracture risk. We evaluated our method using 547 subjects\nfrom the UK Biobank dataset, and experimental results show that ICGM-FRAX\nachieved a sensitivity of 0.9869, demonstrating high accuracy in predicting hip\nfractures."}
{"id": "2504.15509", "pdf": "https://arxiv.org/pdf/2504.15509", "abs": "https://arxiv.org/abs/2504.15509", "authors": ["Keqi Deng", "Wenxi Chen", "Xie Chen", "Philip C. Woodland"], "title": "SimulS2S-LLM: Unlocking Simultaneous Inference of Speech LLMs for Speech-to-Speech Translation", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Simultaneous speech translation (SST) outputs translations in parallel with\nstreaming speech input, balancing translation quality and latency. While large\nlanguage models (LLMs) have been extended to handle the speech modality,\nstreaming remains challenging as speech is prepended as a prompt for the entire\ngeneration process. To unlock LLM streaming capability, this paper proposes\nSimulS2S-LLM, which trains speech LLMs offline and employs a test-time policy\nto guide simultaneous inference. SimulS2S-LLM alleviates the mismatch between\ntraining and inference by extracting boundary-aware speech prompts that allows\nit to be better matched with text input data. SimulS2S-LLM achieves\nsimultaneous speech-to-speech translation (Simul-S2ST) by predicting discrete\noutput speech tokens and then synthesising output speech using a pre-trained\nvocoder. An incremental beam search is designed to expand the search space of\nspeech token prediction without increasing latency. Experiments on the CVSS\nspeech data show that SimulS2S-LLM offers a better translation quality-latency\ntrade-off than existing methods that use the same training data, such as\nimproving ASR-BLEU scores by 3 points at similar latency."}
{"id": "2504.15699", "pdf": "https://arxiv.org/pdf/2504.15699", "abs": "https://arxiv.org/abs/2504.15699", "authors": ["Ning Wang", "Zihan Yan", "Weiyang Li", "Chuan Ma", "He Chen", "Tao Xiang"], "title": "Advancing Embodied Agent Security: From Safety Benchmarks to Input Moderation", "categories": ["cs.AI"], "comment": "9 pages", "summary": "Embodied agents exhibit immense potential across a multitude of domains,\nmaking the assurance of their behavioral safety a fundamental prerequisite for\ntheir widespread deployment. However, existing research predominantly\nconcentrates on the security of general large language models, lacking\nspecialized methodologies for establishing safety benchmarks and input\nmoderation tailored to embodied agents. To bridge this gap, this paper\nintroduces a novel input moderation framework, meticulously designed to\nsafeguard embodied agents. This framework encompasses the entire pipeline,\nincluding taxonomy definition, dataset curation, moderator architecture, model\ntraining, and rigorous evaluation. Notably, we introduce EAsafetyBench, a\nmeticulously crafted safety benchmark engineered to facilitate both the\ntraining and stringent assessment of moderators specifically designed for\nembodied agents. Furthermore, we propose Pinpoint, an innovative\nprompt-decoupled input moderation scheme that harnesses a masked attention\nmechanism to effectively isolate and mitigate the influence of functional\nprompts on moderation tasks. Extensive experiments conducted on diverse\nbenchmark datasets and models validate the feasibility and efficacy of the\nproposed approach. The results demonstrate that our methodologies achieve an\nimpressive average detection accuracy of 94.58%, surpassing the performance of\nexisting state-of-the-art techniques, alongside an exceptional moderation\nprocessing time of merely 0.002 seconds per instance."}
{"id": "2504.15397", "pdf": "https://arxiv.org/pdf/2504.15397", "abs": "https://arxiv.org/abs/2504.15397", "authors": ["Ankit Dhiman", "Manan Shah", "R Venkatesh Babu"], "title": "MirrorVerse: Pushing Diffusion Models to Realistically Reflect the World", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025. Project Page: https://mirror-verse.github.io/", "summary": "Diffusion models have become central to various image editing tasks, yet they\noften fail to fully adhere to physical laws, particularly with effects like\nshadows, reflections, and occlusions. In this work, we address the challenge of\ngenerating photorealistic mirror reflections using diffusion-based generative\nmodels. Despite extensive training data, existing diffusion models frequently\noverlook the nuanced details crucial to authentic mirror reflections. Recent\napproaches have attempted to resolve this by creating synhetic datasets and\nframing reflection generation as an inpainting task; however, they struggle to\ngeneralize across different object orientations and positions relative to the\nmirror. Our method overcomes these limitations by introducing key augmentations\ninto the synthetic data pipeline: (1) random object positioning, (2) randomized\nrotations, and (3) grounding of objects, significantly enhancing generalization\nacross poses and placements. To further address spatial relationships and\nocclusions in scenes with multiple objects, we implement a strategy to pair\nobjects during dataset generation, resulting in a dataset robust enough to\nhandle these complex scenarios. Achieving generalization to real-world scenes\nremains a challenge, so we introduce a three-stage training curriculum to\ndevelop the MirrorFusion 2.0 model to improve real-world performance. We\nprovide extensive qualitative and quantitative evaluations to support our\napproach. The project page is available at: https://mirror-verse.github.io/."}
{"id": "2504.15521", "pdf": "https://arxiv.org/pdf/2504.15521", "abs": "https://arxiv.org/abs/2504.15521", "authors": ["Minghao Wu", "Weixuan Wang", "Sinuo Liu", "Huifeng Yin", "Xintong Wang", "Yu Zhao", "Chenyang Lyu", "Longyue Wang", "Weihua Luo", "Kaifu Zhang"], "title": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks", "categories": ["cs.CL"], "comment": "work in progress; 22 pages, 8 figures, 3 tables;", "summary": "As large language models (LLMs) continue to advance in linguistic\ncapabilities, robust multilingual evaluation has become essential for promoting\nequitable technological progress. This position paper examines over 2,000\nmultilingual (non-English) benchmarks from 148 countries, published between\n2021 and 2024, to evaluate past, present, and future practices in multilingual\nbenchmarking. Our findings reveal that, despite significant investments\namounting to tens of millions of dollars, English remains significantly\noverrepresented in these benchmarks. Additionally, most benchmarks rely on\noriginal language content rather than translations, with the majority sourced\nfrom high-resource countries such as China, India, Germany, the UK, and the\nUSA. Furthermore, a comparison of benchmark performance with human judgments\nhighlights notable disparities. STEM-related tasks exhibit strong correlations\nwith human evaluations (0.70 to 0.85), while traditional NLP tasks like\nquestion answering (e.g., XQuAD) show much weaker correlations (0.11 to 0.30).\nMoreover, translating English benchmarks into other languages proves\ninsufficient, as localized benchmarks demonstrate significantly higher\nalignment with local human judgments (0.68) than their translated counterparts\n(0.47). This underscores the importance of creating culturally and\nlinguistically tailored benchmarks rather than relying solely on translations.\nThrough this comprehensive analysis, we highlight six key limitations in\ncurrent multilingual evaluation practices, propose the guiding principles\naccordingly for effective multilingual benchmarking, and outline five critical\nresearch directions to drive progress in the field. Finally, we call for a\nglobal collaborative effort to develop human-aligned benchmarks that prioritize\nreal-world applications."}
{"id": "2504.15716", "pdf": "https://arxiv.org/pdf/2504.15716", "abs": "https://arxiv.org/abs/2504.15716", "authors": ["Jie Zhu", "Qian Chen", "Huaixia Dou", "Junhui Li", "Lifan Guo", "Feng Chen", "Chi Zhang"], "title": "DianJin-R1: Evaluating and Enhancing Financial Reasoning in Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Effective reasoning remains a core challenge for large language models (LLMs)\nin the financial domain, where tasks often require domain-specific knowledge,\nprecise numerical calculations, and strict adherence to compliance rules. We\npropose DianJin-R1, a reasoning-enhanced framework designed to address these\nchallenges through reasoning-augmented supervision and reinforcement learning.\nCentral to our approach is DianJin-R1-Data, a high-quality dataset constructed\nfrom CFLUE, FinQA, and a proprietary compliance corpus (Chinese Compliance\nCheck, CCC), combining diverse financial reasoning scenarios with verified\nannotations. Our models, DianJin-R1-7B and DianJin-R1-32B, are fine-tuned from\nQwen2.5-7B-Instruct and Qwen2.5-32B-Instruct using a structured format that\ngenerates both reasoning steps and final answers. To further refine reasoning\nquality, we apply Group Relative Policy Optimization (GRPO), a reinforcement\nlearning method that incorporates dual reward signals: one encouraging\nstructured outputs and another rewarding answer correctness. We evaluate our\nmodels on five benchmarks: three financial datasets (CFLUE, FinQA, and CCC) and\ntwo general reasoning benchmarks (MATH-500 and GPQA-Diamond). Experimental\nresults show that DianJin-R1 models consistently outperform their non-reasoning\ncounterparts, especially on complex financial tasks. Moreover, on the\nreal-world CCC dataset, our single-call reasoning models match or even surpass\nthe performance of multi-agent systems that require significantly more\ncomputational cost. These findings demonstrate the effectiveness of DianJin-R1\nin enhancing financial reasoning through structured supervision and\nreward-aligned learning, offering a scalable and practical solution for\nreal-world applications."}
{"id": "2504.15404", "pdf": "https://arxiv.org/pdf/2504.15404", "abs": "https://arxiv.org/abs/2504.15404", "authors": ["Tajamul Ashraf", "Rajes Manna", "Partha Sarathi Purkayastha", "Tavaheed Tariq", "Janibul Bashir"], "title": "Context Aware Grounded Teacher for Source Free Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "We focus on the Source Free Object Detection (SFOD) problem, when source data\nis unavailable during adaptation, and the model must adapt to the unlabeled\ntarget domain. In medical imaging, several approaches have leveraged a\nsemi-supervised student-teacher architecture to bridge domain discrepancy.\nContext imbalance in labeled training data and significant domain shifts\nbetween domains can lead to biased teacher models that produce inaccurate\npseudolabels, degrading the student model's performance and causing a mode\ncollapse. Class imbalance, particularly when one class significantly outnumbers\nanother, leads to contextual bias. To tackle the problem of context bias and\nthe significant performance drop of the student model in the SFOD setting, we\nintroduce Grounded Teacher (GT) as a standard framework. In this study, we\nmodel contextual relationships using a dedicated relational context module and\nleverage it to mitigate inherent biases in the model. This approach enables us\nto apply augmentations to closely related classes, across and within domains,\nenhancing the performance of underrepresented classes while keeping the effect\non dominant classes minimal. We further improve the quality of predictions by\nimplementing an expert foundational branch to supervise the student model. We\nvalidate the effectiveness of our approach in mitigating context bias under the\nSFOD setting through experiments on three medical datasets supported by\ncomprehensive ablation studies. All relevant resources, including preprocessed\ndata, trained model weights, and code, are publicly available at this\nhttps://github.com/Tajamul21/Grounded_Teacher."}
{"id": "2504.15524", "pdf": "https://arxiv.org/pdf/2504.15524", "abs": "https://arxiv.org/abs/2504.15524", "authors": ["Qiyao Wang", "Guhong Chen", "Hongbo Wang", "Huaren Liu", "Minghui Zhu", "Zhifei Qin", "Linwei Li", "Yilin Yue", "Shiqiang Wang", "Jiayan Li", "Yihang Wu", "Ziqiang Liu", "Longze Chen", "Run Luo", "Liyang Fan", "Jiaming Li", "Lei Zhang", "Kan Xu", "Hongfei Lin", "Hamid Alinejad-Rokny", "Shiwen Ni", "Yuan Lin", "Min Yang"], "title": "IPBench: Benchmarking the Knowledge of Large Language Models in Intellectual Property", "categories": ["cs.CL", "cs.AI"], "comment": "89 pages, 75 figures, 55 tables", "summary": "Intellectual Property (IP) is a unique domain that integrates technical and\nlegal knowledge, making it inherently complex and knowledge-intensive. As large\nlanguage models (LLMs) continue to advance, they show great potential for\nprocessing IP tasks, enabling more efficient analysis, understanding, and\ngeneration of IP-related content. However, existing datasets and benchmarks\neither focus narrowly on patents or cover limited aspects of the IP field,\nlacking alignment with real-world scenarios. To bridge this gap, we introduce\nthe first comprehensive IP task taxonomy and a large, diverse bilingual\nbenchmark, IPBench, covering 8 IP mechanisms and 20 tasks. This benchmark is\ndesigned to evaluate LLMs in real-world intellectual property applications,\nencompassing both understanding and generation. We benchmark 16 LLMs, ranging\nfrom general-purpose to domain-specific models, and find that even the\nbest-performing model achieves only 75.8% accuracy, revealing substantial room\nfor improvement. Notably, open-source IP and law-oriented models lag behind\nclosed-source general-purpose models. We publicly release all data and code of\nIPBench and will continue to update it with additional IP-related tasks to\nbetter reflect real-world challenges in the intellectual property domain."}
{"id": "2504.15719", "pdf": "https://arxiv.org/pdf/2504.15719", "abs": "https://arxiv.org/abs/2504.15719", "authors": ["Anna Karnysheva", "Christian Drescher", "Dietrich Klakow"], "title": "Implementing Rational Choice Functions with LLMs and Measuring their Alignment with User Preferences", "categories": ["cs.AI"], "comment": null, "summary": "As large language models (LLMs) become integral to intelligent user\ninterfaces (IUIs), their role as decision-making agents raises critical\nconcerns about alignment. Although extensive research has addressed issues such\nas factuality, bias, and toxicity, comparatively little attention has been paid\nto measuring alignment to preferences, i.e., the relative desirability of\ndifferent alternatives, a concept used in decision making, economics, and\nsocial choice theory. However, a reliable decision-making agent makes choices\nthat align well with user preferences.\n  In this paper, we generalize existing methods that exploit LLMs for ranking\nalternative outcomes by addressing alignment with the broader and more flexible\nconcept of user preferences, which includes both strict preferences and\nindifference among alternatives. To this end, we put forward design principles\nfor using LLMs to implement rational choice functions, and provide the\nnecessary tools to measure preference satisfaction. We demonstrate the\napplicability of our approach through an empirical study in a practical\napplication of an IUI in the automotive domain."}
{"id": "2504.15415", "pdf": "https://arxiv.org/pdf/2504.15415", "abs": "https://arxiv.org/abs/2504.15415", "authors": ["David Ma", "Yuanxing Zhang", "Jincheng Ren", "Jarvis Guo", "Yifan Yao", "Zhenlin Wei", "Zhenzhu Yang", "Zhongyuan Peng", "Boyu Feng", "Jun Ma", "Xiao Gu", "Zhoufutu Wen", "King Zhu", "Yancheng He", "Meng Cao", "Shiwen Ni", "Jiaheng Liu", "Wenhao Huang", "Ge Zhang", "Xiaojie Jin"], "title": "IV-Bench: A Benchmark for Image-Grounded Video Perception and Reasoning in Multimodal LLMs", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Existing evaluation frameworks for Multimodal Large Language Models (MLLMs)\nprimarily focus on image reasoning or general video understanding tasks,\nlargely overlooking the significant role of image context in video\ncomprehension. To bridge this gap, we propose IV-Bench, the first comprehensive\nbenchmark for evaluating Image-Grounded Video Perception and Reasoning.\nIV-Bench consists of 967 videos paired with 2,585 meticulously annotated\nimage-text queries across 13 tasks (7 perception and 6 reasoning tasks) and 5\nrepresentative categories. Extensive evaluations of state-of-the-art\nopen-source (e.g., InternVL2.5, Qwen2.5-VL) and closed-source (e.g., GPT-4o,\nGemini2-Flash and Gemini2-Pro) MLLMs demonstrate that current models\nsubstantially underperform in image-grounded video Perception and Reasoning,\nmerely achieving at most 28.9% accuracy. Further analysis reveals key factors\ninfluencing model performance on IV-Bench, including inference pattern, frame\nnumber, and resolution. Additionally, through a simple data synthesis approach,\nwe demonstratethe challenges of IV- Bench extend beyond merely aligning the\ndata format in the training proecss. These findings collectively provide\nvaluable insights for future research. Our codes and data are released in\nhttps://github.com/multimodal-art-projection/IV-Bench."}
{"id": "2504.15527", "pdf": "https://arxiv.org/pdf/2504.15527", "abs": "https://arxiv.org/abs/2504.15527", "authors": ["Sophia Maria"], "title": "Compass-V2 Technical Report", "categories": ["cs.CL"], "comment": null, "summary": "Predominant LLMs focus on high-resource languages while leaving low-resource\nlanguages, particularly those in Southeast Asia (SEA), underrepresented. In\naddition, those models are general-purpose and pay limited attention to the\ne-commerce domain. To overcome these limitations, we introduce Compass-v2, a\nlightweight Mixture-of-Experts (MoE) model specifically designed for Southeast\nAsian languages and e-commerce applications. To balance model performance and\ninference cost, the model is designed with 30B total parameters and 5B active\nparameters, incorporating both fine-grained and shared expert modules. To\nenhance multilingual performance, we curated and constructed a high-quality,\nindustry-leading SEA dataset, to the best of our knowledge. To boost\nperformance in the e-commerce domain, we built a dataset comprising hundreds of\nbillions of tokens, sourced through external data mining and internal platform\ncollection. Besides, we pioneered a hybrid reasoning model that supports both\nfast thinking and deep thinking within a unified framework to enhance the\nreasoning capabilities, diverging from the conventional industry practice of\ndeploying two separate models. Through extensive experimental evaluations, our\nmodel demonstrates state-of-the-art SEA multilingual and e-commerce performance\namong sub-30B models, while maintaining significantly lower inference cost."}
{"id": "2504.15780", "pdf": "https://arxiv.org/pdf/2504.15780", "abs": "https://arxiv.org/abs/2504.15780", "authors": ["Daocheng Fu", "Zijun Chen", "Renqiu Xia", "Qi Liu", "Yuan Feng", "Hongbin Zhou", "Renrui Zhang", "Shiyang Feng", "Peng Gao", "Junchi Yan", "Botian Shi", "Bo Zhang", "Yu Qiao"], "title": "TrustGeoGen: Scalable and Formal-Verified Data Engine for Trustworthy Multi-modal Geometric Problem Solving", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Mathematical geometric problem solving (GPS) often requires effective\nintegration of multimodal information and verifiable logical coherence. Despite\nthe fast development of large language models in general problem solving, it\nremains unresolved regarding with both methodology and benchmarks, especially\ngiven the fact that exiting synthetic GPS benchmarks are often not\nself-verified and contain noise and self-contradicted information due to the\nillusion of LLMs. In this paper, we propose a scalable data engine called\nTrustGeoGen for problem generation, with formal verification to provide a\nprincipled benchmark, which we believe lays the foundation for the further\ndevelopment of methods for GPS. The engine synthesizes geometric data through\nfour key innovations: 1) multimodal-aligned generation of diagrams, textual\ndescriptions, and stepwise solutions; 2) formal verification ensuring\nrule-compliant reasoning paths; 3) a bootstrapping mechanism enabling\ncomplexity escalation via recursive state generation and 4) our devised\nGeoExplore series algorithms simultaneously produce multi-solution variants and\nself-reflective backtracking traces. By formal logical verification,\nTrustGeoGen produces GeoTrust-200K dataset with guaranteed modality integrity,\nalong with GeoTrust-test testset. Experiments reveal the state-of-the-art\nmodels achieve only 49.17\\% accuracy on GeoTrust-test, demonstrating its\nevaluation stringency. Crucially, models trained on GeoTrust achieve OOD\ngeneralization on GeoQA, significantly reducing logical inconsistencies\nrelative to pseudo-label annotated by OpenAI-o1. Our code is available at\nhttps://github.com/Alpha-Innovator/TrustGeoGen"}
{"id": "2504.15470", "pdf": "https://arxiv.org/pdf/2504.15470", "abs": "https://arxiv.org/abs/2504.15470", "authors": ["Jonathan Brokman", "Amit Giloni", "Omer Hofman", "Roman Vainshtein", "Hisashi Kojima", "Guy Gilboa"], "title": "Manifold Induced Biases for Zero-shot and Few-shot Detection of Generated Images", "categories": ["cs.CV"], "comment": "Accepted to ICLR 2025 (The International Conference on Learning\n  Representations)", "summary": "Distinguishing between real and AI-generated images, commonly referred to as\n'image detection', presents a timely and significant challenge. Despite\nextensive research in the (semi-)supervised regime, zero-shot and few-shot\nsolutions have only recently emerged as promising alternatives. Their main\nadvantage is in alleviating the ongoing data maintenance, which quickly becomes\noutdated due to advances in generative technologies. We identify two main gaps:\n(1) a lack of theoretical grounding for the methods, and (2) significant room\nfor performance improvements in zero-shot and few-shot regimes. Our approach is\nfounded on understanding and quantifying the biases inherent in generated\ncontent, where we use these quantities as criteria for characterizing generated\nimages. Specifically, we explore the biases of the implicit probability\nmanifold, captured by a pre-trained diffusion model. Through score-function\nanalysis, we approximate the curvature, gradient, and bias towards points on\nthe probability manifold, establishing criteria for detection in the zero-shot\nregime. We further extend our contribution to the few-shot setting by employing\na mixture-of-experts methodology. Empirical results across 20 generative models\ndemonstrate that our method outperforms current approaches in both zero-shot\nand few-shot settings. This work advances the theoretical understanding and\npractical usage of generated content biases through the lens of manifold\nanalysis."}
{"id": "2504.15544", "pdf": "https://arxiv.org/pdf/2504.15544", "abs": "https://arxiv.org/abs/2504.15544", "authors": ["Issa Sugiura", "Kouta Nakayama", "Yusuke Oda"], "title": "llm-jp-modernbert: A ModernBERT Model Trained on a Large-Scale Japanese Corpus with Long Context Length", "categories": ["cs.CL"], "comment": "9 pages, 5 figures", "summary": "Encoder-only transformer models like BERT are widely adopted as a pre-trained\nbackbone for tasks like sentence classification and retrieval. However,\npretraining of encoder models with large-scale corpora and long contexts has\nbeen relatively underexplored compared to decoder-only transformers. In this\nwork, we present llm-jp-modernbert, a ModernBERT model trained on a publicly\navailable, massive Japanese corpus with a context length of 8192 tokens. While\nour model does not surpass existing baselines on downstream tasks, it achieves\ngood results on fill-mask test evaluations. We also analyze the effect of\ncontext length expansion through pseudo-perplexity experiments. Furthermore, we\ninvestigate sentence embeddings in detail, analyzing their transitions during\ntraining and comparing them with those from other existing models, confirming\nsimilar trends with models sharing the same architecture. To support\nreproducibility and foster the development of long-context BERT, we release our\nmodel, along with the training and evaluation code."}
{"id": "2504.15785", "pdf": "https://arxiv.org/pdf/2504.15785", "abs": "https://arxiv.org/abs/2504.15785", "authors": ["Siyu Zhou", "Tianyi Zhou", "Yijun Yang", "Guodong Long", "Deheng Ye", "Jing Jiang", "Chengqi Zhang"], "title": "WALL-E 2.0: World Alignment by NeuroSymbolic Learning improves World Model-based LLM Agents", "categories": ["cs.AI"], "comment": "Code is available at https://github.com/elated-sawyer/WALL-E", "summary": "Can we build accurate world models out of large language models (LLMs)? How\ncan world models benefit LLM agents? The gap between the prior knowledge of\nLLMs and the specified environment's dynamics usually bottlenecks LLMs'\nperformance as world models. To bridge the gap, we propose a training-free\n\"world alignment\" that learns an environment's symbolic knowledge complementary\nto LLMs. The symbolic knowledge covers action rules, knowledge graphs, and\nscene graphs, which are extracted by LLMs from exploration trajectories and\nencoded into executable codes to regulate LLM agents' policies. We further\npropose an RL-free, model-based agent \"WALL-E 2.0\" through the model-predictive\ncontrol (MPC) framework. Unlike classical MPC requiring costly optimization on\nthe fly, we adopt an LLM agent as an efficient look-ahead optimizer of future\nsteps' actions by interacting with the neurosymbolic world model. While the LLM\nagent's strong heuristics make it an efficient planner in MPC, the quality of\nits planned actions is also secured by the accurate predictions of the aligned\nworld model. They together considerably improve learning efficiency in a new\nenvironment. On open-world challenges in Mars (Minecraft like) and ALFWorld\n(embodied indoor environments), WALL-E 2.0 significantly outperforms existing\nmethods, e.g., surpassing baselines in Mars by 16.1%-51.6% of success rate and\nby at least 61.7% in score. In ALFWorld, it achieves a new record 98% success\nrate after only 4 iterations."}
{"id": "2504.15473", "pdf": "https://arxiv.org/pdf/2504.15473", "abs": "https://arxiv.org/abs/2504.15473", "authors": ["Berk Tinaz", "Zalan Fabian", "Mahdi Soltanolkotabi"], "title": "Emergence and Evolution of Interpretable Concepts in Diffusion Models", "categories": ["cs.CV", "cs.LG", "eess.IV", "I.2.6; I.2.10"], "comment": "32 pages, 32 figures, preliminary version", "summary": "Diffusion models have become the go-to method for text-to-image generation,\nproducing high-quality images from noise through a process called reverse\ndiffusion. Understanding the dynamics of the reverse diffusion process is\ncrucial in steering the generation and achieving high sample quality. However,\nthe inner workings of diffusion models is still largely a mystery due to their\nblack-box nature and complex, multi-step generation process. Mechanistic\nInterpretability (MI) techniques, such as Sparse Autoencoders (SAEs), aim at\nuncovering the operating principles of models through granular analysis of\ntheir internal representations. These MI techniques have been successful in\nunderstanding and steering the behavior of large language models at scale.\nHowever, the great potential of SAEs has not yet been applied toward gaining\ninsight into the intricate generative process of diffusion models. In this\nwork, we leverage the SAE framework to probe the inner workings of a popular\ntext-to-image diffusion model, and uncover a variety of human-interpretable\nconcepts in its activations. Interestingly, we find that even before the first\nreverse diffusion step is completed, the final composition of the scene can be\npredicted surprisingly well by looking at the spatial distribution of activated\nconcepts. Moreover, going beyond correlational analysis, we show that the\ndiscovered concepts have a causal effect on the model output and can be\nleveraged to steer the generative process. We design intervention techniques\naimed at manipulating image composition and style, and demonstrate that (1) in\nearly stages of diffusion image composition can be effectively controlled, (2)\nin the middle stages of diffusion image composition is finalized, however\nstylistic interventions are effective, and (3) in the final stages of diffusion\nonly minor textural details are subject to change."}
{"id": "2504.15548", "pdf": "https://arxiv.org/pdf/2504.15548", "abs": "https://arxiv.org/abs/2504.15548", "authors": ["Elyas Meguellati", "Assaad Zeghina", "Shazia Sadiq", "Gianluca Demartini"], "title": "LLM-based Semantic Augmentation for Harmful Content Detection", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Recent advances in large language models (LLMs) have demonstrated strong\nperformance on simple text classification tasks, frequently under zero-shot\nsettings. However, their efficacy declines when tackling complex social media\nchallenges such as propaganda detection, hateful meme classification, and\ntoxicity identification. Much of the existing work has focused on using LLMs to\ngenerate synthetic training data, overlooking the potential of LLM-based text\npreprocessing and semantic augmentation. In this paper, we introduce an\napproach that prompts LLMs to clean noisy text and provide context-rich\nexplanations, thereby enhancing training sets without substantial increases in\ndata volume. We systematically evaluate on the SemEval 2024 multi-label\nPersuasive Meme dataset and further validate on the Google Jigsaw toxic\ncomments and Facebook hateful memes datasets to assess generalizability. Our\nresults reveal that zero-shot LLM classification underperforms on these\nhigh-context tasks compared to supervised models. In contrast, integrating\nLLM-based semantic augmentation yields performance on par with approaches that\nrely on human-annotated data, at a fraction of the cost. These findings\nunderscore the importance of strategically incorporating LLMs into machine\nlearning (ML) pipeline for social media classification tasks, offering broad\nimplications for combating harmful content online."}
{"id": "2504.15791", "pdf": "https://arxiv.org/pdf/2504.15791", "abs": "https://arxiv.org/abs/2504.15791", "authors": ["Raquel Fernandez-Peralta", "Javier Fumanal-Idocin", "Javier Andreu-Perez"], "title": "Crisp complexity of fuzzy classifiers", "categories": ["cs.AI"], "comment": null, "summary": "Rule-based systems are a very popular form of explainable AI, particularly in\nthe fuzzy community, where fuzzy rules are widely used for control and\nclassification problems. However, fuzzy rule-based classifiers struggle to\nreach bigger traction outside of fuzzy venues, because users sometimes do not\nknow about fuzzy and because fuzzy partitions are not so easy to interpret in\nsome situations. In this work, we propose a methodology to reduce fuzzy\nrule-based classifiers to crisp rule-based classifiers. We study different\npossible crisp descriptions and implement an algorithm to obtain them. Also, we\nanalyze the complexity of the resulting crisp classifiers. We believe that our\nresults can help both fuzzy and non-fuzzy practitioners understand better the\nway in which fuzzy rule bases partition the feature space and how easily one\nsystem can be translated to another and vice versa. Our complexity metric can\nalso help to choose between different fuzzy classifiers based on what the\nequivalent crisp partitions look like."}
{"id": "2504.15485", "pdf": "https://arxiv.org/pdf/2504.15485", "abs": "https://arxiv.org/abs/2504.15485", "authors": ["Atin Pothiraj", "Elias Stengel-Eskin", "Jaemin Cho", "Mohit Bansal"], "title": "CAPTURe: Evaluating Spatial Reasoning in Vision Language Models via Occluded Object Counting", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Code and data: https://github.com/atinpothiraj/CAPTURe", "summary": "Recognizing and reasoning about occluded (partially or fully hidden) objects\nis vital to understanding visual scenes, as occlusions frequently occur in\nreal-world environments and act as obstacles for spatial comprehension. To test\nmodels' ability to reason about multiple occluded objects, we introduce a novel\ntask, Counting Amodally for Patterns Through Unseen REgions (CAPTURe), which\nrequires a model to count objects arranged in a pattern by inferring how the\npattern continues behind an occluder (an object which blocks parts of the\nscene). CAPTURe requires both recognizing visual patterns and reasoning, making\nit a useful testbed for evaluating vision-language models (VLMs) on whether\nthey understand occluded patterns and possess spatial understanding skills. By\nrequiring models to reason about occluded objects, CAPTURe also tests VLMs'\nability to form world models that would allow them to fill in missing\ninformation. CAPTURe consists of two parts: (1) CAPTURe-real, with manually\nfiltered images of real objects in patterns and (2) CAPTURe-synthetic, a\ncontrolled diagnostic with generated patterned images. We evaluate four strong\nVLMs (GPT-4o, Intern-VL2, Molmo, and Qwen2-VL) on CAPTURe, finding that models\nstruggle to count on both occluded and unoccluded patterns. Crucially, we find\nthat models perform worse with occlusion, suggesting that VLMs are also\ndeficient in inferring unseen spatial relationships: even the strongest VLMs\nlike GPT-4o fail to count with occlusion. In contrast, we find that humans\nachieve very little error on CAPTURe. We also find that providing auxiliary\ninformation of occluded object locations increases performance, underscoring\nthat the model error comes both from an inability to handle occlusion as well\nas difficulty counting in images."}
{"id": "2504.15573", "pdf": "https://arxiv.org/pdf/2504.15573", "abs": "https://arxiv.org/abs/2504.15573", "authors": ["Yuxin Jiang", "Yufei Wang", "Chuhan Wu", "Xinyi Dai", "Yan Xu", "Weinan Gan", "Yasheng Wang", "Xin Jiang", "Lifeng Shang", "Ruiming Tang", "Wei Wang"], "title": "Instruction-Tuning Data Synthesis from Scratch via Web Reconstruction", "categories": ["cs.CL"], "comment": "15 pages, 11 figures, 9 tables", "summary": "The improvement of LLMs' instruction-following capabilities depends\ncritically on the availability of high-quality instruction-response pairs.\nWhile existing automatic data synthetic methods alleviate the burden of manual\ncuration, they often rely heavily on either the quality of seed data or strong\nassumptions about the structure and content of web documents. To tackle these\nchallenges, we propose Web Reconstruction (WebR), a fully automated framework\nfor synthesizing high-quality instruction-tuning (IT) data directly from raw\nweb documents with minimal assumptions. Leveraging the inherent diversity of\nraw web content, we conceptualize web reconstruction as an instruction-tuning\ndata synthesis task via a novel dual-perspective paradigm--Web as Instruction\nand Web as Response--where each web document is designated as either an\ninstruction or a response to trigger the reconstruction process. Comprehensive\nexperiments show that datasets generated by WebR outperform state-of-the-art\nbaselines by up to 16.65% across four instruction-following benchmarks.\nNotably, WebR demonstrates superior compatibility, data efficiency, and\nscalability, enabling enhanced domain adaptation with minimal effort. The data\nand code are publicly available at https://github.com/YJiangcm/WebR."}
{"id": "2504.15829", "pdf": "https://arxiv.org/pdf/2504.15829", "abs": "https://arxiv.org/abs/2504.15829", "authors": ["Modhurita Mitra", "Martine G. de Vos", "Nicola Cortinovis", "Dawa Ometto"], "title": "Generative AI for Research Data Processing: Lessons Learnt From Three Use Cases", "categories": ["cs.AI", "68T50", "I.2.7"], "comment": "10 pages, 4 figures, 6 tables. Published in Proceedings of the 2024\n  IEEE 20th International Conference on e-Science (e-Science), Osaka, Japan", "summary": "There has been enormous interest in generative AI since ChatGPT was launched\nin 2022. However, there are concerns about the accuracy and consistency of the\noutputs of generative AI. We have carried out an exploratory study on the\napplication of this new technology in research data processing. We identified\ntasks for which rule-based or traditional machine learning approaches were\ndifficult to apply, and then performed these tasks using generative AI.\n  We demonstrate the feasibility of using the generative AI model Claude 3 Opus\nin three research projects involving complex data processing tasks:\n  1) Information extraction: We extract plant species names from historical\nseedlists (catalogues of seeds) published by botanical gardens.\n  2) Natural language understanding: We extract certain data points (name of\ndrug, name of health indication, relative effectiveness, cost-effectiveness,\netc.) from documents published by Health Technology Assessment organisations in\nthe EU.\n  3) Text classification: We assign industry codes to projects on the\ncrowdfunding website Kickstarter.\n  We share the lessons we learnt from these use cases: How to determine if\ngenerative AI is an appropriate tool for a given data processing task, and if\nso, how to maximise the accuracy and consistency of the results obtained."}
{"id": "2504.15513", "pdf": "https://arxiv.org/pdf/2504.15513", "abs": "https://arxiv.org/abs/2504.15513", "authors": ["Yixuan Zhu", "Haolin Wang", "Ao Li", "Wenliang Zhao", "Yansong Tang", "Jingxuan Niu", "Lei Chen", "Jie Zhou", "Jiwen Lu"], "title": "InstaRevive: One-Step Image Enhancement via Dynamic Score Matching", "categories": ["cs.CV"], "comment": "Accepted by ICLR 2025", "summary": "Image enhancement finds wide-ranging applications in real-world scenarios due\nto complex environments and the inherent limitations of imaging devices. Recent\ndiffusion-based methods yield promising outcomes but necessitate prolonged and\ncomputationally intensive iterative sampling. In response, we propose\nInstaRevive, a straightforward yet powerful image enhancement framework that\nemploys score-based diffusion distillation to harness potent generative\ncapability and minimize the sampling steps. To fully exploit the potential of\nthe pre-trained diffusion model, we devise a practical and effective diffusion\ndistillation pipeline using dynamic control to address inaccuracies in updating\ndirection during score matching. Our control strategy enables a dynamic\ndiffusing scope, facilitating precise learning of denoising trajectories within\nthe diffusion model and ensuring accurate distribution matching gradients\nduring training. Additionally, to enrich guidance for the generative power, we\nincorporate textual prompts via image captioning as auxiliary conditions,\nfostering further exploration of the diffusion model. Extensive experiments\nsubstantiate the efficacy of our framework across a diverse array of\nchallenging tasks and datasets, unveiling the compelling efficacy and\nefficiency of InstaRevive in delivering high-quality and visually appealing\nresults. Code is available at https://github.com/EternalEvan/InstaRevive."}
{"id": "2504.15604", "pdf": "https://arxiv.org/pdf/2504.15604", "abs": "https://arxiv.org/abs/2504.15604", "authors": ["Pavan Yadav", "Nikhil Khandalkar", "Krishna Shinde", "Lokesh B. Ramegowda", "Rajarshi Das"], "title": "Exploring Next Token Prediction in Theory of Mind (ToM) Tasks: Comparative Experiments with GPT-2 and LLaMA-2 AI Models", "categories": ["cs.CL", "cs.AI"], "comment": "75 pages, 60 figures", "summary": "Language models have made significant progress in generating coherent text\nand predicting next tokens based on input prompts. This study compares the\nnext-token prediction performance of two well-known models: OpenAI's GPT-2 and\nMeta's Llama-2-7b-chat-hf on Theory of Mind (ToM) tasks. To evaluate their\ncapabilities, we built a dataset from 10 short stories sourced from the Explore\nToM Dataset. We enhanced these stories by programmatically inserting additional\nsentences (infills) using GPT-4, creating variations that introduce different\nlevels of contextual complexity. This setup enables analysis of how increasing\ncontext affects model performance. We tested both models under four temperature\nsettings (0.01, 0.5, 1.0, 2.0) and evaluated their ability to predict the next\ntoken across three reasoning levels. Zero-order reasoning involves tracking the\nstate, either current (ground truth) or past (memory). First-order reasoning\nconcerns understanding another's mental state (e.g., \"Does Anne know the apple\nis salted?\"). Second-order reasoning adds recursion (e.g., \"Does Anne think\nthat Charles knows the apple is salted?\").\n  Our results show that adding more infill sentences slightly reduces\nprediction accuracy, as added context increases complexity and ambiguity.\nLlama-2 consistently outperforms GPT-2 in prediction accuracy, especially at\nlower temperatures, demonstrating greater confidence in selecting the most\nprobable token. As reasoning complexity rises, model responses diverge more.\nNotably, GPT-2 and Llama-2 display greater variability in predictions during\nfirst- and second-order reasoning tasks. These findings illustrate how model\narchitecture, temperature, and contextual complexity influence next-token\nprediction, contributing to a better understanding of the strengths and\nlimitations of current language models."}
{"id": "2504.15847", "pdf": "https://arxiv.org/pdf/2504.15847", "abs": "https://arxiv.org/abs/2504.15847", "authors": ["Xiang Liu", "Hau Chan", "Minming Li", "Xianlong Zeng", "Chenchen Fu", "Weiwei Wu"], "title": "CARE: Compatibility-Aware Incentive Mechanisms for Federated Learning with Budgeted Requesters", "categories": ["cs.AI"], "comment": null, "summary": "Federated learning (FL) is a promising approach that allows requesters (\\eg,\nservers) to obtain local training models from workers (e.g., clients). Since\nworkers are typically unwilling to provide training services/models freely and\nvoluntarily, many incentive mechanisms in FL are designed to incentivize\nparticipation by offering monetary rewards from requesters. However, existing\nstudies neglect two crucial aspects of real-world FL scenarios. First, workers\ncan possess inherent incompatibility characteristics (e.g., communication\nchannels and data sources), which can lead to degradation of FL efficiency\n(e.g., low communication efficiency and poor model generalization). Second, the\nrequesters are budgeted, which limits the amount of workers they can hire for\ntheir tasks. In this paper, we investigate the scenario in FL where multiple\nbudgeted requesters seek training services from incompatible workers with\nprivate training costs. We consider two settings: the cooperative budget\nsetting where requesters cooperate to pool their budgets to improve their\noverall utility and the non-cooperative budget setting where each requester\noptimizes their utility within their own budgets. To address efficiency\ndegradation caused by worker incompatibility, we develop novel\ncompatibility-aware incentive mechanisms, CARE-CO and CARE-NO, for both\nsettings to elicit true private costs and determine workers to hire for\nrequesters and their rewards while satisfying requester budget constraints. Our\nmechanisms guarantee individual rationality, truthfulness, budget feasibility,\nand approximation performance. We conduct extensive experiments using\nreal-world datasets to show that the proposed mechanisms significantly\noutperform existing baselines."}
{"id": "2504.15599", "pdf": "https://arxiv.org/pdf/2504.15599", "abs": "https://arxiv.org/abs/2504.15599", "authors": ["Shichen Li", "Chenhui Shao"], "title": "Multi-Modal Fusion of In-Situ Video Data and Process Parameters for Online Forecasting of Cookie Drying Readiness", "categories": ["cs.CV", "cs.LG"], "comment": "17 pages, 12 figures", "summary": "Food drying is essential for food production, extending shelf life, and\nreducing transportation costs. Accurate real-time forecasting of drying\nreadiness is crucial for minimizing energy consumption, improving productivity,\nand ensuring product quality. However, this remains challenging due to the\ndynamic nature of drying, limited data availability, and the lack of effective\npredictive analytical methods. To address this gap, we propose an end-to-end\nmulti-modal data fusion framework that integrates in-situ video data with\nprocess parameters for real-time food drying readiness forecasting. Our\napproach leverages a new encoder-decoder architecture with modality-specific\nencoders and a transformer-based decoder to effectively extract features while\npreserving the unique structure of each modality. We apply our approach to\nsugar cookie drying, where time-to-ready is predicted at each timestamp.\nExperimental results demonstrate that our model achieves an average prediction\nerror of only 15 seconds, outperforming state-of-the-art data fusion methods by\n65.69% and a video-only model by 11.30%. Additionally, our model balances\nprediction accuracy, model size, and computational efficiency, making it\nwell-suited for heterogenous industrial datasets. The proposed model is\nextensible to various other industrial modality fusion tasks for online\ndecision-making."}
{"id": "2504.15630", "pdf": "https://arxiv.org/pdf/2504.15630", "abs": "https://arxiv.org/abs/2504.15630", "authors": ["Xiaowei Yuan", "Zhao Yang", "Ziyang Huang", "Yequan Wang", "Siqi Fan", "Yiming Ju", "Jun Zhao", "Kang Liu"], "title": "Exploiting Contextual Knowledge in LLMs through V-usable Information based Layer Enhancement", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious tasks, yet they often struggle with context-faithfulness generations\nthat properly reflect contextual knowledge. While existing approaches focus on\nenhancing the decoding strategies, they ignore the fundamental mechanism of how\ncontextual information is processed within LLMs' internal states. As a result,\nLLMs remain limited in their ability to fully leverage contextual knowledge. In\nthis paper, we propose Context-aware Layer Enhancement (CaLE), a novel\nintervention method that enhances the utilization of contextual knowledge\nwithin LLMs' internal representations. By employing V-usable information\nanalysis, CaLE strategically amplifies the growth of contextual information at\nan optimal layer, thereby enriching representations in the final layer. Our\nexperiments demonstrate that CaLE effectively improves context-faithful\ngeneration in Question-Answering tasks, particularly in scenarios involving\nunknown or conflicting contextual knowledge."}
{"id": "2504.15903", "pdf": "https://arxiv.org/pdf/2504.15903", "abs": "https://arxiv.org/abs/2504.15903", "authors": ["Nikhil Khandalkar", "Pavan Yadav", "Krishna Shinde", "Lokesh B. Ramegowda", "Rajarshi Das"], "title": "Impact of Noise on LLM-Models Performance in Abstraction and Reasoning Corpus (ARC) Tasks with Model Temperature Considerations", "categories": ["cs.AI"], "comment": "60 pages, 25 figures", "summary": "Recent advancements in Large Language Models (LLMs) have generated growing\ninterest in their structured reasoning capabilities, particularly in tasks\ninvolving abstraction and pattern recognition. The Abstraction and Reasoning\nCorpus (ARC) benchmark plays a crucial role in evaluating these capabilities by\ntesting how well AI models generalize to novel problems. While GPT-4o\ndemonstrates strong performance by solving all ARC tasks under zero-noise\nconditions, other models like DeepSeek R1 and LLaMA 3.2 fail to solve any,\nsuggesting limitations in their ability to reason beyond simple pattern\nmatching. To explore this gap, we systematically evaluate these models across\ndifferent noise levels and temperature settings. Our results reveal that the\nintroduction of noise consistently impairs model performance, regardless of\narchitecture. This decline highlights a shared vulnerability: current LLMs,\ndespite showing signs of abstract reasoning, remain highly sensitive to input\nperturbations. Such fragility raises concerns about their real-world\napplicability, where noise and uncertainty are common. By comparing how\ndifferent model architectures respond to these challenges, we offer insights\ninto the structural weaknesses of modern LLMs in reasoning tasks. This work\nunderscores the need for developing more robust and adaptable AI systems\ncapable of handling the ambiguity and variability inherent in real-world\nscenarios. Our findings aim to guide future research toward enhancing model\ngeneralization, robustness, and alignment with human-like cognitive\nflexibility."}
{"id": "2504.15609", "pdf": "https://arxiv.org/pdf/2504.15609", "abs": "https://arxiv.org/abs/2504.15609", "authors": ["Yunfeng Li", "Bo Wang", "Jiahao Wan", "Xueyi Wu", "Ye Li"], "title": "SonarT165: A Large-scale Benchmark and STFTrack Framework for Acoustic Object Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Underwater observation systems typically integrate optical cameras and\nimaging sonar systems. When underwater visibility is insufficient, only sonar\nsystems can provide stable data, which necessitates exploration of the\nunderwater acoustic object tracking (UAOT) task. Previous studies have explored\ntraditional methods and Siamese networks for UAOT. However, the absence of a\nunified evaluation benchmark has significantly constrained the value of these\nmethods. To alleviate this limitation, we propose the first large-scale UAOT\nbenchmark, SonarT165, comprising 165 square sequences, 165 fan sequences, and\n205K high-quality annotations. Experimental results demonstrate that SonarT165\nreveals limitations in current state-of-the-art SOT trackers. To address these\nlimitations, we propose STFTrack, an efficient framework for acoustic object\ntracking. It includes two novel modules, a multi-view template fusion module\n(MTFM) and an optimal trajectory correction module (OTCM). The MTFM module\nintegrates multi-view feature of both the original image and the binary image\nof the dynamic template, and introduces a cross-attention-like layer to fuse\nthe spatio-temporal target representations. The OTCM module introduces the\nacoustic-response-equivalent pixel property and proposes normalized pixel\nbrightness response scores, thereby suppressing suboptimal matches caused by\ninaccurate Kalman filter prediction boxes. To further improve the model\nfeature, STFTrack introduces a acoustic image enhancement method and a\nFrequency Enhancement Module (FEM) into its tracking pipeline. Comprehensive\nexperiments show the proposed STFTrack achieves state-of-the-art performance on\nthe proposed benchmark. The code is available at\nhttps://github.com/LiYunfengLYF/SonarT165."}
{"id": "2504.15640", "pdf": "https://arxiv.org/pdf/2504.15640", "abs": "https://arxiv.org/abs/2504.15640", "authors": ["Hongtao Wang", "Taiyan Zhang", "Renchi Yang", "Jianliang Xu"], "title": "Cost-Effective Text Clustering with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Text clustering aims to automatically partition a collection of text\ndocuments into distinct clusters based on linguistic features. In the\nliterature, this task is usually framed as metric clustering based on text\nembeddings from pre-trained encoders or a graph clustering problem upon\npairwise similarities from an oracle, e.g., a large ML model. Recently, large\nlanguage models (LLMs) bring significant advancement in this field by offering\ncontextualized text embeddings and highly accurate similarity scores, but\nmeanwhile, present grand challenges to cope with substantial computational\nand/or financial overhead caused by numerous API-based queries or inference\ncalls to the models.\n  In response, this paper proposes TECL, a cost-effective framework that taps\ninto the feedback from LLMs for accurate text clustering within a limited\nbudget of queries to LLMs. Under the hood, TECL adopts our EdgeLLM or\nTriangleLLM to construct must-link/cannot-link constraints for text pairs, and\nfurther leverages such constraints as supervision signals input to our weighted\nconstrained clustering approach to generate clusters. Particularly, EdgeLLM\n(resp. TriangleLLM) enables the identification of informative text pairs (resp.\ntriplets) for querying LLMs via well-thought-out greedy algorithms and accurate\nextraction of pairwise constraints through carefully-crafted prompting\ntechniques. Our experiments on multiple benchmark datasets exhibit that TECL\nconsistently and considerably outperforms existing solutions in unsupervised\ntext clustering under the same query cost for LLMs."}
{"id": "2504.16042", "pdf": "https://arxiv.org/pdf/2504.16042", "abs": "https://arxiv.org/abs/2504.16042", "authors": ["Ismaïl Baaj"], "title": "Approximate matrices of systems of max-min fuzzy relational equations", "categories": ["cs.AI", "cs.LO"], "comment": null, "summary": "In this article, we address the inconsistency of a system of max-min fuzzy\nrelational equations by minimally modifying the matrix governing the system in\norder to achieve consistency. Our method yields consistent systems that\napproximate the original inconsistent system in the following sense: the\nright-hand side vector of each consistent system is that of the inconsistent\nsystem, and the coefficients of the matrix governing each consistent system are\nobtained by modifying, exactly and minimally, the entries of the original\nmatrix that must be corrected to achieve consistency, while leaving all other\nentries unchanged.\n  To obtain a consistent system that closely approximates the considered\ninconsistent system, we study the distance (in terms of a norm among $L_1$,\n$L_2$ or $L_\\infty$) between the matrix of the inconsistent system and the set\nformed by the matrices of consistent systems that use the same right-hand side\nvector as the inconsistent system. We show that our method allows us to\ndirectly compute matrices of consistent systems that use the same right-hand\nside vector as the inconsistent system whose distance in terms of $L_\\infty$\nnorm to the matrix of the inconsistent system is minimal (the computational\ncosts are higher when using $L_1$ norm or $L_2$ norm). We also give an explicit\nanalytical formula for computing this minimal $L_\\infty$ distance. Finally, we\ntranslate our results for systems of min-max fuzzy relational equations and\npresent some potential applications."}
{"id": "2504.15612", "pdf": "https://arxiv.org/pdf/2504.15612", "abs": "https://arxiv.org/abs/2504.15612", "authors": ["Hongxing Peng", "Kang Lin", "Huanai Liu"], "title": "HS-Mamba: Full-Field Interaction Multi-Groups Mamba for Hyperspectral Image Classification", "categories": ["cs.CV"], "comment": null, "summary": "Hyperspectral image (HSI) classification has been one of the hot topics in\nremote sensing fields. Recently, the Mamba architecture based on selective\nstate-space models (S6) has demonstrated great advantages in long sequence\nmodeling. However, the unique properties of hyperspectral data, such as high\ndimensionality and feature inlining, pose challenges to the application of\nMamba to HSI classification. To compensate for these shortcomings, we propose\nan full-field interaction multi-groups Mamba framework (HS-Mamba), which adopts\na strategy different from pixel-patch based or whole-image based, but combines\nthe advantages of both. The patches cut from the whole image are sent to\nmulti-groups Mamba, combined with positional information to perceive local\ninline features in the spatial and spectral domains, and the whole image is\nsent to a lightweight attention module to enhance the global feature\nrepresentation ability. Specifically, HS-Mamba consists of a dual-channel\nspatial-spectral encoder (DCSS-encoder) module and a lightweight global inline\nattention (LGI-Att) branch. The DCSS-encoder module uses multiple groups of\nMamba to decouple and model the local features of dual-channel sequences with\nnon-overlapping patches. The LGI-Att branch uses a lightweight compressed and\nextended attention module to perceive the global features of the spatial and\nspectral domains of the unsegmented whole image. By fusing local and global\nfeatures, high-precision classification of hyperspectral images is achieved.\nExtensive experiments demonstrate the superiority of the proposed HS-Mamba,\noutperforming state-of-the-art methods on four benchmark HSI datasets."}
{"id": "2504.15642", "pdf": "https://arxiv.org/pdf/2504.15642", "abs": "https://arxiv.org/abs/2504.15642", "authors": ["Gerhard Jäger"], "title": "Computational Typology", "categories": ["cs.CL", "q-bio.PE"], "comment": "19 pages, s5 figure", "summary": "Typology is a subfield of linguistics that focuses on the study and\nclassification of languages based on their structural features. Unlike\ngenealogical classification, which examines the historical relationships\nbetween languages, typology seeks to understand the diversity of human\nlanguages by identifying common properties and patterns, known as universals.\nIn recent years, computational methods have played an increasingly important\nrole in typological research, enabling the analysis of large-scale linguistic\ndata and the testing of hypotheses about language structure and evolution. This\narticle provides an illustration of the benefits of computational statistical\nmodeling in typology."}
{"id": "2504.15286", "pdf": "https://arxiv.org/pdf/2504.15286", "abs": "https://arxiv.org/abs/2504.15286", "authors": ["Daniele Gorla", "Shivam Kumar", "Pietro Nicolaus Roselli Lorenzini", "Alireza Alipourfaz"], "title": "CUBETESTERAI: Automated JUnit Test Generation using the LLaMA Model", "categories": ["cs.SE", "cs.AI"], "comment": "Accepted to ICST 2025 Industry Track", "summary": "This paper presents an approach to automating JUnit test generation for Java\napplications using the Spring Boot framework, leveraging the LLaMA (Large\nLanguage Model Architecture) model to enhance the efficiency and accuracy of\nthe testing process. The resulting tool, called CUBETESTERAI, includes a\nuser-friendly web interface and the integration of a CI/CD pipeline using\nGitLab and Docker. These components streamline the automated test generation\nprocess, allowing developers to generate JUnit tests directly from their code\nsnippets with minimal manual intervention. The final implementation executes\nthe LLaMA models through RunPod, an online GPU service, which also enhances the\nprivacy of our tool. Using the advanced natural language processing\ncapabilities of the LLaMA model, CUBETESTERAI is able to generate test cases\nthat provide high code coverage and accurate validation of software\nfunctionalities in Java-based Spring Boot applications. Furthermore, it\nefficiently manages resource-intensive operations and refines the generated\ntests to address common issues like missing imports and handling of private\nmethods. By comparing CUBETESTERAI with some state-of-the-art tools, we show\nthat our proposal consistently demonstrates competitive and, in many cases,\nbetter performance in terms of code coverage in different real-life Java\nprograms."}
{"id": "2504.15619", "pdf": "https://arxiv.org/pdf/2504.15619", "abs": "https://arxiv.org/abs/2504.15619", "authors": ["Jinda Lu", "Jinghan Li", "Yuan Gao", "Junkang Wu", "Jiancan Wu", "Xiang Wang", "Xiangnan He"], "title": "AdaViP: Aligning Multi-modal LLMs via Adaptive Vision-enhanced Preference Optimization", "categories": ["cs.CV"], "comment": null, "summary": "Preference alignment through Direct Preference Optimization (DPO) has\ndemonstrated significant effectiveness in aligning multimodal large language\nmodels (MLLMs) with human preferences. However, existing methods focus\nprimarily on language preferences while neglecting the critical visual context.\nIn this paper, we propose an Adaptive Vision-enhanced Preference optimization\n(AdaViP) that addresses these limitations through two key innovations: (1)\nvision-based preference pair construction, which integrates multiple visual\nfoundation models to strategically remove key visual elements from the image,\nenhancing MLLMs' sensitivity to visual details; and (2) adaptive preference\noptimization that dynamically balances vision- and language-based preferences\nfor more accurate alignment. Extensive evaluations across different benchmarks\ndemonstrate our effectiveness. Notably, our AdaViP-7B achieves 93.7% and 96.4%\nreductions in response-level and mentioned-level hallucination respectively on\nthe Object HalBench, significantly outperforming current state-of-the-art\nmethods."}
{"id": "2504.15683", "pdf": "https://arxiv.org/pdf/2504.15683", "abs": "https://arxiv.org/abs/2504.15683", "authors": ["Simon Jehnen", "Joaquín Ordieres-Meré", "Javier Villalba-Díez"], "title": "FinTextSim: Enhancing Financial Text Analysis with BERTopic", "categories": ["cs.CL", "cs.LG", "econ.GN", "q-fin.EC", "q-fin.GN", "68T50", "I.2.7; I.5.1; J.4"], "comment": null, "summary": "Recent advancements in information availability and computational\ncapabilities have transformed the analysis of annual reports, integrating\ntraditional financial metrics with insights from textual data. To extract\nvaluable insights from this wealth of textual data, automated review processes,\nsuch as topic modeling, are crucial. This study examines the effectiveness of\nBERTopic, a state-of-the-art topic model relying on contextual embeddings, for\nanalyzing Item 7 and Item 7A of 10-K filings from S&P 500 companies\n(2016-2022). Moreover, we introduce FinTextSim, a finetuned\nsentence-transformer model optimized for clustering and semantic search in\nfinancial contexts. Compared to all-MiniLM-L6-v2, the most widely used\nsentence-transformer, FinTextSim increases intratopic similarity by 81% and\nreduces intertopic similarity by 100%, significantly enhancing organizational\nclarity. We assess BERTopic's performance using embeddings from both FinTextSim\nand all-MiniLM-L6-v2. Our findings reveal that BERTopic only forms clear and\ndistinct economic topic clusters when paired with FinTextSim's embeddings.\nWithout FinTextSim, BERTopic struggles with misclassification and overlapping\ntopics. Thus, FinTextSim is pivotal for advancing financial text analysis.\nFinTextSim's enhanced contextual embeddings, tailored for the financial domain,\nelevate the quality of future research and financial information. This improved\nquality of financial information will enable stakeholders to gain a competitive\nadvantage, streamlining resource allocation and decision-making processes.\nMoreover, the improved insights have the potential to leverage business\nvaluation and stock price prediction models."}
{"id": "2504.15296", "pdf": "https://arxiv.org/pdf/2504.15296", "abs": "https://arxiv.org/abs/2504.15296", "authors": ["Yihong Jin", "Ze Yang"], "title": "Scalability Optimization in Cloud-Based AI Inference Services: Strategies for Real-Time Load Balancing and Automated Scaling", "categories": ["cs.DC", "cs.AI", "F.2.2; I.2.8"], "comment": "Accepted to BDICN 2025", "summary": "The rapid expansion of AI inference services in the cloud necessitates a\nrobust scalability solution to manage dynamic workloads and maintain high\nperformance. This study proposes a comprehensive scalability optimization\nframework for cloud AI inference services, focusing on real-time load balancing\nand autoscaling strategies. The proposed model is a hybrid approach that\ncombines reinforcement learning for adaptive load distribution and deep neural\nnetworks for accurate demand forecasting. This multi-layered approach enables\nthe system to anticipate workload fluctuations and proactively adjust\nresources, ensuring maximum resource utilisation and minimising latency.\nFurthermore, the incorporation of a decentralised decision-making process\nwithin the model serves to enhance fault tolerance and reduce response time in\nscaling operations. Experimental results demonstrate that the proposed model\nenhances load balancing efficiency by 35\\ and reduces response delay by 28\\,\nthereby exhibiting a substantial optimization effect in comparison with\nconventional scalability solutions."}
{"id": "2504.15624", "pdf": "https://arxiv.org/pdf/2504.15624", "abs": "https://arxiv.org/abs/2504.15624", "authors": ["Jingzhi Li", "Changjiang Luo", "Ruoyu Chen", "Hua Zhang", "Wenqi Ren", "Jianhou Gan", "Xiaochun Cao"], "title": "FaceInsight: A Multimodal Large Language Model for Face Perception", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in multimodal large language models (MLLMs) have demonstrated\nstrong capabilities in understanding general visual content. However, these\ngeneral-domain MLLMs perform poorly in face perception tasks, often producing\ninaccurate or misleading responses to face-specific queries. To address this\ngap, we propose FaceInsight, the versatile face perception MLLM that provides\nfine-grained facial information. Our approach introduces visual-textual\nalignment of facial knowledge to model both uncertain dependencies and\ndeterministic relationships among facial information, mitigating the\nlimitations of language-driven reasoning. Additionally, we incorporate face\nsegmentation maps as an auxiliary perceptual modality, enriching the visual\ninput with localized structural cues to enhance semantic understanding.\nComprehensive experiments and analyses across three face perception tasks\ndemonstrate that FaceInsight consistently outperforms nine compared MLLMs under\nboth training-free and fine-tuned settings."}
{"id": "2504.15688", "pdf": "https://arxiv.org/pdf/2504.15688", "abs": "https://arxiv.org/abs/2504.15688", "authors": ["Mandy Cartner", "Matthew Kogan", "Nikolas Webster", "Matthew Wagers", "Ivy Sichel"], "title": "Subject islands do not reduce to construction-specific discourse function", "categories": ["cs.CL"], "comment": null, "summary": "The term islands in linguistics refers to phrases from which extracting an\nelement results in ungrammaticality (Ross, 1967). Grammatical subjects are\nconsidered islands because extracting a sub-part of a subject results in an\nill-formed sentence, despite having a clear intended meaning (e.g., \"Which\ntopic did the article about inspire you?\"). The generative tradition, which\nviews syntax as autonomous of meaning and function, attributes this\nungrammaticality to the abstract movement dependency between the wh-phrase and\nthe subject-internal position with which it is associated for interpretation.\nHowever, research on language that emphasizes its communicative function\nsuggests instead that syntactic constraints, including islands, can be\nexplained based on the way different constructions package information.\nAccordingly, Abeill\\'e et al. (2020) suggest that the islandhood of subjects is\nspecific to the information structure of wh-questions, and propose that\nsubjects are not islands for movement, but for focusing, due to their\ndiscourse-backgroundedness. This predicts that other constructions that differ\nin their information structure from wh-questions, but still involve movement,\nshould not create a subject island effect. We test this prediction in three\nlarge-scale acceptability studies, using a super-additive design that singles\nout subject island violations, in three different constructions: wh-questions,\nrelative clauses, and topicalization. We report evidence for a subject island\neffect in each construction type, despite only wh-questions introducing what\nAbeill\\'e et al. (2020) call \"a clash in information structure.\" We argue that\nthis motivates an account of islands in terms of abstract, syntactic\nrepresentations, independent of the communicative function associated with the\nconstructions."}
{"id": "2504.15299", "pdf": "https://arxiv.org/pdf/2504.15299", "abs": "https://arxiv.org/abs/2504.15299", "authors": ["Haodong Wang", "Qihua Zhou", "Zicong Hong", "Song Guo"], "title": "D$^{2}$MoE: Dual Routing and Dynamic Scheduling for Efficient On-Device MoE-based LLM Serving", "categories": ["cs.DC", "cs.AI"], "comment": "Accepted by MobiCom 2025", "summary": "The mixture of experts (MoE) model is a sparse variant of large language\nmodels (LLMs), designed to hold a better balance between intelligent capability\nand computational overhead. Despite its benefits, MoE is still too expensive to\ndeploy on resource-constrained edge devices, especially with the demands of\non-device inference services. Recent research efforts often apply model\ncompression techniques, such as quantization, pruning and merging, to restrict\nMoE complexity. Unfortunately, due to their predefined static model\noptimization strategies, they cannot always achieve the desired\nquality-overhead trade-off when handling multiple requests, finally degrading\nthe on-device quality of service. These limitations motivate us to propose the\nD$^2$MoE, an algorithm-system co-design framework that matches diverse task\nrequirements by dynamically allocating the most proper bit-width to each\nexpert. Specifically, inspired by the nested structure of matryoshka dolls, we\npropose the matryoshka weight quantization (MWQ) to progressively compress\nexpert weights in a bit-nested manner and reduce the required runtime memory.\nOn top of it, we further optimize the I/O-computation pipeline and design a\nheuristic scheduling algorithm following our hottest-expert-bit-first (HEBF)\nprinciple, which maximizes the expert parallelism between I/O and computation\nqueue under constrained memory budgets, thus significantly reducing the idle\ntemporal bubbles waiting for the experts to load. Evaluations on real edge\ndevices show that D$^2$MoE improves the overall inference throughput by up to\n1.39$\\times$ and reduces the peak memory footprint by up to 53% over the latest\non-device inference frameworks, while still preserving comparable serving\naccuracy as its INT8 counterparts."}
{"id": "2504.15627", "pdf": "https://arxiv.org/pdf/2504.15627", "abs": "https://arxiv.org/abs/2504.15627", "authors": ["Doanh C. Bui", "Hoai Luan Pham", "Vu Trung Duong Le", "Tuan Hai Vu", "Van Duy Tran", "Yasuhiko Nakashima"], "title": "ZeroSlide: Is Zero-Shot Classification Adequate for Lifelong Learning in Whole-Slide Image Analysis in the Era of Pathology Vision-Language Foundation Models?", "categories": ["cs.CV"], "comment": "10 pages, 3 figures, 1 table, conference submission", "summary": "Lifelong learning for whole slide images (WSIs) poses the challenge of\ntraining a unified model to perform multiple WSI-related tasks, such as cancer\nsubtyping and tumor classification, in a distributed, continual fashion. This\nis a practical and applicable problem in clinics and hospitals, as WSIs are\nlarge, require storage, processing, and transfer time. Training new models\nwhenever new tasks are defined is time-consuming. Recent work has applied\nregularization- and rehearsal-based methods to this setting. However, the rise\nof vision-language foundation models that align diagnostic text with pathology\nimages raises the question: are these models alone sufficient for lifelong WSI\nlearning using zero-shot classification, or is further investigation into\ncontinual learning strategies needed to improve performance? To our knowledge,\nthis is the first study to compare conventional continual-learning approaches\nwith vision-language zero-shot classification for WSIs. Our source code and\nexperimental results will be available soon."}
{"id": "2504.15777", "pdf": "https://arxiv.org/pdf/2504.15777", "abs": "https://arxiv.org/abs/2504.15777", "authors": ["Shangshang Wang", "Julian Asilis", "Ömer Faruk Akgül", "Enes Burak Bilgin", "Ollie Liu", "Willie Neiswanger"], "title": "Tina: Tiny Reasoning Models via LoRA", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "How cost-effectively can strong reasoning abilities be achieved in language\nmodels? Driven by this fundamental question, we present Tina, a family of tiny\nreasoning models achieved with high cost-efficiency. Notably, Tina demonstrates\nthat substantial reasoning performance can be developed using only minimal\nresources, by applying parameter-efficient updates during reinforcement\nlearning (RL), using low-rank adaptation (LoRA), to an already tiny 1.5B\nparameter base model. This minimalist approach produces models that achieve\nreasoning performance which is competitive with, and sometimes surpasses, SOTA\nRL reasoning models built upon the same base model. Crucially, this is achieved\nat a tiny fraction of the computational post-training cost employed by existing\nSOTA models. In fact, the best Tina model achieves a >20\\% reasoning\nperformance increase and 43.33\\% Pass@1 accuracy on AIME24, at only \\$9 USD\npost-training and evaluation cost (i.e., an estimated 260x cost reduction). Our\nwork reveals the surprising effectiveness of efficient RL reasoning via LoRA.\nWe validate this across multiple open-source reasoning datasets and various\nablation settings starting with a single, fixed set of hyperparameters.\nFurthermore, we hypothesize that this effectiveness and efficiency stem from\nLoRA rapidly adapting the model to the structural format of reasoning rewarded\nby RL, while largely preserving the base model's underlying knowledge. In\nservice of accessibility and open research, we fully open-source all code,\ntraining logs, and model weights \\& checkpoints."}
{"id": "2504.15301", "pdf": "https://arxiv.org/pdf/2504.15301", "abs": "https://arxiv.org/abs/2504.15301", "authors": ["Zoi Lygizou", "Dimitris Kalles"], "title": "A biologically Inspired Trust Model for Open Multi-Agent Systems that is Resilient to Rapid Performance Fluctuations", "categories": ["cs.MA", "cs.AI", "cs.DC"], "comment": null, "summary": "Trust management provides an alternative solution for securing open, dynamic,\nand distributed multi-agent systems, where conventional cryptographic methods\nprove to be impractical. However, existing trust models face challenges related\nto agent mobility, changing behaviors, and the cold start problem. To address\nthese issues we introduced a biologically inspired trust model in which\ntrustees assess their own capabilities and store trust data locally. This\ndesign improves mobility support, reduces communication overhead, resists\ndisinformation, and preserves privacy. Despite these advantages, prior\nevaluations revealed limitations of our model in adapting to provider\npopulation changes and continuous performance fluctuations. This study proposes\na novel algorithm, incorporating a self-classification mechanism for providers\nto detect performance drops potentially harmful for the service consumers.\nSimulation results demonstrate that the new algorithm outperforms its original\nversion and FIRE, a well-known trust and reputation model, particularly in\nhandling dynamic trustee behavior. While FIRE remains competitive under extreme\nenvironmental changes, the proposed algorithm demonstrates greater adaptability\nacross various conditions. In contrast to existing trust modeling research,\nthis study conducts a comprehensive evaluation of our model using widely\nrecognized trust model criteria, assessing its resilience against common\ntrust-related attacks while identifying strengths, weaknesses, and potential\ncountermeasures. Finally, several key directions for future research are\nproposed."}
{"id": "2504.15650", "pdf": "https://arxiv.org/pdf/2504.15650", "abs": "https://arxiv.org/abs/2504.15650", "authors": ["Dengyang Jiang", "Mengmeng Wang", "Teli Ma", "Hengzhuang Li", "Yong liu", "Guang Dai", "Lei Zhang"], "title": "AffordanceSAM: Segment Anything Once More in Affordance Grounding", "categories": ["cs.CV"], "comment": "SAM Meets Affordance Grounding", "summary": "Improving the generalization ability of an affordance grounding model to\nrecognize regions for unseen objects and affordance functions is crucial for\nreal-world application. However, current models are still far away from such\nstandards. To address this problem, we introduce AffordanceSAM, an effective\napproach that extends SAM's generalization capacity to the domain of affordance\ngrounding. For the purpose of thoroughly transferring SAM's robust performance\nin segmentation to affordance, we initially propose an affordance-adaption\nmodule in order to help modify SAM's segmentation output to be adapted to the\nspecific functional regions required for affordance grounding. We concurrently\nmake a coarse-to-fine training recipe to make SAM first be aware of affordance\nobjects and actions coarsely, and then be able to generate affordance heatmaps\nfinely. Both quantitative and qualitative experiments show the strong\ngeneralization capacity of our AffordanceSAM, which not only surpasses previous\nmethods under AGD20K benchmark but also shows evidence to handle the task with\nnovel objects and affordance functions."}
{"id": "2504.15784", "pdf": "https://arxiv.org/pdf/2504.15784", "abs": "https://arxiv.org/abs/2504.15784", "authors": ["Ruizhe Li", "Chiwei Zhu", "Benfeng Xu", "Xiaorui Wang", "Zhendong Mao"], "title": "Automated Creativity Evaluation for Large Language Models: A Reference-Based Approach", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Creative writing is a key capability of Large Language Models (LLMs), with\npotential applications in literature, storytelling, and various creative\ndomains. However, evaluating the creativity of machine-generated texts remains\na significant challenge, as existing methods either rely on costly manual\nannotations or fail to align closely with human assessments. In this paper, we\npropose an effective automated evaluation method based on the Torrance Test of\nCreative Writing (TTCW), which evaluates creativity as product. Our method\nemploys a reference-based Likert-style approach, scoring generated creative\ntexts relative to high-quality reference texts across various tests.\nExperimental results demonstrate that our method significantly improves the\nalignment between LLM evaluations and human assessments, achieving a pairwise\naccuracy of 0.75 (+15\\%)."}
{"id": "2504.15303", "pdf": "https://arxiv.org/pdf/2504.15303", "abs": "https://arxiv.org/abs/2504.15303", "authors": ["Yi Xiong", "Jinqi Huang", "Wenjie Huang", "Xuebing Yu", "Entong Li", "Zhixiong Ning", "Jinhua Zhou", "Li Zeng", "Xin Chen"], "title": "High-Throughput LLM inference on Heterogeneous Clusters", "categories": ["cs.DC", "cs.AI"], "comment": null, "summary": "Nowadays, many companies possess various types of AI accelerators, forming\nheterogeneous clusters. Efficiently leveraging these clusters for\nhigh-throughput large language model (LLM) inference services can significantly\nreduce costs and expedite task processing. However, LLM inference on\nheterogeneous clusters presents two main challenges. Firstly, different\ndeployment configurations can result in vastly different performance. The\nnumber of possible configurations is large, and evaluating the effectiveness of\na specific setup is complex. Thus, finding an optimal configuration is not an\neasy task. Secondly, LLM inference instances within a heterogeneous cluster\npossess varying processing capacities, leading to different processing speeds\nfor handling inference requests. Evaluating these capacities and designing a\nrequest scheduling algorithm that fully maximizes the potential of each\ninstance is challenging. In this paper, we propose a high-throughput inference\nservice system on heterogeneous clusters. First, the deployment configuration\nis optimized by modeling the resource amount and expected throughput and using\nthe exhaustive search method. Second, a novel mechanism is proposed to schedule\nrequests among instances, which fully considers the different processing\ncapabilities of various instances. Extensive experiments show that the proposed\nscheduler improves throughput by 122.5% and 33.6% on two heterogeneous\nclusters, respectively."}
{"id": "2504.15661", "pdf": "https://arxiv.org/pdf/2504.15661", "abs": "https://arxiv.org/abs/2504.15661", "authors": ["Xian Wu", "Chang Liu"], "title": "DiTPainter: Efficient Video Inpainting with Diffusion Transformers", "categories": ["cs.CV"], "comment": null, "summary": "Many existing video inpainting algorithms utilize optical flows to construct\nthe corresponding maps and then propagate pixels from adjacent frames to\nmissing areas by mapping. Despite the effectiveness of the propagation\nmechanism, they might encounter blurry and inconsistencies when dealing with\ninaccurate optical flows or large masks. Recently, Diffusion Transformer (DiT)\nhas emerged as a revolutionary technique for video generation tasks. However,\npretrained DiT models for video generation all contain a large amount of\nparameters, which makes it very time consuming to apply to video inpainting\ntasks. In this paper, we present DiTPainter, an end-to-end video inpainting\nmodel based on Diffusion Transformer (DiT). DiTPainter uses an efficient\ntransformer network designed for video inpainting, which is trained from\nscratch instead of initializing from any large pretrained models. DiTPainter\ncan address videos with arbitrary lengths and can be applied to video\ndecaptioning and video completion tasks with an acceptable time cost.\nExperiments show that DiTPainter outperforms existing video inpainting\nalgorithms with higher quality and better spatial-temporal consistency."}
{"id": "2504.15801", "pdf": "https://arxiv.org/pdf/2504.15801", "abs": "https://arxiv.org/abs/2504.15801", "authors": ["Valeria Lerman", "Yaniv Dover"], "title": "A closer look at how large language models trust humans: patterns and biases", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "As large language models (LLMs) and LLM-based agents increasingly interact\nwith humans in decision-making contexts, understanding the trust dynamics\nbetween humans and AI agents becomes a central concern. While considerable\nliterature studies how humans trust AI agents, it is much less understood how\nLLM-based agents develop effective trust in humans. LLM-based agents likely\nrely on some sort of implicit effective trust in trust-related contexts (e.g.,\nevaluating individual loan applications) to assist and affect decision making.\nUsing established behavioral theories, we develop an approach that studies\nwhether LLMs trust depends on the three major trustworthiness dimensions:\ncompetence, benevolence and integrity of the human subject. We also study how\ndemographic variables affect effective trust. Across 43,200 simulated\nexperiments, for five popular language models, across five different scenarios\nwe find that LLM trust development shows an overall similarity to human trust\ndevelopment. We find that in most, but not all cases, LLM trust is strongly\npredicted by trustworthiness, and in some cases also biased by age, religion\nand gender, especially in financial scenarios. This is particularly true for\nscenarios common in the literature and for newer models. While the overall\npatterns align with human-like mechanisms of effective trust formation,\ndifferent models exhibit variation in how they estimate trust; in some cases,\ntrustworthiness and demographic factors are weak predictors of effective trust.\nThese findings call for a better understanding of AI-to-human trust dynamics\nand monitoring of biases and trust development patterns to prevent unintended\nand potentially harmful outcomes in trust-sensitive applications of AI."}
{"id": "2504.15310", "pdf": "https://arxiv.org/pdf/2504.15310", "abs": "https://arxiv.org/abs/2504.15310", "authors": ["Syeda Tahreem Zahra", "Syed Kashif Imdad", "Sohail Khan", "Sohail Khalid", "Nauman Anwar Baig"], "title": "Power Transformer Health Index and Life Span Assessment: A Comprehensive Review of Conventional and Machine Learning based Approaches", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Power transformers play a critical role within the electrical power system,\nmaking their health assessment and the prediction of their remaining lifespan\nparamount for the purpose of ensuring efficient operation and facilitating\neffective maintenance planning. This paper undertakes a comprehensive\nexamination of existent literature, with a primary focus on both conventional\nand cutting-edge techniques employed within this domain. The merits and\ndemerits of recent methodologies and techniques are subjected to meticulous\nscrutiny and explication. Furthermore, this paper expounds upon intelligent\nfault diagnosis methodologies and delves into the most widely utilized\nintelligent algorithms for the assessment of transformer conditions. Diverse\nArtificial Intelligence (AI) approaches, including Artificial Neural Networks\n(ANN) and Convolutional Neural Network (CNN), Support Vector Machine (SVM),\nRandom Forest (RF), Genetic Algorithm (GA), and Particle Swarm Optimization\n(PSO), are elucidated offering pragmatic solutions for enhancing the\nperformance of transformer fault diagnosis. The amalgamation of multiple AI\nmethodologies and the exploration of timeseries analysis further contribute to\nthe augmentation of diagnostic precision and the early detection of faults in\ntransformers. By furnishing a comprehensive panorama of AI applications in the\nfield of transformer fault diagnosis, this study lays the groundwork for future\nresearch endeavors and the progression of this critical area of study."}
{"id": "2504.15665", "pdf": "https://arxiv.org/pdf/2504.15665", "abs": "https://arxiv.org/abs/2504.15665", "authors": ["Pei Liu", "Yisi Luo", "Wenzhen Wang", "Xiangyong Cao"], "title": "Motion-Enhanced Nonlocal Similarity Implicit Neural Representation for Infrared Dim and Small Target Detection", "categories": ["cs.CV"], "comment": null, "summary": "Infrared dim and small target detection presents a significant challenge due\nto dynamic multi-frame scenarios and weak target signatures in the infrared\nmodality. Traditional low-rank plus sparse models often fail to capture dynamic\nbackgrounds and global spatial-temporal correlations, which results in\nbackground leakage or target loss. In this paper, we propose a novel\nmotion-enhanced nonlocal similarity implicit neural representation (INR)\nframework to address these challenges. We first integrate motion estimation via\noptical flow to capture subtle target movements, and propose multi-frame fusion\nto enhance motion saliency. Second, we leverage nonlocal similarity to\nconstruct patch tensors with strong low-rank properties, and propose an\ninnovative tensor decomposition-based INR model to represent the nonlocal patch\ntensor, effectively encoding both the nonlocal low-rankness and\nspatial-temporal correlations of background through continuous neural\nrepresentations. An alternating direction method of multipliers is developed\nfor the nonlocal INR model, which enjoys theoretical fixed-point convergence.\nExperimental results show that our approach robustly separates dim targets from\ncomplex infrared backgrounds, outperforming state-of-the-art methods in\ndetection accuracy and robustness."}
{"id": "2504.15815", "pdf": "https://arxiv.org/pdf/2504.15815", "abs": "https://arxiv.org/abs/2504.15815", "authors": ["Michael A. Hedderich", "Anyi Wang", "Raoyuan Zhao", "Florian Eichin", "Barbara Plank"], "title": "What's the Difference? Supporting Users in Identifying the Effects of Prompt and Model Changes Through Token Patterns", "categories": ["cs.CL", "cs.HC", "cs.LG"], "comment": null, "summary": "Prompt engineering for large language models is challenging, as even small\nprompt perturbations or model changes can significantly impact the generated\noutput texts. Existing evaluation methods, either automated metrics or human\nevaluation, have limitations, such as providing limited insights or being\nlabor-intensive. We propose Spotlight, a new approach that combines both\nautomation and human analysis. Based on data mining techniques, we\nautomatically distinguish between random (decoding) variations and systematic\ndifferences in language model outputs. This process provides token patterns\nthat describe the systematic differences and guide the user in manually\nanalyzing the effects of their prompt and model changes efficiently. We create\nthree benchmarks to quantitatively test the reliability of token pattern\nextraction methods and demonstrate that our approach provides new insights into\nestablished prompt data. From a human-centric perspective, through\ndemonstration studies and a user study, we show that our token pattern approach\nhelps users understand the systematic differences of language model outputs,\nand we are able to discover relevant differences caused by prompt and model\nchanges (e.g. related to gender or culture), thus supporting the prompt\nengineering process and human-centric model behavior research."}
{"id": "2504.15311", "pdf": "https://arxiv.org/pdf/2504.15311", "abs": "https://arxiv.org/abs/2504.15311", "authors": ["Fei Shang", "Haohua Du", "Dawei Yan", "Panlong Yang", "Xiang-Yang Li"], "title": "RINN: One Sample Radio Frequency Imaging based on Physics Informed Neural Network", "categories": ["eess.IV", "cs.AI"], "comment": null, "summary": "Due to its ability to work in non-line-of-sight and low-light environments,\nradio frequency (RF) imaging technology is expected to bring new possibilities\nfor embodied intelligence and multimodal sensing. However, widely used RF\ndevices (such as Wi-Fi) often struggle to provide high-precision\nelectromagnetic measurements and large-scale datasets, hindering the\napplication of RF imaging technology. In this paper, we combine the ideas of\nPINN to design the RINN network, using physical constraints instead of true\nvalue comparison constraints and adapting it with the characteristics of\nubiquitous RF signals, allowing the RINN network to achieve RF imaging using\nonly one sample without phase and with amplitude noise. Our numerical\nevaluation results show that compared with 5 classic algorithms based on phase\ndata for imaging results, RINN's imaging results based on phaseless data are\ngood, with indicators such as RRMSE (0.11) performing similarly well. RINN\nprovides new possibilities for the universal development of radio frequency\nimaging technology."}
{"id": "2504.15669", "pdf": "https://arxiv.org/pdf/2504.15669", "abs": "https://arxiv.org/abs/2504.15669", "authors": ["Wei Zhuo", "Zhiyue Tang", "Wufeng Xue", "Hao Ding", "Linlin Shen"], "title": "DINOv2-powered Few-Shot Semantic Segmentation: A Unified Framework via Cross-Model Distillation and 4D Correlation Mining", "categories": ["cs.CV"], "comment": null, "summary": "Few-shot semantic segmentation has gained increasing interest due to its\ngeneralization capability, i.e., segmenting pixels of novel classes requiring\nonly a few annotated images. Prior work has focused on meta-learning for\nsupport-query matching, with extensive development in both prototype-based and\naggregation-based methods. To address data scarcity, recent approaches have\nturned to foundation models to enhance representation transferability for novel\nclass segmentation. Among them, a hybrid dual-modal framework including both\nDINOv2 and SAM has garnered attention due to their complementary capabilities.\nWe wonder \"can we build a unified model with knowledge from both foundation\nmodels?\" To this end, we propose FS-DINO, with only DINOv2's encoder and a\nlightweight segmenter. The segmenter features a bottleneck adapter, a\nmeta-visual prompt generator based on dense similarities and semantic\nembeddings, and a decoder. Through coarse-to-fine cross-model distillation, we\neffectively integrate SAM's knowledge into our lightweight segmenter, which can\nbe further enhanced by 4D correlation mining on support-query pairs. Extensive\nexperiments on COCO-20i, PASCAL-5i, and FSS-1000 demonstrate the effectiveness\nand superiority of our method."}
{"id": "2504.15843", "pdf": "https://arxiv.org/pdf/2504.15843", "abs": "https://arxiv.org/abs/2504.15843", "authors": ["Junshu Pan", "Wei Shen", "Shulin Huang", "Qiji Zhou", "Yue Zhang"], "title": "Pre-DPO: Improving Data Utilization in Direct Preference Optimization Using a Guiding Reference Model", "categories": ["cs.CL"], "comment": null, "summary": "Direct Preference Optimization (DPO) simplifies reinforcement learning from\nhuman feedback (RLHF) for large language models (LLMs) by directly optimizing\nhuman preferences without an explicit reward model. We find that during DPO\ntraining, the reference model plays the role of a data weight adjuster.\nHowever, the common practice of initializing the policy and reference models\nidentically in DPO can lead to inefficient data utilization and impose a\nperformance ceiling. Meanwhile, the lack of a reference model in Simple\nPreference Optimization (SimPO) reduces training robustness and necessitates\nstricter conditions to prevent catastrophic forgetting. In this work, we\npropose Pre-DPO, a simple yet effective DPO-based training paradigm that\nenhances preference optimization performance by leveraging a guiding reference\nmodel. This reference model provides foresight into the optimal policy state\nachievable through the training preference data, serving as a guiding mechanism\nthat adaptively assigns higher weights to samples more suitable for the model\nand lower weights to those less suitable. Extensive experiments on AlpacaEval\n2.0 and Arena-Hard v0.1 benchmarks demonstrate that Pre-DPO consistently\nimproves the performance of both DPO and SimPO, without relying on external\nmodels or additional data."}
{"id": "2504.15315", "pdf": "https://arxiv.org/pdf/2504.15315", "abs": "https://arxiv.org/abs/2504.15315", "authors": ["Noa Cohen", "Rotem Dror", "Itzik Klein"], "title": "Diffusion-Driven Inertial Generated Data for Smartphone Location Classification", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Despite the crucial role of inertial measurements in motion tracking and\nnavigation systems, the time-consuming and resource-intensive nature of\ncollecting extensive inertial data has hindered the development of robust\nmachine learning models in this field. In recent years, diffusion models have\nemerged as a revolutionary class of generative models, reshaping the landscape\nof artificial data generation. These models surpass generative adversarial\nnetworks and other state-of-the-art approaches to complex tasks. In this work,\nwe propose diffusion-driven specific force-generated data for smartphone\nlocation recognition. We provide a comprehensive evaluation methodology by\ncomparing synthetic and real recorded specific force data across multiple\nmetrics. Our results demonstrate that our diffusion-based generative model\nsuccessfully captures the distinctive characteristics of specific force signals\nacross different smartphone placement conditions. Thus, by creating diverse,\nrealistic synthetic data, we can reduce the burden of extensive data collection\nwhile providing high-quality training data for machine learning models."}
{"id": "2504.15681", "pdf": "https://arxiv.org/pdf/2504.15681", "abs": "https://arxiv.org/abs/2504.15681", "authors": ["Vidi Team", "Celong Liu", "Chia-Wen Kuo", "Dawei Du", "Fan Chen", "Guang Chen", "Jiamin Yuan", "Lingxi Zhang", "Lu Guo", "Lusha Li", "Longyin Wen", "Qingyu Chen", "Rachel Deng", "Sijie Zhu", "Stuart Siew", "Tong Jin", "Wei Lu", "Wen Zhong", "Xiaohui Shen", "Xin Gu", "Xing Mei", "Xueqiong Qu"], "title": "Vidi: Large Multimodal Models for Video Understanding and Editing", "categories": ["cs.CV"], "comment": null, "summary": "Humans naturally share information with those they are connected to, and\nvideo has become one of the dominant mediums for communication and expression\non the Internet. To support the creation of high-quality large-scale video\ncontent, a modern pipeline requires a comprehensive understanding of both the\nraw input materials (e.g., the unedited footage captured by cameras) and the\nediting components (e.g., visual effects). In video editing scenarios, models\nmust process multiple modalities (e.g., vision, audio, text) with strong\nbackground knowledge and handle flexible input lengths (e.g., hour-long raw\nvideos), which poses significant challenges for traditional models. In this\nreport, we introduce Vidi, a family of Large Multimodal Models (LMMs) for a\nwide range of video understand editing scenarios. The first release focuses on\ntemporal retrieval, i.e., identifying the time ranges within the input videos\ncorresponding to a given text query, which plays a critical role in intelligent\nediting. The model is capable of processing hour-long videos with strong\ntemporal understanding capability, e.g., retrieve time ranges for certain\nqueries. To support a comprehensive evaluation in real-world scenarios, we also\npresent the VUE-TR benchmark, which introduces five key advancements. 1) Video\nduration: significantly longer than existing temporal retrival datasets, 2)\nAudio support: includes audio-based queries, 3) Query format: diverse query\nlengths/formats, 4) Annotation quality: ground-truth time ranges are manually\nannotated. 5) Evaluation metric: a refined IoU metric to support evaluation\nover multiple time ranges. Remarkably, Vidi significantly outperforms leading\nproprietary models, e.g., GPT-4o and Gemini, on the temporal retrieval task,\nindicating its superiority in video editing scenarios."}
{"id": "2504.15848", "pdf": "https://arxiv.org/pdf/2504.15848", "abs": "https://arxiv.org/abs/2504.15848", "authors": ["Luwei Xiao", "Rui Mao", "Shuai Zhao", "Qika Lin", "Yanhao Jia", "Liang He", "Erik Cambria"], "title": "Exploring Cognitive and Aesthetic Causality for Multimodal Aspect-Based Sentiment Analysis", "categories": ["cs.CL"], "comment": "Accepted by TAFFC 2025", "summary": "Multimodal aspect-based sentiment classification (MASC) is an emerging task\ndue to an increase in user-generated multimodal content on social platforms,\naimed at predicting sentiment polarity toward specific aspect targets (i.e.,\nentities or attributes explicitly mentioned in text-image pairs). Despite\nextensive efforts and significant achievements in existing MASC, substantial\ngaps remain in understanding fine-grained visual content and the cognitive\nrationales derived from semantic content and impressions (cognitive\ninterpretations of emotions evoked by image content). In this study, we present\nChimera: a cognitive and aesthetic sentiment causality understanding framework\nto derive fine-grained holistic features of aspects and infer the fundamental\ndrivers of sentiment expression from both semantic perspectives and\naffective-cognitive resonance (the synergistic effect between emotional\nresponses and cognitive interpretations). Specifically, this framework first\nincorporates visual patch features for patch-word alignment. Meanwhile, it\nextracts coarse-grained visual features (e.g., overall image representation)\nand fine-grained visual regions (e.g., aspect-related regions) and translates\nthem into corresponding textual descriptions (e.g., facial, aesthetic).\nFinally, we leverage the sentimental causes and impressions generated by a\nlarge language model (LLM) to enhance the model's awareness of sentimental cues\nevoked by semantic content and affective-cognitive resonance. Experimental\nresults on standard MASC datasets demonstrate the effectiveness of the proposed\nmodel, which also exhibits greater flexibility to MASC compared to LLMs such as\nGPT-4o. We have publicly released the complete implementation and dataset at\nhttps://github.com/Xillv/Chimera"}
{"id": "2504.15317", "pdf": "https://arxiv.org/pdf/2504.15317", "abs": "https://arxiv.org/abs/2504.15317", "authors": ["Meher Boulaabi", "Takwa Ben Aïcha Gader", "Afef Kacem Echi", "Zied Bouraoui"], "title": "Enhancing DR Classification with Swin Transformer and Shifted Window Attention", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Diabetic retinopathy (DR) is a leading cause of blindness worldwide,\nunderscoring the importance of early detection for effective treatment.\nHowever, automated DR classification remains challenging due to variations in\nimage quality, class imbalance, and pixel-level similarities that hinder model\ntraining. To address these issues, we propose a robust preprocessing pipeline\nincorporating image cropping, Contrast-Limited Adaptive Histogram Equalization\n(CLAHE), and targeted data augmentation to improve model generalization and\nresilience. Our approach leverages the Swin Transformer, which utilizes\nhierarchical token processing and shifted window attention to efficiently\ncapture fine-grained features while maintaining linear computational\ncomplexity. We validate our method on the Aptos and IDRiD datasets for\nmulti-class DR classification, achieving accuracy rates of 89.65% and 97.40%,\nrespectively. These results demonstrate the effectiveness of our model,\nparticularly in detecting early-stage DR, highlighting its potential for\nimproving automated retinal screening in clinical settings."}
{"id": "2504.15694", "pdf": "https://arxiv.org/pdf/2504.15694", "abs": "https://arxiv.org/abs/2504.15694", "authors": ["Jun Dong", "Wenli Wu", "Jintao Cheng", "Xiaoyu Tang"], "title": "You Sense Only Once Beneath: Ultra-Light Real-Time Underwater Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Despite the remarkable achievements in object detection, the model's accuracy\nand efficiency still require further improvement under challenging underwater\nconditions, such as low image quality and limited computational resources. To\naddress this, we propose an Ultra-Light Real-Time Underwater Object Detection\nframework, You Sense Only Once Beneath (YSOOB). Specifically, we utilize a\nMulti-Spectrum Wavelet Encoder (MSWE) to perform frequency-domain encoding on\nthe input image, minimizing the semantic loss caused by underwater optical\ncolor distortion. Furthermore, we revisit the unique characteristics of\neven-sized and transposed convolutions, allowing the model to dynamically\nselect and enhance key information during the resampling process, thereby\nimproving its generalization ability. Finally, we eliminate model redundancy\nthrough a simple yet effective channel compression and reconstructed large\nkernel convolution (RLKC) to achieve model lightweight. As a result, forms a\nhigh-performance underwater object detector YSOOB with only 1.2 million\nparameters. Extensive experimental results demonstrate that, with the fewest\nparameters, YSOOB achieves mAP50 of 83.1% and 82.9% on the URPC2020 and DUO\ndatasets, respectively, comparable to the current SOTA detectors. The inference\nspeed reaches 781.3 FPS and 57.8 FPS on the T4 GPU (TensorRT FP16) and the edge\ncomputing device Jetson Xavier NX (TensorRT FP16), surpassing YOLOv12-N by\n28.1% and 22.5%, respectively."}
{"id": "2504.15895", "pdf": "https://arxiv.org/pdf/2504.15895", "abs": "https://arxiv.org/abs/2504.15895", "authors": ["Chenxu Yang", "Qingyi Si", "Yongjie Duan", "Zheliang Zhu", "Chenyu Zhu", "Zheng Lin", "Li Cao", "Weiping Wang"], "title": "Dynamic Early Exit in Reasoning Models", "categories": ["cs.CL", "cs.AI"], "comment": "19 pages, 11 figures", "summary": "Recent advances in large reasoning language models (LRLMs) rely on test-time\nscaling, which extends long chain-of-thought (CoT) generation to solve complex\ntasks. However, overthinking in long CoT not only slows down the efficiency of\nproblem solving, but also risks accuracy loss due to the extremely detailed or\nredundant reasoning steps. We propose a simple yet effective method that allows\nLLMs to self-truncate CoT sequences by early exit during generation. Instead of\nrelying on fixed heuristics, the proposed method monitors model behavior at\npotential reasoning transition points (e.g.,\"Wait\" tokens) and dynamically\nterminates the next reasoning chain's generation when the model exhibits high\nconfidence in a trial answer. Our method requires no additional training and\ncan be seamlessly integrated into existing o1-like reasoning LLMs. Experiments\non multiple reasoning benchmarks MATH-500, AMC 2023, GPQA Diamond and AIME 2024\nshow that the proposed method is consistently effective on deepseek-series\nreasoning LLMs, reducing the length of CoT sequences by an average of 31% to\n43% while improving accuracy by 1.7% to 5.7%."}
{"id": "2504.15322", "pdf": "https://arxiv.org/pdf/2504.15322", "abs": "https://arxiv.org/abs/2504.15322", "authors": ["Xiao Zhou", "Yuze Sun", "Jie Wu", "Xiaomeng Huang"], "title": "How to systematically develop an effective AI-based bias correction model?", "categories": ["cs.LG", "cs.AI", "physics.ao-ph"], "comment": null, "summary": "This study introduces ReSA-ConvLSTM, an artificial intelligence (AI)\nframework for systematic bias correction in numerical weather prediction (NWP).\nWe propose three innovations by integrating dynamic climatological\nnormalization, ConvLSTM with temporal causality constraints, and residual\nself-attention mechanisms. The model establishes a physics-aware nonlinear\nmapping between ECMWF forecasts and ERA5 reanalysis data. Using 41 years\n(1981-2021) of global atmospheric data, the framework reduces systematic biases\nin 2-m air temperature (T2m), 10-m winds (U10/V10), and sea-level pressure\n(SLP), achieving up to 20% RMSE reduction over 1-7 day forecasts compared to\noperational ECMWF outputs. The lightweight architecture (10.6M parameters)\nenables efficient generalization to multiple variables and downstream\napplications, reducing retraining time by 85% for cross-variable correction\nwhile improving ocean model skill through bias-corrected boundary conditions.\nThe ablation experiments demonstrate that our innovations significantly improve\nthe model's correction performance, suggesting that incorporating variable\ncharacteristics into the model helps enhance forecasting skills."}
{"id": "2504.15707", "pdf": "https://arxiv.org/pdf/2504.15707", "abs": "https://arxiv.org/abs/2504.15707", "authors": ["Yannic Neuhaus", "Matthias Hein"], "title": "RePOPE: Impact of Annotation Errors on the POPE Benchmark", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Since data annotation is costly, benchmark datasets often incorporate labels\nfrom established image datasets. In this work, we assess the impact of label\nerrors in MSCOCO on the frequently used object hallucination benchmark POPE. We\nre-annotate the benchmark images and identify an imbalance in annotation errors\nacross different subsets. Evaluating multiple models on the revised labels,\nwhich we denote as RePOPE, we observe notable shifts in model rankings,\nhighlighting the impact of label quality. Code and data are available at\nhttps://github.com/YanNeu/RePOPE ."}
{"id": "2504.15900", "pdf": "https://arxiv.org/pdf/2504.15900", "abs": "https://arxiv.org/abs/2504.15900", "authors": ["Cheng Wen", "Tingwei Guo", "Shuaijiang Zhao", "Wei Zou", "Xiangang Li"], "title": "SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning", "categories": ["cs.CL"], "comment": null, "summary": "Recent work shows that reinforcement learning(RL) can markedly sharpen the\nreasoning ability of large language models (LLMs) by prompting them to \"think\nbefore answering.\" Yet whether and how these gains transfer to audio-language\nreasoning remains largely unexplored. We extend the Group-Relative Policy\nOptimization (GRPO) framework from DeepSeek-R1 to a Large Audio-Language Model\n(LALM), and construct a 32k sample multiple-choice corpus. Using a two-stage\nregimen supervised fine-tuning on structured and unstructured\nchains-of-thought, followed by curriculum-guided GRPO, we systematically\ncompare implicit vs. explicit, and structured vs. free form reasoning under\nidentical architectures. Our structured audio reasoning model, SARI (Structured\nAudio Reasoning via Curriculum-Guided Reinforcement Learning), achieves a\n16.35% improvement in average accuracy over the base model\nQwen2-Audio-7B-Instruct. Furthermore, the variant built upon Qwen2.5-Omni\nreaches state-of-the-art performance of 67.08% on the MMAU test-mini benchmark.\nAblation experiments show that on the base model we use: (i) SFT warm-up is\nimportant for stable RL training, (ii) structured chains yield more robust\ngeneralization than unstructured ones, and (iii) easy-to-hard curricula\naccelerate convergence and improve final performance. These findings\ndemonstrate that explicit, structured reasoning and curriculum learning\nsubstantially enhances audio-language understanding."}
{"id": "2504.15323", "pdf": "https://arxiv.org/pdf/2504.15323", "abs": "https://arxiv.org/abs/2504.15323", "authors": ["Donggyun Kim", "Chanwoo Kim", "Seunghoon Hong"], "title": "HyperFlow: Gradient-Free Emulation of Few-Shot Fine-Tuning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "While test-time fine-tuning is beneficial in few-shot learning, the need for\nmultiple backpropagation steps can be prohibitively expensive in real-time or\nlow-resource scenarios. To address this limitation, we propose an approach that\nemulates gradient descent without computing gradients, enabling efficient\ntest-time adaptation. Specifically, we formulate gradient descent as an Euler\ndiscretization of an ordinary differential equation (ODE) and train an\nauxiliary network to predict the task-conditional drift using only the few-shot\nsupport set. The adaptation then reduces to a simple numerical integration\n(e.g., via the Euler method), which requires only a few forward passes of the\nauxiliary network -- no gradients or forward passes of the target model are\nneeded. In experiments on cross-domain few-shot classification using the\nMeta-Dataset and CDFSL benchmarks, our method significantly improves\nout-of-domain performance over the non-fine-tuned baseline while incurring only\n6\\% of the memory cost and 0.02\\% of the computation time of standard\nfine-tuning, thus establishing a practical middle ground between direct\ntransfer and fully fine-tuned approaches."}
{"id": "2504.15723", "pdf": "https://arxiv.org/pdf/2504.15723", "abs": "https://arxiv.org/abs/2504.15723", "authors": ["Dasol Jeong", "Donggoo Kang", "Jiwon Park", "Hyebean Lee", "Joonki Paik"], "title": "Structure-Preserving Zero-Shot Image Editing via Stage-Wise Latent Injection in Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "We propose a diffusion-based framework for zero-shot image editing that\nunifies text-guided and reference-guided approaches without requiring\nfine-tuning. Our method leverages diffusion inversion and timestep-specific\nnull-text embeddings to preserve the structural integrity of the source image.\nBy introducing a stage-wise latent injection strategy-shape injection in early\nsteps and attribute injection in later steps-we enable precise, fine-grained\nmodifications while maintaining global consistency. Cross-attention with\nreference latents facilitates semantic alignment between the source and\nreference. Extensive experiments across expression transfer, texture\ntransformation, and style infusion demonstrate state-of-the-art performance,\nconfirming the method's scalability and adaptability to diverse image editing\nscenarios."}
{"id": "2504.15941", "pdf": "https://arxiv.org/pdf/2504.15941", "abs": "https://arxiv.org/abs/2504.15941", "authors": ["Fanny Jourdan", "Yannick Chevalier", "Cécile Favre"], "title": "FairTranslate: An English-French Dataset for Gender Bias Evaluation in Machine Translation by Overcoming Gender Binarity", "categories": ["cs.CL", "cs.AI"], "comment": "FAccT 2025", "summary": "Large Language Models (LLMs) are increasingly leveraged for translation tasks\nbut often fall short when translating inclusive language -- such as texts\ncontaining the singular 'they' pronoun or otherwise reflecting fair linguistic\nprotocols. Because these challenges span both computational and societal\ndomains, it is imperative to critically evaluate how well LLMs handle inclusive\ntranslation with a well-founded framework.\n  This paper presents FairTranslate, a novel, fully human-annotated dataset\ndesigned to evaluate non-binary gender biases in machine translation systems\nfrom English to French. FairTranslate includes 2418 English-French sentence\npairs related to occupations, annotated with rich metadata such as the\nstereotypical alignment of the occupation, grammatical gender indicator\nambiguity, and the ground-truth gender label (male, female, or inclusive).\n  We evaluate four leading LLMs (Gemma2-2B, Mistral-7B, Llama3.1-8B,\nLlama3.3-70B) on this dataset under different prompting procedures. Our results\nreveal substantial biases in gender representation across LLMs, highlighting\npersistent challenges in achieving equitable outcomes in machine translation.\nThese findings underscore the need for focused strategies and interventions\naimed at ensuring fair and inclusive language usage in LLM-based translation\nsystems.\n  We make the FairTranslate dataset publicly available on Hugging Face, and\ndisclose the code for all experiments on GitHub."}
{"id": "2504.15324", "pdf": "https://arxiv.org/pdf/2504.15324", "abs": "https://arxiv.org/abs/2504.15324", "authors": ["Vuong M. Ngo", "Edward Bolger", "Stan Goodwin", "John O'Sullivan", "Dinh Viet Cuong", "Mark Roantree"], "title": "A Graph Based Raman Spectral Processing Technique for Exosome Classification", "categories": ["q-bio.QM", "cs.AI", "cs.IT", "cs.LG", "math.IT"], "comment": "The 23rd International Conference on Artificial Intelligence in\n  Medicine (AIME 2025), LNAI, Springer, 11 pages", "summary": "Exosomes are small vesicles crucial for cell signaling and disease\nbiomarkers. Due to their complexity, an \"omics\" approach is preferable to\nindividual biomarkers. While Raman spectroscopy is effective for exosome\nanalysis, it requires high sample concentrations and has limited sensitivity to\nlipids and proteins. Surface-enhanced Raman spectroscopy helps overcome these\nchallenges. In this study, we leverage Neo4j graph databases to organize 3,045\nRaman spectra of exosomes, enhancing data generalization. To further refine\nspectral analysis, we introduce a novel spectral filtering process that\nintegrates the PageRank Filter with optimal Dimensionality Reduction. This\nmethod improves feature selection, resulting in superior classification\nperformance. Specifically, the Extra Trees model, using our spectral processing\napproach, achieves 0.76 and 0.857 accuracy in classifying hyperglycemic,\nhypoglycemic, and normal exosome samples based on Raman spectra and surface,\nrespectively, with group 10-fold cross-validation. Our results show that\ngraph-based spectral filtering combined with optimal dimensionality reduction\nsignificantly improves classification accuracy by reducing noise while\npreserving key biomarker signals. This novel framework enhances Raman-based\nexosome analysis, expanding its potential for biomedical applications, disease\ndiagnostics, and biomarker discovery."}
{"id": "2504.15728", "pdf": "https://arxiv.org/pdf/2504.15728", "abs": "https://arxiv.org/abs/2504.15728", "authors": ["Manjunath D", "Aniruddh Sikdar", "Prajwal Gurunath", "Sumanth Udupa", "Suresh Sundaram"], "title": "SAGA: Semantic-Aware Gray color Augmentation for Visible-to-Thermal Domain Adaptation across Multi-View Drone and Ground-Based Vision Systems", "categories": ["cs.CV"], "comment": "Accepted at CVPR-W PBVS 2025", "summary": "Domain-adaptive thermal object detection plays a key role in facilitating\nvisible (RGB)-to-thermal (IR) adaptation by reducing the need for co-registered\nimage pairs and minimizing reliance on large annotated IR datasets. However,\ninherent limitations of IR images, such as the lack of color and texture cues,\npose challenges for RGB-trained models, leading to increased false positives\nand poor-quality pseudo-labels. To address this, we propose Semantic-Aware Gray\ncolor Augmentation (SAGA), a novel strategy for mitigating color bias and\nbridging the domain gap by extracting object-level features relevant to IR\nimages. Additionally, to validate the proposed SAGA for drone imagery, we\nintroduce the IndraEye, a multi-sensor (RGB-IR) dataset designed for diverse\napplications. The dataset contains 5,612 images with 145,666 instances,\ncaptured from diverse angles, altitudes, backgrounds, and times of day,\noffering valuable opportunities for multimodal learning, domain adaptation for\nobject detection and segmentation, and exploration of sensor-specific strengths\nand weaknesses. IndraEye aims to enhance the development of more robust and\naccurate aerial perception systems, especially in challenging environments.\nExperimental results show that SAGA significantly improves RGB-to-IR adaptation\nfor autonomous driving and IndraEye dataset, achieving consistent performance\ngains of +0.4% to +7.6% (mAP) when integrated with state-of-the-art domain\nadaptation techniques. The dataset and codes are available at\nhttps://github.com/airliisc/IndraEye."}
{"id": "2504.15983", "pdf": "https://arxiv.org/pdf/2504.15983", "abs": "https://arxiv.org/abs/2504.15983", "authors": ["Shang Wang"], "title": "W-PCA Based Gradient-Free Proxy for Efficient Search of Lightweight Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "ICLR 2025", "summary": "The demand for efficient natural language processing (NLP) systems has led to\nthe development of lightweight language models. Previous work in this area has\nprimarily focused on manual design or training-based neural architecture search\n(NAS) methods. Recently, zero-shot NAS methods have been proposed for\nevaluating language models without the need for training. However, prevailing\napproaches to zero-shot NAS often face challenges such as biased evaluation\nmetrics and computational inefficiencies. In this paper, we introduce\nweight-weighted PCA (W-PCA), a novel zero-shot NAS method specifically tailored\nfor lightweight language models. Our approach utilizes two evaluation proxies:\nthe parameter count and the number of principal components with cumulative\ncontribution exceeding $\\eta$ in the feed-forward neural (FFN) layer.\nAdditionally, by eliminating the need for gradient computations, we optimize\nthe evaluation time, thus enhancing the efficiency of designing and evaluating\nlightweight language models. We conduct a comparative analysis on the GLUE and\nSQuAD datasets to evaluate our approach. The results demonstrate that our\nmethod significantly reduces training time compared to one-shot NAS methods and\nachieves higher scores in the testing phase compared to previous\nstate-of-the-art training-based methods. Furthermore, we perform ranking\nevaluations on a dataset sampled from the FlexiBERT search space. Our approach\nexhibits superior ranking correlation and further reduces solving time compared\nto other zero-shot NAS methods that require gradient computation."}
{"id": "2504.15325", "pdf": "https://arxiv.org/pdf/2504.15325", "abs": "https://arxiv.org/abs/2504.15325", "authors": ["Alberto Casagrande", "Francesco Fabris", "Rossano Girometti", "Roberto Pagliarini"], "title": "Significativity Indices for Agreement Values", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "27 pages, 6 figures", "summary": "Agreement measures, such as Cohen's kappa or intraclass correlation, gauge\nthe matching between two or more classifiers. They are used in a wide range of\ncontexts from medicine, where they evaluate the effectiveness of medical\ntreatments and clinical trials, to artificial intelligence, where they can\nquantify the approximation due to the reduction of a classifier. The\nconsistency of different classifiers to a golden standard can be compared\nsimply by using the order induced by their agreement measure with respect to\nthe golden standard itself. Nevertheless, labelling an approach as good or bad\nexclusively by using the value of an agreement measure requires a scale or a\nsignificativity index. Some quality scales have been proposed in the literature\nfor Cohen's kappa, but they are mainly naive, and their boundaries are\narbitrary. This work proposes a general approach to evaluate the\nsignificativity of any agreement value between two classifiers and introduces\ntwo significativity indices: one dealing with finite data sets, the other one\nhandling classification probability distributions. Moreover, this manuscript\nconsiders the computational issues of evaluating such indices and identifies\nsome efficient algorithms to evaluate them."}
{"id": "2504.15751", "pdf": "https://arxiv.org/pdf/2504.15751", "abs": "https://arxiv.org/abs/2504.15751", "authors": ["Menan Velayuthan", "Asiri Gawesha", "Purushoth Velayuthan", "Nuwan Kodagoda", "Dharshana Kasthurirathna", "Pradeepa Samarasinghe"], "title": "GADS: A Super Lightweight Model for Head Pose Estimation", "categories": ["cs.CV"], "comment": "16 pages, 5 tables, 10 figures, not submitted to any conference or\n  journal", "summary": "In human-computer interaction, head pose estimation profoundly influences\napplication functionality. Although utilizing facial landmarks is valuable for\nthis purpose, existing landmark-based methods prioritize precision over\nsimplicity and model size, limiting their deployment on edge devices and in\ncompute-poor environments. To bridge this gap, we propose \\textbf{Grouped\nAttention Deep Sets (GADS)}, a novel architecture based on the Deep Set\nframework. By grouping landmarks into regions and employing small Deep Set\nlayers, we reduce computational complexity. Our multihead attention mechanism\nextracts and combines inter-group information, resulting in a model that is\n$7.5\\times$ smaller and executes $25\\times$ faster than the current lightest\nstate-of-the-art model. Notably, our method achieves an impressive reduction,\nbeing $4321\\times$ smaller than the best-performing model. We introduce vanilla\nGADS and Hybrid-GADS (landmarks + RGB) and evaluate our models on three\nbenchmark datasets -- AFLW2000, BIWI, and 300W-LP. We envision our architecture\nas a robust baseline for resource-constrained head pose estimation methods."}
{"id": "2504.15987", "pdf": "https://arxiv.org/pdf/2504.15987", "abs": "https://arxiv.org/abs/2504.15987", "authors": ["Zhenkai Qin", "Dongze Wu", "Yuxin Liu", "Guifang Yang"], "title": "Few-shot Hate Speech Detection Based on the MindSpore Framework", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "The proliferation of hate speech on social media poses a significant threat\nto online communities, requiring effective detection systems. While deep\nlearning models have shown promise, their performance often deteriorates in\nfew-shot or low-resource settings due to reliance on large annotated corpora.\nTo address this, we propose MS-FSLHate, a prompt-enhanced neural framework for\nfew-shot hate speech detection implemented on the MindSpore deep learning\nplatform. The model integrates learnable prompt embeddings, a CNN-BiLSTM\nbackbone with attention pooling, and synonym-based adversarial data\naugmentation to improve generalization. Experimental results on two benchmark\ndatasets-HateXplain and HSOL-demonstrate that our approach outperforms\ncompetitive baselines in precision, recall, and F1-score. Additionally, the\nframework shows high efficiency and scalability, suggesting its suitability for\ndeployment in resource-constrained environments. These findings highlight the\npotential of combining prompt-based learning with adversarial augmentation for\nrobust and adaptable hate speech detection in few-shot scenarios."}
{"id": "2504.15328", "pdf": "https://arxiv.org/pdf/2504.15328", "abs": "https://arxiv.org/abs/2504.15328", "authors": ["Usevalad Milasheuski", "Luca Barbieri", "Sanaz Kianoush", "Monica Nicoli", "Stefano Savazzi"], "title": "Bayesian Federated Learning for Continual Training", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Bayesian Federated Learning (BFL) enables uncertainty quantification and\nrobust adaptation in distributed learning. In contrast to the frequentist\napproach, it estimates the posterior distribution of a global model, offering\ninsights into model reliability. However, current BFL methods neglect continual\nlearning challenges in dynamic environments where data distributions shift over\ntime. We propose a continual BFL framework applied to human sensing with radar\ndata collected over several days. Using Stochastic Gradient Langevin Dynamics\n(SGLD), our approach sequentially updates the model, leveraging past posteriors\nto construct the prior for the new tasks. We assess the accuracy, the expected\ncalibration error (ECE) and the convergence speed of our approach against\nseveral baselines. Results highlight the effectiveness of continual Bayesian\nupdates in preserving knowledge and adapting to evolving data."}
{"id": "2504.15756", "pdf": "https://arxiv.org/pdf/2504.15756", "abs": "https://arxiv.org/abs/2504.15756", "authors": ["Qirui Yang", "Fangpu Zhang", "Yeying Jin", "Qihua Cheng", "Pengtao Jiang", "Huanjing Yue", "Jingyu Yang"], "title": "DSDNet: Raw Domain Demoiréing via Dual Color-Space Synergy", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "With the rapid advancement of mobile imaging, capturing screens using\nsmartphones has become a prevalent practice in distance learning and conference\nrecording. However, moir\\'e artifacts, caused by frequency aliasing between\ndisplay screens and camera sensors, are further amplified by the image signal\nprocessing pipeline, leading to severe visual degradation. Existing sRGB domain\ndemoir\\'eing methods struggle with irreversible information loss, while recent\ntwo-stage raw domain approaches suffer from information bottlenecks and\ninference inefficiency. To address these limitations, we propose a single-stage\nraw domain demoir\\'eing framework, Dual-Stream Demoir\\'eing Network (DSDNet),\nwhich leverages the synergy of raw and YCbCr images to remove moir\\'e while\npreserving luminance and color fidelity. Specifically, to guide luminance\ncorrection and moir\\'e removal, we design a raw-to-YCbCr mapping pipeline and\nintroduce the Synergic Attention with Dynamic Modulation (SADM) module. This\nmodule enriches the raw-to-sRGB conversion with cross-domain contextual\nfeatures. Furthermore, to better guide color fidelity, we develop a\nLuminance-Chrominance Adaptive Transformer (LCAT), which decouples luminance\nand chrominance representations. Extensive experiments demonstrate that DSDNet\noutperforms state-of-the-art methods in both visual quality and quantitative\nevaluation, and achieves an inference speed $\\mathrm{\\textbf{2.4x}}$ faster\nthan the second-best method, highlighting its practical advantages. We provide\nan anonymous online demo at https://xxxxxxxxdsdnet.github.io/DSDNet/."}
{"id": "2504.16005", "pdf": "https://arxiv.org/pdf/2504.16005", "abs": "https://arxiv.org/abs/2504.16005", "authors": ["Tom Zehle", "Moritz Schlager", "Timo Heiß", "Matthias Feurer"], "title": "CAPO: Cost-Aware Prompt Optimization", "categories": ["cs.CL", "cs.AI", "cs.NE", "stat.ML"], "comment": "Submitted to AutoML 2025", "summary": "Large language models (LLMs) have revolutionized natural language processing\nby solving a wide range of tasks simply guided by a prompt. Yet their\nperformance is highly sensitive to prompt formulation. While automated prompt\noptimization addresses this challenge by finding optimal prompts, current\nmethods require a substantial number of LLM calls and input tokens, making\nprompt optimization expensive. We introduce CAPO (Cost-Aware Prompt\nOptimization), an algorithm that enhances prompt optimization efficiency by\nintegrating AutoML techniques. CAPO is an evolutionary approach with LLMs as\noperators, incorporating racing to save evaluations and multi-objective\noptimization to balance performance with prompt length. It jointly optimizes\ninstructions and few-shot examples while leveraging task descriptions for\nimproved robustness. Our extensive experiments across diverse datasets and LLMs\ndemonstrate that CAPO outperforms state-of-the-art discrete prompt optimization\nmethods in 11/15 cases with improvements up to 21%p. Our algorithm achieves\nbetter performances already with smaller budgets, saves evaluations through\nracing, and decreases average prompt length via a length penalty, making it\nboth cost-efficient and cost-aware. Even without few-shot examples, CAPO\noutperforms its competitors and generally remains robust to initial prompts.\nCAPO represents an important step toward making prompt optimization more\npowerful and accessible by improving cost-efficiency."}
{"id": "2504.15330", "pdf": "https://arxiv.org/pdf/2504.15330", "abs": "https://arxiv.org/abs/2504.15330", "authors": ["Mohit Gupta", "Akiko Aizawa", "Rajiv Ratn Shah"], "title": "Med-CoDE: Medical Critique based Disagreement Evaluation Framework", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "8 pages, 4 figures, NAACL SRW 2025", "summary": "The emergence of large language models (LLMs) has significantly influenced\nnumerous fields, including healthcare, by enhancing the capabilities of\nautomated systems to process and generate human-like text. However, despite\ntheir advancements, the reliability and accuracy of LLMs in medical contexts\nremain critical concerns. Current evaluation methods often lack robustness and\nfail to provide a comprehensive assessment of LLM performance, leading to\npotential risks in clinical settings. In this work, we propose Med-CoDE, a\nspecifically designed evaluation framework for medical LLMs to address these\nchallenges. The framework leverages a critique-based approach to quantitatively\nmeasure the degree of disagreement between model-generated responses and\nestablished medical ground truths. This framework captures both accuracy and\nreliability in medical settings. The proposed evaluation framework aims to fill\nthe existing gap in LLM assessment by offering a systematic method to evaluate\nthe quality and trustworthiness of medical LLMs. Through extensive experiments\nand case studies, we illustrate the practicality of our framework in providing\na comprehensive and reliable evaluation of medical LLMs."}
{"id": "2504.15770", "pdf": "https://arxiv.org/pdf/2504.15770", "abs": "https://arxiv.org/abs/2504.15770", "authors": ["Lei Xu", "Mehmet Yamac", "Mete Ahishali", "Moncef Gabbouj"], "title": "Multi-Scale Tensorial Summation and Dimensional Reduction Guided Neural Network for Edge Detection", "categories": ["cs.CV"], "comment": null, "summary": "Edge detection has attracted considerable attention thanks to its exceptional\nability to enhance performance in downstream computer vision tasks. In recent\nyears, various deep learning methods have been explored for edge detection\ntasks resulting in a significant performance improvement compared to\nconventional computer vision algorithms. In neural networks, edge detection\ntasks require considerably large receptive fields to provide satisfactory\nperformance. In a typical convolutional operation, such a large receptive field\ncan be achieved by utilizing a significant number of consecutive layers, which\nyields deep network structures. Recently, a Multi-scale Tensorial Summation\n(MTS) factorization operator was presented, which can achieve very large\nreceptive fields even from the initial layers. In this paper, we propose a\nnovel MTS Dimensional Reduction (MTS-DR) module guided neural network,\nMTS-DR-Net, for the edge detection task. The MTS-DR-Net uses MTS layers, and\ncorresponding MTS-DR blocks as a new backbone to remove redundant information\ninitially. Such a dimensional reduction module enables the neural network to\nfocus specifically on relevant information (i.e., necessary subspaces).\nFinally, a weight U-shaped refinement module follows MTS-DR blocks in the\nMTS-DR-Net. We conducted extensive experiments on two benchmark edge detection\ndatasets: BSDS500 and BIPEDv2 to verify the effectiveness of our model. The\nimplementation of the proposed MTS-DR-Net can be found at\nhttps://github.com/LeiXuAI/MTS-DR-Net.git."}
{"id": "2504.16007", "pdf": "https://arxiv.org/pdf/2504.16007", "abs": "https://arxiv.org/abs/2504.16007", "authors": ["Igor Rozhkov", "Natalia Loukachevitch"], "title": "Methods for Recognizing Nested Terms", "categories": ["cs.CL"], "comment": "To be published in Computational Linguistics and Intellectual\n  Technologies proceedings", "summary": "In this paper, we describe our participation in the RuTermEval competition\ndevoted to extracting nested terms. We apply the Binder model, which was\npreviously successfully applied to the recognition of nested named entities, to\nextract nested terms. We obtained the best results of term recognition in all\nthree tracks of the RuTermEval competition. In addition, we study the new task\nof recognition of nested terms from flat training data annotated with terms\nwithout nestedness. We can conclude that several approaches we proposed in this\nwork are viable enough to retrieve nested terms effectively without nested\nlabeling of them."}
{"id": "2504.15369", "pdf": "https://arxiv.org/pdf/2504.15369", "abs": "https://arxiv.org/abs/2504.15369", "authors": ["Calvin Luo", "Zilai Zeng", "Yilun Du", "Chen Sun"], "title": "Solving New Tasks by Adapting Internet Video Knowledge", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "ICLR 2025. Project Webpage:\n  https://diffusion-supervision.github.io/adapt2act/", "summary": "Video generative models demonstrate great promise in robotics by serving as\nvisual planners or as policy supervisors. When pretrained on internet-scale\ndata, such video models intimately understand alignment with natural language,\nand can thus facilitate generalization to novel downstream behavior through\ntext-conditioning. However, they may not be sensitive to the specificities of\nthe particular environment the agent inhabits. On the other hand, training\nvideo models on in-domain examples of robotic behavior naturally encodes\nenvironment-specific intricacies, but the scale of available demonstrations may\nnot be sufficient to support generalization to unseen tasks via natural\nlanguage specification. In this work, we investigate different adaptation\ntechniques that integrate in-domain information with large-scale pretrained\nvideo models, and explore the extent to which they enable novel\ntext-conditioned generalization for robotic tasks, while also considering their\nindependent data and resource considerations. We successfully demonstrate\nacross robotic environments that adapting powerful video models with small\nscales of example data can successfully facilitate generalization to novel\nbehaviors. In particular, we present a novel adaptation strategy, termed\nInverse Probabilistic Adaptation, that not only consistently achieves strong\ngeneralization performance across robotic tasks and settings, but also exhibits\nrobustness to the quality of adaptation data, successfully solving novel tasks\neven when only suboptimal in-domain demonstrations are available."}
{"id": "2504.15776", "pdf": "https://arxiv.org/pdf/2504.15776", "abs": "https://arxiv.org/abs/2504.15776", "authors": ["Quentin Herau", "Nathan Piasco", "Moussab Bennehar", "Luis Rolado", "Dzmitry Tsishkou", "Bingbing Liu", "Cyrille Migniot", "Pascal Vasseur", "Cédric Demonceaux"], "title": "Pose Optimization for Autonomous Driving Datasets using Neural Rendering Models", "categories": ["cs.CV", "cs.RO"], "comment": "under review", "summary": "Autonomous driving systems rely on accurate perception and localization of\nthe ego car to ensure safety and reliability in challenging real-world driving\nscenarios. Public datasets play a vital role in benchmarking and guiding\nadvancement in research by providing standardized resources for model\ndevelopment and evaluation. However, potential inaccuracies in sensor\ncalibration and vehicle poses within these datasets can lead to erroneous\nevaluations of downstream tasks, adversely impacting the reliability and\nperformance of the autonomous systems. To address this challenge, we propose a\nrobust optimization method based on Neural Radiance Fields (NeRF) to refine\nsensor poses and calibration parameters, enhancing the integrity of dataset\nbenchmarks. To validate improvement in accuracy of our optimized poses without\nground truth, we present a thorough evaluation process, relying on reprojection\nmetrics, Novel View Synthesis rendering quality, and geometric alignment. We\ndemonstrate that our method achieves significant improvements in sensor pose\naccuracy. By optimizing these critical parameters, our approach not only\nimproves the utility of existing datasets but also paves the way for more\nreliable autonomous driving models. To foster continued progress in this field,\nwe make the optimized sensor poses publicly available, providing a valuable\nresource for the research community."}
{"id": "2504.16046", "pdf": "https://arxiv.org/pdf/2504.16046", "abs": "https://arxiv.org/abs/2504.16046", "authors": ["Jingyu Zhang", "Jiacan Yu", "Marc Marone", "Benjamin Van Durme", "Daniel Khashabi"], "title": "Certified Mitigation of Worst-Case LLM Copyright Infringement", "categories": ["cs.CL"], "comment": null, "summary": "The exposure of large language models (LLMs) to copyrighted material during\npre-training raises concerns about unintentional copyright infringement post\ndeployment. This has driven the development of \"copyright takedown\" methods,\npost-training approaches aimed at preventing models from generating content\nsubstantially similar to copyrighted ones. While current mitigation approaches\nare somewhat effective for average-case risks, we demonstrate that they\noverlook worst-case copyright risks exhibits by the existence of long, verbatim\nquotes from copyrighted sources. We propose BloomScrub, a remarkably simple yet\nhighly effective inference-time approach that provides certified copyright\ntakedown. Our method repeatedly interleaves quote detection with rewriting\ntechniques to transform potentially infringing segments. By leveraging\nefficient data sketches (Bloom filters), our approach enables scalable\ncopyright screening even for large-scale real-world corpora. When quotes beyond\na length threshold cannot be removed, the system can abstain from responding,\noffering certified risk reduction. Experimental results show that BloomScrub\nreduces infringement risk, preserves utility, and accommodates different levels\nof enforcement stringency with adaptive abstention. Our results suggest that\nlightweight, inference-time methods can be surprisingly effective for copyright\nprevention."}
{"id": "2504.15376", "pdf": "https://arxiv.org/pdf/2504.15376", "abs": "https://arxiv.org/abs/2504.15376", "authors": ["Zhiqiu Lin", "Siyuan Cen", "Daniel Jiang", "Jay Karhade", "Hewei Wang", "Chancharik Mitra", "Tiffany Ling", "Yuhan Huang", "Sifan Liu", "Mingyu Chen", "Rushikesh Zawar", "Xue Bai", "Yilun Du", "Chuang Gan", "Deva Ramanan"], "title": "Towards Understanding Camera Motions in Any Video", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "Project site: https://linzhiqiu.github.io/papers/camerabench/", "summary": "We introduce CameraBench, a large-scale dataset and benchmark designed to\nassess and improve camera motion understanding. CameraBench consists of ~3,000\ndiverse internet videos, annotated by experts through a rigorous multi-stage\nquality control process. One of our contributions is a taxonomy of camera\nmotion primitives, designed in collaboration with cinematographers. We find,\nfor example, that some motions like \"follow\" (or tracking) require\nunderstanding scene content like moving subjects. We conduct a large-scale\nhuman study to quantify human annotation performance, revealing that domain\nexpertise and tutorial-based training can significantly enhance accuracy. For\nexample, a novice may confuse zoom-in (a change of intrinsics) with translating\nforward (a change of extrinsics), but can be trained to differentiate the two.\nUsing CameraBench, we evaluate Structure-from-Motion (SfM) and Video-Language\nModels (VLMs), finding that SfM models struggle to capture semantic primitives\nthat depend on scene content, while VLMs struggle to capture geometric\nprimitives that require precise estimation of trajectories. We then fine-tune a\ngenerative VLM on CameraBench to achieve the best of both worlds and showcase\nits applications, including motion-augmented captioning, video question\nanswering, and video-text retrieval. We hope our taxonomy, benchmark, and\ntutorials will drive future efforts towards the ultimate goal of understanding\ncamera motions in any video."}
{"id": "2504.15782", "pdf": "https://arxiv.org/pdf/2504.15782", "abs": "https://arxiv.org/abs/2504.15782", "authors": ["Daniele Baieri", "Riccardo Cicciarella", "Michael Krützen", "Emanuele Rodolà", "Silvia Zuffi"], "title": "Model-based Metric 3D Shape and Motion Reconstruction of Wild Bottlenose Dolphins in Drone-Shot Videos", "categories": ["cs.CV", "cs.GR", "I.4.8; J.3"], "comment": "9 pages, 7 figures", "summary": "We address the problem of estimating the metric 3D shape and motion of wild\ndolphins from monocular video, with the aim of assessing their body condition.\nWhile considerable progress has been made in reconstructing 3D models of\nterrestrial quadrupeds, aquatic animals remain unexplored due to the difficulty\nof observing them in their natural underwater environment. To address this, we\npropose a model-based approach that incorporates a transmission model to\naccount for water-induced occlusion. We apply our method to video captured\nunder different sea conditions. We estimate mass and volume, and compare our\nresults to a manual 2D measurements-based method."}
{"id": "2504.16053", "pdf": "https://arxiv.org/pdf/2504.16053", "abs": "https://arxiv.org/abs/2504.16053", "authors": ["Zhifan Ye", "Kejing Xia", "Yonggan Fu", "Xin Dong", "Jihoon Hong", "Xiangchi Yuan", "Shizhe Diao", "Jan Kautz", "Pavlo Molchanov", "Yingyan Celine Lin"], "title": "LongMamba: Enhancing Mamba's Long Context Capabilities via Training-Free Receptive Field Enlargement", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ICLR 2025", "summary": "State space models (SSMs) have emerged as an efficient alternative to\nTransformer models for language modeling, offering linear computational\ncomplexity and constant memory usage as context length increases. However,\ndespite their efficiency in handling long contexts, recent studies have shown\nthat SSMs, such as Mamba models, generally underperform compared to\nTransformers in long-context understanding tasks. To address this significant\nshortfall and achieve both efficient and accurate long-context understanding,\nwe propose LongMamba, a training-free technique that significantly enhances the\nlong-context capabilities of Mamba models. LongMamba builds on our discovery\nthat the hidden channels in Mamba can be categorized into local and global\nchannels based on their receptive field lengths, with global channels primarily\nresponsible for long-context capability. These global channels can become the\nkey bottleneck as the input context lengthens. Specifically, when input lengths\nlargely exceed the training sequence length, global channels exhibit\nlimitations in adaptively extend their receptive fields, leading to Mamba's\npoor long-context performance. The key idea of LongMamba is to mitigate the\nhidden state memory decay in these global channels by preventing the\naccumulation of unimportant tokens in their memory. This is achieved by first\nidentifying critical tokens in the global channels and then applying token\nfiltering to accumulate only those critical tokens. Through extensive\nbenchmarking across synthetic and real-world long-context scenarios, LongMamba\nsets a new standard for Mamba's long-context performance, significantly\nextending its operational range without requiring additional training. Our code\nis available at https://github.com/GATECH-EIC/LongMamba."}
{"id": "2504.15417", "pdf": "https://arxiv.org/pdf/2504.15417", "abs": "https://arxiv.org/abs/2504.15417", "authors": ["Van-Giang Trinh", "Belaid Benhamou", "Sylvain Soliman", "François Fages"], "title": "On the Boolean Network Theory of Datalog$^\\neg$", "categories": ["cs.LO", "cs.AI"], "comment": "48 pages, 7 figures", "summary": "Datalog$^\\neg$ is a central formalism used in a variety of domains ranging\nfrom deductive databases and abstract argumentation frameworks to answer set\nprogramming. Its model theory is the finite counterpart of the logical\nsemantics developed for normal logic programs, mainly based on the notions of\nClark's completion and two-valued or three-valued canonical models including\nsupported, stable, regular and well-founded models. In this paper we establish\na formal link between Datalog$^\\neg$ and Boolean network theory, which was\ninitially introduced by Stuart Kaufman and Ren\\'e Thomas to reason about gene\nregulatory networks. We use previous results from Boolean network theory to\nprove that in the absence of odd cycles in a Datalog$^\\neg$ program, the\nregular models coincide with the stable models, which entails the existence of\nstable models, and in the absence of even cycles, we show the uniqueness of\nstable partial models, which entails the uniqueness of regular models. These\nresults on regular models have been claimed by You and Yuan in 1994 for normal\nlogic programs but we show problems in their definition of well-founded\nstratification and in their proofs that we can fix for negative normal logic\nprograms only. We also give upper bounds on the numbers of stable partial,\nregular, and stable models of a Datalog$^\\neg$ program using the cardinality of\na feedback vertex set in its atom dependency graph. Interestingly, our\nconnection to Boolean network theory also points us to the notion of trap\nspaces for Datalog$^\\neg$ programs. We relate the notions of supported or\nstable trap spaces to the other semantics of Datalog$^\\neg$, and show the\nequivalence between subset-minimal stable trap spaces and regular models."}
{"id": "2504.15783", "pdf": "https://arxiv.org/pdf/2504.15783", "abs": "https://arxiv.org/abs/2504.15783", "authors": ["Johan Öfverstedt", "Elin Lundström", "Håkan Ahlström", "Joel Kullberg"], "title": "Towards prediction of morphological heart age from computed tomography angiography", "categories": ["cs.CV"], "comment": "24 pages", "summary": "Age prediction from medical images or other health-related non-imaging data\nis an important approach to data-driven aging research, providing knowledge of\nhow much information a specific tissue or organ carries about the chronological\nage of the individual. In this work, we studied the prediction of age from\ncomputed tomography angiography (CTA) images, which provide detailed\nrepresentations of the heart morphology, with the goals of (i) studying the\nrelationship between morphology and aging, and (ii) developing a novel\n\\emph{morphological heart age} biomarker. We applied an image\nregistration-based method that standardizes the images from the whole cohort\ninto a single space. We then extracted supervoxels (using unsupervised\nsegmentation), and corresponding robust features of density and local volume,\nwhich provide a detailed representation of the heart morphology while being\nrobust to registration errors. Machine learning models are then trained to fit\nregression models from these features to the chronological age. We applied the\nmethod to a subset of the images from the Swedish CArdioPulomonary bioImage\nStudy (SCAPIS) dataset, consisting of 721 females and 666 males. We observe a\nmean absolute error of $2.74$ years for females and $2.77$ years for males. The\npredictions from different sub-regions of interest were observed to be more\nhighly correlated with the predictions from the whole heart, compared to the\nchronological age, revealing a high consistency in the predictions from\nmorphology. Saliency analysis was also performed on the prediction models to\nstudy what regions are associated positively and negatively with the predicted\nage. This resulted in detailed association maps where the density and volume of\nknown, as well as some novel sub-regions of interest, are determined to be\nimportant. The saliency analysis aids in the interpretability of the models and\ntheir predictions."}
{"id": "2504.16056", "pdf": "https://arxiv.org/pdf/2504.16056", "abs": "https://arxiv.org/abs/2504.16056", "authors": ["Daniel Hendriks", "Philipp Spitzer", "Niklas Kühl", "Gerhard Satzger"], "title": "Honey, I Shrunk the Language Model: Impact of Knowledge Distillation Methods on Performance and Explainability", "categories": ["cs.CL"], "comment": null, "summary": "Artificial Intelligence (AI) has increasingly influenced modern society,\nrecently in particular through significant advancements in Large Language\nModels (LLMs). However, high computational and storage demands of LLMs still\nlimit their deployment in resource-constrained environments. Knowledge\ndistillation addresses this challenge by training a small student model from a\nlarger teacher model. Previous research has introduced several distillation\nmethods for both generating training data and for training the student model.\nDespite their relevance, the effects of state-of-the-art distillation methods\non model performance and explainability have not been thoroughly investigated\nand compared. In this work, we enlarge the set of available methods by applying\ncritique-revision prompting to distillation for data generation and by\nsynthesizing existing methods for training. For these methods, we provide a\nsystematic comparison based on the widely used Commonsense Question-Answering\n(CQA) dataset. While we measure performance via student model accuracy, we\nemploy a human-grounded study to evaluate explainability. We contribute new\ndistillation methods and their comparison in terms of both performance and\nexplainability. This should further advance the distillation of small language\nmodels and, thus, contribute to broader applicability and faster diffusion of\nLLM technology."}
{"id": "2504.15424", "pdf": "https://arxiv.org/pdf/2504.15424", "abs": "https://arxiv.org/abs/2504.15424", "authors": ["Nishath Rajiv Ranasinghe", "Shawn M. Jones", "Michal Kucer", "Ayan Biswas", "Daniel O'Malley", "Alexander Buschmann Most", "Selma Liliane Wanna", "Ajay Sreekumar"], "title": "LLM-Assisted Translation of Legacy FORTRAN Codes to C++: A Cross-Platform Study", "categories": ["cs.SE", "cs.AI", "I.2.2; I.2.7; D.2.3; D.2.4"], "comment": "12 pages, 7 figures, 2 tables", "summary": "Large Language Models (LLMs) are increasingly being leveraged for generating\nand translating scientific computer codes by both domain-experts and non-domain\nexperts. Fortran has served as one of the go to programming languages in legacy\nhigh-performance computing (HPC) for scientific discoveries. Despite growing\nadoption, LLM-based code translation of legacy code-bases has not been\nthoroughly assessed or quantified for its usability. Here, we studied the\napplicability of LLM-based translation of Fortran to C++ as a step towards\nbuilding an agentic-workflow using open-weight LLMs on two different\ncomputational platforms. We statistically quantified the compilation accuracy\nof the translated C++ codes, measured the similarity of the LLM translated code\nto the human translated C++ code, and statistically quantified the output\nsimilarity of the Fortran to C++ translation."}
{"id": "2504.15786", "pdf": "https://arxiv.org/pdf/2504.15786", "abs": "https://arxiv.org/abs/2504.15786", "authors": ["Ningli Xu", "Rongjun Qin"], "title": "Satellite to GroundScape -- Large-scale Consistent Ground View Generation from Satellite Views", "categories": ["cs.CV"], "comment": "8 figures", "summary": "Generating consistent ground-view images from satellite imagery is\nchallenging, primarily due to the large discrepancies in viewing angles and\nresolution between satellite and ground-level domains. Previous efforts mainly\nconcentrated on single-view generation, often resulting in inconsistencies\nacross neighboring ground views. In this work, we propose a novel cross-view\nsynthesis approach designed to overcome these challenges by ensuring\nconsistency across ground-view images generated from satellite views. Our\nmethod, based on a fixed latent diffusion model, introduces two conditioning\nmodules: satellite-guided denoising, which extracts high-level scene layout to\nguide the denoising process, and satellite-temporal denoising, which captures\ncamera motion to maintain consistency across multiple generated views. We\nfurther contribute a large-scale satellite-ground dataset containing over\n100,000 perspective pairs to facilitate extensive ground scene or video\ngeneration. Experimental results demonstrate that our approach outperforms\nexisting methods on perceptual and temporal metrics, achieving high\nphotorealism and consistency in multi-view outputs."}
{"id": "2504.16060", "pdf": "https://arxiv.org/pdf/2504.16060", "abs": "https://arxiv.org/abs/2504.16060", "authors": ["Ziqiao Ma", "Jing Ding", "Xuejun Zhang", "Dezhi Luo", "Jiahe Ding", "Sihan Xu", "Yuchen Huang", "Run Peng", "Joyce Chai"], "title": "Vision-Language Models Are Not Pragmatically Competent in Referring Expression Generation", "categories": ["cs.CL"], "comment": "Homepage: https://vlm-reg.github.io/", "summary": "Referring Expression Generation (REG) is a core task for evaluating the\npragmatic competence of vision-language systems, requiring not only accurate\nsemantic grounding but also adherence to principles of cooperative\ncommunication (Grice, 1975). However, current evaluations of vision-language\nmodels (VLMs) often overlook the pragmatic dimension, reducing REG to a\nregion-based captioning task and neglecting Gricean maxims. In this work, we\nrevisit REG from a pragmatic perspective, introducing a new dataset (RefOI) of\n1.5k images annotated with both written and spoken referring expressions.\nThrough a systematic evaluation of state-of-the-art VLMs, we identify three key\nfailures of pragmatic competence: (1) failure to uniquely identify the\nreferent, (2) inclusion of excessive or irrelevant information, and (3)\nmisalignment with human pragmatic preference, such as the underuse of minimal\nspatial cues. We also show that standard automatic evaluations fail to capture\nthese pragmatic violations, reinforcing superficial cues rather than genuine\nreferential success. Our findings call for a renewed focus on pragmatically\ninformed models and evaluation frameworks that align with real human\ncommunication."}
{"id": "2504.15425", "pdf": "https://arxiv.org/pdf/2504.15425", "abs": "https://arxiv.org/abs/2504.15425", "authors": ["Songyuan Zhang", "Oswin So", "Mitchell Black", "Zachary Serlin", "Chuchu Fan"], "title": "Solving Multi-Agent Safe Optimal Control with Distributed Epigraph Form MARL", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA", "math.OC"], "comment": "28 pages, 16 figures; Accepted by Robotics: Science and Systems 2025", "summary": "Tasks for multi-robot systems often require the robots to collaborate and\ncomplete a team goal while maintaining safety. This problem is usually\nformalized as a constrained Markov decision process (CMDP), which targets\nminimizing a global cost and bringing the mean of constraint violation below a\nuser-defined threshold. Inspired by real-world robotic applications, we define\nsafety as zero constraint violation. While many safe multi-agent reinforcement\nlearning (MARL) algorithms have been proposed to solve CMDPs, these algorithms\nsuffer from unstable training in this setting. To tackle this, we use the\nepigraph form for constrained optimization to improve training stability and\nprove that the centralized epigraph form problem can be solved in a distributed\nfashion by each agent. This results in a novel centralized training distributed\nexecution MARL algorithm named Def-MARL. Simulation experiments on 8 different\ntasks across 2 different simulators show that Def-MARL achieves the best\noverall performance, satisfies safety constraints, and maintains stable\ntraining. Real-world hardware experiments on Crazyflie quadcopters demonstrate\nthe ability of Def-MARL to safely coordinate agents to complete complex\ncollaborative tasks compared to other methods."}
{"id": "2504.15792", "pdf": "https://arxiv.org/pdf/2504.15792", "abs": "https://arxiv.org/abs/2504.15792", "authors": ["Dinh Nam Pham", "Torsten Rahne"], "title": "Development and evaluation of a deep learning algorithm for German word recognition from lip movements", "categories": ["cs.CV"], "comment": "English version of journal article in HNO 2022", "summary": "When reading lips, many people benefit from additional visual information\nfrom the lip movements of the speaker, which is, however, very error prone.\nAlgorithms for lip reading with artificial intelligence based on artificial\nneural networks significantly improve word recognition but are not available\nfor the German language. A total of 1806 video clips with only one\nGerman-speaking person each were selected, split into word segments, and\nassigned to word classes using speech-recognition software. In 38,391 video\nsegments with 32 speakers, 18 polysyllabic, visually distinguishable words were\nused to train and validate a neural network. The 3D Convolutional Neural\nNetwork and Gated Recurrent Units models and a combination of both models\n(GRUConv) were compared, as were different image sections and color spaces of\nthe videos. The accuracy was determined in 5000 training epochs. Comparison of\nthe color spaces did not reveal any relevant different correct classification\nrates in the range from 69% to 72%. With a cut to the lips, a significantly\nhigher accuracy of 70% was achieved than when cut to the entire speaker's face\n(34%). With the GRUConv model, the maximum accuracies were 87% with known\nspeakers and 63% in the validation with unknown speakers. The neural network\nfor lip reading, which was first developed for the German language, shows a\nvery high level of accuracy, comparable to English-language algorithms. It\nworks with unknown speakers as well and can be generalized with more word\nclasses."}
{"id": "2504.16063", "pdf": "https://arxiv.org/pdf/2504.16063", "abs": "https://arxiv.org/abs/2504.16063", "authors": ["A. Fronzetti Colladon", "R. Vestrelli"], "title": "A Python Tool for Reconstructing Full News Text from GDELT", "categories": ["cs.CL", "cs.DB", "cs.IR", "I.2.7; H.2.8; H.3.1"], "comment": null, "summary": "News data have become an essential resource across various disciplines,\nincluding economics, finance, management, social sciences, and computer\nscience. Researchers leverage newspaper articles to study economic trends,\nmarket dynamics, corporate strategies, public perception, political discourse,\nand the evolution of public opinion. Additionally, news datasets have been\ninstrumental in training large-scale language models, with applications in\nsentiment analysis, fake news detection, and automated news summarization.\nDespite their significance, access to comprehensive news corpora remains a key\nchallenge. Many full-text news providers, such as Factiva and LexisNexis,\nrequire costly subscriptions, while free alternatives often suffer from\nincomplete data and transparency issues. This paper presents a novel approach\nto obtaining full-text newspaper articles at near-zero cost by leveraging data\nfrom the Global Database of Events, Language, and Tone (GDELT). Specifically,\nwe focus on the GDELT Web News NGrams 3.0 dataset, which provides\nhigh-frequency updates of n-grams extracted from global online news sources. We\nprovide Python code to reconstruct full-text articles from these n-grams by\nidentifying overlapping textual fragments and intelligently merging them. Our\nmethod enables researchers to access structured, large-scale newspaper data for\ntext analysis while overcoming the limitations of existing proprietary\ndatasets. The proposed approach enhances the accessibility of news data for\nempirical research, facilitating applications in economic forecasting,\ncomputational social science, and natural language processing."}
{"id": "2504.15431", "pdf": "https://arxiv.org/pdf/2504.15431", "abs": "https://arxiv.org/abs/2504.15431", "authors": ["Sungjun Han", "Juyoung Suk", "Suyeong An", "Hyungguk Kim", "Kyuseok Kim", "Wonsuk Yang", "Seungtaek Choi", "Jamin Shin"], "title": "Trillion 7B Technical Report", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Preview version", "summary": "We introduce Trillion-7B, the most token-efficient Korean-centric\nmultilingual LLM available. Our novel Cross-lingual Document Attention (XLDA)\nmechanism enables highly efficient and effective knowledge transfer from\nEnglish to target languages like Korean and Japanese. Combined with optimized\ndata mixtures, language-specific filtering, and tailored tokenizer\nconstruction, Trillion-7B achieves competitive performance while dedicating\nonly 10\\% of its 2T training tokens to multilingual data and requiring just\n59.4K H100 GPU hours (\\$148K) for full training. Comprehensive evaluations\nacross 27 benchmarks in four languages demonstrate Trillion-7B's robust\nmultilingual performance and exceptional cross-lingual consistency."}
{"id": "2504.15796", "pdf": "https://arxiv.org/pdf/2504.15796", "abs": "https://arxiv.org/abs/2504.15796", "authors": ["Jiaqi Tang", "Yinsong Xu", "Qingchao Chen"], "title": "Locating and Mitigating Gradient Conflicts in Point Cloud Domain Adaptation via Saliency Map Skewness", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Object classification models utilizing point cloud data are fundamental for\n3D media understanding, yet they often struggle with unseen or\nout-of-distribution (OOD) scenarios. Existing point cloud unsupervised domain\nadaptation (UDA) methods typically employ a multi-task learning (MTL) framework\nthat combines primary classification tasks with auxiliary self-supervision\ntasks to bridge the gap between cross-domain feature distributions. However,\nour further experiments demonstrate that not all gradients from\nself-supervision tasks are beneficial and some may negatively impact the\nclassification performance. In this paper, we propose a novel solution, termed\nSaliency Map-based Data Sampling Block (SM-DSB), to mitigate these gradient\nconflicts. Specifically, our method designs a new scoring mechanism based on\nthe skewness of 3D saliency maps to estimate gradient conflicts without\nrequiring target labels. Leveraging this, we develop a sample selection\nstrategy that dynamically filters out samples whose self-supervision gradients\nare not beneficial for the classification. Our approach is scalable,\nintroducing modest computational overhead, and can be integrated into all the\npoint cloud UDA MTL frameworks. Extensive evaluations demonstrate that our\nmethod outperforms state-of-the-art approaches. In addition, we provide a new\nperspective on understanding the UDA problem through back-propagation analysis."}
{"id": "2504.16073", "pdf": "https://arxiv.org/pdf/2504.16073", "abs": "https://arxiv.org/abs/2504.16073", "authors": ["Zhiyuan Hu", "Shiyun Xiong", "Yifan Zhang", "See-Kiong Ng", "Anh Tuan Luu", "Bo An", "Shuicheng Yan", "Bryan Hooi"], "title": "Guiding VLM Agents with Process Rewards at Inference Time for GUI Navigation", "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in visual language models (VLMs) have notably enhanced\ntheir capabilities in handling complex Graphical User Interface (GUI)\ninteraction tasks. Despite these improvements, current frameworks often\nstruggle to generate correct actions in challenging GUI environments.\nState-of-the-art commercial VLMs are black-boxes, and fine-tuning open-source\nVLMs for GUI tasks requires significant resources. Additionally, existing\ntrajectory-level evaluation and refinement techniques frequently fall short due\nto delayed feedback and local optimization issues. To address these challenges,\nwe propose an approach that guides VLM agents with process supervision by a\nreward model during GUI navigation and control at inference time. This guidance\nallows the VLM agent to optimize actions at each inference step, thereby\nimproving performance in both static and dynamic environments. In particular,\nour method demonstrates significant performance gains in three GUI navigation\ntasks, achieving a 3.4% improvement in single step action accuracy for static\nenvironments, along with a around 33% increase in task success rate in one\ndynamic environment. With further integration of trajectory reflection and\nretry mechanisms, we also demonstrate even greater enhancement in task success."}
{"id": "2504.15440", "pdf": "https://arxiv.org/pdf/2504.15440", "abs": "https://arxiv.org/abs/2504.15440", "authors": ["Andrey Fradkin"], "title": "Demand for LLMs: Descriptive Evidence on Substitution, Market Expansion, and Multihoming", "categories": ["cs.CY", "cs.AI", "econ.GN", "q-fin.EC", "K.4; I.2"], "comment": null, "summary": "This paper documents three stylized facts about the demand for Large Language\nModels (LLMs) using data from OpenRouter, a prominent LLM marketplace. First,\nnew models experience rapid initial adoption that stabilizes within weeks.\nSecond, model releases differ substantially in whether they primarily attract\nnew users or substitute demand from competing models. Third, multihoming, using\nmultiple models simultaneously, is common among apps. These findings suggest\nsignificant horizontal and vertical differentiation in the LLM market, implying\nopportunities for providers to maintain demand and pricing power despite rapid\ntechnological advances."}
{"id": "2504.15823", "pdf": "https://arxiv.org/pdf/2504.15823", "abs": "https://arxiv.org/abs/2504.15823", "authors": ["Songyan Xie", "Jinghang Wen", "Encheng Su", "Qiucheng Yu"], "title": "Human-Imperceptible Physical Adversarial Attack for NIR Face Recognition Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Near-infrared (NIR) face recognition systems, which can operate effectively\nin low-light conditions or in the presence of makeup, exhibit vulnerabilities\nwhen subjected to physical adversarial attacks. To further demonstrate the\npotential risks in real-world applications, we design a novel, stealthy, and\npractical adversarial patch to attack NIR face recognition systems in a\nblack-box setting. We achieved this by utilizing human-imperceptible\ninfrared-absorbing ink to generate multiple patches with digitally optimized\nshapes and positions for infrared images. To address the optimization mismatch\nbetween digital and real-world NIR imaging, we develop a light reflection model\nfor human skin to minimize pixel-level discrepancies by simulating NIR light\nreflection.\n  Compared to state-of-the-art (SOTA) physical attacks on NIR face recognition\nsystems, the experimental results show that our method improves the attack\nsuccess rate in both digital and physical domains, particularly maintaining\neffectiveness across various face postures. Notably, the proposed approach\noutperforms SOTA methods, achieving an average attack success rate of 82.46% in\nthe physical domain across different models, compared to 64.18% for existing\nmethods. The artifact is available at\nhttps://anonymous.4open.science/r/Human-imperceptible-adversarial-patch-0703/."}
{"id": "2504.16074", "pdf": "https://arxiv.org/pdf/2504.16074", "abs": "https://arxiv.org/abs/2504.16074", "authors": ["Shi Qiu", "Shaoyang Guo", "Zhuo-Yang Song", "Yunbo Sun", "Zeyu Cai", "Jiashen Wei", "Tianyu Luo", "Yixuan Yin", "Haoxu Zhang", "Yi Hu", "Chenyang Wang", "Chencheng Tang", "Haoling Chang", "Qi Liu", "Ziheng Zhou", "Tianyu Zhang", "Jingtian Zhang", "Zhangyi Liu", "Minghao Li", "Yuku Zhang", "Boxuan Jing", "Xianqi Yin", "Yutong Ren", "Zizhuo Fu", "Weike Wang", "Xudong Tian", "Anqi Lv", "Laifu Man", "Jianxiang Li", "Feiyu Tao", "Qihua Sun", "Zhou Liang", "Yushu Mu", "Zhongxuan Li", "Jing-Jun Zhang", "Shutao Zhang", "Xiaotian Li", "Xingqi Xia", "Jiawei Lin", "Zheyu Shen", "Jiahang Chen", "Qiuhao Xiong", "Binran Wang", "Fengyuan Wang", "Ziyang Ni", "Bohan Zhang", "Fan Cui", "Changkun Shao", "Qing-Hong Cao", "Ming-xing Luo", "Muhan Zhang", "Hua Xing Zhu"], "title": "PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models", "categories": ["cs.CL"], "comment": "21 pages ,8 figures, 4 tables", "summary": "We introduce PHYBench, a novel, high-quality benchmark designed for\nevaluating reasoning capabilities of large language models (LLMs) in physical\ncontexts. PHYBench consists of 500 meticulously curated physics problems based\non real-world physical scenarios, designed to assess the ability of models to\nunderstand and reason about realistic physical processes. Covering mechanics,\nelectromagnetism, thermodynamics, optics, modern physics, and advanced physics,\nthe benchmark spans difficulty levels from high school exercises to\nundergraduate problems and Physics Olympiad challenges. Additionally, we\npropose the Expression Edit Distance (EED) Score, a novel evaluation metric\nbased on the edit distance between mathematical expressions, which effectively\ncaptures differences in model reasoning processes and results beyond\ntraditional binary scoring methods. We evaluate various LLMs on PHYBench and\ncompare their performance with human experts. Our results reveal that even\nstate-of-the-art reasoning models significantly lag behind human experts,\nhighlighting their limitations and the need for improvement in complex physical\nreasoning scenarios. Our benchmark results and dataset are publicly available\nat https://phybench-official.github.io/phybench-demo/."}
{"id": "2504.15485", "pdf": "https://arxiv.org/pdf/2504.15485", "abs": "https://arxiv.org/abs/2504.15485", "authors": ["Atin Pothiraj", "Elias Stengel-Eskin", "Jaemin Cho", "Mohit Bansal"], "title": "CAPTURe: Evaluating Spatial Reasoning in Vision Language Models via Occluded Object Counting", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Code and data: https://github.com/atinpothiraj/CAPTURe", "summary": "Recognizing and reasoning about occluded (partially or fully hidden) objects\nis vital to understanding visual scenes, as occlusions frequently occur in\nreal-world environments and act as obstacles for spatial comprehension. To test\nmodels' ability to reason about multiple occluded objects, we introduce a novel\ntask, Counting Amodally for Patterns Through Unseen REgions (CAPTURe), which\nrequires a model to count objects arranged in a pattern by inferring how the\npattern continues behind an occluder (an object which blocks parts of the\nscene). CAPTURe requires both recognizing visual patterns and reasoning, making\nit a useful testbed for evaluating vision-language models (VLMs) on whether\nthey understand occluded patterns and possess spatial understanding skills. By\nrequiring models to reason about occluded objects, CAPTURe also tests VLMs'\nability to form world models that would allow them to fill in missing\ninformation. CAPTURe consists of two parts: (1) CAPTURe-real, with manually\nfiltered images of real objects in patterns and (2) CAPTURe-synthetic, a\ncontrolled diagnostic with generated patterned images. We evaluate four strong\nVLMs (GPT-4o, Intern-VL2, Molmo, and Qwen2-VL) on CAPTURe, finding that models\nstruggle to count on both occluded and unoccluded patterns. Crucially, we find\nthat models perform worse with occlusion, suggesting that VLMs are also\ndeficient in inferring unseen spatial relationships: even the strongest VLMs\nlike GPT-4o fail to count with occlusion. In contrast, we find that humans\nachieve very little error on CAPTURe. We also find that providing auxiliary\ninformation of occluded object locations increases performance, underscoring\nthat the model error comes both from an inability to handle occlusion as well\nas difficulty counting in images."}
{"id": "2504.15835", "pdf": "https://arxiv.org/pdf/2504.15835", "abs": "https://arxiv.org/abs/2504.15835", "authors": ["Yiqian Wu", "Malte Prinzler", "Xiaogang Jin", "Siyu Tang"], "title": "Text-based Animatable 3D Avatars with Morphable Model Alignment", "categories": ["cs.CV"], "comment": null, "summary": "The generation of high-quality, animatable 3D head avatars from text has\nenormous potential in content creation applications such as games, movies, and\nembodied virtual assistants. Current text-to-3D generation methods typically\ncombine parametric head models with 2D diffusion models using score\ndistillation sampling to produce 3D-consistent results. However, they struggle\nto synthesize realistic details and suffer from misalignments between the\nappearance and the driving parametric model, resulting in unnatural animation\nresults. We discovered that these limitations stem from ambiguities in the 2D\ndiffusion predictions during 3D avatar distillation, specifically: i) the\navatar's appearance and geometry is underconstrained by the text input, and ii)\nthe semantic alignment between the predictions and the parametric head model is\ninsufficient because the diffusion model alone cannot incorporate information\nfrom the parametric model. In this work, we propose a novel framework,\nAnimPortrait3D, for text-based realistic animatable 3DGS avatar generation with\nmorphable model alignment, and introduce two key strategies to address these\nchallenges. First, we tackle appearance and geometry ambiguities by utilizing\nprior information from a pretrained text-to-3D model to initialize a 3D avatar\nwith robust appearance, geometry, and rigging relationships to the morphable\nmodel. Second, we refine the initial 3D avatar for dynamic expressions using a\nControlNet that is conditioned on semantic and normal maps of the morphable\nmodel to ensure accurate alignment. As a result, our method outperforms\nexisting approaches in terms of synthesis quality, alignment, and animation\nfidelity. Our experiments show that the proposed method advances the state of\nthe art in text-based, animatable 3D head avatar generation."}
{"id": "2504.16084", "pdf": "https://arxiv.org/pdf/2504.16084", "abs": "https://arxiv.org/abs/2504.16084", "authors": ["Yuxin Zuo", "Kaiyan Zhang", "Shang Qu", "Li Sheng", "Xuekai Zhu", "Biqing Qi", "Youbang Sun", "Ganqu Cui", "Ning Ding", "Bowen Zhou"], "title": "TTRL: Test-Time Reinforcement Learning", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This paper investigates Reinforcement Learning (RL) on data without explicit\nlabels for reasoning tasks in Large Language Models (LLMs). The core challenge\nof the problem is reward estimation during inference while not having access to\nground-truth information. While this setting appears elusive, we find that\ncommon practices in Test-Time Scaling (TTS), such as majority voting, yield\nsurprisingly effective rewards suitable for driving RL training. In this work,\nwe introduce Test-Time Reinforcement Learning (TTRL), a novel method for\ntraining LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs\nby utilizing the priors in the pre-trained models. Our experiments demonstrate\nthat TTRL consistently improves performance across a variety of tasks and\nmodels. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by\napproximately 159% on the AIME 2024 with only unlabeled test data. Furthermore,\nalthough TTRL is only supervised by the Maj@N metric, TTRL has demonstrated\nperformance to consistently surpass the upper limit of the initial model, and\napproach the performance of models trained directly on test data with\nground-truth labels. Our experimental findings validate the general\neffectiveness of TTRL across various tasks, and highlight TTRL's potential for\nbroader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL"}
{"id": "2504.15497", "pdf": "https://arxiv.org/pdf/2504.15497", "abs": "https://arxiv.org/abs/2504.15497", "authors": ["Noah Subedar", "Taeui Kim", "Saathwick Venkataramalingam"], "title": "Scalable APT Malware Classification via Parallel Feature Extraction and GPU-Accelerated Learning", "categories": ["cs.CR", "cs.AI", "I.2.0; I.2.6; K.6.5"], "comment": "26 pages, 54 figures, 14 tables", "summary": "This paper presents an underlying framework for both automating and\naccelerating malware classification, more specifically, mapping malicious\nexecutables to known Advanced Persistent Threat (APT) groups. The main feature\nof this analysis is the assembly-level instructions present in executables\nwhich are also known as opcodes. The collection of such opcodes on many\nmalicious samples is a lengthy process; hence, open-source reverse engineering\ntools are used in tandem with scripts that leverage parallel computing to\nanalyze multiple files at once. Traditional and deep learning models are\napplied to create models capable of classifying malware samples. One-gram and\ntwo-gram datasets are constructed and used to train models such as SVM, KNN,\nand Decision Tree; however, they struggle to provide adequate results without\nrelying on metadata to support n-gram sequences. The computational limitations\nof such models are overcome with convolutional neural networks (CNNs) and\nheavily accelerated using graphical compute unit (GPU) resources."}
{"id": "2504.15863", "pdf": "https://arxiv.org/pdf/2504.15863", "abs": "https://arxiv.org/abs/2504.15863", "authors": ["Diego de Oliveira Hitzges", "Suman Ghosh", "Guillermo Gallego"], "title": "DERD-Net: Learning Depth from Event-based Ray Densities", "categories": ["cs.CV", "cs.LG", "cs.RO", "eess.SP"], "comment": "13 pages, 3 figures, 14 tables. Project page:\n  https://github.com/tub-rip/DERD-Net", "summary": "Event cameras offer a promising avenue for multi-view stereo depth estimation\nand Simultaneous Localization And Mapping (SLAM) due to their ability to detect\nblur-free 3D edges at high-speed and over broad illumination conditions.\nHowever, traditional deep learning frameworks designed for conventional cameras\nstruggle with the asynchronous, stream-like nature of event data, as their\narchitectures are optimized for discrete, image-like inputs. We propose a\nscalable, flexible and adaptable framework for pixel-wise depth estimation with\nevent cameras in both monocular and stereo setups. The 3D scene structure is\nencoded into disparity space images (DSIs), representing spatial densities of\nrays obtained by back-projecting events into space via known camera poses. Our\nneural network processes local subregions of the DSIs combining 3D convolutions\nand a recurrent structure to recognize valuable patterns for depth prediction.\nLocal processing enables fast inference with full parallelization and ensures\nconstant ultra-low model complexity and memory costs, regardless of camera\nresolution. Experiments on standard benchmarks (MVSEC and DSEC datasets)\ndemonstrate unprecedented effectiveness: (i) using purely monocular data, our\nmethod achieves comparable results to existing stereo methods; (ii) when\napplied to stereo data, it strongly outperforms all state-of-the-art (SOTA)\napproaches, reducing the mean absolute error by at least 42%; (iii) our method\nalso allows for increases in depth completeness by more than 3-fold while still\nyielding a reduction in median absolute error of at least 30%. Given its\nremarkable performance and effective processing of event-data, our framework\nholds strong potential to become a standard approach for using deep learning\nfor event-based depth estimation and SLAM. Project page:\nhttps://github.com/tub-rip/DERD-Net"}
{"id": "2504.15330", "pdf": "https://arxiv.org/pdf/2504.15330", "abs": "https://arxiv.org/abs/2504.15330", "authors": ["Mohit Gupta", "Akiko Aizawa", "Rajiv Ratn Shah"], "title": "Med-CoDE: Medical Critique based Disagreement Evaluation Framework", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "8 pages, 4 figures, NAACL SRW 2025", "summary": "The emergence of large language models (LLMs) has significantly influenced\nnumerous fields, including healthcare, by enhancing the capabilities of\nautomated systems to process and generate human-like text. However, despite\ntheir advancements, the reliability and accuracy of LLMs in medical contexts\nremain critical concerns. Current evaluation methods often lack robustness and\nfail to provide a comprehensive assessment of LLM performance, leading to\npotential risks in clinical settings. In this work, we propose Med-CoDE, a\nspecifically designed evaluation framework for medical LLMs to address these\nchallenges. The framework leverages a critique-based approach to quantitatively\nmeasure the degree of disagreement between model-generated responses and\nestablished medical ground truths. This framework captures both accuracy and\nreliability in medical settings. The proposed evaluation framework aims to fill\nthe existing gap in LLM assessment by offering a systematic method to evaluate\nthe quality and trustworthiness of medical LLMs. Through extensive experiments\nand case studies, we illustrate the practicality of our framework in providing\na comprehensive and reliable evaluation of medical LLMs."}
{"id": "2504.15499", "pdf": "https://arxiv.org/pdf/2504.15499", "abs": "https://arxiv.org/abs/2504.15499", "authors": ["James Mickens", "Sarah Radway", "Ravi Netravali"], "title": "Guillotine: Hypervisors for Isolating Malicious AIs", "categories": ["cs.CR", "cs.AI", "cs.OS"], "comment": "To be published in the ACM SIGOPS 2025 Workshop on Hot Topics in\n  Operating Systems", "summary": "As AI models become more embedded in critical sectors like finance,\nhealthcare, and the military, their inscrutable behavior poses ever-greater\nrisks to society. To mitigate this risk, we propose Guillotine, a hypervisor\narchitecture for sandboxing powerful AI models -- models that, by accident or\nmalice, can generate existential threats to humanity. Although Guillotine\nborrows some well-known virtualization techniques, Guillotine must also\nintroduce fundamentally new isolation mechanisms to handle the unique threat\nmodel posed by existential-risk AIs. For example, a rogue AI may try to\nintrospect upon hypervisor software or the underlying hardware substrate to\nenable later subversion of that control plane; thus, a Guillotine hypervisor\nrequires careful co-design of the hypervisor software and the CPUs, RAM, NIC,\nand storage devices that support the hypervisor software, to thwart side\nchannel leakage and more generally eliminate mechanisms for AI to exploit\nreflection-based vulnerabilities. Beyond such isolation at the software,\nnetwork, and microarchitectural layers, a Guillotine hypervisor must also\nprovide physical fail-safes more commonly associated with nuclear power plants,\navionic platforms, and other types of mission critical systems. Physical\nfail-safes, e.g., involving electromechanical disconnection of network cables,\nor the flooding of a datacenter which holds a rogue AI, provide defense in\ndepth if software, network, and microarchitectural isolation is compromised and\na rogue AI must be temporarily shut down or permanently destroyed."}
{"id": "2504.15865", "pdf": "https://arxiv.org/pdf/2504.15865", "abs": "https://arxiv.org/abs/2504.15865", "authors": ["Lotfi Abdelkrim Mecharbat", "Ibrahim Elmakky", "Martin Takac", "Mohammed Yaqub"], "title": "MedNNS: Supernet-based Medical Task-Adaptive Neural Network Search", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Deep learning (DL) has achieved remarkable progress in the field of medical\nimaging. However, adapting DL models to medical tasks remains a significant\nchallenge, primarily due to two key factors: (1) architecture selection, as\ndifferent tasks necessitate specialized model designs, and (2) weight\ninitialization, which directly impacts the convergence speed and final\nperformance of the models. Although transfer learning from ImageNet is a widely\nadopted strategy, its effectiveness is constrained by the substantial\ndifferences between natural and medical images. To address these challenges, we\nintroduce Medical Neural Network Search (MedNNS), the first Neural Network\nSearch framework for medical imaging applications. MedNNS jointly optimizes\narchitecture selection and weight initialization by constructing a meta-space\nthat encodes datasets and models based on how well they perform together. We\nbuild this space using a Supernetwork-based approach, expanding the model zoo\nsize by 51x times over previous state-of-the-art (SOTA) methods. Moreover, we\nintroduce rank loss and Fr\\'echet Inception Distance (FID) loss into the\nconstruction of the space to capture inter-model and inter-dataset\nrelationships, thereby achieving more accurate alignment in the meta-space.\nExperimental results across multiple datasets demonstrate that MedNNS\nsignificantly outperforms both ImageNet pre-trained DL models and SOTA Neural\nArchitecture Search (NAS) methods, achieving an average accuracy improvement of\n1.7% across datasets while converging substantially faster. The code and the\nprocessed meta-space is available at https://github.com/BioMedIA-MBZUAI/MedNNS."}
{"id": "2504.15362", "pdf": "https://arxiv.org/pdf/2504.15362", "abs": "https://arxiv.org/abs/2504.15362", "authors": ["Yuan-Hong Liao", "Sven Elflein", "Liu He", "Laura Leal-Taixé", "Yejin Choi", "Sanja Fidler", "David Acuna"], "title": "LongPerceptualThoughts: Distilling System-2 Reasoning for System-1 Perception", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "24 pages, 10 figures, in submission. Project page:\n  https://andrewliao11.github.io/LongPerceptualThoughts", "summary": "Recent reasoning models through test-time scaling have demonstrated that long\nchain-of-thoughts can unlock substantial performance boosts in hard reasoning\ntasks such as math and code. However, the benefit of such long thoughts for\nsystem-2 reasoning is relatively less explored in other domains such as\nperceptual tasks where shallower, system-1 reasoning seems sufficient. In this\npaper, we introduce LongPerceptualThoughts, a new synthetic dataset with 30K\nlong-thought traces for perceptual tasks. The key challenges in synthesizing\nelaborate reasoning thoughts for perceptual tasks are that off-the-shelf models\nare not yet equipped with such thinking behavior and that it is not\nstraightforward to build a reliable process verifier for perceptual tasks.\nThus, we propose a novel three-stage data synthesis framework that first\nsynthesizes verifiable multiple-choice questions from dense image descriptions,\nthen extracts simple CoTs from VLMs for those verifiable problems, and finally\nexpands those simple thoughts to elaborate long thoughts via frontier reasoning\nmodels. In controlled experiments with a strong instruction-tuned 7B model, we\ndemonstrate notable improvements over existing visual reasoning data-generation\nmethods. Our model, trained on the generated dataset, achieves an average +3.4\npoints improvement over 5 vision-centric benchmarks, including +11.8 points on\nV$^*$ Bench. Notably, despite being tuned for vision tasks, it also improves\nperformance on the text reasoning benchmark, MMLU-Pro, by +2 points."}
{"id": "2504.15515", "pdf": "https://arxiv.org/pdf/2504.15515", "abs": "https://arxiv.org/abs/2504.15515", "authors": ["Wuchen Li"], "title": "Transport f divergences", "categories": ["math.ST", "cs.AI", "cs.IT", "math.IT", "stat.TH"], "comment": "Comments are welcome", "summary": "We define a class of divergences to measure differences between probability\ndensity functions in one-dimensional sample space. The construction is based on\nthe convex function with the Jacobi operator of mapping function that\npushforwards one density to the other. We call these information measures {\\em\ntransport $f$-divergences}. We present several properties of transport\n$f$-divergences, including invariances, convexities, variational formulations,\nand Taylor expansions in terms of mapping functions. Examples of transport\n$f$-divergences in generative models are provided."}
{"id": "2504.15883", "pdf": "https://arxiv.org/pdf/2504.15883", "abs": "https://arxiv.org/abs/2504.15883", "authors": ["Farida Mohsen", "Samir Belhaouari", "Zubair Shah"], "title": "Integrating Non-Linear Radon Transformation for Diabetic Retinopathy Grading", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Diabetic retinopathy is a serious ocular complication that poses a\nsignificant threat to patients' vision and overall health. Early detection and\naccurate grading are essential to prevent vision loss. Current automatic\ngrading methods rely heavily on deep learning applied to retinal fundus images,\nbut the complex, irregular patterns of lesions in these images, which vary in\nshape and distribution, make it difficult to capture subtle changes. This study\nintroduces RadFuse, a multi-representation deep learning framework that\nintegrates non-linear RadEx-transformed sinogram images with traditional fundus\nimages to enhance diabetic retinopathy detection and grading. Our RadEx\ntransformation, an optimized non-linear extension of the Radon transform,\ngenerates sinogram representations to capture complex retinal lesion patterns.\nBy leveraging both spatial and transformed domain information, RadFuse enriches\nthe feature set available to deep learning models, improving the\ndifferentiation of severity levels. We conducted extensive experiments on two\nbenchmark datasets, APTOS-2019 and DDR, using three convolutional neural\nnetworks (CNNs): ResNeXt-50, MobileNetV2, and VGG19. RadFuse showed significant\nimprovements over fundus-image-only models across all three CNN architectures\nand outperformed state-of-the-art methods on both datasets. For severity\ngrading across five stages, RadFuse achieved a quadratic weighted kappa of\n93.24%, an accuracy of 87.07%, and an F1-score of 87.17%. In binary\nclassification between healthy and diabetic retinopathy cases, the method\nreached an accuracy of 99.09%, precision of 98.58%, and recall of 99.6%,\nsurpassing previously established models. These results demonstrate RadFuse's\ncapacity to capture complex non-linear features, advancing diabetic retinopathy\nclassification and promoting the integration of advanced mathematical\ntransforms in medical image analysis."}
{"id": "2504.15376", "pdf": "https://arxiv.org/pdf/2504.15376", "abs": "https://arxiv.org/abs/2504.15376", "authors": ["Zhiqiu Lin", "Siyuan Cen", "Daniel Jiang", "Jay Karhade", "Hewei Wang", "Chancharik Mitra", "Tiffany Ling", "Yuhan Huang", "Sifan Liu", "Mingyu Chen", "Rushikesh Zawar", "Xue Bai", "Yilun Du", "Chuang Gan", "Deva Ramanan"], "title": "Towards Understanding Camera Motions in Any Video", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "Project site: https://linzhiqiu.github.io/papers/camerabench/", "summary": "We introduce CameraBench, a large-scale dataset and benchmark designed to\nassess and improve camera motion understanding. CameraBench consists of ~3,000\ndiverse internet videos, annotated by experts through a rigorous multi-stage\nquality control process. One of our contributions is a taxonomy of camera\nmotion primitives, designed in collaboration with cinematographers. We find,\nfor example, that some motions like \"follow\" (or tracking) require\nunderstanding scene content like moving subjects. We conduct a large-scale\nhuman study to quantify human annotation performance, revealing that domain\nexpertise and tutorial-based training can significantly enhance accuracy. For\nexample, a novice may confuse zoom-in (a change of intrinsics) with translating\nforward (a change of extrinsics), but can be trained to differentiate the two.\nUsing CameraBench, we evaluate Structure-from-Motion (SfM) and Video-Language\nModels (VLMs), finding that SfM models struggle to capture semantic primitives\nthat depend on scene content, while VLMs struggle to capture geometric\nprimitives that require precise estimation of trajectories. We then fine-tune a\ngenerative VLM on CameraBench to achieve the best of both worlds and showcase\nits applications, including motion-augmented captioning, video question\nanswering, and video-text retrieval. We hope our taxonomy, benchmark, and\ntutorials will drive future efforts towards the ultimate goal of understanding\ncamera motions in any video."}
{"id": "2504.15524", "pdf": "https://arxiv.org/pdf/2504.15524", "abs": "https://arxiv.org/abs/2504.15524", "authors": ["Qiyao Wang", "Guhong Chen", "Hongbo Wang", "Huaren Liu", "Minghui Zhu", "Zhifei Qin", "Linwei Li", "Yilin Yue", "Shiqiang Wang", "Jiayan Li", "Yihang Wu", "Ziqiang Liu", "Longze Chen", "Run Luo", "Liyang Fan", "Jiaming Li", "Lei Zhang", "Kan Xu", "Hongfei Lin", "Hamid Alinejad-Rokny", "Shiwen Ni", "Yuan Lin", "Min Yang"], "title": "IPBench: Benchmarking the Knowledge of Large Language Models in Intellectual Property", "categories": ["cs.CL", "cs.AI"], "comment": "89 pages, 75 figures, 55 tables", "summary": "Intellectual Property (IP) is a unique domain that integrates technical and\nlegal knowledge, making it inherently complex and knowledge-intensive. As large\nlanguage models (LLMs) continue to advance, they show great potential for\nprocessing IP tasks, enabling more efficient analysis, understanding, and\ngeneration of IP-related content. However, existing datasets and benchmarks\neither focus narrowly on patents or cover limited aspects of the IP field,\nlacking alignment with real-world scenarios. To bridge this gap, we introduce\nthe first comprehensive IP task taxonomy and a large, diverse bilingual\nbenchmark, IPBench, covering 8 IP mechanisms and 20 tasks. This benchmark is\ndesigned to evaluate LLMs in real-world intellectual property applications,\nencompassing both understanding and generation. We benchmark 16 LLMs, ranging\nfrom general-purpose to domain-specific models, and find that even the\nbest-performing model achieves only 75.8% accuracy, revealing substantial room\nfor improvement. Notably, open-source IP and law-oriented models lag behind\nclosed-source general-purpose models. We publicly release all data and code of\nIPBench and will continue to update it with additional IP-related tasks to\nbetter reflect real-world challenges in the intellectual property domain."}
{"id": "2504.15888", "pdf": "https://arxiv.org/pdf/2504.15888", "abs": "https://arxiv.org/abs/2504.15888", "authors": ["Zhiqiang Wei", "Lianqing Zheng", "Jianan Liu", "Tao Huang", "Qing-Long Han", "Wenwen Zhang", "Fengdeng Zhang"], "title": "MS-Occ: Multi-Stage LiDAR-Camera Fusion for 3D Semantic Occupancy Prediction", "categories": ["cs.CV"], "comment": "8 pages, 5 figures", "summary": "Accurate 3D semantic occupancy perception is essential for autonomous driving\nin complex environments with diverse and irregular objects. While\nvision-centric methods suffer from geometric inaccuracies, LiDAR-based\napproaches often lack rich semantic information. To address these limitations,\nMS-Occ, a novel multi-stage LiDAR-camera fusion framework which includes\nmiddle-stage fusion and late-stage fusion, is proposed, integrating LiDAR's\ngeometric fidelity with camera-based semantic richness via hierarchical\ncross-modal fusion. The framework introduces innovations at two critical\nstages: (1) In the middle-stage feature fusion, the Gaussian-Geo module\nleverages Gaussian kernel rendering on sparse LiDAR depth maps to enhance 2D\nimage features with dense geometric priors, and the Semantic-Aware module\nenriches LiDAR voxels with semantic context via deformable cross-attention; (2)\nIn the late-stage voxel fusion, the Adaptive Fusion (AF) module dynamically\nbalances voxel features across modalities, while the High Classification\nConfidence Voxel Fusion (HCCVF) module resolves semantic inconsistencies using\nself-attention-based refinement. Experiments on the nuScenes-OpenOccupancy\nbenchmark show that MS-Occ achieves an Intersection over Union (IoU) of 32.1%\nand a mean IoU (mIoU) of 25.3%, surpassing the state-of-the-art by +0.7% IoU\nand +2.4% mIoU. Ablation studies further validate the contribution of each\nmodule, with substantial improvements in small-object perception, demonstrating\nthe practical value of MS-Occ for safety-critical autonomous driving scenarios."}
{"id": "2504.15415", "pdf": "https://arxiv.org/pdf/2504.15415", "abs": "https://arxiv.org/abs/2504.15415", "authors": ["David Ma", "Yuanxing Zhang", "Jincheng Ren", "Jarvis Guo", "Yifan Yao", "Zhenlin Wei", "Zhenzhu Yang", "Zhongyuan Peng", "Boyu Feng", "Jun Ma", "Xiao Gu", "Zhoufutu Wen", "King Zhu", "Yancheng He", "Meng Cao", "Shiwen Ni", "Jiaheng Liu", "Wenhao Huang", "Ge Zhang", "Xiaojie Jin"], "title": "IV-Bench: A Benchmark for Image-Grounded Video Perception and Reasoning in Multimodal LLMs", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Existing evaluation frameworks for Multimodal Large Language Models (MLLMs)\nprimarily focus on image reasoning or general video understanding tasks,\nlargely overlooking the significant role of image context in video\ncomprehension. To bridge this gap, we propose IV-Bench, the first comprehensive\nbenchmark for evaluating Image-Grounded Video Perception and Reasoning.\nIV-Bench consists of 967 videos paired with 2,585 meticulously annotated\nimage-text queries across 13 tasks (7 perception and 6 reasoning tasks) and 5\nrepresentative categories. Extensive evaluations of state-of-the-art\nopen-source (e.g., InternVL2.5, Qwen2.5-VL) and closed-source (e.g., GPT-4o,\nGemini2-Flash and Gemini2-Pro) MLLMs demonstrate that current models\nsubstantially underperform in image-grounded video Perception and Reasoning,\nmerely achieving at most 28.9% accuracy. Further analysis reveals key factors\ninfluencing model performance on IV-Bench, including inference pattern, frame\nnumber, and resolution. Additionally, through a simple data synthesis approach,\nwe demonstratethe challenges of IV- Bench extend beyond merely aligning the\ndata format in the training proecss. These findings collectively provide\nvaluable insights for future research. Our codes and data are released in\nhttps://github.com/multimodal-art-projection/IV-Bench."}
{"id": "2504.15546", "pdf": "https://arxiv.org/pdf/2504.15546", "abs": "https://arxiv.org/abs/2504.15546", "authors": ["Jayachandu Bandlamudi", "Ritwik Chaudhuri", "Neelamadhav Gantayat", "Kushal Mukherjee", "Prerna Agarwal", "Renuka Sindhgatta", "Sameep Mehta"], "title": "A Framework for Testing and Adapting REST APIs as LLM Tools", "categories": ["cs.SE", "cs.AI", "I.2.7"], "comment": null, "summary": "Large Language Models (LLMs) are enabling autonomous agents to perform\ncomplex workflows using external tools or functions, often provided via REST\nAPIs in enterprise systems. However, directly utilizing these APIs as tools\nposes challenges due to their complex input schemas, elaborate responses, and\noften ambiguous documentation. Current benchmarks for tool testing do not\nadequately address these complexities, leading to a critical gap in evaluating\nAPI readiness for agent-driven automation. In this work, we present a novel\ntesting framework aimed at evaluating and enhancing the readiness of REST APIs\nto function as tools for LLM-based agents. Our framework transforms apis as\ntools, generates comprehensive test cases for the APIs, translates tests cases\ninto natural language instructions suitable for agents, enriches tool\ndefinitions and evaluates the agent's ability t correctly invoke the API and\nprocess its inputs and responses. To provide actionable insights, we analyze\nthe outcomes of 750 test cases, presenting a detailed taxonomy of errors,\nincluding input misinterpretation, output handling inconsistencies, and schema\nmismatches. Additionally, we classify these test cases to streamline debugging\nand refinement of tool integrations. This work offers a foundational step\ntoward enabling enterprise APIs as tools, improving their usability in\nagent-based applications."}
{"id": "2504.15918", "pdf": "https://arxiv.org/pdf/2504.15918", "abs": "https://arxiv.org/abs/2504.15918", "authors": ["Chang Zong", "Bin Li", "Shoujun Zhou", "Jian Wan", "Lei Zhang"], "title": "Ask2Loc: Learning to Locate Instructional Visual Answers by Asking Questions", "categories": ["cs.CV", "cs.AI", "cs.HC", "68T45, 68T20"], "comment": "16 pages, 8 figures", "summary": "Locating specific segments within an instructional video is an efficient way\nto acquire guiding knowledge. Generally, the task of obtaining video segments\nfor both verbal explanations and visual demonstrations is known as visual\nanswer localization (VAL). However, users often need multiple interactions to\nobtain answers that align with their expectations when using the system. During\nthese interactions, humans deepen their understanding of the video content by\nasking themselves questions, thereby accurately identifying the location.\nTherefore, we propose a new task, named In-VAL, to simulate the multiple\ninteractions between humans and videos in the procedure of obtaining visual\nanswers. The In-VAL task requires interactively addressing several semantic gap\nissues, including 1) the ambiguity of user intent in the input questions, 2)\nthe incompleteness of language in video subtitles, and 3) the fragmentation of\ncontent in video segments. To address these issues, we propose Ask2Loc, a\nframework for resolving In-VAL by asking questions. It includes three key\nmodules: 1) a chatting module to refine initial questions and uncover clear\nintentions, 2) a rewriting module to generate fluent language and create\ncomplete descriptions, and 3) a searching module to broaden local context and\nprovide integrated content. We conduct extensive experiments on three\nreconstructed In-VAL datasets. Compared to traditional end-to-end and two-stage\nmethods, our proposed Ask2Loc can improve performance by up to 14.91 (mIoU) on\nthe In-VAL task. Our code and datasets can be accessed at\nhttps://github.com/changzong/Ask2Loc."}
{"id": "2504.15448", "pdf": "https://arxiv.org/pdf/2504.15448", "abs": "https://arxiv.org/abs/2504.15448", "authors": ["Yanampally Abhiram Reddy", "Siddhi Agarwal", "Vikram Parashar", "Arshiya Arora"], "title": "Real-Time Sentiment Insights from X Using VADER, DistilBERT, and Web-Scraped Data", "categories": ["econ.GN", "cs.CL", "q-fin.EC"], "comment": "19 pages, 2 figures", "summary": "In the age of social media, understanding public sentiment toward major\ncorporations is crucial for investors, policymakers, and researchers. This\npaper presents a comprehensive sentiment analysis system tailored for corporate\nreputation monitoring, combining Natural Language Processing (NLP) and machine\nlearning techniques to accurately interpret public opinion in real time. The\nmethodology integrates a hybrid sentiment detection framework leveraging both\nrule-based models (VADER) and transformer-based deep learning models\n(DistilBERT), applied to social media data from multiple platforms. The system\nbegins with robust preprocessing involving noise removal and text\nnormalization, followed by sentiment classification using an ensemble approach\nto ensure both interpretability and contextual accuracy. Results are visualized\nthrough sentiment distribution plots, comparative analyses, and temporal\nsentiment trends for enhanced interpretability. Our analysis reveals\nsignificant disparities in public sentiment across major corporations, with\ncompanies like Amazon (81.2) and Samsung (45.8) receiving excellent sentiment\nscores, while Microsoft (21.7) and Walmart (21.9) exhibit poor sentiment\nprofiles. These findings demonstrate the utility of our multi-source sentiment\nframework in providing actionable insights regarding corporate public\nperception, enabling stakeholders to make informed strategic decisions based on\ncomprehensive sentiment analysis."}
{"id": "2504.15549", "pdf": "https://arxiv.org/pdf/2504.15549", "abs": "https://arxiv.org/abs/2504.15549", "authors": ["Anjali Khurana", "Xiaotian Su", "April Yi Wang", "Parmit K Chilana"], "title": "Do It For Me vs. Do It With Me: Investigating User Perceptions of Different Paradigms of Automation in Copilots for Feature-Rich Software", "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": "Accepted for publication in the CHI Conference on Human Factors in\n  Computing Systems (CHI 2025), April 26 - May 1, 2025, Yokohama, Japan", "summary": "Large Language Model (LLM)-based in-application assistants, or copilots, can\nautomate software tasks, but users often prefer learning by doing, raising\nquestions about the optimal level of automation for an effective user\nexperience. We investigated two automation paradigms by designing and\nimplementing a fully automated copilot (AutoCopilot) and a semi-automated\ncopilot (GuidedCopilot) that automates trivial steps while offering\nstep-by-step visual guidance. In a user study (N=20) across data analysis and\nvisual design tasks, GuidedCopilot outperformed AutoCopilot in user control,\nsoftware utility, and learnability, especially for exploratory and creative\ntasks, while AutoCopilot saved time for simpler visual tasks. A follow-up\ndesign exploration (N=10) enhanced GuidedCopilot with task-and state-aware\nfeatures, including in-context preview clips and adaptive instructions. Our\nfindings highlight the critical role of user control and tailored guidance in\ndesigning the next generation of copilots that enhance productivity, support\ndiverse skill levels, and foster deeper software engagement."}
{"id": "2504.15921", "pdf": "https://arxiv.org/pdf/2504.15921", "abs": "https://arxiv.org/abs/2504.15921", "authors": ["Jian Hu", "Dimitrios Korkinof", "Shaogang Gong", "Mariano Beguerisse-Diaz"], "title": "ViSMaP: Unsupervised Hour-long Video Summarisation by Meta-Prompting", "categories": ["cs.CV"], "comment": null, "summary": "We introduce ViSMap: Unsupervised Video Summarisation by Meta Prompting, a\nsystem to summarise hour long videos with no-supervision. Most existing video\nunderstanding models work well on short videos of pre-segmented events, yet\nthey struggle to summarise longer videos where relevant events are sparsely\ndistributed and not pre-segmented. Moreover, long-form video understanding\noften relies on supervised hierarchical training that needs extensive\nannotations which are costly, slow and prone to inconsistency. With ViSMaP we\nbridge the gap between short videos (where annotated data is plentiful) and\nlong ones (where it's not). We rely on LLMs to create optimised\npseudo-summaries of long videos using segment descriptions from short ones.\nThese pseudo-summaries are used as training data for a model that generates\nlong-form video summaries, bypassing the need for expensive annotations of long\nvideos. Specifically, we adopt a meta-prompting strategy to iteratively\ngenerate and refine creating pseudo-summaries of long videos. The strategy\nleverages short clip descriptions obtained from a supervised short video model\nto guide the summary. Each iteration uses three LLMs working in sequence: one\nto generate the pseudo-summary from clip descriptions, another to evaluate it,\nand a third to optimise the prompt of the generator. This iteration is\nnecessary because the quality of the pseudo-summaries is highly dependent on\nthe generator prompt, and varies widely among videos. We evaluate our summaries\nextensively on multiple datasets; our results show that ViSMaP achieves\nperformance comparable to fully supervised state-of-the-art models while\ngeneralising across domains without sacrificing performance. Code will be\nreleased upon publication."}
{"id": "2504.15466", "pdf": "https://arxiv.org/pdf/2504.15466", "abs": "https://arxiv.org/abs/2504.15466", "authors": ["Jiayi Pan", "Xiuyu Li", "Long Lian", "Charlie Snell", "Yifei Zhou", "Adam Yala", "Trevor Darrell", "Kurt Keutzer", "Alane Suhr"], "title": "Learning Adaptive Parallel Reasoning with Language Models", "categories": ["cs.AI", "cs.CL"], "comment": "Code, model, and data are available at\n  https://github.com/Parallel-Reasoning/APR. The first three authors\n  contributed equally to this work", "summary": "Scaling inference-time computation has substantially improved the reasoning\ncapabilities of language models. However, existing methods have significant\nlimitations: serialized chain-of-thought approaches generate overly long\noutputs, leading to increased latency and exhausted context windows, while\nparallel methods such as self-consistency suffer from insufficient\ncoordination, resulting in redundant computations and limited performance\ngains. To address these shortcomings, we propose Adaptive Parallel Reasoning\n(APR), a novel reasoning framework that enables language models to orchestrate\nboth serialized and parallel computations end-to-end. APR generalizes existing\nreasoning methods by enabling adaptive multi-threaded inference using spawn()\nand join() operations. A key innovation is our end-to-end reinforcement\nlearning strategy, optimizing both parent and child inference threads to\nenhance task success rate without requiring predefined reasoning structures.\nExperiments on the Countdown reasoning task demonstrate significant benefits of\nAPR: (1) higher performance within the same context window (83.4% vs. 60.0% at\n4k context); (2) superior scalability with increased computation (80.1% vs.\n66.6% at 20k total tokens); (3) improved accuracy at equivalent latency (75.2%\nvs. 57.3% at approximately 5,000ms). APR represents a step towards enabling\nlanguage models to autonomously optimize their reasoning processes through\nadaptive allocation of computation."}
{"id": "2504.15564", "pdf": "https://arxiv.org/pdf/2504.15564", "abs": "https://arxiv.org/abs/2504.15564", "authors": ["Musfiqur Rahman", "SayedHassan Khatoonabadi", "Emad Shihab"], "title": "A Large-scale Class-level Benchmark Dataset for Code Generation with LLMs", "categories": ["cs.SE", "cs.AI", "cs.LG"], "comment": "This paper was submitted to the 29th International Conference on\n  Evaluation and Assessment in Software Engineering (EASE 2025) AI models/data\n  track", "summary": "Recent advancements in large language models (LLMs) have demonstrated\npromising capabilities in code generation tasks. However, most existing\nbenchmarks focus on isolated functions and fail to capture the complexity of\nreal-world, class-level software structures. To address this gap, we introduce\na large-scale, Python class-level dataset curated from $13{,}174$ real-world\nopen-source projects. The dataset contains over 842,000 class skeletons, each\nincluding class and method signatures, along with associated docstrings when\navailable. We preserve structural and contextual dependencies critical to\nrealistic software development scenarios and enrich the dataset with static\ncode metrics to support downstream analysis. To evaluate the usefulness of this\ndataset, we use extracted class skeletons as prompts for GPT-4 to generate full\nclass implementations. Results show that the LLM-generated classes exhibit\nstrong lexical and structural similarity to human-written counterparts, with\naverage ROUGE@L, BLEU, and TSED scores of 0.80, 0.59, and 0.73, respectively.\nThese findings confirm that well-structured prompts derived from real-world\nclass skeletons significantly enhance LLM performance in class-level code\ngeneration. This dataset offers a valuable resource for benchmarking, training,\nand improving LLMs in realistic software engineering contexts."}
{"id": "2504.15928", "pdf": "https://arxiv.org/pdf/2504.15928", "abs": "https://arxiv.org/abs/2504.15928", "authors": ["Meng Wang", "Tian Lin", "Qingshan Hou", "Aidi Lin", "Jingcheng Wang", "Qingsheng Peng", "Truong X. Nguyen", "Danqi Fang", "Ke Zou", "Ting Xu", "Cancan Xue", "Ten Cheer Quek", "Qinkai Yu", "Minxin Liu", "Hui Zhou", "Zixuan Xiao", "Guiqin He", "Huiyu Liang", "Tingkun Shi", "Man Chen", "Linna Liu", "Yuanyuan Peng", "Lianyu Wang", "Qiuming Hu", "Junhong Chen", "Zhenhua Zhang", "Cheng Chen", "Yitian Zhao", "Dianbo Liu", "Jianhua Wu", "Xinjian Chen", "Changqing Zhang", "Triet Thanh Nguyen", "Yanda Meng", "Yalin Zheng", "Yih Chung Tham", "Carol Y. Cheung", "Huazhu Fu", "Haoyu Chen", "Ching-Yu Cheng"], "title": "A Clinician-Friendly Platform for Ophthalmic Image Analysis Without Technical Barriers", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Artificial intelligence (AI) shows remarkable potential in medical imaging\ndiagnostics, but current models typically require retraining when deployed\nacross different clinical centers, limiting their widespread adoption. We\nintroduce GlobeReady, a clinician-friendly AI platform that enables ocular\ndisease diagnosis without retraining/fine-tuning or technical expertise.\nGlobeReady achieves high accuracy across imaging modalities: 93.9-98.5% for an\n11-category fundus photo dataset and 87.2-92.7% for a 15-category OCT dataset.\nThrough training-free local feature augmentation, it addresses domain shifts\nacross centers and populations, reaching an average accuracy of 88.9% across\nfive centers in China, 86.3% in Vietnam, and 90.2% in the UK. The built-in\nconfidence-quantifiable diagnostic approach further boosted accuracy to\n94.9-99.4% (fundus) and 88.2-96.2% (OCT), while identifying out-of-distribution\ncases at 86.3% (49 CFP categories) and 90.6% (13 OCT categories). Clinicians\nfrom multiple countries rated GlobeReady highly (average 4.6 out of 5) for its\nusability and clinical relevance. These results demonstrate GlobeReady's\nrobust, scalable diagnostic capability and potential to support ophthalmic care\nwithout technical barriers."}
{"id": "2504.15485", "pdf": "https://arxiv.org/pdf/2504.15485", "abs": "https://arxiv.org/abs/2504.15485", "authors": ["Atin Pothiraj", "Elias Stengel-Eskin", "Jaemin Cho", "Mohit Bansal"], "title": "CAPTURe: Evaluating Spatial Reasoning in Vision Language Models via Occluded Object Counting", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Code and data: https://github.com/atinpothiraj/CAPTURe", "summary": "Recognizing and reasoning about occluded (partially or fully hidden) objects\nis vital to understanding visual scenes, as occlusions frequently occur in\nreal-world environments and act as obstacles for spatial comprehension. To test\nmodels' ability to reason about multiple occluded objects, we introduce a novel\ntask, Counting Amodally for Patterns Through Unseen REgions (CAPTURe), which\nrequires a model to count objects arranged in a pattern by inferring how the\npattern continues behind an occluder (an object which blocks parts of the\nscene). CAPTURe requires both recognizing visual patterns and reasoning, making\nit a useful testbed for evaluating vision-language models (VLMs) on whether\nthey understand occluded patterns and possess spatial understanding skills. By\nrequiring models to reason about occluded objects, CAPTURe also tests VLMs'\nability to form world models that would allow them to fill in missing\ninformation. CAPTURe consists of two parts: (1) CAPTURe-real, with manually\nfiltered images of real objects in patterns and (2) CAPTURe-synthetic, a\ncontrolled diagnostic with generated patterned images. We evaluate four strong\nVLMs (GPT-4o, Intern-VL2, Molmo, and Qwen2-VL) on CAPTURe, finding that models\nstruggle to count on both occluded and unoccluded patterns. Crucially, we find\nthat models perform worse with occlusion, suggesting that VLMs are also\ndeficient in inferring unseen spatial relationships: even the strongest VLMs\nlike GPT-4o fail to count with occlusion. In contrast, we find that humans\nachieve very little error on CAPTURe. We also find that providing auxiliary\ninformation of occluded object locations increases performance, underscoring\nthat the model error comes both from an inability to handle occlusion as well\nas difficulty counting in images."}
{"id": "2504.15585", "pdf": "https://arxiv.org/pdf/2504.15585", "abs": "https://arxiv.org/abs/2504.15585", "authors": ["Kun Wang", "Guibin Zhang", "Zhenhong Zhou", "Jiahao Wu", "Miao Yu", "Shiqian Zhao", "Chenlong Yin", "Jinhu Fu", "Yibo Yan", "Hanjun Luo", "Liang Lin", "Zhihao Xu", "Haolang Lu", "Xinye Cao", "Xinyun Zhou", "Weifei Jin", "Fanci Meng", "Junyuan Mao", "Hao Wu", "Minghe Wang", "Fan Zhang", "Junfeng Fang", "Chengwei Liu", "Yifan Zhang", "Qiankun Li", "Chongye Guo", "Yalan Qin", "Yi Ding", "Donghai Hong", "Jiaming Ji", "Xinfeng Li", "Yifan Jiang", "Dongxia Wang", "Yihao Huang", "Yufei Guo", "Jen-tse Huang", "Yanwei Yue", "Wenke Huang", "Guancheng Wan", "Tianlin Li", "Lei Bai", "Jie Zhang", "Qing Guo", "Jingyi Wang", "Tianlong Chen", "Joey Tianyi Zhou", "Xiaojun Jia", "Weisong Sun", "Cong Wu", "Jing Chen", "Xuming Hu", "Yiming Li", "Xiao Wang", "Ningyu Zhang", "Luu Anh Tuan", "Guowen Xu", "Tianwei Zhang", "Xingjun Ma", "Xiang Wang", "Bo An", "Jun Sun", "Mohit Bansal", "Shirui Pan", "Yuval Elovici", "Bhavya Kailkhura", "Bo Li", "Yaodong Yang", "Hongwei Li", "Wenyuan Xu", "Yizhou Sun", "Wei Wang", "Qing Li", "Ke Tang", "Yu-Gang Jiang", "Felix Juefei-Xu", "Hui Xiong", "Xiaofeng Wang", "Shuicheng Yan", "Dacheng Tao", "Philip S. Yu", "Qingsong Wen", "Yang Liu"], "title": "A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "The remarkable success of Large Language Models (LLMs) has illuminated a\npromising pathway toward achieving Artificial General Intelligence for both\nacademic and industrial communities, owing to their unprecedented performance\nacross various applications. As LLMs continue to gain prominence in both\nresearch and commercial domains, their security and safety implications have\nbecome a growing concern, not only for researchers and corporations but also\nfor every nation. Currently, existing surveys on LLM safety primarily focus on\nspecific stages of the LLM lifecycle, e.g., deployment phase or fine-tuning\nphase, lacking a comprehensive understanding of the entire \"lifechain\" of LLMs.\nTo address this gap, this paper introduces, for the first time, the concept of\n\"full-stack\" safety to systematically consider safety issues throughout the\nentire process of LLM training, deployment, and eventual commercialization.\nCompared to the off-the-shelf LLM safety surveys, our work demonstrates several\ndistinctive advantages: (I) Comprehensive Perspective. We define the complete\nLLM lifecycle as encompassing data preparation, pre-training, post-training,\ndeployment and final commercialization. To our knowledge, this represents the\nfirst safety survey to encompass the entire lifecycle of LLMs. (II) Extensive\nLiterature Support. Our research is grounded in an exhaustive review of over\n800+ papers, ensuring comprehensive coverage and systematic organization of\nsecurity issues within a more holistic understanding. (III) Unique Insights.\nThrough systematic literature analysis, we have developed reliable roadmaps and\nperspectives for each chapter. Our work identifies promising research\ndirections, including safety in data generation, alignment techniques, model\nediting, and LLM-based agent systems. These insights provide valuable guidance\nfor researchers pursuing future work in this field."}
{"id": "2504.15929", "pdf": "https://arxiv.org/pdf/2504.15929", "abs": "https://arxiv.org/abs/2504.15929", "authors": ["Saban Ozturk", "Melih B. Yilmaz", "Muti Kara", "M. Talat Yavuz", "Aykut Koç", "Tolga Çukur"], "title": "Meta-Entity Driven Triplet Mining for Aligning Medical Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "18 pages, 7 figures, 6 tables", "summary": "Diagnostic imaging relies on interpreting both images and radiology reports,\nbut the growing data volumes place significant pressure on medical experts,\nyielding increased errors and workflow backlogs. Medical vision-language models\n(med-VLMs) have emerged as a powerful framework to efficiently process\nmultimodal imaging data, particularly in chest X-ray (CXR) evaluations, albeit\ntheir performance hinges on how well image and text representations are\naligned. Existing alignment methods, predominantly based on contrastive\nlearning, prioritize separation between disease classes over segregation of\nfine-grained pathology attributes like location, size or severity, leading to\nsuboptimal representations. Here, we propose MedTrim (Meta-entity-driven\nTriplet mining), a novel method that enhances image-text alignment through\nmultimodal triplet learning synergistically guided by disease class as well as\nadjectival and directional pathology descriptors. Unlike common alignment\nmethods that separate broad disease classes, MedTrim leverages structured\nmeta-entity information to preserve subtle but clinically significant\nintra-class variations. For this purpose, we first introduce an ontology-based\nentity recognition module that extracts pathology-specific meta-entities from\nCXR reports, as annotations on pathology attributes are rare in public\ndatasets. For refined sample selection in triplet mining, we then introduce a\nnovel score function that captures an aggregate measure of inter-sample\nsimilarity based on disease classes and adjectival/directional descriptors.\nLastly, we introduce a multimodal triplet alignment objective for explicit\nwithin- and cross-modal alignment between samples sharing detailed pathology\ncharacteristics. Our demonstrations indicate that MedTrim improves performance\nin downstream retrieval and classification tasks compared to state-of-the-art\nalignment methods."}
{"id": "2504.15585", "pdf": "https://arxiv.org/pdf/2504.15585", "abs": "https://arxiv.org/abs/2504.15585", "authors": ["Kun Wang", "Guibin Zhang", "Zhenhong Zhou", "Jiahao Wu", "Miao Yu", "Shiqian Zhao", "Chenlong Yin", "Jinhu Fu", "Yibo Yan", "Hanjun Luo", "Liang Lin", "Zhihao Xu", "Haolang Lu", "Xinye Cao", "Xinyun Zhou", "Weifei Jin", "Fanci Meng", "Junyuan Mao", "Hao Wu", "Minghe Wang", "Fan Zhang", "Junfeng Fang", "Chengwei Liu", "Yifan Zhang", "Qiankun Li", "Chongye Guo", "Yalan Qin", "Yi Ding", "Donghai Hong", "Jiaming Ji", "Xinfeng Li", "Yifan Jiang", "Dongxia Wang", "Yihao Huang", "Yufei Guo", "Jen-tse Huang", "Yanwei Yue", "Wenke Huang", "Guancheng Wan", "Tianlin Li", "Lei Bai", "Jie Zhang", "Qing Guo", "Jingyi Wang", "Tianlong Chen", "Joey Tianyi Zhou", "Xiaojun Jia", "Weisong Sun", "Cong Wu", "Jing Chen", "Xuming Hu", "Yiming Li", "Xiao Wang", "Ningyu Zhang", "Luu Anh Tuan", "Guowen Xu", "Tianwei Zhang", "Xingjun Ma", "Xiang Wang", "Bo An", "Jun Sun", "Mohit Bansal", "Shirui Pan", "Yuval Elovici", "Bhavya Kailkhura", "Bo Li", "Yaodong Yang", "Hongwei Li", "Wenyuan Xu", "Yizhou Sun", "Wei Wang", "Qing Li", "Ke Tang", "Yu-Gang Jiang", "Felix Juefei-Xu", "Hui Xiong", "Xiaofeng Wang", "Shuicheng Yan", "Dacheng Tao", "Philip S. Yu", "Qingsong Wen", "Yang Liu"], "title": "A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "The remarkable success of Large Language Models (LLMs) has illuminated a\npromising pathway toward achieving Artificial General Intelligence for both\nacademic and industrial communities, owing to their unprecedented performance\nacross various applications. As LLMs continue to gain prominence in both\nresearch and commercial domains, their security and safety implications have\nbecome a growing concern, not only for researchers and corporations but also\nfor every nation. Currently, existing surveys on LLM safety primarily focus on\nspecific stages of the LLM lifecycle, e.g., deployment phase or fine-tuning\nphase, lacking a comprehensive understanding of the entire \"lifechain\" of LLMs.\nTo address this gap, this paper introduces, for the first time, the concept of\n\"full-stack\" safety to systematically consider safety issues throughout the\nentire process of LLM training, deployment, and eventual commercialization.\nCompared to the off-the-shelf LLM safety surveys, our work demonstrates several\ndistinctive advantages: (I) Comprehensive Perspective. We define the complete\nLLM lifecycle as encompassing data preparation, pre-training, post-training,\ndeployment and final commercialization. To our knowledge, this represents the\nfirst safety survey to encompass the entire lifecycle of LLMs. (II) Extensive\nLiterature Support. Our research is grounded in an exhaustive review of over\n800+ papers, ensuring comprehensive coverage and systematic organization of\nsecurity issues within a more holistic understanding. (III) Unique Insights.\nThrough systematic literature analysis, we have developed reliable roadmaps and\nperspectives for each chapter. Our work identifies promising research\ndirections, including safety in data generation, alignment techniques, model\nediting, and LLM-based agent systems. These insights provide valuable guidance\nfor researchers pursuing future work in this field."}
{"id": "2504.15587", "pdf": "https://arxiv.org/pdf/2504.15587", "abs": "https://arxiv.org/abs/2504.15587", "authors": ["Zimo Yan", "Jie Zhang", "Zheng Xie", "Chang Liu", "Yizhen Liu", "Yiping Song"], "title": "MetaMolGen: A Neural Graph Motif Generation Model for De Novo Molecular Design", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Molecular generation plays an important role in drug discovery and materials\nscience, especially in data-scarce scenarios where traditional generative\nmodels often struggle to achieve satisfactory conditional generalization. To\naddress this challenge, we propose MetaMolGen, a first-order\nmeta-learning-based molecular generator designed for few-shot and\nproperty-conditioned molecular generation. MetaMolGen standardizes the\ndistribution of graph motifs by mapping them to a normalized latent space, and\nemploys a lightweight autoregressive sequence model to generate SMILES\nsequences that faithfully reflect the underlying molecular structure. In\naddition, it supports conditional generation of molecules with target\nproperties through a learnable property projector integrated into the\ngenerative process.Experimental results demonstrate that MetaMolGen\nconsistently generates valid and diverse SMILES sequences under low-data\nregimes, outperforming conventional baselines. This highlights its advantage in\nfast adaptation and efficient conditional generation for practical molecular\ndesign."}
{"id": "2504.15931", "pdf": "https://arxiv.org/pdf/2504.15931", "abs": "https://arxiv.org/abs/2504.15931", "authors": ["Ekaterina Kondrateva", "Sandzhi Barg", "Mikhail Vasiliev"], "title": "Benchmarking the Reproducibility of Brain MRI Segmentation Across Scanners and Time", "categories": ["cs.CV"], "comment": null, "summary": "Accurate and reproducible brain morphometry from structural MRI is critical\nfor monitoring neuroanatomical changes across time and across imaging domains.\nAlthough deep learning has accelerated segmentation workflows, scanner-induced\nvariability and reproducibility limitations remain-especially in longitudinal\nand multi-site settings. In this study, we benchmark two modern segmentation\npipelines, FastSurfer and SynthSeg, both integrated into FreeSurfer, one of the\nmost widely adopted tools in neuroimaging.\n  Using two complementary datasets - a 17-year longitudinal cohort (SIMON) and\na 9-site test-retest cohort (SRPBS)-we quantify inter-scan segmentation\nvariability using Dice coefficient, Surface Dice, Hausdorff Distance (HD95),\nand Mean Absolute Percentage Error (MAPE). Our results reveal up to 7-8% volume\nvariation in small subcortical structures such as the amygdala and ventral\ndiencephalon, even under controlled test-retest conditions. This raises a key\nquestion: is it feasible to detect subtle longitudinal changes on the order of\n5-10% in pea-sized brain regions, given the magnitude of domain-induced\nmorphometric noise?\n  We further analyze the effects of registration templates and interpolation\nmodes, and propose surface-based quality filtering to improve segmentation\nreliability. This study provides a reproducible benchmark for morphometric\nreproducibility and emphasizes the need for harmonization strategies in\nreal-world neuroimaging studies.\n  Code and figures: https://github.com/kondratevakate/brain-mri-segmentation"}
{"id": "2504.15629", "pdf": "https://arxiv.org/pdf/2504.15629", "abs": "https://arxiv.org/abs/2504.15629", "authors": ["Harsh Maheshwari", "Srikanth Tenneti", "Alwarappan Nakkiran"], "title": "CiteFix: Enhancing RAG Accuracy Through Post-Processing Citation Correction", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Retrieval Augmented Generation (RAG) has emerged as a powerful application of\nLarge Language Models (LLMs), revolutionizing information search and\nconsumption. RAG systems combine traditional search capabilities with LLMs to\ngenerate comprehensive answers to user queries, ideally with accurate\ncitations. However, in our experience of developing a RAG product, LLMs often\nstruggle with source attribution, aligning with other industry studies\nreporting citation accuracy rates of only about 74% for popular generative\nsearch engines. To address this, we present efficient post-processing\nalgorithms to improve citation accuracy in LLM-generated responses, with\nminimal impact on latency and cost. Our approaches cross-check generated\ncitations against retrieved articles using methods including keyword + semantic\nmatching, fine tuned model with BERTScore, and a lightweight LLM-based\ntechnique. Our experimental results demonstrate a relative improvement of\n15.46% in the overall accuracy metrics of our RAG system. This significant\nenhancement potentially enables a shift from our current larger language model\nto a relatively smaller model that is approximately 12x more cost-effective and\n3x faster in inference time, while maintaining comparable performance. This\nresearch contributes to enhancing the reliability and trustworthiness of\nAI-generated content in information retrieval and summarization tasks which is\ncritical to gain customer trust especially in commercial products."}
{"id": "2504.15604", "pdf": "https://arxiv.org/pdf/2504.15604", "abs": "https://arxiv.org/abs/2504.15604", "authors": ["Pavan Yadav", "Nikhil Khandalkar", "Krishna Shinde", "Lokesh B. Ramegowda", "Rajarshi Das"], "title": "Exploring Next Token Prediction in Theory of Mind (ToM) Tasks: Comparative Experiments with GPT-2 and LLaMA-2 AI Models", "categories": ["cs.CL", "cs.AI"], "comment": "75 pages, 60 figures", "summary": "Language models have made significant progress in generating coherent text\nand predicting next tokens based on input prompts. This study compares the\nnext-token prediction performance of two well-known models: OpenAI's GPT-2 and\nMeta's Llama-2-7b-chat-hf on Theory of Mind (ToM) tasks. To evaluate their\ncapabilities, we built a dataset from 10 short stories sourced from the Explore\nToM Dataset. We enhanced these stories by programmatically inserting additional\nsentences (infills) using GPT-4, creating variations that introduce different\nlevels of contextual complexity. This setup enables analysis of how increasing\ncontext affects model performance. We tested both models under four temperature\nsettings (0.01, 0.5, 1.0, 2.0) and evaluated their ability to predict the next\ntoken across three reasoning levels. Zero-order reasoning involves tracking the\nstate, either current (ground truth) or past (memory). First-order reasoning\nconcerns understanding another's mental state (e.g., \"Does Anne know the apple\nis salted?\"). Second-order reasoning adds recursion (e.g., \"Does Anne think\nthat Charles knows the apple is salted?\").\n  Our results show that adding more infill sentences slightly reduces\nprediction accuracy, as added context increases complexity and ambiguity.\nLlama-2 consistently outperforms GPT-2 in prediction accuracy, especially at\nlower temperatures, demonstrating greater confidence in selecting the most\nprobable token. As reasoning complexity rises, model responses diverge more.\nNotably, GPT-2 and Llama-2 display greater variability in predictions during\nfirst- and second-order reasoning tasks. These findings illustrate how model\narchitecture, temperature, and contextual complexity influence next-token\nprediction, contributing to a better understanding of the strengths and\nlimitations of current language models."}
{"id": "2504.15932", "pdf": "https://arxiv.org/pdf/2504.15932", "abs": "https://arxiv.org/abs/2504.15932", "authors": ["Wang Lin", "Liyu Jia", "Wentao Hu", "Kaihang Pan", "Zhongqi Yue", "Wei Zhao", "Jingyuan Chen", "Fei Wu", "Hanwang Zhang"], "title": "Reasoning Physical Video Generation with Diffusion Timestep Tokens via Reinforcement Learning", "categories": ["cs.CV"], "comment": null, "summary": "Despite recent progress in video generation, producing videos that adhere to\nphysical laws remains a significant challenge. Traditional diffusion-based\nmethods struggle to extrapolate to unseen physical conditions (eg, velocity)\ndue to their reliance on data-driven approximations. To address this, we\npropose to integrate symbolic reasoning and reinforcement learning to enforce\nphysical consistency in video generation. We first introduce the Diffusion\nTimestep Tokenizer (DDT), which learns discrete, recursive visual tokens by\nrecovering visual attributes lost during the diffusion process. The recursive\nvisual tokens enable symbolic reasoning by a large language model. Based on it,\nwe propose the Phys-AR framework, which consists of two stages: The first stage\nuses supervised fine-tuning to transfer symbolic knowledge, while the second\nstage applies reinforcement learning to optimize the model's reasoning\nabilities through reward functions based on physical conditions. Our approach\nallows the model to dynamically adjust and improve the physical properties of\ngenerated videos, ensuring adherence to physical laws. Experimental results\ndemonstrate that PhysAR can generate videos that are physically consistent."}
{"id": "2504.15659", "pdf": "https://arxiv.org/pdf/2504.15659", "abs": "https://arxiv.org/abs/2504.15659", "authors": ["Anjiang Wei", "Huanmi Tan", "Tarun Suresh", "Daniel Mendoza", "Thiago S. F. X. Teixeira", "Ke Wang", "Caroline Trippel", "Alex Aiken"], "title": "VeriCoder: Enhancing LLM-Based RTL Code Generation through Functional Correctness Validation", "categories": ["cs.AR", "cs.AI", "cs.CL", "cs.LG", "cs.SE"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have sparked growing interest\nin applying them to Electronic Design Automation (EDA) tasks, particularly\nRegister Transfer Level (RTL) code generation. While several RTL datasets have\nbeen introduced, most focus on syntactic validity rather than functional\nvalidation with tests, leading to training examples that compile but may not\nimplement the intended behavior. We present VERICODER, a model for RTL code\ngeneration fine-tuned on a dataset validated for functional correctness. This\nfine-tuning dataset is constructed using a novel methodology that combines unit\ntest generation with feedback-directed refinement. Given a natural language\nspecification and an initial RTL design, we prompt a teacher model\n(GPT-4o-mini) to generate unit tests and iteratively revise the RTL design\nbased on its simulation results using the generated tests. If necessary, the\nteacher model also updates the tests to ensure they comply with the natural\nlanguage specification. As a result of this process, every example in our\ndataset is functionally validated, consisting of a natural language\ndescription, an RTL implementation, and passing tests. Fine-tuned on this\ndataset of over 125,000 examples, VERICODER achieves state-of-the-art metrics\nin functional correctness on VerilogEval and RTLLM, with relative gains of up\nto 71.7% and 27.4% respectively. An ablation study further shows that models\ntrained on our functionally validated dataset outperform those trained on\nfunctionally non-validated datasets, underscoring the importance of\nhigh-quality datasets in RTL code generation."}
{"id": "2504.15634", "pdf": "https://arxiv.org/pdf/2504.15634", "abs": "https://arxiv.org/abs/2504.15634", "authors": ["Peizheng Liu", "Hitoshi Iba"], "title": "Enhancing Reinforcement learning in 3-Dimensional Hydrophobic-Polar Protein Folding Model with Attention-based layers", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Transformer-based architectures have recently propelled advances in sequence\nmodeling across domains, but their application to the hydrophobic-hydrophilic\n(H-P) model for protein folding remains relatively unexplored. In this work, we\nadapt a Deep Q-Network (DQN) integrated with attention mechanisms\n(Transformers) to address the 3D H-P protein folding problem. Our system\nformulates folding decisions as a self-avoiding walk in a reinforced\nenvironment, and employs a specialized reward function based on favorable\nhydrophobic interactions. To improve performance, the method incorporates\nvalidity check including symmetry-breaking constraints, dueling and double\nQ-learning, and prioritized replay to focus learning on critical transitions.\nExperimental evaluations on standard benchmark sequences demonstrate that our\napproach achieves several known best solutions for shorter sequences, and\nobtains near-optimal results for longer chains. This study underscores the\npromise of attention-based reinforcement learning for protein folding, and\ncreated a prototype of Transformer-based Q-network structure for 3-dimensional\nlattice models."}
{"id": "2504.15958", "pdf": "https://arxiv.org/pdf/2504.15958", "abs": "https://arxiv.org/abs/2504.15958", "authors": ["Zebin Yao", "Lei Ren", "Huixing Jiang", "Chen Wei", "Xiaojie Wang", "Ruifan Li", "Fangxiang Feng"], "title": "FreeGraftor: Training-Free Cross-Image Feature Grafting for Subject-Driven Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Subject-driven image generation aims to synthesize novel scenes that\nfaithfully preserve subject identity from reference images while adhering to\ntextual guidance, yet existing methods struggle with a critical trade-off\nbetween fidelity and efficiency. Tuning-based approaches rely on time-consuming\nand resource-intensive subject-specific optimization, while zero-shot methods\nfail to maintain adequate subject consistency. In this work, we propose\nFreeGraftor, a training-free framework that addresses these limitations through\ncross-image feature grafting. Specifically, FreeGraftor employs semantic\nmatching and position-constrained attention fusion to transfer visual details\nfrom reference subjects to the generated image. Additionally, our framework\nincorporates a novel noise initialization strategy to preserve geometry priors\nof reference subjects for robust feature matching. Extensive qualitative and\nquantitative experiments demonstrate that our method enables precise subject\nidentity transfer while maintaining text-aligned scene synthesis. Without\nrequiring model fine-tuning or additional training, FreeGraftor significantly\noutperforms existing zero-shot and training-free approaches in both subject\nfidelity and text alignment. Furthermore, our framework can seamlessly extend\nto multi-subject generation, making it practical for real-world deployment. Our\ncode is available at https://github.com/Nihukat/FreeGraftor."}
{"id": "2504.15780", "pdf": "https://arxiv.org/pdf/2504.15780", "abs": "https://arxiv.org/abs/2504.15780", "authors": ["Daocheng Fu", "Zijun Chen", "Renqiu Xia", "Qi Liu", "Yuan Feng", "Hongbin Zhou", "Renrui Zhang", "Shiyang Feng", "Peng Gao", "Junchi Yan", "Botian Shi", "Bo Zhang", "Yu Qiao"], "title": "TrustGeoGen: Scalable and Formal-Verified Data Engine for Trustworthy Multi-modal Geometric Problem Solving", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Mathematical geometric problem solving (GPS) often requires effective\nintegration of multimodal information and verifiable logical coherence. Despite\nthe fast development of large language models in general problem solving, it\nremains unresolved regarding with both methodology and benchmarks, especially\ngiven the fact that exiting synthetic GPS benchmarks are often not\nself-verified and contain noise and self-contradicted information due to the\nillusion of LLMs. In this paper, we propose a scalable data engine called\nTrustGeoGen for problem generation, with formal verification to provide a\nprincipled benchmark, which we believe lays the foundation for the further\ndevelopment of methods for GPS. The engine synthesizes geometric data through\nfour key innovations: 1) multimodal-aligned generation of diagrams, textual\ndescriptions, and stepwise solutions; 2) formal verification ensuring\nrule-compliant reasoning paths; 3) a bootstrapping mechanism enabling\ncomplexity escalation via recursive state generation and 4) our devised\nGeoExplore series algorithms simultaneously produce multi-solution variants and\nself-reflective backtracking traces. By formal logical verification,\nTrustGeoGen produces GeoTrust-200K dataset with guaranteed modality integrity,\nalong with GeoTrust-test testset. Experiments reveal the state-of-the-art\nmodels achieve only 49.17\\% accuracy on GeoTrust-test, demonstrating its\nevaluation stringency. Crucially, models trained on GeoTrust achieve OOD\ngeneralization on GeoQA, significantly reducing logical inconsistencies\nrelative to pseudo-label annotated by OpenAI-o1. Our code is available at\nhttps://github.com/Alpha-Innovator/TrustGeoGen"}
{"id": "2504.15637", "pdf": "https://arxiv.org/pdf/2504.15637", "abs": "https://arxiv.org/abs/2504.15637", "authors": ["Farnaz Behrang", "Zhizhou Zhang", "Georgian-Vlad Saioc", "Peng Liu", "Milind Chabbi"], "title": "DR.FIX: Automatically Fixing Data Races at Industry Scale", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.PL", "cs.SE"], "comment": "To appear in PLDI 2025", "summary": "Data races are a prevalent class of concurrency bugs in shared-memory\nparallel programs, posing significant challenges to software reliability and\nreproducibility. While there is an extensive body of research on detecting data\nraces and a wealth of practical detection tools across various programming\nlanguages, considerably less effort has been directed toward automatically\nfixing data races at an industrial scale. In large codebases, data races are\ncontinuously introduced and exhibit myriad patterns, making automated fixing\nparticularly challenging.\n  In this paper, we tackle the problem of automatically fixing data races at an\nindustrial scale. We present Dr.Fix, a tool that combines large language models\n(LLMs) with program analysis to generate fixes for data races in real-world\nsettings, effectively addressing a broad spectrum of racy patterns in complex\ncode contexts. Implemented for Go--the programming language widely used in\nmodern microservice architectures where concurrency is pervasive and data races\nare common--Dr.Fix seamlessly integrates into existing development workflows.\nWe detail the design of Dr.Fix and examine how individual design choices\ninfluence the quality of the fixes produced. Over the past 18 months, Dr.Fix\nhas been integrated into developer workflows at Uber demonstrating its\npractical utility. During this period, Dr.Fix produced patches for 224 (55%)\nfrom a corpus of 404 data races spanning various categories; 193 of these\npatches (86%) were accepted by more than a hundred developers via code reviews\nand integrated into the codebase."}
{"id": "2504.15991", "pdf": "https://arxiv.org/pdf/2504.15991", "abs": "https://arxiv.org/abs/2504.15991", "authors": ["Leonardo Olivi", "Edoardo Santero Mormile", "Enzo Tartaglione"], "title": "Efficient Adaptation of Deep Neural Networks for Semantic Segmentation in Space Applications", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "In recent years, the application of Deep Learning techniques has shown\nremarkable success in various computer vision tasks, paving the way for their\ndeployment in extraterrestrial exploration. Transfer learning has emerged as a\npowerful strategy for addressing the scarcity of labeled data in these novel\nenvironments. This paper represents one of the first efforts in evaluating the\nfeasibility of employing adapters toward efficient transfer learning for rock\nsegmentation in extraterrestrial landscapes, mainly focusing on lunar and\nmartian terrains. Our work suggests that the use of adapters, strategically\nintegrated into a pre-trained backbone model, can be successful in reducing\nboth bandwidth and memory requirements for the target extraterrestrial device.\nIn this study, we considered two memory-saving strategies: layer fusion (to\nreduce to zero the inference overhead) and an ``adapter ranking'' (to also\nreduce the transmission cost). Finally, we evaluate these results in terms of\ntask performance, memory, and computation on embedded devices, evidencing\ntrade-offs that open the road to more research in the field."}
{"id": "2504.16000", "pdf": "https://arxiv.org/pdf/2504.16000", "abs": "https://arxiv.org/abs/2504.16000", "authors": ["Soham Bonnerjee", "Zhen Wei", "Yeon", "Anna Asch", "Sagnik Nandy", "Promit Ghosal"], "title": "How Private is Your Attention? Bridging Privacy with In-Context Learning", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "In-context learning (ICL)-the ability of transformer-based models to perform\nnew tasks from examples provided at inference time-has emerged as a hallmark of\nmodern language models. While recent works have investigated the mechanisms\nunderlying ICL, its feasibility under formal privacy constraints remains\nlargely unexplored. In this paper, we propose a differentially private\npretraining algorithm for linear attention heads and present the first\ntheoretical analysis of the privacy-accuracy trade-off for ICL in linear\nregression. Our results characterize the fundamental tension between\noptimization and privacy-induced noise, formally capturing behaviors observed\nin private training via iterative methods. Additionally, we show that our\nmethod is robust to adversarial perturbations of training prompts, unlike\nstandard ridge regression. All theoretical findings are supported by extensive\nsimulations across diverse settings."}
{"id": "2504.15640", "pdf": "https://arxiv.org/pdf/2504.15640", "abs": "https://arxiv.org/abs/2504.15640", "authors": ["Hongtao Wang", "Taiyan Zhang", "Renchi Yang", "Jianliang Xu"], "title": "Cost-Effective Text Clustering with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Text clustering aims to automatically partition a collection of text\ndocuments into distinct clusters based on linguistic features. In the\nliterature, this task is usually framed as metric clustering based on text\nembeddings from pre-trained encoders or a graph clustering problem upon\npairwise similarities from an oracle, e.g., a large ML model. Recently, large\nlanguage models (LLMs) bring significant advancement in this field by offering\ncontextualized text embeddings and highly accurate similarity scores, but\nmeanwhile, present grand challenges to cope with substantial computational\nand/or financial overhead caused by numerous API-based queries or inference\ncalls to the models.\n  In response, this paper proposes TECL, a cost-effective framework that taps\ninto the feedback from LLMs for accurate text clustering within a limited\nbudget of queries to LLMs. Under the hood, TECL adopts our EdgeLLM or\nTriangleLLM to construct must-link/cannot-link constraints for text pairs, and\nfurther leverages such constraints as supervision signals input to our weighted\nconstrained clustering approach to generate clusters. Particularly, EdgeLLM\n(resp. TriangleLLM) enables the identification of informative text pairs (resp.\ntriplets) for querying LLMs via well-thought-out greedy algorithms and accurate\nextraction of pairwise constraints through carefully-crafted prompting\ntechniques. Our experiments on multiple benchmark datasets exhibit that TECL\nconsistently and considerably outperforms existing solutions in unsupervised\ntext clustering under the same query cost for LLMs."}
{"id": "2504.16003", "pdf": "https://arxiv.org/pdf/2504.16003", "abs": "https://arxiv.org/abs/2504.16003", "authors": ["Yachun Mi", "Yu Li", "Weicheng Meng", "Chaofeng Chen", "Chen Hui", "Shaohui Liu"], "title": "MVQA: Mamba with Unified Sampling for Efficient Video Quality Assessment", "categories": ["cs.CV"], "comment": null, "summary": "The rapid growth of long-duration, high-definition videos has made efficient\nvideo quality assessment (VQA) a critical challenge. Existing research\ntypically tackles this problem through two main strategies: reducing model\nparameters and resampling inputs. However, light-weight Convolution Neural\nNetworks (CNN) and Transformers often struggle to balance efficiency with high\nperformance due to the requirement of long-range modeling capabilities.\nRecently, the state-space model, particularly Mamba, has emerged as a promising\nalternative, offering linear complexity with respect to sequence length.\nMeanwhile, efficient VQA heavily depends on resampling long sequences to\nminimize computational costs, yet current resampling methods are often weak in\npreserving essential semantic information. In this work, we present MVQA, a\nMamba-based model designed for efficient VQA along with a novel Unified\nSemantic and Distortion Sampling (USDS) approach. USDS combines semantic patch\nsampling from low-resolution videos and distortion patch sampling from\noriginal-resolution videos. The former captures semantically dense regions,\nwhile the latter retains critical distortion details. To prevent computation\nincrease from dual inputs, we propose a fusion mechanism using pre-defined\nmasks, enabling a unified sampling strategy that captures both semantic and\nquality information without additional computational burden. Experiments show\nthat the proposed MVQA, equipped with USDS, achieve comparable performance to\nstate-of-the-art methods while being $2\\times$ as fast and requiring only $1/5$\nGPU memory."}
{"id": "2504.16081", "pdf": "https://arxiv.org/pdf/2504.16081", "abs": "https://arxiv.org/abs/2504.16081", "authors": ["Yimu Wang", "Xuye Liu", "Wei Pang", "Li Ma", "Shuai Yuan", "Paul Debevec", "Ning Yu"], "title": "Survey of Video Diffusion Models: Foundations, Implementations, and Applications", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Recent advances in diffusion models have revolutionized video generation,\noffering superior temporal consistency and visual quality compared to\ntraditional generative adversarial networks-based approaches. While this\nemerging field shows tremendous promise in applications, it faces significant\nchallenges in motion consistency, computational efficiency, and ethical\nconsiderations. This survey provides a comprehensive review of diffusion-based\nvideo generation, examining its evolution, technical foundations, and practical\napplications. We present a systematic taxonomy of current methodologies,\nanalyze architectural innovations and optimization strategies, and investigate\napplications across low-level vision tasks such as denoising and\nsuper-resolution. Additionally, we explore the synergies between diffusionbased\nvideo generation and related domains, including video representation learning,\nquestion answering, and retrieval. Compared to the existing surveys (Lei et\nal., 2024a;b; Melnik et al., 2024; Cao et al., 2023; Xing et al., 2024c) which\nfocus on specific aspects of video generation, such as human video synthesis\n(Lei et al., 2024a) or long-form content generation (Lei et al., 2024b), our\nwork provides a broader, more updated, and more fine-grained perspective on\ndiffusion-based approaches with a special section for evaluation metrics,\nindustry solutions, and training engineering techniques in video generation.\nThis survey serves as a foundational resource for researchers and practitioners\nworking at the intersection of diffusion models and video generation, providing\ninsights into both the theoretical frameworks and practical implementations\nthat drive this rapidly evolving field. A structured list of related works\ninvolved in this survey is also available on\nhttps://github.com/Eyeline-Research/Survey-Video-Diffusion."}
{"id": "2504.15654", "pdf": "https://arxiv.org/pdf/2504.15654", "abs": "https://arxiv.org/abs/2504.15654", "authors": ["Md Abdul Baset Sarker", "Art Nguyen", "Sigmond Kukla", "Kevin Fite", "Masudul H. Imtiaz"], "title": "A Vision-Enabled Prosthetic Hand for Children with Upper Limb Disabilities", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "This paper introduces a novel AI vision-enabled pediatric prosthetic hand\ndesigned to assist children aged 10-12 with upper limb disabilities. The\nprosthesis features an anthropomorphic appearance, multi-articulating\nfunctionality, and a lightweight design that mimics a natural hand, making it\nboth accessible and affordable for low-income families. Using 3D printing\ntechnology and integrating advanced machine vision, sensing, and embedded\ncomputing, the prosthetic hand offers a low-cost, customizable solution that\naddresses the limitations of current myoelectric prostheses. A micro camera is\ninterfaced with a low-power FPGA for real-time object detection and assists\nwith precise grasping. The onboard DL-based object detection and grasp\nclassification models achieved accuracies of 96% and 100% respectively. In the\nforce prediction, the mean absolute error was found to be 0.018. The features\nof the proposed prosthetic hand can thus be summarized as: a) a wrist-mounted\nmicro camera for artificial sensing, enabling a wide range of hand-based tasks;\nb) real-time object detection and distance estimation for precise grasping; and\nc) ultra-low-power operation that delivers high performance within constrained\npower and resource limits."}
{"id": "2504.16016", "pdf": "https://arxiv.org/pdf/2504.16016", "abs": "https://arxiv.org/abs/2504.16016", "authors": ["Xinyuan Song", "Yangfan He", "Sida Li", "Jianhui Wang", "Hongyang He", "Xinhang Yuan", "Ruoyu Wang", "Jiaqi Chen", "Keqin Li", "Kuan Lu", "Menghao Huo", "Binxu Li", "Pei Liu"], "title": "Efficient Temporal Consistency in Diffusion-Based Video Editing with Adaptor Modules: A Theoretical Framework", "categories": ["cs.CV"], "comment": "arXiv admin note: substantial text overlap with arXiv:2501.04606", "summary": "Adapter-based methods are commonly used to enhance model performance with\nminimal additional complexity, especially in video editing tasks that require\nframe-to-frame consistency. By inserting small, learnable modules into\npretrained diffusion models, these adapters can maintain temporal coherence\nwithout extensive retraining. Approaches that incorporate prompt learning with\nboth shared and frame-specific tokens are particularly effective in preserving\ncontinuity across frames at low training cost. In this work, we want to provide\na general theoretical framework for adapters that maintain frame consistency in\nDDIM-based models under a temporal consistency loss. First, we prove that the\ntemporal consistency objective is differentiable under bounded feature norms,\nand we establish a Lipschitz bound on its gradient. Second, we show that\ngradient descent on this objective decreases the loss monotonically and\nconverges to a local minimum if the learning rate is within an appropriate\nrange. Finally, we analyze the stability of modules in the DDIM inversion\nprocedure, showing that the associated error remains controlled. These\ntheoretical findings will reinforce the reliability of diffusion-based video\nediting methods that rely on adapter strategies and provide theoretical\ninsights in video generation tasks."}
{"id": "2504.15659", "pdf": "https://arxiv.org/pdf/2504.15659", "abs": "https://arxiv.org/abs/2504.15659", "authors": ["Anjiang Wei", "Huanmi Tan", "Tarun Suresh", "Daniel Mendoza", "Thiago S. F. X. Teixeira", "Ke Wang", "Caroline Trippel", "Alex Aiken"], "title": "VeriCoder: Enhancing LLM-Based RTL Code Generation through Functional Correctness Validation", "categories": ["cs.AR", "cs.AI", "cs.CL", "cs.LG", "cs.SE"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have sparked growing interest\nin applying them to Electronic Design Automation (EDA) tasks, particularly\nRegister Transfer Level (RTL) code generation. While several RTL datasets have\nbeen introduced, most focus on syntactic validity rather than functional\nvalidation with tests, leading to training examples that compile but may not\nimplement the intended behavior. We present VERICODER, a model for RTL code\ngeneration fine-tuned on a dataset validated for functional correctness. This\nfine-tuning dataset is constructed using a novel methodology that combines unit\ntest generation with feedback-directed refinement. Given a natural language\nspecification and an initial RTL design, we prompt a teacher model\n(GPT-4o-mini) to generate unit tests and iteratively revise the RTL design\nbased on its simulation results using the generated tests. If necessary, the\nteacher model also updates the tests to ensure they comply with the natural\nlanguage specification. As a result of this process, every example in our\ndataset is functionally validated, consisting of a natural language\ndescription, an RTL implementation, and passing tests. Fine-tuned on this\ndataset of over 125,000 examples, VERICODER achieves state-of-the-art metrics\nin functional correctness on VerilogEval and RTLLM, with relative gains of up\nto 71.7% and 27.4% respectively. An ablation study further shows that models\ntrained on our functionally validated dataset outperform those trained on\nfunctionally non-validated datasets, underscoring the importance of\nhigh-quality datasets in RTL code generation."}
{"id": "2504.16023", "pdf": "https://arxiv.org/pdf/2504.16023", "abs": "https://arxiv.org/abs/2504.16023", "authors": ["Song Wang", "Xiaolu Liu", "Lingdong Kong", "Jianyun Xu", "Chunyong Hu", "Gongfan Fang", "Wentong Li", "Jianke Zhu", "Xinchao Wang"], "title": "PointLoRA: Low-Rank Adaptation with Token Selection for Point Cloud Learning", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Self-supervised representation learning for point cloud has demonstrated\neffectiveness in improving pre-trained model performance across diverse tasks.\nHowever, as pre-trained models grow in complexity, fully fine-tuning them for\ndownstream applications demands substantial computational and storage\nresources. Parameter-efficient fine-tuning (PEFT) methods offer a promising\nsolution to mitigate these resource requirements, yet most current approaches\nrely on complex adapter and prompt mechanisms that increase tunable parameters.\nIn this paper, we propose PointLoRA, a simple yet effective method that\ncombines low-rank adaptation (LoRA) with multi-scale token selection to\nefficiently fine-tune point cloud models. Our approach embeds LoRA layers\nwithin the most parameter-intensive components of point cloud transformers,\nreducing the need for tunable parameters while enhancing global feature\ncapture. Additionally, multi-scale token selection extracts critical local\ninformation to serve as prompts for downstream fine-tuning, effectively\ncomplementing the global context captured by LoRA. The experimental results\nacross various pre-trained models and three challenging public datasets\ndemonstrate that our approach achieves competitive performance with only 3.43%\nof the trainable parameters, making it highly effective for\nresource-constrained applications. Source code is available at:\nhttps://github.com/songw-zju/PointLoRA."}
{"id": "2504.15663", "pdf": "https://arxiv.org/pdf/2504.15663", "abs": "https://arxiv.org/abs/2504.15663", "authors": ["Ju Yeon Kang", "Ji Won Yoon", "Semin Kim", "Min Hyun Han", "Nam Soo Kim"], "title": "FADEL: Uncertainty-aware Fake Audio Detection with Evidential Deep Learning", "categories": ["eess.AS", "cs.AI"], "comment": "Accepted at ICASSP 2025", "summary": "Recently, fake audio detection has gained significant attention, as\nadvancements in speech synthesis and voice conversion have increased the\nvulnerability of automatic speaker verification (ASV) systems to spoofing\nattacks. A key challenge in this task is generalizing models to detect unseen,\nout-of-distribution (OOD) attacks. Although existing approaches have shown\npromising results, they inherently suffer from overconfidence issues due to the\nusage of softmax for classification, which can produce unreliable predictions\nwhen encountering unpredictable spoofing attempts. To deal with this\nlimitation, we propose a novel framework called fake audio detection with\nevidential learning (FADEL). By modeling class probabilities with a Dirichlet\ndistribution, FADEL incorporates model uncertainty into its predictions,\nthereby leading to more robust performance in OOD scenarios. Experimental\nresults on the ASVspoof2019 Logical Access (LA) and ASVspoof2021 LA datasets\nindicate that the proposed method significantly improves the performance of\nbaseline models. Furthermore, we demonstrate the validity of uncertainty\nestimation by analyzing a strong correlation between average uncertainty and\nequal error rate (EER) across different spoofing algorithms."}
{"id": "2504.16030", "pdf": "https://arxiv.org/pdf/2504.16030", "abs": "https://arxiv.org/abs/2504.16030", "authors": ["Joya Chen", "Ziyun Zeng", "Yiqi Lin", "Wei Li", "Zejun Ma", "Mike Zheng Shou"], "title": "LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale", "categories": ["cs.CV"], "comment": "CVPR 2025. If any references are missing, please contact\n  joyachen@u.nus.edu", "summary": "Recent video large language models (Video LLMs) often depend on costly human\nannotations or proprietary model APIs (e.g., GPT-4o) to produce training data,\nwhich limits their training at scale. In this paper, we explore large-scale\ntraining for Video LLM with cheap automatic speech recognition (ASR)\ntranscripts. Specifically, we propose a novel streaming training approach that\ndensely interleaves the ASR words and video frames according to their\ntimestamps. Compared to previous studies in vision-language representation with\nASR, our method naturally fits the streaming characteristics of ASR, thus\nenabling the model to learn temporally-aligned, fine-grained vision-language\nmodeling. To support the training algorithm, we introduce a data production\npipeline to process YouTube videos and their closed captions (CC, same as ASR),\nresulting in Live-CC-5M dataset for pre-training and Live-WhisperX-526K dataset\nfor high-quality supervised fine-tuning (SFT). Remarkably, even without SFT,\nthe ASR-only pre-trained LiveCC-7B-Base model demonstrates competitive general\nvideo QA performance and exhibits a new capability in real-time video\ncommentary. To evaluate this, we carefully design a new LiveSports-3K\nbenchmark, using LLM-as-a-judge to measure the free-form commentary.\nExperiments show our final LiveCC-7B-Instruct model can surpass advanced 72B\nmodels (Qwen2.5-VL-72B-Instruct, LLaVA-Video-72B) in commentary quality even\nworking in a real-time mode. Meanwhile, it achieves state-of-the-art results at\nthe 7B/8B scale on popular video QA benchmarks such as VideoMME and OVOBench,\ndemonstrating the broad generalizability of our approach. All resources of this\npaper have been released at https://showlab.github.io/livecc."}
{"id": "2504.15707", "pdf": "https://arxiv.org/pdf/2504.15707", "abs": "https://arxiv.org/abs/2504.15707", "authors": ["Yannic Neuhaus", "Matthias Hein"], "title": "RePOPE: Impact of Annotation Errors on the POPE Benchmark", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Since data annotation is costly, benchmark datasets often incorporate labels\nfrom established image datasets. In this work, we assess the impact of label\nerrors in MSCOCO on the frequently used object hallucination benchmark POPE. We\nre-annotate the benchmark images and identify an imbalance in annotation errors\nacross different subsets. Evaluating multiple models on the revised labels,\nwhich we denote as RePOPE, we observe notable shifts in model rankings,\nhighlighting the impact of label quality. Code and data are available at\nhttps://github.com/YanNeu/RePOPE ."}
{"id": "2504.16047", "pdf": "https://arxiv.org/pdf/2504.16047", "abs": "https://arxiv.org/abs/2504.16047", "authors": ["Frank Li", "Hari Trivedi", "Bardia Khosravi", "Theo Dapamede", "Mohammadreza Chavoshi", "Abdulhameed Dere", "Rohan Satya Isaac", "Aawez Mansuri", "Janice Newsome", "Saptarshi Purkayastha", "Judy Gichoya"], "title": "Evaluating Vision Language Models (VLMs) for Radiology: A Comprehensive Analysis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Foundation models, trained on vast amounts of data using self-supervised\ntechniques, have emerged as a promising frontier for advancing artificial\nintelligence (AI) applications in medicine. This study evaluates three\ndifferent vision-language foundation models (RAD-DINO, CheXagent, and\nBiomedCLIP) on their ability to capture fine-grained imaging features for\nradiology tasks. The models were assessed across classification, segmentation,\nand regression tasks for pneumothorax and cardiomegaly on chest radiographs.\nSelf-supervised RAD-DINO consistently excelled in segmentation tasks, while\ntext-supervised CheXagent demonstrated superior classification performance.\nBiomedCLIP showed inconsistent performance across tasks. A custom segmentation\nmodel that integrates global and local features substantially improved\nperformance for all foundation models, particularly for challenging\npneumothorax segmentation. The findings highlight that pre-training methodology\nsignificantly influences model performance on specific downstream tasks. For\nfine-grained segmentation tasks, models trained without text supervision\nperformed better, while text-supervised models offered advantages in\nclassification and interpretability. These insights provide guidance for\nselecting foundation models based on specific clinical applications in\nradiology."}
{"id": "2504.15724", "pdf": "https://arxiv.org/pdf/2504.15724", "abs": "https://arxiv.org/abs/2504.15724", "authors": ["Yiannis Papageorgiou", "Yannis Thomas", "Alexios Filippakopoulos", "Ramin Khalili", "Iordanis Koutsopoulos"], "title": "Collaborative Split Federated Learning with Parallel Training and Aggregation", "categories": ["cs.DC", "cs.AI"], "comment": null, "summary": "Federated learning (FL) operates based on model exchanges between the server\nand the clients, and it suffers from significant client-side computation and\ncommunication burden. Split federated learning (SFL) arises a promising\nsolution by splitting the model into two parts, that are trained sequentially:\nthe clients train the first part of the model (client-side model) and transmit\nit to the server that trains the second (server-side model). Existing SFL\nschemes though still exhibit long training delays and significant communication\noverhead, especially when clients of different computing capability\nparticipate. Thus, we propose Collaborative-Split Federated Learning~(C-SFL), a\nnovel scheme that splits the model into three parts, namely the model parts\ntrained at the computationally weak clients, the ones trained at the\ncomputationally strong clients, and the ones at the server. Unlike existing\nworks, C-SFL enables parallel training and aggregation of model's parts at the\nclients and at the server, resulting in reduced training delays and\ncommmunication overhead while improving the model's accuracy. Experiments\nverify the multiple gains of C-SFL against the existing schemes."}
{"id": "2504.16061", "pdf": "https://arxiv.org/pdf/2504.16061", "abs": "https://arxiv.org/abs/2504.16061", "authors": ["Sangeet Khemlani", "Tyler Tran", "Nathaniel Gyory", "Anthony M. Harrison", "Wallace E. Lawson", "Ravenna Thielstrom", "Hunter Thompson", "Taaren Singh", "J. Gregory Trafton"], "title": "Vision language models are unreliable at trivial spatial cognition", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision language models (VLMs) are designed to extract relevant visuospatial\ninformation from images. Some research suggests that VLMs can exhibit humanlike\nscene understanding, while other investigations reveal difficulties in their\nability to process relational information. To achieve widespread applicability,\nVLMs must perform reliably, yielding comparable competence across a wide\nvariety of related tasks. We sought to test how reliable these architectures\nare at engaging in trivial spatial cognition, e.g., recognizing whether one\nobject is left of another in an uncluttered scene. We developed a benchmark\ndataset -- TableTest -- whose images depict 3D scenes of objects arranged on a\ntable, and used it to evaluate state-of-the-art VLMs. Results show that\nperformance could be degraded by minor variations of prompts that use logically\nequivalent descriptions. These analyses suggest limitations in how VLMs may\nreason about spatial relations in real-world applications. They also reveal\nnovel opportunities for bolstering image caption corpora for more efficient\ntraining and testing."}
{"id": "2504.15743", "pdf": "https://arxiv.org/pdf/2504.15743", "abs": "https://arxiv.org/abs/2504.15743", "authors": ["Seung Gyu Jeong", "Sung Woo Nam", "Seong Kwan Jung", "Seong-Eun Kim"], "title": "iMedic: Towards Smartphone-based Self-Auscultation Tool for AI-Powered Pediatric Respiratory Assessment", "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "Respiratory auscultation is crucial for early detection of pediatric\npneumonia, a condition that can quickly worsen without timely intervention. In\nareas with limited physician access, effective auscultation is challenging. We\npresent a smartphone-based system that leverages built-in microphones and\nadvanced deep learning algorithms to detect abnormal respiratory sounds\nindicative of pneumonia risk. Our end-to-end deep learning framework employs\ndomain generalization to integrate a large electronic stethoscope dataset with\na smaller smartphone-derived dataset, enabling robust feature learning for\naccurate respiratory assessments without expensive equipment. The accompanying\nmobile application guides caregivers in collecting high-quality lung sound\nsamples and provides immediate feedback on potential pneumonia risks. User\nstudies show strong classification performance and high acceptance,\ndemonstrating the system's ability to facilitate proactive interventions and\nreduce preventable childhood pneumonia deaths. By seamlessly integrating into\nubiquitous smartphones, this approach offers a promising avenue for more\nequitable and comprehensive remote pediatric care."}
{"id": "2504.16064", "pdf": "https://arxiv.org/pdf/2504.16064", "abs": "https://arxiv.org/abs/2504.16064", "authors": ["Theodoros Kouzelis", "Efstathios Karypidis", "Ioannis Kakogeorgiou", "Spyros Gidaris", "Nikos Komodakis"], "title": "Boosting Generative Image Modeling via Joint Image-Feature Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "Latent diffusion models (LDMs) dominate high-quality image generation, yet\nintegrating representation learning with generative modeling remains a\nchallenge. We introduce a novel generative image modeling framework that\nseamlessly bridges this gap by leveraging a diffusion model to jointly model\nlow-level image latents (from a variational autoencoder) and high-level\nsemantic features (from a pretrained self-supervised encoder like DINO). Our\nlatent-semantic diffusion approach learns to generate coherent image-feature\npairs from pure noise, significantly enhancing both generative quality and\ntraining efficiency, all while requiring only minimal modifications to standard\nDiffusion Transformer architectures. By eliminating the need for complex\ndistillation objectives, our unified design simplifies training and unlocks a\npowerful new inference strategy: Representation Guidance, which leverages\nlearned semantics to steer and refine image generation. Evaluated in both\nconditional and unconditional settings, our method delivers substantial\nimprovements in image quality and training convergence speed, establishing a\nnew direction for representation-aware generative modeling."}
{"id": "2504.15766", "pdf": "https://arxiv.org/pdf/2504.15766", "abs": "https://arxiv.org/abs/2504.15766", "authors": ["Tobias Demmler", "Lennart Hartung", "Andreas Tamke", "Thao Dang", "Alexander Hegai", "Karsten Haug", "Lars Mikelsons"], "title": "Dynamic Intent Queries for Motion Transformer-based Trajectory Prediction", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "In autonomous driving, accurately predicting the movements of other traffic\nparticipants is crucial, as it significantly influences a vehicle's planning\nprocesses. Modern trajectory prediction models strive to interpret complex\npatterns and dependencies from agent and map data. The Motion Transformer (MTR)\narchitecture and subsequent work define the most accurate methods in common\nbenchmarks such as the Waymo Open Motion Benchmark. The MTR model employs\npre-generated static intention points as initial goal points for trajectory\nprediction. However, the static nature of these points frequently leads to\nmisalignment with map data in specific traffic scenarios, resulting in\nunfeasible or unrealistic goal points. Our research addresses this limitation\nby integrating scene-specific dynamic intention points into the MTR model. This\nadaptation of the MTR model was trained and evaluated on the Waymo Open Motion\nDataset. Our findings demonstrate that incorporating dynamic intention points\nhas a significant positive impact on trajectory prediction accuracy, especially\nfor predictions over long time horizons. Furthermore, we analyze the impact on\nground truth trajectories which are not compliant with the map data or are\nillegal maneuvers."}
{"id": "2504.16072", "pdf": "https://arxiv.org/pdf/2504.16072", "abs": "https://arxiv.org/abs/2504.16072", "authors": ["Long Lian", "Yifan Ding", "Yunhao Ge", "Sifei Liu", "Hanzi Mao", "Boyi Li", "Marco Pavone", "Ming-Yu Liu", "Trevor Darrell", "Adam Yala", "Yin Cui"], "title": "Describe Anything: Detailed Localized Image and Video Captioning", "categories": ["cs.CV", "cs.AI"], "comment": "Project page: https://describe-anything.github.io/", "summary": "Generating detailed and accurate descriptions for specific regions in images\nand videos remains a fundamental challenge for vision-language models. We\nintroduce the Describe Anything Model (DAM), a model designed for detailed\nlocalized captioning (DLC). DAM preserves both local details and global context\nthrough two key innovations: a focal prompt, which ensures high-resolution\nencoding of targeted regions, and a localized vision backbone, which integrates\nprecise localization with its broader context. To tackle the scarcity of\nhigh-quality DLC data, we propose a Semi-supervised learning (SSL)-based Data\nPipeline (DLC-SDP). DLC-SDP starts with existing segmentation datasets and\nexpands to unlabeled web images using SSL. We introduce DLC-Bench, a benchmark\ndesigned to evaluate DLC without relying on reference captions. DAM sets new\nstate-of-the-art on 7 benchmarks spanning keyword-level, phrase-level, and\ndetailed multi-sentence localized image and video captioning."}
{"id": "2504.15773", "pdf": "https://arxiv.org/pdf/2504.15773", "abs": "https://arxiv.org/abs/2504.15773", "authors": ["Cong Liu", "Sharvaree Vadgama", "David Ruhe", "Erik Bekkers", "Patrick Forrè"], "title": "Clifford Group Equivariant Diffusion Models for 3D Molecular Generation", "categories": ["cs.LG", "cs.AI"], "comment": "7 pages, 1 figure, 1 table", "summary": "This paper explores leveraging the Clifford algebra's expressive power for\n$\\E(n)$-equivariant diffusion models. We utilize the geometric products between\nClifford multivectors and the rich geometric information encoded in Clifford\nsubspaces in \\emph{Clifford Diffusion Models} (CDMs). We extend the diffusion\nprocess beyond just Clifford one-vectors to incorporate all higher-grade\nmultivector subspaces. The data is embedded in grade-$k$ subspaces, allowing us\nto apply latent diffusion across complete multivectors. This enables CDMs to\ncapture the joint distribution across different subspaces of the algebra,\nincorporating richer geometric information through higher-order features. We\nprovide empirical results for unconditional molecular generation on the QM9\ndataset, showing that CDMs provide a promising avenue for generative modeling."}
{"id": "2504.16080", "pdf": "https://arxiv.org/pdf/2504.16080", "abs": "https://arxiv.org/abs/2504.16080", "authors": ["Le Zhuo", "Liangbing Zhao", "Sayak Paul", "Yue Liao", "Renrui Zhang", "Yi Xin", "Peng Gao", "Mohamed Elhoseiny", "Hongsheng Li"], "title": "From Reflection to Perfection: Scaling Inference-Time Optimization for Text-to-Image Diffusion Models via Reflection Tuning", "categories": ["cs.CV"], "comment": "All code, checkpoints, and datasets are available at\n  \\url{https://diffusion-cot.github.io/reflection2perfection}", "summary": "Recent text-to-image diffusion models achieve impressive visual quality\nthrough extensive scaling of training data and model parameters, yet they often\nstruggle with complex scenes and fine-grained details. Inspired by the\nself-reflection capabilities emergent in large language models, we propose\nReflectionFlow, an inference-time framework enabling diffusion models to\niteratively reflect upon and refine their outputs. ReflectionFlow introduces\nthree complementary inference-time scaling axes: (1) noise-level scaling to\noptimize latent initialization; (2) prompt-level scaling for precise semantic\nguidance; and most notably, (3) reflection-level scaling, which explicitly\nprovides actionable reflections to iteratively assess and correct previous\ngenerations. To facilitate reflection-level scaling, we construct GenRef, a\nlarge-scale dataset comprising 1 million triplets, each containing a\nreflection, a flawed image, and an enhanced image. Leveraging this dataset, we\nefficiently perform reflection tuning on state-of-the-art diffusion\ntransformer, FLUX.1-dev, by jointly modeling multimodal inputs within a unified\nframework. Experimental results show that ReflectionFlow significantly\noutperforms naive noise-level scaling methods, offering a scalable and\ncompute-efficient solution toward higher-quality image synthesis on challenging\ntasks."}
{"id": "2504.15779", "pdf": "https://arxiv.org/pdf/2504.15779", "abs": "https://arxiv.org/abs/2504.15779", "authors": ["Aaron J. Gutknecht", "Fernando E. Rosas", "David A. Ehrlich", "Abdullah Makkeh", "Pedro A. M. Mediano", "Michael Wibral"], "title": "Shannon invariants: A scalable approach to information decomposition", "categories": ["cs.IT", "cs.AI", "cs.LG", "math.IT", "nlin.AO", "physics.data-an"], "comment": "16 pages, 4 Figures", "summary": "Distributed systems, such as biological and artificial neural networks,\nprocess information via complex interactions engaging multiple subsystems,\nresulting in high-order patterns with distinct properties across scales.\nInvestigating how these systems process information remains challenging due to\ndifficulties in defining appropriate multivariate metrics and ensuring their\nscalability to large systems. To address these challenges, we introduce a novel\nframework based on what we call \"Shannon invariants\" -- quantities that capture\nessential properties of high-order information processing in a way that depends\nonly on the definition of entropy and can be efficiently calculated for large\nsystems. Our theoretical results demonstrate how Shannon invariants can be used\nto resolve long-standing ambiguities regarding the interpretation of widely\nused multivariate information-theoretic measures. Moreover, our practical\nresults reveal distinctive information-processing signatures of various deep\nlearning architectures across layers, which lead to new insights into how these\nsystems process information and how this evolves during training. Overall, our\nframework resolves fundamental limitations in analyzing high-order phenomena\nand offers broad opportunities for theoretical developments and empirical\nanalyses."}
{"id": "2504.16081", "pdf": "https://arxiv.org/pdf/2504.16081", "abs": "https://arxiv.org/abs/2504.16081", "authors": ["Yimu Wang", "Xuye Liu", "Wei Pang", "Li Ma", "Shuai Yuan", "Paul Debevec", "Ning Yu"], "title": "Survey of Video Diffusion Models: Foundations, Implementations, and Applications", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Recent advances in diffusion models have revolutionized video generation,\noffering superior temporal consistency and visual quality compared to\ntraditional generative adversarial networks-based approaches. While this\nemerging field shows tremendous promise in applications, it faces significant\nchallenges in motion consistency, computational efficiency, and ethical\nconsiderations. This survey provides a comprehensive review of diffusion-based\nvideo generation, examining its evolution, technical foundations, and practical\napplications. We present a systematic taxonomy of current methodologies,\nanalyze architectural innovations and optimization strategies, and investigate\napplications across low-level vision tasks such as denoising and\nsuper-resolution. Additionally, we explore the synergies between diffusionbased\nvideo generation and related domains, including video representation learning,\nquestion answering, and retrieval. Compared to the existing surveys (Lei et\nal., 2024a;b; Melnik et al., 2024; Cao et al., 2023; Xing et al., 2024c) which\nfocus on specific aspects of video generation, such as human video synthesis\n(Lei et al., 2024a) or long-form content generation (Lei et al., 2024b), our\nwork provides a broader, more updated, and more fine-grained perspective on\ndiffusion-based approaches with a special section for evaluation metrics,\nindustry solutions, and training engineering techniques in video generation.\nThis survey serves as a foundational resource for researchers and practitioners\nworking at the intersection of diffusion models and video generation, providing\ninsights into both the theoretical frameworks and practical implementations\nthat drive this rapidly evolving field. A structured list of related works\ninvolved in this survey is also available on\nhttps://github.com/Eyeline-Research/Survey-Video-Diffusion."}
{"id": "2504.15784", "pdf": "https://arxiv.org/pdf/2504.15784", "abs": "https://arxiv.org/abs/2504.15784", "authors": ["Ruizhe Li", "Chiwei Zhu", "Benfeng Xu", "Xiaorui Wang", "Zhendong Mao"], "title": "Automated Creativity Evaluation for Large Language Models: A Reference-Based Approach", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Creative writing is a key capability of Large Language Models (LLMs), with\npotential applications in literature, storytelling, and various creative\ndomains. However, evaluating the creativity of machine-generated texts remains\na significant challenge, as existing methods either rely on costly manual\nannotations or fail to align closely with human assessments. In this paper, we\npropose an effective automated evaluation method based on the Torrance Test of\nCreative Writing (TTCW), which evaluates creativity as product. Our method\nemploys a reference-based Likert-style approach, scoring generated creative\ntexts relative to high-quality reference texts across various tests.\nExperimental results demonstrate that our method significantly improves the\nalignment between LLM evaluations and human assessments, achieving a pairwise\naccuracy of 0.75 (+15\\%)."}
{"id": "2504.16082", "pdf": "https://arxiv.org/pdf/2504.16082", "abs": "https://arxiv.org/abs/2504.16082", "authors": ["Ziqi Pang", "Yu-Xiong Wang"], "title": "MR. Video: \"MapReduce\" is the Principle for Long Video Understanding", "categories": ["cs.CV"], "comment": "Preprint", "summary": "We propose MR. Video, an agentic long video understanding framework that\ndemonstrates the simple yet effective MapReduce principle for processing long\nvideos: (1) Map: independently and densely perceiving short video clips, and\n(2) Reduce: jointly aggregating information from all clips. Compared with\nsequence-to-sequence vision-language models (VLMs), MR. Video performs detailed\nshort video perception without being limited by context length. Compared with\nexisting video agents that typically rely on sequential key segment selection,\nthe Map operation enables simpler and more scalable sequence parallel\nperception of short video segments. Its Reduce step allows for more\ncomprehensive context aggregation and reasoning, surpassing explicit key\nsegment retrieval. This MapReduce principle is applicable to both VLMs and\nvideo agents, and we use LLM agents to validate its effectiveness.\n  In practice, MR. Video employs two MapReduce stages: (A) Captioning:\ngenerating captions for short video clips (map), then standardizing repeated\ncharacters and objects into shared names (reduce); (B) Analysis: for each user\nquestion, analyzing relevant information from individual short videos (map),\nand integrating them into a final answer (reduce). MR. Video achieves over 10%\naccuracy improvement on the challenging LVBench compared to state-of-the-art\nVLMs and video agents.\n  Code is available at: https://github.com/ziqipang/MR-Video"}
{"id": "2504.15801", "pdf": "https://arxiv.org/pdf/2504.15801", "abs": "https://arxiv.org/abs/2504.15801", "authors": ["Valeria Lerman", "Yaniv Dover"], "title": "A closer look at how large language models trust humans: patterns and biases", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "As large language models (LLMs) and LLM-based agents increasingly interact\nwith humans in decision-making contexts, understanding the trust dynamics\nbetween humans and AI agents becomes a central concern. While considerable\nliterature studies how humans trust AI agents, it is much less understood how\nLLM-based agents develop effective trust in humans. LLM-based agents likely\nrely on some sort of implicit effective trust in trust-related contexts (e.g.,\nevaluating individual loan applications) to assist and affect decision making.\nUsing established behavioral theories, we develop an approach that studies\nwhether LLMs trust depends on the three major trustworthiness dimensions:\ncompetence, benevolence and integrity of the human subject. We also study how\ndemographic variables affect effective trust. Across 43,200 simulated\nexperiments, for five popular language models, across five different scenarios\nwe find that LLM trust development shows an overall similarity to human trust\ndevelopment. We find that in most, but not all cases, LLM trust is strongly\npredicted by trustworthiness, and in some cases also biased by age, religion\nand gender, especially in financial scenarios. This is particularly true for\nscenarios common in the literature and for newer models. While the overall\npatterns align with human-like mechanisms of effective trust formation,\ndifferent models exhibit variation in how they estimate trust; in some cases,\ntrustworthiness and demographic factors are weak predictors of effective trust.\nThese findings call for a better understanding of AI-to-human trust dynamics\nand monitoring of biases and trust development patterns to prevent unintended\nand potentially harmful outcomes in trust-sensitive applications of AI."}
{"id": "2504.16083", "pdf": "https://arxiv.org/pdf/2504.16083", "abs": "https://arxiv.org/abs/2504.16083", "authors": ["Yucheng Li", "Huiqiang Jiang", "Chengruidong Zhang", "Qianhui Wu", "Xufang Luo", "Surin Ahn", "Amir H. Abdi", "Dongsheng Li", "Jianfeng Gao", "Yuqing Yang", "Lili Qiu"], "title": "MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The integration of long-context capabilities with visual understanding\nunlocks unprecedented potential for Vision Language Models (VLMs). However, the\nquadratic attention complexity during the pre-filling phase remains a\nsignificant obstacle to real-world deployment. To overcome this limitation, we\nintroduce MMInference (Multimodality Million tokens Inference), a dynamic\nsparse attention method that accelerates the prefilling stage for long-context\nmulti-modal inputs. First, our analysis reveals that the temporal and spatial\nlocality of video input leads to a unique sparse pattern, the Grid pattern.\nSimultaneously, VLMs exhibit markedly different sparse distributions across\ndifferent modalities. We introduce a permutation-based method to leverage the\nunique Grid pattern and handle modality boundary issues. By offline search the\noptimal sparse patterns for each head, MMInference constructs the sparse\ndistribution dynamically based on the input. We also provide optimized GPU\nkernels for efficient sparse computations. Notably, MMInference integrates\nseamlessly into existing VLM pipelines without any model modifications or\nfine-tuning. Experiments on multi-modal benchmarks-including Video QA,\nCaptioning, VisionNIAH, and Mixed-Modality NIAH-with state-of-the-art\nlong-context VLMs (LongVila, LlavaVideo, VideoChat-Flash, Qwen2.5-VL) show that\nMMInference accelerates the pre-filling stage by up to 8.3x at 1M tokens while\nmaintaining accuracy. Our code is available at https://aka.ms/MMInference."}
{"id": "2504.15804", "pdf": "https://arxiv.org/pdf/2504.15804", "abs": "https://arxiv.org/abs/2504.15804", "authors": ["Ning Wang", "Bingkun Yao", "Jie Zhou", "Yuchen Hu", "Xi Wang", "Nan Guan", "Zhe Jiang"], "title": "Insights from Verification: Training a Verilog Generation LLM with Reinforcement Learning with Testbench Feedback", "categories": ["cs.AR", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have shown strong performance in Verilog\ngeneration from natural language description. However, ensuring the functional\ncorrectness of the generated code remains a significant challenge. This paper\nintroduces a method that integrates verification insights from testbench into\nthe training of Verilog generation LLMs, aligning the training with the\nfundamental goal of hardware design: functional correctness. The main obstacle\nin using LLMs for Verilog code generation is the lack of sufficient functional\nverification data, particularly testbenches paired with design specifications\nand code. To address this problem, we introduce an automatic testbench\ngeneration pipeline that decomposes the process and uses feedback from the\nVerilog compiler simulator (VCS) to reduce hallucination and ensure\ncorrectness. We then use the testbench to evaluate the generated codes and\ncollect them for further training, where verification insights are introduced.\nOur method applies reinforcement learning (RL), specifically direct preference\noptimization (DPO), to align Verilog code generation with functional\ncorrectness by training preference pairs based on testbench outcomes. In\nevaluations on VerilogEval-Machine, VerilogEval-Human, RTLLM v1.1, RTLLM v2,\nand VerilogEval v2, our approach consistently outperforms state-of-the-art\nbaselines in generating functionally correct Verilog code. We open source all\ntraining code, data, and models at\nhttps://anonymous.4open.science/r/VeriPrefer-E88B."}
{"id": "2504.09697", "pdf": "https://arxiv.org/pdf/2504.09697", "abs": "https://arxiv.org/abs/2504.09697", "authors": ["Kenan Tang", "Yanhong Li", "Yao Qin"], "title": "SPICE: A Synergistic, Precise, Iterative, and Customizable Image Editing Workflow", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "24 pages, 21 figures. Figure 9(b) has been accepted by CVPR AI Art\n  Gallery 2025", "summary": "Recent prompt-based image editing models have demonstrated impressive\nprompt-following capability at structural editing tasks. However, existing\nmodels still fail to perform local edits, follow detailed editing prompts, or\nmaintain global image quality beyond a single editing step. To address these\nchallenges, we introduce SPICE, a training-free workflow that accepts arbitrary\nresolutions and aspect ratios, accurately follows user requirements, and\nimproves image quality consistently during more than 100 editing steps. By\nsynergizing the strengths of a base diffusion model and a Canny edge ControlNet\nmodel, SPICE robustly handles free-form editing instructions from the user.\nSPICE outperforms state-of-the-art baselines on a challenging realistic\nimage-editing dataset consisting of semantic editing (object addition, removal,\nreplacement, and background change), stylistic editing (texture changes), and\nstructural editing (action change) tasks. Not only does SPICE achieve the\nhighest quantitative performance according to standard evaluation metrics, but\nit is also consistently preferred by users over existing image-editing methods.\nWe release the workflow implementation for popular diffusion model Web UIs to\nsupport further research and artistic exploration."}
{"id": "2504.15806", "pdf": "https://arxiv.org/pdf/2504.15806", "abs": "https://arxiv.org/abs/2504.15806", "authors": ["Kai Luo", "Juan Tang", "Mingchao Cai", "Xiaoqing Zeng", "Manqi Xie", "Ming Yan"], "title": "DAE-KAN: A Kolmogorov-Arnold Network Model for High-Index Differential-Algebraic Equations", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to\nMulti-Layer Perceptrons (MLPs) due to their superior function-fitting abilities\nin data-driven modeling. In this paper, we propose a novel framework, DAE-KAN,\nfor solving high-index differential-algebraic equations (DAEs) by integrating\nKANs with Physics-Informed Neural Networks (PINNs). This framework not only\npreserves the ability of traditional PINNs to model complex systems governed by\nphysical laws but also enhances their performance by leveraging the\nfunction-fitting strengths of KANs. Numerical experiments demonstrate that for\nDAE systems ranging from index-1 to index-3, DAE-KAN reduces the absolute\nerrors of both differential and algebraic variables by 1 to 2 orders of\nmagnitude compared to traditional PINNs. To assess the effectiveness of this\napproach, we analyze the drift-off error and find that both PINNs and DAE-KAN\noutperform classical numerical methods in controlling this phenomenon. Our\nresults highlight the potential of neural network methods, particularly\nDAE-KAN, in solving high-index DAEs with substantial computational accuracy and\ngeneralization, offering a promising solution for challenging partial\ndifferential-algebraic equations."}
{"id": "2504.15305", "pdf": "https://arxiv.org/pdf/2504.15305", "abs": "https://arxiv.org/abs/2504.15305", "authors": ["Abhishek Tyagi", "Charu Gaur"], "title": "SLAM-Based Navigation and Fault Resilience in a Surveillance Quadcopter with Embedded Vision Systems", "categories": ["cs.RO", "cs.CV", "cs.SY", "eess.SY", "68T40, 68U10, 70Q05", "I.2.9; I.4.8; I.2.10; C.3"], "comment": "18 pages, 21 figures, 4 tables. Onboard processing using Raspberry Pi\n  4 and Arduino Nano. Includes ORB-SLAM3-based navigation, LQR control, rotor\n  fault recovery, object detection, and PCA face recognition. Real-world and\n  simulation tests included. Designed for GPS-denied autonomous UAV\n  surveillance", "summary": "We present an autonomous aerial surveillance platform, Veg, designed as a\nfault-tolerant quadcopter system that integrates visual SLAM for\nGPS-independent navigation, advanced control architecture for dynamic\nstability, and embedded vision modules for real-time object and face\nrecognition. The platform features a cascaded control design with an LQR\ninner-loop and PD outer-loop trajectory control. It leverages ORB-SLAM3 for\n6-DoF localization and loop closure, and supports waypoint-based navigation\nthrough Dijkstra path planning over SLAM-derived maps. A real-time Failure\nDetection and Identification (FDI) system detects rotor faults and executes\nemergency landing through re-routing. The embedded vision system, based on a\nlightweight CNN and PCA, enables onboard object detection and face recognition\nwith high precision. The drone operates fully onboard using a Raspberry Pi 4\nand Arduino Nano, validated through simulations and real-world testing. This\nwork consolidates real-time localization, fault recovery, and embedded AI on a\nsingle platform suitable for constrained environments."}
{"id": "2504.15812", "pdf": "https://arxiv.org/pdf/2504.15812", "abs": "https://arxiv.org/abs/2504.15812", "authors": ["Xuchuang Wang", "Qirun Zeng", "Jinhang Zuo", "Xutong Liu", "Mohammad Hajiesmaili", "John C. S. Lui", "Adam Wierman"], "title": "Fusing Reward and Dueling Feedback in Stochastic Bandits", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This paper investigates the fusion of absolute (reward) and relative\n(dueling) feedback in stochastic bandits, where both feedback types are\ngathered in each decision round. We derive a regret lower bound, demonstrating\nthat an efficient algorithm may incur only the smaller among the reward and\ndueling-based regret for each individual arm. We propose two fusion approaches:\n(1) a simple elimination fusion algorithm that leverages both feedback types to\nexplore all arms and unifies collected information by sharing a common\ncandidate arm set, and (2) a decomposition fusion algorithm that selects the\nmore effective feedback to explore the corresponding arms and randomly assigns\none feedback type for exploration and the other for exploitation in each round.\nThe elimination fusion experiences a suboptimal multiplicative term of the\nnumber of arms in regret due to the intrinsic suboptimality of dueling\nelimination. In contrast, the decomposition fusion achieves regret matching the\nlower bound up to a constant under a common assumption. Extensive experiments\nconfirm the efficacy of our algorithms and theoretical results."}
{"id": "2504.15317", "pdf": "https://arxiv.org/pdf/2504.15317", "abs": "https://arxiv.org/abs/2504.15317", "authors": ["Meher Boulaabi", "Takwa Ben Aïcha Gader", "Afef Kacem Echi", "Zied Bouraoui"], "title": "Enhancing DR Classification with Swin Transformer and Shifted Window Attention", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Diabetic retinopathy (DR) is a leading cause of blindness worldwide,\nunderscoring the importance of early detection for effective treatment.\nHowever, automated DR classification remains challenging due to variations in\nimage quality, class imbalance, and pixel-level similarities that hinder model\ntraining. To address these issues, we propose a robust preprocessing pipeline\nincorporating image cropping, Contrast-Limited Adaptive Histogram Equalization\n(CLAHE), and targeted data augmentation to improve model generalization and\nresilience. Our approach leverages the Swin Transformer, which utilizes\nhierarchical token processing and shifted window attention to efficiently\ncapture fine-grained features while maintaining linear computational\ncomplexity. We validate our method on the Aptos and IDRiD datasets for\nmulti-class DR classification, achieving accuracy rates of 89.65% and 97.40%,\nrespectively. These results demonstrate the effectiveness of our model,\nparticularly in detecting early-stage DR, highlighting its potential for\nimproving automated retinal screening in clinical settings."}
{"id": "2504.15823", "pdf": "https://arxiv.org/pdf/2504.15823", "abs": "https://arxiv.org/abs/2504.15823", "authors": ["Songyan Xie", "Jinghang Wen", "Encheng Su", "Qiucheng Yu"], "title": "Human-Imperceptible Physical Adversarial Attack for NIR Face Recognition Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Near-infrared (NIR) face recognition systems, which can operate effectively\nin low-light conditions or in the presence of makeup, exhibit vulnerabilities\nwhen subjected to physical adversarial attacks. To further demonstrate the\npotential risks in real-world applications, we design a novel, stealthy, and\npractical adversarial patch to attack NIR face recognition systems in a\nblack-box setting. We achieved this by utilizing human-imperceptible\ninfrared-absorbing ink to generate multiple patches with digitally optimized\nshapes and positions for infrared images. To address the optimization mismatch\nbetween digital and real-world NIR imaging, we develop a light reflection model\nfor human skin to minimize pixel-level discrepancies by simulating NIR light\nreflection.\n  Compared to state-of-the-art (SOTA) physical attacks on NIR face recognition\nsystems, the experimental results show that our method improves the attack\nsuccess rate in both digital and physical domains, particularly maintaining\neffectiveness across various face postures. Notably, the proposed approach\noutperforms SOTA methods, achieving an average attack success rate of 82.46% in\nthe physical domain across different models, compared to 64.18% for existing\nmethods. The artifact is available at\nhttps://anonymous.4open.science/r/Human-imperceptible-adversarial-patch-0703/."}
{"id": "2504.15323", "pdf": "https://arxiv.org/pdf/2504.15323", "abs": "https://arxiv.org/abs/2504.15323", "authors": ["Donggyun Kim", "Chanwoo Kim", "Seunghoon Hong"], "title": "HyperFlow: Gradient-Free Emulation of Few-Shot Fine-Tuning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "While test-time fine-tuning is beneficial in few-shot learning, the need for\nmultiple backpropagation steps can be prohibitively expensive in real-time or\nlow-resource scenarios. To address this limitation, we propose an approach that\nemulates gradient descent without computing gradients, enabling efficient\ntest-time adaptation. Specifically, we formulate gradient descent as an Euler\ndiscretization of an ordinary differential equation (ODE) and train an\nauxiliary network to predict the task-conditional drift using only the few-shot\nsupport set. The adaptation then reduces to a simple numerical integration\n(e.g., via the Euler method), which requires only a few forward passes of the\nauxiliary network -- no gradients or forward passes of the target model are\nneeded. In experiments on cross-domain few-shot classification using the\nMeta-Dataset and CDFSL benchmarks, our method significantly improves\nout-of-domain performance over the non-fine-tuned baseline while incurring only\n6\\% of the memory cost and 0.02\\% of the computation time of standard\nfine-tuning, thus establishing a practical middle ground between direct\ntransfer and fully fine-tuned approaches."}
{"id": "2504.15827", "pdf": "https://arxiv.org/pdf/2504.15827", "abs": "https://arxiv.org/abs/2504.15827", "authors": ["Xuyang Zhong", "Haochen Luo", "Chen Liu"], "title": "DualOptim: Enhancing Efficacy and Stability in Machine Unlearning with Dual Optimizers", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Existing machine unlearning (MU) approaches exhibit significant sensitivity\nto hyperparameters, requiring meticulous tuning that limits practical\ndeployment. In this work, we first empirically demonstrate the instability and\nsuboptimal performance of existing popular MU methods when deployed in\ndifferent scenarios. To address this issue, we propose Dual Optimizer\n(DualOptim), which incorporates adaptive learning rate and decoupled momentum\nfactors. Empirical and theoretical evidence demonstrates that DualOptim\ncontributes to effective and stable unlearning. Through extensive experiments,\nwe show that DualOptim can significantly boost MU efficacy and stability across\ndiverse tasks, including image classification, image generation, and large\nlanguage models, making it a versatile approach to empower existing MU\nalgorithms."}
{"id": "2504.15329", "pdf": "https://arxiv.org/pdf/2504.15329", "abs": "https://arxiv.org/abs/2504.15329", "authors": ["Yike Zhang", "Eduardo Davalos", "Jack Noble"], "title": "Vision6D: 3D-to-2D Interactive Visualization and Annotation Tool for 6D Pose Estimation", "categories": ["cs.GR", "cs.CV", "cs.HC", "cs.RO"], "comment": null, "summary": "Accurate 6D pose estimation has gained more attention over the years for\nrobotics-assisted tasks that require precise interaction with physical objects.\nThis paper presents an interactive 3D-to-2D visualization and annotation tool\nto support the 6D pose estimation research community. To the best of our\nknowledge, the proposed work is the first tool that allows users to visualize\nand manipulate 3D objects interactively on a 2D real-world scene, along with a\ncomprehensive user study. This system supports robust 6D camera pose annotation\nby providing both visual cues and spatial relationships to determine object\nposition and orientation in various environments. The annotation feature in\nVision6D is particularly helpful in scenarios where the transformation matrix\nbetween the camera and world objects is unknown, as it enables accurate\nannotation of these objects' poses using only the camera intrinsic matrix. This\ncapability serves as a foundational step in developing and training advanced\npose estimation models across various domains. We evaluate Vision6D's\neffectiveness by utilizing widely-used open-source pose estimation datasets\nLinemod and HANDAL through comparisons between the default ground-truth camera\nposes with manual annotations. A user study was performed to show that Vision6D\ngenerates accurate pose annotations via visual cues in an intuitive 3D user\ninterface. This approach aims to bridge the gap between 2D scene projections\nand 3D scenes, offering an effective way for researchers and developers to\nsolve 6D pose annotation related problems. The software is open-source and\npublicly available at https://github.com/InteractiveGL/vision6D."}
{"id": "2504.15865", "pdf": "https://arxiv.org/pdf/2504.15865", "abs": "https://arxiv.org/abs/2504.15865", "authors": ["Lotfi Abdelkrim Mecharbat", "Ibrahim Elmakky", "Martin Takac", "Mohammed Yaqub"], "title": "MedNNS: Supernet-based Medical Task-Adaptive Neural Network Search", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Deep learning (DL) has achieved remarkable progress in the field of medical\nimaging. However, adapting DL models to medical tasks remains a significant\nchallenge, primarily due to two key factors: (1) architecture selection, as\ndifferent tasks necessitate specialized model designs, and (2) weight\ninitialization, which directly impacts the convergence speed and final\nperformance of the models. Although transfer learning from ImageNet is a widely\nadopted strategy, its effectiveness is constrained by the substantial\ndifferences between natural and medical images. To address these challenges, we\nintroduce Medical Neural Network Search (MedNNS), the first Neural Network\nSearch framework for medical imaging applications. MedNNS jointly optimizes\narchitecture selection and weight initialization by constructing a meta-space\nthat encodes datasets and models based on how well they perform together. We\nbuild this space using a Supernetwork-based approach, expanding the model zoo\nsize by 51x times over previous state-of-the-art (SOTA) methods. Moreover, we\nintroduce rank loss and Fr\\'echet Inception Distance (FID) loss into the\nconstruction of the space to capture inter-model and inter-dataset\nrelationships, thereby achieving more accurate alignment in the meta-space.\nExperimental results across multiple datasets demonstrate that MedNNS\nsignificantly outperforms both ImageNet pre-trained DL models and SOTA Neural\nArchitecture Search (NAS) methods, achieving an average accuracy improvement of\n1.7% across datasets while converging substantially faster. The code and the\nprocessed meta-space is available at https://github.com/BioMedIA-MBZUAI/MedNNS."}
{"id": "2504.15434", "pdf": "https://arxiv.org/pdf/2504.15434", "abs": "https://arxiv.org/abs/2504.15434", "authors": ["Sarath Shekkizhar", "Romain Cosentino"], "title": "AGI Is Coming... Right After AI Learns to Play Wordle", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "This paper investigates multimodal agents, in particular, OpenAI's\nComputer-User Agent (CUA), trained to control and complete tasks through a\nstandard computer interface, similar to humans. We evaluated the agent's\nperformance on the New York Times Wordle game to elicit model behaviors and\nidentify shortcomings. Our findings revealed a significant discrepancy in the\nmodel's ability to recognize colors correctly depending on the context. The\nmodel had a $5.36\\%$ success rate over several hundred runs across a week of\nWordle. Despite the immense enthusiasm surrounding AI agents and their\npotential to usher in Artificial General Intelligence (AGI), our findings\nreinforce the fact that even simple tasks present substantial challenges for\ntoday's frontier AI models. We conclude with a discussion of the potential\nunderlying causes, implications for future development, and research directions\nto improve these AI systems."}
{"id": "2504.15876", "pdf": "https://arxiv.org/pdf/2504.15876", "abs": "https://arxiv.org/abs/2504.15876", "authors": ["Qizhen Wu Lei Chen", "Kexin Liu", "Jinhu Lü"], "title": "Bidirectional Task-Motion Planning Based on Hierarchical Reinforcement Learning for Strategic Confrontation", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "In swarm robotics, confrontation scenarios, including strategic\nconfrontations, require efficient decision-making that integrates discrete\ncommands and continuous actions. Traditional task and motion planning methods\nseparate decision-making into two layers, but their unidirectional structure\nfails to capture the interdependence between these layers, limiting\nadaptability in dynamic environments. Here, we propose a novel bidirectional\napproach based on hierarchical reinforcement learning, enabling dynamic\ninteraction between the layers. This method effectively maps commands to task\nallocation and actions to path planning, while leveraging cross-training\ntechniques to enhance learning across the hierarchical framework. Furthermore,\nwe introduce a trajectory prediction model that bridges abstract task\nrepresentations with actionable planning goals. In our experiments, it achieves\nover 80\\% in confrontation win rate and under 0.01 seconds in decision time,\noutperforming existing approaches. Demonstrations through large-scale tests and\nreal-world robot experiments further emphasize the generalization capabilities\nand practical applicability of our method."}
{"id": "2504.15479", "pdf": "https://arxiv.org/pdf/2504.15479", "abs": "https://arxiv.org/abs/2504.15479", "authors": ["Jeremy Goldwasser", "Giles Hooker"], "title": "Unifying Image Counterfactuals and Feature Attributions with Latent-Space Adversarial Attacks", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Counterfactuals are a popular framework for interpreting machine learning\npredictions. These what if explanations are notoriously challenging to create\nfor computer vision models: standard gradient-based methods are prone to\nproduce adversarial examples, in which imperceptible modifications to image\npixels provoke large changes in predictions. We introduce a new,\neasy-to-implement framework for counterfactual images that can flexibly adapt\nto contemporary advances in generative modeling. Our method, Counterfactual\nAttacks, resembles an adversarial attack on the representation of the image\nalong a low-dimensional manifold. In addition, given an auxiliary dataset of\nimage descriptors, we show how to accompany counterfactuals with feature\nattribution that quantify the changes between the original and counterfactual\nimages. These importance scores can be aggregated into global counterfactual\nexplanations that highlight the overall features driving model predictions.\nWhile this unification is possible for any counterfactual method, it has\nparticular computational efficiency for ours. We demonstrate the efficacy of\nour approach with the MNIST and CelebA datasets."}
{"id": "2504.15883", "pdf": "https://arxiv.org/pdf/2504.15883", "abs": "https://arxiv.org/abs/2504.15883", "authors": ["Farida Mohsen", "Samir Belhaouari", "Zubair Shah"], "title": "Integrating Non-Linear Radon Transformation for Diabetic Retinopathy Grading", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Diabetic retinopathy is a serious ocular complication that poses a\nsignificant threat to patients' vision and overall health. Early detection and\naccurate grading are essential to prevent vision loss. Current automatic\ngrading methods rely heavily on deep learning applied to retinal fundus images,\nbut the complex, irregular patterns of lesions in these images, which vary in\nshape and distribution, make it difficult to capture subtle changes. This study\nintroduces RadFuse, a multi-representation deep learning framework that\nintegrates non-linear RadEx-transformed sinogram images with traditional fundus\nimages to enhance diabetic retinopathy detection and grading. Our RadEx\ntransformation, an optimized non-linear extension of the Radon transform,\ngenerates sinogram representations to capture complex retinal lesion patterns.\nBy leveraging both spatial and transformed domain information, RadFuse enriches\nthe feature set available to deep learning models, improving the\ndifferentiation of severity levels. We conducted extensive experiments on two\nbenchmark datasets, APTOS-2019 and DDR, using three convolutional neural\nnetworks (CNNs): ResNeXt-50, MobileNetV2, and VGG19. RadFuse showed significant\nimprovements over fundus-image-only models across all three CNN architectures\nand outperformed state-of-the-art methods on both datasets. For severity\ngrading across five stages, RadFuse achieved a quadratic weighted kappa of\n93.24%, an accuracy of 87.07%, and an F1-score of 87.17%. In binary\nclassification between healthy and diabetic retinopathy cases, the method\nreached an accuracy of 99.09%, precision of 98.58%, and recall of 99.6%,\nsurpassing previously established models. These results demonstrate RadFuse's\ncapacity to capture complex non-linear features, advancing diabetic retinopathy\nclassification and promoting the integration of advanced mathematical\ntransforms in medical image analysis."}
{"id": "2504.15481", "pdf": "https://arxiv.org/pdf/2504.15481", "abs": "https://arxiv.org/abs/2504.15481", "authors": ["Michel Berthier", "Nicoletta Prencipe", "Edoardo Provenzi"], "title": "Split-quaternions for perceptual white balance", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "We propose a perceptual chromatic adaptation transform for white balance that\nmakes use of split-quaternions. The novelty of the present work, which is\nmotivated by a recently developed quantum-like model of color perception,\nconsists at stressing the link between the algebraic structures appearing in\nthis model and a certain sub-algebra of the split-quaternions. We show the\npotentiality of this approach for color image processing applications by\nproposing a chromatic adaptation transform, implemented via an appropriate use\nof the split-quaternion multiplication. Moreover, quantitative comparisons with\nthe widely used state-of-the art von Kries chromatic adaptation transform are\nprovided."}
{"id": "2504.15894", "pdf": "https://arxiv.org/pdf/2504.15894", "abs": "https://arxiv.org/abs/2504.15894", "authors": ["Chengbo Zheng", "Tim Miller", "Alina Bialkowski", "H Peter Soyer", "Monika Janda"], "title": "Supporting Data-Frame Dynamics in AI-assisted Decision Making", "categories": ["cs.HC", "cs.AI"], "comment": "Presented at the 2025 ACM Workshop on Human-AI Interaction for\n  Augmented Reasoning, Report Number: CHI25-WS-AUGMENTED-REASONING", "summary": "High stakes decision-making often requires a continuous interplay between\nevolving evidence and shifting hypotheses, a dynamic that is not well supported\nby current AI decision support systems. In this paper, we introduce a\nmixed-initiative framework for AI assisted decision making that is grounded in\nthe data-frame theory of sensemaking and the evaluative AI paradigm. Our\napproach enables both humans and AI to collaboratively construct, validate, and\nadapt hypotheses. We demonstrate our framework with an AI-assisted skin cancer\ndiagnosis prototype that leverages a concept bottleneck model to facilitate\ninterpretable interactions and dynamic updates to diagnostic hypotheses."}
{"id": "2504.15496", "pdf": "https://arxiv.org/pdf/2504.15496", "abs": "https://arxiv.org/abs/2504.15496", "authors": ["Eammon A. Littler", "Emmanuel A. Mannoh", "Ethan P. M. LaRochelle"], "title": "Fluorescence Reference Target Quantitative Analysis Library", "categories": ["physics.med-ph", "cs.CV", "eess.IV", "q-bio.QM"], "comment": "12 pages, 1 table, 4 figures. Code available:\n  https://github.com/QUEL-Imaging/quel-qal), PyPi: quel-qal", "summary": "Standardized performance evaluation of fluorescence imaging systems remains a\ncritical unmet need in the field of fluorescence-guided surgery (FGS). While\nthe American Association of Physicists in Medicine (AAPM) TG311 report and\nrecent FDA draft guidance provide recommended metrics for system\ncharacterization, practical tools for extracting these metrics remain limited,\ninconsistent, and often inaccessible. We present QUEL-QAL, an open-source\nPython library designed to streamline and standardize the quantitative analysis\nof fluorescence images using solid reference targets. The library provides a\nmodular, reproducible workflow that includes region of interest (ROI)\ndetection, statistical analysis, and visualization capabilities. QUEL-QAL\nsupports key metrics such as response linearity, limit of detection, depth\nsensitivity, and spatial resolution, in alignment with regulatory and academic\nguidance. Built on widely adopted Python packages, the library is designed to\nbe extensible, enabling users to adapt it to novel target designs and analysis\nprotocols. By promoting transparency, reproducibility, and regulatory\nalignment, QUEL-QAL offers a foundational tool to support standardized\nbenchmarking and accelerate the development and evaluation of fluorescence\nimaging systems."}
{"id": "2504.15895", "pdf": "https://arxiv.org/pdf/2504.15895", "abs": "https://arxiv.org/abs/2504.15895", "authors": ["Chenxu Yang", "Qingyi Si", "Yongjie Duan", "Zheliang Zhu", "Chenyu Zhu", "Zheng Lin", "Li Cao", "Weiping Wang"], "title": "Dynamic Early Exit in Reasoning Models", "categories": ["cs.CL", "cs.AI"], "comment": "19 pages, 11 figures", "summary": "Recent advances in large reasoning language models (LRLMs) rely on test-time\nscaling, which extends long chain-of-thought (CoT) generation to solve complex\ntasks. However, overthinking in long CoT not only slows down the efficiency of\nproblem solving, but also risks accuracy loss due to the extremely detailed or\nredundant reasoning steps. We propose a simple yet effective method that allows\nLLMs to self-truncate CoT sequences by early exit during generation. Instead of\nrelying on fixed heuristics, the proposed method monitors model behavior at\npotential reasoning transition points (e.g.,\"Wait\" tokens) and dynamically\nterminates the next reasoning chain's generation when the model exhibits high\nconfidence in a trial answer. Our method requires no additional training and\ncan be seamlessly integrated into existing o1-like reasoning LLMs. Experiments\non multiple reasoning benchmarks MATH-500, AMC 2023, GPQA Diamond and AIME 2024\nshow that the proposed method is consistently effective on deepseek-series\nreasoning LLMs, reducing the length of CoT sequences by an average of 31% to\n43% while improving accuracy by 1.7% to 5.7%."}
{"id": "2504.15545", "pdf": "https://arxiv.org/pdf/2504.15545", "abs": "https://arxiv.org/abs/2504.15545", "authors": ["Zizhi Chen", "Xinyu Zhang", "Minghao Han", "Yizhou Liu", "Ziyun Qian", "Weifeng Zhang", "Xukun Zhang", "Jingwei Wei", "Lihua Zhang"], "title": "VLM-based Prompts as the Optimal Assistant for Unpaired Histopathology Virtual Staining", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "In histopathology, tissue sections are typically stained using common H&E\nstaining or special stains (MAS, PAS, PASM, etc.) to clearly visualize specific\ntissue structures. The rapid advancement of deep learning offers an effective\nsolution for generating virtually stained images, significantly reducing the\ntime and labor costs associated with traditional histochemical staining.\nHowever, a new challenge arises in separating the fundamental visual\ncharacteristics of tissue sections from the visual differences induced by\nstaining agents. Additionally, virtual staining often overlooks essential\npathological knowledge and the physical properties of staining, resulting in\nonly style-level transfer. To address these issues, we introduce, for the first\ntime in virtual staining tasks, a pathological vision-language large model\n(VLM) as an auxiliary tool. We integrate contrastive learnable prompts,\nfoundational concept anchors for tissue sections, and staining-specific concept\nanchors to leverage the extensive knowledge of the pathological VLM. This\napproach is designed to describe, frame, and enhance the direction of virtual\nstaining. Furthermore, we have developed a data augmentation method based on\nthe constraints of the VLM. This method utilizes the VLM's powerful image\ninterpretation capabilities to further integrate image style and structural\ninformation, proving beneficial in high-precision pathological diagnostics.\nExtensive evaluations on publicly available multi-domain unpaired staining\ndatasets demonstrate that our method can generate highly realistic images and\nenhance the accuracy of downstream tasks, such as glomerular detection and\nsegmentation. Our code is available at:\nhttps://github.com/CZZZZZZZZZZZZZZZZZ/VPGAN-HARBOR"}
{"id": "2504.15905", "pdf": "https://arxiv.org/pdf/2504.15905", "abs": "https://arxiv.org/abs/2504.15905", "authors": ["Wenjing Xiao", "Chenglong Shi", "Miaojiang Chen", "Zhiquan Liu", "Min Chen", "H. Herbert Song"], "title": "GraphEdge: Dynamic Graph Partition and Task Scheduling for GNNs Computing in Edge Network", "categories": ["cs.LG", "cs.AI"], "comment": "17 pages,12 figures", "summary": "With the exponential growth of Internet of Things (IoT) devices, edge\ncomputing (EC) is gradually playing an important role in providing\ncost-effective services. However, existing approaches struggle to perform well\nin graph-structured scenarios where user data is correlated, such as traffic\nflow prediction and social relationship recommender systems. In particular,\ngraph neural network (GNN)-based approaches lead to expensive server\ncommunication cost. To address this problem, we propose GraphEdge, an efficient\nGNN-based EC architecture. It considers the EC system of GNN tasks, where there\nare associations between users and it needs to take into account the task data\nof its neighbors when processing the tasks of a user. Specifically, the\narchitecture first perceives the user topology and represents their data\nassociations as a graph layout at each time step. Then the graph layout is\noptimized by calling our proposed hierarchical traversal graph cut algorithm\n(HiCut), which cuts the graph layout into multiple weakly associated subgraphs\nbased on the aggregation characteristics of GNN, and the communication cost\nbetween different subgraphs during GNN inference is minimized. Finally, based\non the optimized graph layout, our proposed deep reinforcement learning (DRL)\nbased graph offloading algorithm (DRLGO) is executed to obtain the optimal\noffloading strategy for the tasks of users, the offloading strategy is\nsubgraph-based, it tries to offload user tasks in a subgraph to the same edge\nserver as possible while minimizing the task processing time and energy\nconsumption of the EC system. Experimental results show the good effectiveness\nand dynamic adaptation of our proposed architecture and it also performs well\neven in dynamic scenarios."}
{"id": "2504.15562", "pdf": "https://arxiv.org/pdf/2504.15562", "abs": "https://arxiv.org/abs/2504.15562", "authors": ["Dip Roy"], "title": "Bayesian Autoencoder for Medical Anomaly Detection: Uncertainty-Aware Approach for Brain 2 MRI Analysis", "categories": ["cs.LG", "cs.CV"], "comment": "16 pages, 6 figures", "summary": "In medical imaging, anomaly detection is a vital element of healthcare\ndiagnostics, especially for neurological conditions which can be\nlife-threatening. Conventional deterministic methods often fall short when it\ncomes to capturing the inherent uncertainty of anomaly detection tasks. This\npaper introduces a Bayesian Variational Autoencoder (VAE) equipped with\nmulti-head attention mechanisms for detecting anomalies in brain magnetic\nresonance imaging (MRI). For the purpose of improving anomaly detection\nperformance, we incorporate both epistemic and aleatoric uncertainty estimation\nthrough Bayesian inference. The model was tested on the BraTS2020 dataset, and\nthe findings were a 0.83 ROC AUC and a 0.83 PR AUC. The data in our paper\nsuggests that modeling uncertainty is an essential component of anomaly\ndetection, enhancing both performance and interpretability and providing\nconfidence estimates, as well as anomaly predictions, for clinicians to\nleverage in making medical decisions."}
{"id": "2504.15912", "pdf": "https://arxiv.org/pdf/2504.15912", "abs": "https://arxiv.org/abs/2504.15912", "authors": ["Riley Pierson", "Armin Moin"], "title": "Automated Bug Report Prioritization in Large Open-Source Projects", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Large open-source projects receive a large number of issues (known as bugs),\nincluding software defect (i.e., bug) reports and new feature requests from\ntheir user and developer communities at a fast rate. The often limited project\nresources do not allow them to deal with all issues. Instead, they have to\nprioritize them according to the project's priorities and the issues'\nseverities. In this paper, we propose a novel approach to automated bug\nprioritization based on the natural language text of the bug reports that are\nstored in the open bug repositories of the issue-tracking systems. We conduct\ntopic modeling using a variant of LDA called TopicMiner-MTM and text\nclassification with the BERT large language model to achieve a higher\nperformance level compared to the state-of-the-art. Experimental results using\nan existing reference dataset containing 85,156 bug reports of the Eclipse\nPlatform project indicate that we outperform existing approaches in terms of\nAccuracy, Precision, Recall, and F1-measure of the bug report priority\nprediction."}
{"id": "2504.15594", "pdf": "https://arxiv.org/pdf/2504.15594", "abs": "https://arxiv.org/abs/2504.15594", "authors": ["Tatsuhito Hasegawa", "Shunsuke Sakai"], "title": "Analytical Softmax Temperature Setting from Feature Dimensions for Model- and Domain-Robust Classification", "categories": ["cs.LG", "cs.CV"], "comment": "22 pages, 11 figures, under review", "summary": "In deep learning-based classification tasks, the softmax function's\ntemperature parameter $T$ critically influences the output distribution and\noverall performance. This study presents a novel theoretical insight that the\noptimal temperature $T^*$ is uniquely determined by the dimensionality of the\nfeature representations, thereby enabling training-free determination of $T^*$.\nDespite this theoretical grounding, empirical evidence reveals that $T^*$\nfluctuates under practical conditions owing to variations in models, datasets,\nand other confounding factors. To address these influences, we propose and\noptimize a set of temperature determination coefficients that specify how $T^*$\nshould be adjusted based on the theoretical relationship to feature\ndimensionality. Additionally, we insert a batch normalization layer immediately\nbefore the output layer, effectively stabilizing the feature space. Building on\nthese coefficients and a suite of large-scale experiments, we develop an\nempirical formula to estimate $T^*$ without additional training while also\nintroducing a corrective scheme to refine $T^*$ based on the number of classes\nand task complexity. Our findings confirm that the derived temperature not only\naligns with the proposed theoretical perspective but also generalizes\neffectively across diverse tasks, consistently enhancing classification\nperformance and offering a practical, training-free solution for determining\n$T^*$."}
{"id": "2504.15918", "pdf": "https://arxiv.org/pdf/2504.15918", "abs": "https://arxiv.org/abs/2504.15918", "authors": ["Chang Zong", "Bin Li", "Shoujun Zhou", "Jian Wan", "Lei Zhang"], "title": "Ask2Loc: Learning to Locate Instructional Visual Answers by Asking Questions", "categories": ["cs.CV", "cs.AI", "cs.HC", "68T45, 68T20"], "comment": "16 pages, 8 figures", "summary": "Locating specific segments within an instructional video is an efficient way\nto acquire guiding knowledge. Generally, the task of obtaining video segments\nfor both verbal explanations and visual demonstrations is known as visual\nanswer localization (VAL). However, users often need multiple interactions to\nobtain answers that align with their expectations when using the system. During\nthese interactions, humans deepen their understanding of the video content by\nasking themselves questions, thereby accurately identifying the location.\nTherefore, we propose a new task, named In-VAL, to simulate the multiple\ninteractions between humans and videos in the procedure of obtaining visual\nanswers. The In-VAL task requires interactively addressing several semantic gap\nissues, including 1) the ambiguity of user intent in the input questions, 2)\nthe incompleteness of language in video subtitles, and 3) the fragmentation of\ncontent in video segments. To address these issues, we propose Ask2Loc, a\nframework for resolving In-VAL by asking questions. It includes three key\nmodules: 1) a chatting module to refine initial questions and uncover clear\nintentions, 2) a rewriting module to generate fluent language and create\ncomplete descriptions, and 3) a searching module to broaden local context and\nprovide integrated content. We conduct extensive experiments on three\nreconstructed In-VAL datasets. Compared to traditional end-to-end and two-stage\nmethods, our proposed Ask2Loc can improve performance by up to 14.91 (mIoU) on\nthe In-VAL task. Our code and datasets can be accessed at\nhttps://github.com/changzong/Ask2Loc."}
{"id": "2504.15616", "pdf": "https://arxiv.org/pdf/2504.15616", "abs": "https://arxiv.org/abs/2504.15616", "authors": ["Kai Chen", "Xiaodong Zhao", "Yujie Huang", "Guoyu Fang", "Xiao Song", "Ruiping Wang", "Ziyuan Wang"], "title": "SocialMOIF: Multi-Order Intention Fusion for Pedestrian Trajectory Prediction", "categories": ["cs.LG", "cs.CV"], "comment": "11 pages,6 figures", "summary": "The analysis and prediction of agent trajectories are crucial for\ndecision-making processes in intelligent systems, with precise short-term\ntrajectory forecasting being highly significant across a range of applications.\nAgents and their social interactions have been quantified and modeled by\nresearchers from various perspectives; however, substantial limitations exist\nin the current work due to the inherent high uncertainty of agent intentions\nand the complex higher-order influences among neighboring groups. SocialMOIF is\nproposed to tackle these challenges, concentrating on the higher-order\nintention interactions among neighboring groups while reinforcing the primary\nrole of first-order intention interactions between neighbors and the target\nagent. This method develops a multi-order intention fusion model to achieve a\nmore comprehensive understanding of both direct and indirect intention\ninformation. Within SocialMOIF, a trajectory distribution approximator is\ndesigned to guide the trajectories toward values that align more closely with\nthe actual data, thereby enhancing model interpretability. Furthermore, a\nglobal trajectory optimizer is introduced to enable more accurate and efficient\nparallel predictions. By incorporating a novel loss function that accounts for\ndistance and direction during training, experimental results demonstrate that\nthe model outperforms previous state-of-the-art baselines across multiple\nmetrics in both dynamic and static datasets."}
{"id": "2504.15924", "pdf": "https://arxiv.org/pdf/2504.15924", "abs": "https://arxiv.org/abs/2504.15924", "authors": ["Alycia Carey", "Xintao Wu"], "title": "Achieving Distributive Justice in Federated Learning via Uncertainty Quantification", "categories": ["cs.LG", "cs.AI", "stat.ML", "68T01", "I.2.0"], "comment": "21 pages, 1 figure, 7 tables", "summary": "Client-level fairness metrics for federated learning are used to ensure that\nall clients in a federation either: a) have similar final performance on their\nlocal data distributions (i.e., client parity), or b) obtain final performance\non their local data distributions relative to their contribution to the\nfederated learning process (i.e., contribution fairness). While a handful of\nworks that propose either client-parity or contribution-based fairness metrics\nground their definitions and decisions in social theories of equality -- such\nas distributive justice -- most works arbitrarily choose what notion of\nfairness to align with which makes it difficult for practitioners to choose\nwhich fairness metric aligns best with their fairness ethics. In this work, we\npropose UDJ-FL (Uncertainty-based Distributive Justice for Federated Learning),\na flexible federated learning framework that can achieve multiple distributive\njustice-based client-level fairness metrics. Namely, by utilizing techniques\ninspired by fair resource allocation, in conjunction with performing aleatoric\nuncertainty-based client weighing, our UDJ-FL framework is able to achieve\negalitarian, utilitarian, Rawls' difference principle, or desert-based\nclient-level fairness. We empirically show the ability of UDJ-FL to achieve all\nfour defined distributive justice-based client-level fairness metrics in\naddition to providing fairness equivalent to (or surpassing) other popular fair\nfederated learning works. Further, we provide justification for why aleatoric\nuncertainty weighing is necessary to the construction of our UDJ-FL framework\nas well as derive theoretical guarantees for the generalization bounds of\nUDJ-FL. Our code is publicly available at\nhttps://github.com/alycia-noel/UDJ-FL."}
{"id": "2504.15649", "pdf": "https://arxiv.org/pdf/2504.15649", "abs": "https://arxiv.org/abs/2504.15649", "authors": ["Biao Wu", "Diankai Zhang", "Shaoli Liu", "Si Gao", "Chengjian Zheng", "Ning Wang"], "title": "RepNet-VSR: Reparameterizable Architecture for High-Fidelity Video Super-Resolution", "categories": ["eess.IV", "cs.CV"], "comment": "Champion Solution for CVPR 2025 MAI VSR Track", "summary": "As a fundamental challenge in visual computing, video super-resolution (VSR)\nfocuses on reconstructing highdefinition video sequences from their degraded\nlowresolution counterparts. While deep convolutional neural networks have\ndemonstrated state-of-the-art performance in spatial-temporal super-resolution\ntasks, their computationally intensive nature poses significant deployment\nchallenges for resource-constrained edge devices, particularly in real-time\nmobile video processing scenarios where power efficiency and latency\nconstraints coexist. In this work, we propose a Reparameterizable Architecture\nfor High Fidelity Video Super Resolution method, named RepNet-VSR, for\nreal-time 4x video super-resolution. On the REDS validation set, the proposed\nmodel achieves 27.79 dB PSNR when processing 180p to 720p frames in 103 ms per\n10 frames on a MediaTek Dimensity NPU. The competition results demonstrate an\nexcellent balance between restoration quality and deployment efficiency. The\nproposed method scores higher than the previous champion algorithm of MAI video\nsuper-resolution challenge."}
{"id": "2504.15927", "pdf": "https://arxiv.org/pdf/2504.15927", "abs": "https://arxiv.org/abs/2504.15927", "authors": ["Ling Cheng", "Jiashu Pu", "Ruicheng Liang", "Qian Shao", "Hezhe Qiao", "Feida Zhu"], "title": "New Recipe for Semi-supervised Community Detection: Clique Annealing under Crystallization Kinetics", "categories": ["cs.SI", "cs.AI"], "comment": "arXiv admin note: text overlap with arXiv:2203.05898 by other authors", "summary": "Semi-supervised community detection methods are widely used for identifying\nspecific communities due to the label scarcity. Existing semi-supervised\ncommunity detection methods typically involve two learning stages learning in\nboth initial identification and subsequent adjustment, which often starts from\nan unreasonable community core candidate. Moreover, these methods encounter\nscalability issues because they depend on reinforcement learning and generative\nadversarial networks, leading to higher computational costs and restricting the\nselection of candidates. To address these limitations, we draw a parallel\nbetween crystallization kinetics and community detection to integrate the\nspontaneity of the annealing process into community detection. Specifically, we\nliken community detection to identifying a crystal subgrain (core) that expands\ninto a complete grain (community) through a process similar to annealing. Based\non this finding, we propose CLique ANNealing (CLANN), which applies kinetics\nconcepts to community detection by integrating these principles into the\noptimization process to strengthen the consistency of the community core.\nSubsequently, a learning-free Transitive Annealer was employed to refine the\nfirst-stage candidates by merging neighboring cliques and repositioning the\ncommunity core, enabling a spontaneous growth process that enhances\nscalability. Extensive experiments on \\textbf{43} different network settings\ndemonstrate that CLANN outperforms state-of-the-art methods across multiple\nreal-world datasets, showcasing its exceptional efficacy and efficiency in\ncommunity detection."}
{"id": "2504.15654", "pdf": "https://arxiv.org/pdf/2504.15654", "abs": "https://arxiv.org/abs/2504.15654", "authors": ["Md Abdul Baset Sarker", "Art Nguyen", "Sigmond Kukla", "Kevin Fite", "Masudul H. Imtiaz"], "title": "A Vision-Enabled Prosthetic Hand for Children with Upper Limb Disabilities", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "This paper introduces a novel AI vision-enabled pediatric prosthetic hand\ndesigned to assist children aged 10-12 with upper limb disabilities. The\nprosthesis features an anthropomorphic appearance, multi-articulating\nfunctionality, and a lightweight design that mimics a natural hand, making it\nboth accessible and affordable for low-income families. Using 3D printing\ntechnology and integrating advanced machine vision, sensing, and embedded\ncomputing, the prosthetic hand offers a low-cost, customizable solution that\naddresses the limitations of current myoelectric prostheses. A micro camera is\ninterfaced with a low-power FPGA for real-time object detection and assists\nwith precise grasping. The onboard DL-based object detection and grasp\nclassification models achieved accuracies of 96% and 100% respectively. In the\nforce prediction, the mean absolute error was found to be 0.018. The features\nof the proposed prosthetic hand can thus be summarized as: a) a wrist-mounted\nmicro camera for artificial sensing, enabling a wide range of hand-based tasks;\nb) real-time object detection and distance estimation for precise grasping; and\nc) ultra-low-power operation that delivers high performance within constrained\npower and resource limits."}
{"id": "2504.15928", "pdf": "https://arxiv.org/pdf/2504.15928", "abs": "https://arxiv.org/abs/2504.15928", "authors": ["Meng Wang", "Tian Lin", "Qingshan Hou", "Aidi Lin", "Jingcheng Wang", "Qingsheng Peng", "Truong X. Nguyen", "Danqi Fang", "Ke Zou", "Ting Xu", "Cancan Xue", "Ten Cheer Quek", "Qinkai Yu", "Minxin Liu", "Hui Zhou", "Zixuan Xiao", "Guiqin He", "Huiyu Liang", "Tingkun Shi", "Man Chen", "Linna Liu", "Yuanyuan Peng", "Lianyu Wang", "Qiuming Hu", "Junhong Chen", "Zhenhua Zhang", "Cheng Chen", "Yitian Zhao", "Dianbo Liu", "Jianhua Wu", "Xinjian Chen", "Changqing Zhang", "Triet Thanh Nguyen", "Yanda Meng", "Yalin Zheng", "Yih Chung Tham", "Carol Y. Cheung", "Huazhu Fu", "Haoyu Chen", "Ching-Yu Cheng"], "title": "A Clinician-Friendly Platform for Ophthalmic Image Analysis Without Technical Barriers", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Artificial intelligence (AI) shows remarkable potential in medical imaging\ndiagnostics, but current models typically require retraining when deployed\nacross different clinical centers, limiting their widespread adoption. We\nintroduce GlobeReady, a clinician-friendly AI platform that enables ocular\ndisease diagnosis without retraining/fine-tuning or technical expertise.\nGlobeReady achieves high accuracy across imaging modalities: 93.9-98.5% for an\n11-category fundus photo dataset and 87.2-92.7% for a 15-category OCT dataset.\nThrough training-free local feature augmentation, it addresses domain shifts\nacross centers and populations, reaching an average accuracy of 88.9% across\nfive centers in China, 86.3% in Vietnam, and 90.2% in the UK. The built-in\nconfidence-quantifiable diagnostic approach further boosted accuracy to\n94.9-99.4% (fundus) and 88.2-96.2% (OCT), while identifying out-of-distribution\ncases at 86.3% (49 CFP categories) and 90.6% (13 OCT categories). Clinicians\nfrom multiple countries rated GlobeReady highly (average 4.6 out of 5) for its\nusability and clinical relevance. These results demonstrate GlobeReady's\nrobust, scalable diagnostic capability and potential to support ophthalmic care\nwithout technical barriers."}
{"id": "2504.15664", "pdf": "https://arxiv.org/pdf/2504.15664", "abs": "https://arxiv.org/abs/2504.15664", "authors": ["Phuong Quynh Le", "Jörg Schlötterer", "Christin Seifert"], "title": "An XAI-based Analysis of Shortcut Learning in Neural Networks", "categories": ["cs.LG", "cs.CV"], "comment": "Accepted at The World Conference on eXplainable Artificial\n  Intelligence 2025 (XAI-2025)", "summary": "Machine learning models tend to learn spurious features - features that\nstrongly correlate with target labels but are not causal. Existing approaches\nto mitigate models' dependence on spurious features work in some cases, but\nfail in others. In this paper, we systematically analyze how and where neural\nnetworks encode spurious correlations. We introduce the neuron spurious score,\nan XAI-based diagnostic measure to quantify a neuron's dependence on spurious\nfeatures. We analyze both convolutional neural networks (CNNs) and vision\ntransformers (ViTs) using architecture-specific methods. Our results show that\nspurious features are partially disentangled, but the degree of disentanglement\nvaries across model architectures. Furthermore, we find that the assumptions\nbehind existing mitigation methods are incomplete. Our results lay the\ngroundwork for the development of novel methods to mitigate spurious\ncorrelations and make AI models safer to use in practice."}
{"id": "2504.15929", "pdf": "https://arxiv.org/pdf/2504.15929", "abs": "https://arxiv.org/abs/2504.15929", "authors": ["Saban Ozturk", "Melih B. Yilmaz", "Muti Kara", "M. Talat Yavuz", "Aykut Koç", "Tolga Çukur"], "title": "Meta-Entity Driven Triplet Mining for Aligning Medical Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "18 pages, 7 figures, 6 tables", "summary": "Diagnostic imaging relies on interpreting both images and radiology reports,\nbut the growing data volumes place significant pressure on medical experts,\nyielding increased errors and workflow backlogs. Medical vision-language models\n(med-VLMs) have emerged as a powerful framework to efficiently process\nmultimodal imaging data, particularly in chest X-ray (CXR) evaluations, albeit\ntheir performance hinges on how well image and text representations are\naligned. Existing alignment methods, predominantly based on contrastive\nlearning, prioritize separation between disease classes over segregation of\nfine-grained pathology attributes like location, size or severity, leading to\nsuboptimal representations. Here, we propose MedTrim (Meta-entity-driven\nTriplet mining), a novel method that enhances image-text alignment through\nmultimodal triplet learning synergistically guided by disease class as well as\nadjectival and directional pathology descriptors. Unlike common alignment\nmethods that separate broad disease classes, MedTrim leverages structured\nmeta-entity information to preserve subtle but clinically significant\nintra-class variations. For this purpose, we first introduce an ontology-based\nentity recognition module that extracts pathology-specific meta-entities from\nCXR reports, as annotations on pathology attributes are rare in public\ndatasets. For refined sample selection in triplet mining, we then introduce a\nnovel score function that captures an aggregate measure of inter-sample\nsimilarity based on disease classes and adjectival/directional descriptors.\nLastly, we introduce a multimodal triplet alignment objective for explicit\nwithin- and cross-modal alignment between samples sharing detailed pathology\ncharacteristics. Our demonstrations indicate that MedTrim improves performance\nin downstream retrieval and classification tasks compared to state-of-the-art\nalignment methods."}
{"id": "2504.15667", "pdf": "https://arxiv.org/pdf/2504.15667", "abs": "https://arxiv.org/abs/2504.15667", "authors": ["Jingchen Zou", "Jianqiang Li", "Gabriel Jimenez", "Qing Zhao", "Daniel Racoceanu", "Matias Cosarinsky", "Enzo Ferrante", "Guanghui Fu"], "title": "Performance Estimation for Supervised Medical Image Segmentation Models on Unlabeled Data Using UniverSeg", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "The performance of medical image segmentation models is usually evaluated\nusing metrics like the Dice score and Hausdorff distance, which compare\npredicted masks to ground truth annotations. However, when applying the model\nto unseen data, such as in clinical settings, it is often impractical to\nannotate all the data, making the model's performance uncertain. To address\nthis challenge, we propose the Segmentation Performance Evaluator (SPE), a\nframework for estimating segmentation models' performance on unlabeled data.\nThis framework is adaptable to various evaluation metrics and model\narchitectures. Experiments on six publicly available datasets across six\nevaluation metrics including pixel-based metrics such as Dice score and\ndistance-based metrics like HD95, demonstrated the versatility and\neffectiveness of our approach, achieving a high correlation (0.956$\\pm$0.046)\nand low MAE (0.025$\\pm$0.019) compare with real Dice score on the independent\ntest set. These results highlight its ability to reliably estimate model\nperformance without requiring annotations. The SPE framework integrates\nseamlessly into any model training process without adding training overhead,\nenabling performance estimation and facilitating the real-world application of\nmedical image segmentation algorithms. The source code is publicly available"}
{"id": "2504.15941", "pdf": "https://arxiv.org/pdf/2504.15941", "abs": "https://arxiv.org/abs/2504.15941", "authors": ["Fanny Jourdan", "Yannick Chevalier", "Cécile Favre"], "title": "FairTranslate: An English-French Dataset for Gender Bias Evaluation in Machine Translation by Overcoming Gender Binarity", "categories": ["cs.CL", "cs.AI"], "comment": "FAccT 2025", "summary": "Large Language Models (LLMs) are increasingly leveraged for translation tasks\nbut often fall short when translating inclusive language -- such as texts\ncontaining the singular 'they' pronoun or otherwise reflecting fair linguistic\nprotocols. Because these challenges span both computational and societal\ndomains, it is imperative to critically evaluate how well LLMs handle inclusive\ntranslation with a well-founded framework.\n  This paper presents FairTranslate, a novel, fully human-annotated dataset\ndesigned to evaluate non-binary gender biases in machine translation systems\nfrom English to French. FairTranslate includes 2418 English-French sentence\npairs related to occupations, annotated with rich metadata such as the\nstereotypical alignment of the occupation, grammatical gender indicator\nambiguity, and the ground-truth gender label (male, female, or inclusive).\n  We evaluate four leading LLMs (Gemma2-2B, Mistral-7B, Llama3.1-8B,\nLlama3.3-70B) on this dataset under different prompting procedures. Our results\nreveal substantial biases in gender representation across LLMs, highlighting\npersistent challenges in achieving equitable outcomes in machine translation.\nThese findings underscore the need for focused strategies and interventions\naimed at ensuring fair and inclusive language usage in LLM-based translation\nsystems.\n  We make the FairTranslate dataset publicly available on Hugging Face, and\ndisclose the code for all experiments on GitHub."}
{"id": "2504.15899", "pdf": "https://arxiv.org/pdf/2504.15899", "abs": "https://arxiv.org/abs/2504.15899", "authors": ["Blerim Abdullai", "Tony Wang", "Xinyuan Qiao", "Florian Shkurti", "Timothy D. Barfoot"], "title": "RaSCL: Radar to Satellite Crossview Localization", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "GNSS is unreliable, inaccurate, and insufficient in many real-time autonomous\nfield applications. In this work, we present a GNSS-free global localization\nsolution that contains a method of registering imaging radar on the ground with\noverhead RGB imagery, with joint optimization of relative poses from odometry\nand global poses from our overhead registration. Previous works have used\nvarious combinations of ground sensors and overhead imagery, and different\nfeature extraction and matching methods. These include various handcrafted and\ndeep-learning-based methods for extracting features from overhead imagery. Our\nwork presents insights on extracting essential features from RGB overhead\nimages for effective global localization against overhead imagery using only\nground radar and a single georeferenced initial guess. We motivate our method\nby evaluating it on datasets in diverse geographic conditions and robotic\nplatforms, including on an Unmanned Surface Vessel (USV) as well as urban and\nsuburban driving datasets."}
{"id": "2504.15956", "pdf": "https://arxiv.org/pdf/2504.15956", "abs": "https://arxiv.org/abs/2504.15956", "authors": ["Jerry Yao-Chieh Hu", "Hude Liu", "Hong-Yu Chen", "Weimin Wu", "Han Liu"], "title": "Universal Approximation with Softmax Attention", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "We prove that with linear transformations, both (i) two-layer self-attention\nand (ii) one-layer self-attention followed by a softmax function are universal\napproximators for continuous sequence-to-sequence functions on compact domains.\nOur main technique is a new interpolation-based method for analyzing\nattention's internal mechanism. This leads to our key insight: self-attention\nis able to approximate a generalized version of ReLU to arbitrary precision,\nand hence subsumes many known universal approximators. Building on these, we\nshow that two-layer multi-head attention alone suffices as a\nsequence-to-sequence universal approximator. In contrast, prior works rely on\nfeed-forward networks to establish universal approximation in Transformers.\nFurthermore, we extend our techniques to show that, (softmax-)attention-only\nlayers are capable of approximating various statistical models in-context. We\nbelieve these techniques hold independent interest."}
{"id": "2504.15953", "pdf": "https://arxiv.org/pdf/2504.15953", "abs": "https://arxiv.org/abs/2504.15953", "authors": ["Chance J. Hamilton", "Alfredo Weitzenfeld"], "title": "Visual Place Cell Encoding: A Computational Model for Spatial Representation and Cognitive Mapping", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "This paper presents the Visual Place Cell Encoding (VPCE) model, a\nbiologically inspired computational framework for simulating place cell-like\nactivation using visual input. Drawing on evidence that visual landmarks play a\ncentral role in spatial encoding, the proposed VPCE model activates visual\nplace cells by clustering high-dimensional appearance features extracted from\nimages captured by a robot-mounted camera. Each cluster center defines a\nreceptive field, and activation is computed based on visual similarity using a\nradial basis function. We evaluate whether the resulting activation patterns\ncorrelate with key properties of biological place cells, including spatial\nproximity, orientation alignment, and boundary differentiation. Experiments\ndemonstrate that the VPCE can distinguish between visually similar yet\nspatially distinct locations and adapt to environment changes such as the\ninsertion or removal of walls. These results suggest that structured visual\ninput, even in the absence of motion cues or reward-driven learning, is\nsufficient to generate place-cell-like spatial representations and support\nbiologically inspired cognitive mapping."}
{"id": "2504.15972", "pdf": "https://arxiv.org/pdf/2504.15972", "abs": "https://arxiv.org/abs/2504.15972", "authors": ["Sophie C. Pope", "Andrew Barovic", "Armin Moin"], "title": "Bug Destiny Prediction in Large Open-Source Software Repositories through Sentiment Analysis and BERT Topic Modeling", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "This study explores a novel approach to predicting key bug-related outcomes,\nincluding the time to resolution, time to fix, and ultimate status of a bug,\nusing data from the Bugzilla Eclipse Project. Specifically, we leverage\nfeatures available before a bug is resolved to enhance predictive accuracy. Our\nmethodology incorporates sentiment analysis to derive both an emotionality\nscore and a sentiment classification (positive or negative). Additionally, we\nintegrate the bug's priority level and its topic, extracted using a BERTopic\nmodel, as features for a Convolutional Neural Network (CNN) and a Multilayer\nPerceptron (MLP). Our findings indicate that the combination of BERTopic and\nsentiment analysis can improve certain model performance metrics. Furthermore,\nwe observe that balancing model inputs enhances practical applicability, albeit\nat the cost of a significant reduction in accuracy in most cases. To address\nour primary objectives, predicting time-to-resolution, time-to-fix, and bug\ndestiny, we employ both binary classification and exact time value predictions,\nallowing for a comparative evaluation of their predictive effectiveness.\nResults demonstrate that sentiment analysis serves as a valuable predictor of a\nbug's eventual outcome, particularly in determining whether it will be fixed.\nHowever, its utility is less pronounced when classifying bugs into more complex\nor unconventional outcome categories."}
{"id": "2504.15970", "pdf": "https://arxiv.org/pdf/2504.15970", "abs": "https://arxiv.org/abs/2504.15970", "authors": ["Baichuan Zeng"], "title": "Recent Advances and Future Directions in Extended Reality (XR): Exploring AI-Powered Spatial Intelligence", "categories": ["cs.HC", "cs.CV", "cs.MA"], "comment": "7 pages,4 figures", "summary": "Extended Reality (XR), encompassing Augmented Reality (AR), Virtual Reality\n(VR) and Mixed Reality (MR), is a transformative technology bridging the\nphysical and virtual world and it has diverse potential which will be\nubiquitous in the future. This review examines XR's evolution through\nfoundational framework - hardware ranging from monitors to sensors and software\nranging from visual tasks to user interface; highlights state of the art (SOTA)\nXR products with the comparison and analysis of performance based on their\nfoundational framework; discusses how commercial XR devices can support the\ndemand of high-quality performance focusing on spatial intelligence. For future\ndirections, attention should be given to the integration of multi-modal AI and\nIoT-driven digital twins to enable adaptive XR systems. With the concept of\nspatial intelligence, future XR should establish a new digital space with\nrealistic experience that benefits humanity. This review underscores the\npivotal role of AI in unlocking XR as the next frontier in human-computer\ninteraction."}
{"id": "2504.15983", "pdf": "https://arxiv.org/pdf/2504.15983", "abs": "https://arxiv.org/abs/2504.15983", "authors": ["Shang Wang"], "title": "W-PCA Based Gradient-Free Proxy for Efficient Search of Lightweight Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "ICLR 2025", "summary": "The demand for efficient natural language processing (NLP) systems has led to\nthe development of lightweight language models. Previous work in this area has\nprimarily focused on manual design or training-based neural architecture search\n(NAS) methods. Recently, zero-shot NAS methods have been proposed for\nevaluating language models without the need for training. However, prevailing\napproaches to zero-shot NAS often face challenges such as biased evaluation\nmetrics and computational inefficiencies. In this paper, we introduce\nweight-weighted PCA (W-PCA), a novel zero-shot NAS method specifically tailored\nfor lightweight language models. Our approach utilizes two evaluation proxies:\nthe parameter count and the number of principal components with cumulative\ncontribution exceeding $\\eta$ in the feed-forward neural (FFN) layer.\nAdditionally, by eliminating the need for gradient computations, we optimize\nthe evaluation time, thus enhancing the efficiency of designing and evaluating\nlightweight language models. We conduct a comparative analysis on the GLUE and\nSQuAD datasets to evaluate our approach. The results demonstrate that our\nmethod significantly reduces training time compared to one-shot NAS methods and\nachieves higher scores in the testing phase compared to previous\nstate-of-the-art training-based methods. Furthermore, we perform ranking\nevaluations on a dataset sampled from the FlexiBERT search space. Our approach\nexhibits superior ranking correlation and further reduces solving time compared\nto other zero-shot NAS methods that require gradient computation."}
{"id": "2504.15975", "pdf": "https://arxiv.org/pdf/2504.15975", "abs": "https://arxiv.org/abs/2504.15975", "authors": ["Peter Fletcher"], "title": "A New Graph Grammar Formalism for Robust Syntactic Pattern Recognition", "categories": ["cs.FL", "cs.CV", "F.4.2; F.4.3"], "comment": "64 pages, 23 figures", "summary": "I introduce a formalism for representing the syntax of recursively structured\ngraph-like patterns. It does not use production rules, like a conventional\ngraph grammar, but represents the syntactic structure in a more direct and\ndeclarative way. The grammar and the pattern are both represented as networks,\nand parsing is seen as the construction of a homomorphism from the pattern to\nthe grammar. The grammars can represent iterative, hierarchical and nested\nrecursive structure in more than one dimension.\n  This supports a highly parallel style of parsing, in which all aspects of\npattern recognition (feature detection, segmentation, parsing, filling in\nmissing symbols, top-down and bottom-up inference) are integrated into a single\nprocess, to exploit the synergy between them.\n  The emphasis of this paper is on underlying theoretical issues, but I also\ngive some example runs to illustrate the error-tolerant parsing of complex\nrecursively structured patterns of 50-1000 symbols, involving variability in\ngeometric relationships, blurry and indistinct symbols, overlapping symbols,\ncluttered images, and erased patches."}
{"id": "2504.15995", "pdf": "https://arxiv.org/pdf/2504.15995", "abs": "https://arxiv.org/abs/2504.15995", "authors": ["Sindhuja Madabushi", "Ahmad Faraz Khan", "Haider Ali", "Jin-Hee Cho"], "title": "OPUS-VFL: Incentivizing Optimal Privacy-Utility Tradeoffs in Vertical Federated Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Vertical Federated Learning (VFL) enables organizations with disjoint feature\nspaces but shared user bases to collaboratively train models without sharing\nraw data. However, existing VFL systems face critical limitations: they often\nlack effective incentive mechanisms, struggle to balance privacy-utility\ntradeoffs, and fail to accommodate clients with heterogeneous resource\ncapabilities. These challenges hinder meaningful participation, degrade model\nperformance, and limit practical deployment. To address these issues, we\npropose OPUS-VFL, an Optimal Privacy-Utility tradeoff Strategy for VFL.\nOPUS-VFL introduces a novel, privacy-aware incentive mechanism that rewards\nclients based on a principled combination of model contribution, privacy\npreservation, and resource investment. It employs a lightweight leave-one-out\n(LOO) strategy to quantify feature importance per client, and integrates an\nadaptive differential privacy mechanism that enables clients to dynamically\ncalibrate noise levels to optimize their individual utility. Our framework is\ndesigned to be scalable, budget-balanced, and robust to inference and poisoning\nattacks. Extensive experiments on benchmark datasets (MNIST, CIFAR-10, and\nCIFAR-100) demonstrate that OPUS-VFL significantly outperforms state-of-the-art\nVFL baselines in both efficiency and robustness. It reduces label inference\nattack success rates by up to 20%, increases feature inference reconstruction\nerror (MSE) by over 30%, and achieves up to 25% higher incentives for clients\nthat contribute meaningfully while respecting privacy and cost constraints.\nThese results highlight the practicality and innovation of OPUS-VFL as a\nsecure, fair, and performance-driven solution for real-world VFL."}
{"id": "2504.16062", "pdf": "https://arxiv.org/pdf/2504.16062", "abs": "https://arxiv.org/abs/2504.16062", "authors": ["Hardik Shah", "Jiaxu Xing", "Nico Messikommer", "Boyang Sun", "Marc Pollefeys", "Davide Scaramuzza"], "title": "ForesightNav: Learning Scene Imagination for Efficient Exploration", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Understanding how humans leverage prior knowledge to navigate unseen\nenvironments while making exploratory decisions is essential for developing\nautonomous robots with similar abilities. In this work, we propose\nForesightNav, a novel exploration strategy inspired by human imagination and\nreasoning. Our approach equips robotic agents with the capability to predict\ncontextual information, such as occupancy and semantic details, for unexplored\nregions. These predictions enable the robot to efficiently select meaningful\nlong-term navigation goals, significantly enhancing exploration in unseen\nenvironments. We validate our imagination-based approach using the Structured3D\ndataset, demonstrating accurate occupancy prediction and superior performance\nin anticipating unseen scene geometry. Our experiments show that the\nimagination module improves exploration efficiency in unseen environments,\nachieving a 100% completion rate for PointNav and an SPL of 67% for ObjectNav\non the Structured3D Validation split. These contributions demonstrate the power\nof imagination-driven reasoning for autonomous systems to enhance generalizable\nand efficient exploration."}
{"id": "2504.16000", "pdf": "https://arxiv.org/pdf/2504.16000", "abs": "https://arxiv.org/abs/2504.16000", "authors": ["Soham Bonnerjee", "Zhen Wei", "Yeon", "Anna Asch", "Sagnik Nandy", "Promit Ghosal"], "title": "How Private is Your Attention? Bridging Privacy with In-Context Learning", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "In-context learning (ICL)-the ability of transformer-based models to perform\nnew tasks from examples provided at inference time-has emerged as a hallmark of\nmodern language models. While recent works have investigated the mechanisms\nunderlying ICL, its feasibility under formal privacy constraints remains\nlargely unexplored. In this paper, we propose a differentially private\npretraining algorithm for linear attention heads and present the first\ntheoretical analysis of the privacy-accuracy trade-off for ICL in linear\nregression. Our results characterize the fundamental tension between\noptimization and privacy-induced noise, formally capturing behaviors observed\nin private training via iterative methods. Additionally, we show that our\nmethod is robust to adversarial perturbations of training prompts, unlike\nstandard ridge regression. All theoretical findings are supported by extensive\nsimulations across diverse settings."}
{"id": "2504.16005", "pdf": "https://arxiv.org/pdf/2504.16005", "abs": "https://arxiv.org/abs/2504.16005", "authors": ["Tom Zehle", "Moritz Schlager", "Timo Heiß", "Matthias Feurer"], "title": "CAPO: Cost-Aware Prompt Optimization", "categories": ["cs.CL", "cs.AI", "cs.NE", "stat.ML"], "comment": "Submitted to AutoML 2025", "summary": "Large language models (LLMs) have revolutionized natural language processing\nby solving a wide range of tasks simply guided by a prompt. Yet their\nperformance is highly sensitive to prompt formulation. While automated prompt\noptimization addresses this challenge by finding optimal prompts, current\nmethods require a substantial number of LLM calls and input tokens, making\nprompt optimization expensive. We introduce CAPO (Cost-Aware Prompt\nOptimization), an algorithm that enhances prompt optimization efficiency by\nintegrating AutoML techniques. CAPO is an evolutionary approach with LLMs as\noperators, incorporating racing to save evaluations and multi-objective\noptimization to balance performance with prompt length. It jointly optimizes\ninstructions and few-shot examples while leveraging task descriptions for\nimproved robustness. Our extensive experiments across diverse datasets and LLMs\ndemonstrate that CAPO outperforms state-of-the-art discrete prompt optimization\nmethods in 11/15 cases with improvements up to 21%p. Our algorithm achieves\nbetter performances already with smaller budgets, saves evaluations through\nracing, and decreases average prompt length via a length penalty, making it\nboth cost-efficient and cost-aware. Even without few-shot examples, CAPO\noutperforms its competitors and generally remains robust to initial prompts.\nCAPO represents an important step toward making prompt optimization more\npowerful and accessible by improving cost-efficiency."}
{"id": "2504.16020", "pdf": "https://arxiv.org/pdf/2504.16020", "abs": "https://arxiv.org/abs/2504.16020", "authors": ["Soham Sane"], "title": "AlphaGrad: Non-Linear Gradient Normalization Optimizer", "categories": ["cs.LG", "cs.AI", "cs.NE", "stat.ML"], "comment": null, "summary": "We introduce AlphaGrad, a memory-efficient, conditionally stateless optimizer\naddressing the memory overhead and hyperparameter complexity of adaptive\nmethods like Adam. AlphaGrad enforces scale invariance via tensor-wise L2\ngradient normalization followed by a smooth hyperbolic tangent transformation,\n$g' = \\tanh(\\alpha \\cdot \\tilde{g})$, controlled by a single steepness\nparameter $\\alpha$. Our contributions include: (1) the AlphaGrad algorithm\nformulation; (2) a formal non-convex convergence analysis guaranteeing\nstationarity; (3) extensive empirical evaluation on diverse RL benchmarks (DQN,\nTD3, PPO). Compared to Adam, AlphaGrad demonstrates a highly context-dependent\nperformance profile. While exhibiting instability in off-policy DQN, it\nprovides enhanced training stability with competitive results in TD3 (requiring\ncareful $\\alpha$ tuning) and achieves substantially superior performance in\non-policy PPO. These results underscore the critical importance of empirical\n$\\alpha$ selection, revealing strong interactions between the optimizer's\ndynamics and the underlying RL algorithm. AlphaGrad presents a compelling\nalternative optimizer for memory-constrained scenarios and shows significant\npromise for on-policy learning regimes where its stability and efficiency\nadvantages can be particularly impactful."}
{"id": "2504.16021", "pdf": "https://arxiv.org/pdf/2504.16021", "abs": "https://arxiv.org/abs/2504.16021", "authors": ["Dinithi Dissanayake", "Suranga Nanayakkara"], "title": "Navigating the State of Cognitive Flow: Context-Aware AI Interventions for Effective Reasoning Support", "categories": ["cs.HC", "cs.AI"], "comment": "Presented at the 2025 ACM Workshop on Human-AI Interaction for\n  Augmented Reasoning, Report Number: CHI25-WS-AUGMENTED-REASONING", "summary": "Flow theory describes an optimal cognitive state where individuals experience\ndeep focus and intrinsic motivation when a task's difficulty aligns with their\nskill level. In AI-augmented reasoning, interventions that disrupt the state of\ncognitive flow can hinder rather than enhance decision-making. This paper\nproposes a context-aware cognitive augmentation framework that adapts\ninterventions based on three key contextual factors: type, timing, and scale.\nBy leveraging multimodal behavioral cues (e.g., gaze behavior, typing\nhesitation, interaction speed), AI can dynamically adjust cognitive support to\nmaintain or restore flow. We introduce the concept of cognitive flow, an\nextension of flow theory in AI-augmented reasoning, where interventions are\npersonalized, adaptive, and minimally intrusive. By shifting from static\ninterventions to context-aware augmentation, our approach ensures that AI\nsystems support deep engagement in complex decision-making and reasoning\nwithout disrupting cognitive immersion."}
{"id": "2504.16026", "pdf": "https://arxiv.org/pdf/2504.16026", "abs": "https://arxiv.org/abs/2504.16026", "authors": ["Konstantin F. Pilz", "James Sanders", "Robi Rahman", "Lennart Heim"], "title": "Trends in AI Supercomputers", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Frontier AI development relies on powerful AI supercomputers, yet analysis of\nthese systems is limited. We create a dataset of 500 AI supercomputers from\n2019 to 2025 and analyze key trends in performance, power needs, hardware cost,\nownership, and global distribution. We find that the computational performance\nof AI supercomputers has doubled every nine months, while hardware acquisition\ncost and power needs both doubled every year. The leading system in March 2025,\nxAI's Colossus, used 200,000 AI chips, had a hardware cost of \\$7B, and\nrequired 300 MW of power, as much as 250,000 households. As AI supercomputers\nevolved from tools for science to industrial machines, companies rapidly\nexpanded their share of total AI supercomputer performance, while the share of\ngovernments and academia diminished. Globally, the United States accounts for\nabout 75% of total performance in our dataset, with China in second place at\n15%. If the observed trends continue, the leading AI supercomputer in 2030 will\nachieve $2\\times10^{22}$ 16-bit FLOP/s, use two million AI chips, have a\nhardware cost of \\$200 billion, and require 9 GW of power. Our analysis\nprovides visibility into the AI supercomputer landscape, allowing policymakers\nto assess key AI trends like resource needs, ownership, and national\ncompetitiveness."}
{"id": "2504.16027", "pdf": "https://arxiv.org/pdf/2504.16027", "abs": "https://arxiv.org/abs/2504.16027", "authors": ["Ahmed R. Sadik", "Siddhata Govind"], "title": "Benchmarking LLM for Code Smells Detection: OpenAI GPT-4.0 vs DeepSeek-V3", "categories": ["cs.SE", "cs.AI", "cs.LG", "cs.PL"], "comment": null, "summary": "Determining the most effective Large Language Model for code smell detection\npresents a complex challenge. This study introduces a structured methodology\nand evaluation matrix to tackle this issue, leveraging a curated dataset of\ncode samples consistently annotated with known smells. The dataset spans four\nprominent programming languages Java, Python, JavaScript, and C++; allowing for\ncross language comparison. We benchmark two state of the art LLMs, OpenAI GPT\n4.0 and DeepSeek-V3, using precision, recall, and F1 score as evaluation\nmetrics. Our analysis covers three levels of detail: overall performance,\ncategory level performance, and individual code smell type performance.\nAdditionally, we explore cost effectiveness by comparing the token based\ndetection approach of GPT 4.0 with the pattern-matching techniques employed by\nDeepSeek V3. The study also includes a cost analysis relative to traditional\nstatic analysis tools such as SonarQube. The findings offer valuable guidance\nfor practitioners in selecting an efficient, cost effective solution for\nautomated code smell detection"}
{"id": "2504.16032", "pdf": "https://arxiv.org/pdf/2504.16032", "abs": "https://arxiv.org/abs/2504.16032", "authors": ["Yazan Otoum", "Arghavan Asad", "Amiya Nayak"], "title": "LLMs meet Federated Learning for Scalable and Secure IoT Management", "categories": ["cs.LG", "cs.AI", "cs.ET"], "comment": "This work has been submitted to the IEEE Global Communications\n  Conference (GLOBECOM) 2025 for possible publication", "summary": "The rapid expansion of IoT ecosystems introduces severe challenges in\nscalability, security, and real-time decision-making. Traditional centralized\narchitectures struggle with latency, privacy concerns, and excessive resource\nconsumption, making them unsuitable for modern large-scale IoT deployments.\nThis paper presents a novel Federated Learning-driven Large Language Model\n(FL-LLM) framework, designed to enhance IoT system intelligence while ensuring\ndata privacy and computational efficiency. The framework integrates Generative\nIoT (GIoT) models with a Gradient Sensing Federated Strategy (GSFS),\ndynamically optimizing model updates based on real-time network conditions. By\nleveraging a hybrid edge-cloud processing architecture, our approach balances\nintelligence, scalability, and security in distributed IoT environments.\nEvaluations on the IoT-23 dataset demonstrate that our framework improves model\naccuracy, reduces response latency, and enhances energy efficiency,\noutperforming traditional FL techniques (i.e., FedAvg, FedOpt). These findings\nhighlight the potential of integrating LLM-powered federated learning into\nlarge-scale IoT ecosystems, paving the way for more secure, scalable, and\nadaptive IoT management solutions."}
{"id": "2504.16041", "pdf": "https://arxiv.org/pdf/2504.16041", "abs": "https://arxiv.org/abs/2504.16041", "authors": ["Amund Tveit", "Bjørn Remseth", "Arve Skogvold"], "title": "Muon Optimizer Accelerates Grokking", "categories": ["cs.LG", "cs.AI", "I.2"], "comment": "8 pages, 4 figures", "summary": "This paper investigates the impact of different optimizers on the grokking\nphenomenon, where models exhibit delayed generalization. We conducted\nexperiments across seven numerical tasks (primarily modular arithmetic) using a\nmodern Transformer architecture. The experimental configuration systematically\nvaried the optimizer (Muon vs. AdamW) and the softmax activation function\n(standard softmax, stablemax, and sparsemax) to assess their combined effect on\nlearning dynamics. Our empirical evaluation reveals that the Muon optimizer,\ncharacterized by its use of spectral norm constraints and second-order\ninformation, significantly accelerates the onset of grokking compared to the\nwidely used AdamW optimizer. Specifically, Muon reduced the mean grokking epoch\nfrom 153.09 to 102.89 across all configurations, a statistically significant\ndifference (t = 5.0175, p = 6.33e-08). This suggests that the optimizer choice\nplays a crucial role in facilitating the transition from memorization to\ngeneralization."}
{"id": "2504.16047", "pdf": "https://arxiv.org/pdf/2504.16047", "abs": "https://arxiv.org/abs/2504.16047", "authors": ["Frank Li", "Hari Trivedi", "Bardia Khosravi", "Theo Dapamede", "Mohammadreza Chavoshi", "Abdulhameed Dere", "Rohan Satya Isaac", "Aawez Mansuri", "Janice Newsome", "Saptarshi Purkayastha", "Judy Gichoya"], "title": "Evaluating Vision Language Models (VLMs) for Radiology: A Comprehensive Analysis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Foundation models, trained on vast amounts of data using self-supervised\ntechniques, have emerged as a promising frontier for advancing artificial\nintelligence (AI) applications in medicine. This study evaluates three\ndifferent vision-language foundation models (RAD-DINO, CheXagent, and\nBiomedCLIP) on their ability to capture fine-grained imaging features for\nradiology tasks. The models were assessed across classification, segmentation,\nand regression tasks for pneumothorax and cardiomegaly on chest radiographs.\nSelf-supervised RAD-DINO consistently excelled in segmentation tasks, while\ntext-supervised CheXagent demonstrated superior classification performance.\nBiomedCLIP showed inconsistent performance across tasks. A custom segmentation\nmodel that integrates global and local features substantially improved\nperformance for all foundation models, particularly for challenging\npneumothorax segmentation. The findings highlight that pre-training methodology\nsignificantly influences model performance on specific downstream tasks. For\nfine-grained segmentation tasks, models trained without text supervision\nperformed better, while text-supervised models offered advantages in\nclassification and interpretability. These insights provide guidance for\nselecting foundation models based on specific clinical applications in\nradiology."}
{"id": "2504.16053", "pdf": "https://arxiv.org/pdf/2504.16053", "abs": "https://arxiv.org/abs/2504.16053", "authors": ["Zhifan Ye", "Kejing Xia", "Yonggan Fu", "Xin Dong", "Jihoon Hong", "Xiangchi Yuan", "Shizhe Diao", "Jan Kautz", "Pavlo Molchanov", "Yingyan Celine Lin"], "title": "LongMamba: Enhancing Mamba's Long Context Capabilities via Training-Free Receptive Field Enlargement", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ICLR 2025", "summary": "State space models (SSMs) have emerged as an efficient alternative to\nTransformer models for language modeling, offering linear computational\ncomplexity and constant memory usage as context length increases. However,\ndespite their efficiency in handling long contexts, recent studies have shown\nthat SSMs, such as Mamba models, generally underperform compared to\nTransformers in long-context understanding tasks. To address this significant\nshortfall and achieve both efficient and accurate long-context understanding,\nwe propose LongMamba, a training-free technique that significantly enhances the\nlong-context capabilities of Mamba models. LongMamba builds on our discovery\nthat the hidden channels in Mamba can be categorized into local and global\nchannels based on their receptive field lengths, with global channels primarily\nresponsible for long-context capability. These global channels can become the\nkey bottleneck as the input context lengthens. Specifically, when input lengths\nlargely exceed the training sequence length, global channels exhibit\nlimitations in adaptively extend their receptive fields, leading to Mamba's\npoor long-context performance. The key idea of LongMamba is to mitigate the\nhidden state memory decay in these global channels by preventing the\naccumulation of unimportant tokens in their memory. This is achieved by first\nidentifying critical tokens in the global channels and then applying token\nfiltering to accumulate only those critical tokens. Through extensive\nbenchmarking across synthetic and real-world long-context scenarios, LongMamba\nsets a new standard for Mamba's long-context performance, significantly\nextending its operational range without requiring additional training. Our code\nis available at https://github.com/GATECH-EIC/LongMamba."}
{"id": "2504.16061", "pdf": "https://arxiv.org/pdf/2504.16061", "abs": "https://arxiv.org/abs/2504.16061", "authors": ["Sangeet Khemlani", "Tyler Tran", "Nathaniel Gyory", "Anthony M. Harrison", "Wallace E. Lawson", "Ravenna Thielstrom", "Hunter Thompson", "Taaren Singh", "J. Gregory Trafton"], "title": "Vision language models are unreliable at trivial spatial cognition", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision language models (VLMs) are designed to extract relevant visuospatial\ninformation from images. Some research suggests that VLMs can exhibit humanlike\nscene understanding, while other investigations reveal difficulties in their\nability to process relational information. To achieve widespread applicability,\nVLMs must perform reliably, yielding comparable competence across a wide\nvariety of related tasks. We sought to test how reliable these architectures\nare at engaging in trivial spatial cognition, e.g., recognizing whether one\nobject is left of another in an uncluttered scene. We developed a benchmark\ndataset -- TableTest -- whose images depict 3D scenes of objects arranged on a\ntable, and used it to evaluate state-of-the-art VLMs. Results show that\nperformance could be degraded by minor variations of prompts that use logically\nequivalent descriptions. These analyses suggest limitations in how VLMs may\nreason about spatial relations in real-world applications. They also reveal\nnovel opportunities for bolstering image caption corpora for more efficient\ntraining and testing."}
{"id": "2504.16072", "pdf": "https://arxiv.org/pdf/2504.16072", "abs": "https://arxiv.org/abs/2504.16072", "authors": ["Long Lian", "Yifan Ding", "Yunhao Ge", "Sifei Liu", "Hanzi Mao", "Boyi Li", "Marco Pavone", "Ming-Yu Liu", "Trevor Darrell", "Adam Yala", "Yin Cui"], "title": "Describe Anything: Detailed Localized Image and Video Captioning", "categories": ["cs.CV", "cs.AI"], "comment": "Project page: https://describe-anything.github.io/", "summary": "Generating detailed and accurate descriptions for specific regions in images\nand videos remains a fundamental challenge for vision-language models. We\nintroduce the Describe Anything Model (DAM), a model designed for detailed\nlocalized captioning (DLC). DAM preserves both local details and global context\nthrough two key innovations: a focal prompt, which ensures high-resolution\nencoding of targeted regions, and a localized vision backbone, which integrates\nprecise localization with its broader context. To tackle the scarcity of\nhigh-quality DLC data, we propose a Semi-supervised learning (SSL)-based Data\nPipeline (DLC-SDP). DLC-SDP starts with existing segmentation datasets and\nexpands to unlabeled web images using SSL. We introduce DLC-Bench, a benchmark\ndesigned to evaluate DLC without relying on reference captions. DAM sets new\nstate-of-the-art on 7 benchmarks spanning keyword-level, phrase-level, and\ndetailed multi-sentence localized image and video captioning."}
{"id": "2504.16078", "pdf": "https://arxiv.org/pdf/2504.16078", "abs": "https://arxiv.org/abs/2504.16078", "authors": ["Thomas Schmied", "Jörg Bornschein", "Jordi Grau-Moya", "Markus Wulfmeier", "Razvan Pascanu"], "title": "LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making Abilities", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The success of Large Language Models (LLMs) has sparked interest in various\nagentic applications. A key hypothesis is that LLMs, leveraging common sense\nand Chain-of-Thought (CoT) reasoning, can effectively explore and efficiently\nsolve complex domains. However, LLM agents have been found to suffer from\nsub-optimal exploration and the knowing-doing gap, the inability to effectively\nact on knowledge present in the model. In this work, we systematically study\nwhy LLMs perform sub-optimally in decision-making scenarios. In particular, we\nclosely examine three prevalent failure modes: greediness, frequency bias, and\nthe knowing-doing gap. We propose mitigation of these shortcomings by\nfine-tuning via Reinforcement Learning (RL) on self-generated CoT rationales.\nOur experiments across multi-armed bandits, contextual bandits, and\nTic-tac-toe, demonstrate that RL fine-tuning enhances the decision-making\nabilities of LLMs by increasing exploration and narrowing the knowing-doing\ngap. Finally, we study both classic exploration mechanisms, such as\n$\\epsilon$-greedy, and LLM-specific approaches, such as self-correction and\nself-consistency, to enable more effective fine-tuning of LLMs for\ndecision-making."}
