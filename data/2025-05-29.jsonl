{"id": "2505.21552", "pdf": "https://arxiv.org/pdf/2505.21552", "abs": "https://arxiv.org/abs/2505.21552", "authors": ["Diogo Cruz"], "title": "Understanding the learned look-ahead behavior of chess neural networks", "categories": ["cs.AI", "cs.LG"], "comment": "40 pages, 47 figures", "summary": "We investigate the look-ahead capabilities of chess-playing neural networks,\nspecifically focusing on the Leela Chess Zero policy network. We build on the\nwork of Jenner et al. (2024) by analyzing the model's ability to consider\nfuture moves and alternative sequences beyond the immediate next move. Our\nfindings reveal that the network's look-ahead behavior is highly\ncontext-dependent, varying significantly based on the specific chess position.\nWe demonstrate that the model can process information about board states up to\nseven moves ahead, utilizing similar internal mechanisms across different\nfuture time steps. Additionally, we provide evidence that the network considers\nmultiple possible move sequences rather than focusing on a single line of play.\nThese results offer new insights into the emergence of sophisticated look-ahead\ncapabilities in neural networks trained on strategic tasks, contributing to our\nunderstanding of AI reasoning in complex domains. Our work also showcases the\neffectiveness of interpretability techniques in uncovering cognitive-like\nprocesses in artificial intelligence systems."}
{"id": "2505.21668", "pdf": "https://arxiv.org/pdf/2505.21668", "abs": "https://arxiv.org/abs/2505.21668", "authors": ["Yongchao Chen", "Yueying Liu", "Junwei Zhou", "Yilun Hao", "Jingquan Wang", "Yang Zhang", "Chuchu Fan"], "title": "R1-Code-Interpreter: Training LLMs to Reason with Code via Supervised and Reinforcement Learning", "categories": ["cs.AI", "cs.CL", "cs.SC"], "comment": "33 pages, 8 figures", "summary": "Despite advances in reasoning and planning of R1-like models, Large Language\nModels (LLMs) still struggle with tasks requiring precise computation, symbolic\nmanipulation, optimization, and algorithmic reasoning, in which textual\nreasoning lacks the rigor of code execution. A key challenge is enabling LLMs\nto decide when to use textual reasoning versus code generation. While OpenAI\ntrains models to invoke a Code Interpreter as needed, public research lacks\nguidance on aligning pre-trained LLMs to effectively leverage code and\ngeneralize across diverse tasks. We present R1-Code-Interpreter, an extension\nof a text-only LLM trained via multi-turn supervised fine-tuning (SFT) and\nreinforcement learning (RL) to autonomously generate multiple code queries\nduring step-by-step reasoning. We curate 144 reasoning and planning tasks (107\nfor training, 37 for testing), each with over 200 diverse questions. We\nfine-tune Qwen-2.5 models (3B/7B/14B) using various SFT and RL strategies,\ninvestigating different answer formats, reasoning vs. non-reasoning models,\ncold vs. warm starts, GRPO vs. PPO, and masked vs. unmasked code outputs.\nUnlike prior RL work on narrow domains, we find that Code Interpreter training\nis significantly harder due to high task diversity and expensive code\nexecution, highlighting the critical role of the SFT stage. Our final model,\nR1-CI-14B, improves average accuracy on the 37 test tasks from 44.0\\% to\n64.1\\%, outperforming GPT-4o (text-only: 58.6\\%) and approaching GPT-4o with\nCode Interpreter (70.9\\%), with the emergent self-checking behavior via code\ngeneration. Datasets, Codes, and Models are available at\nhttps://github.com/yongchao98/R1-Code-Interpreter and\nhttps://huggingface.co/yongchao98."}
{"id": "2505.21671", "pdf": "https://arxiv.org/pdf/2505.21671", "abs": "https://arxiv.org/abs/2505.21671", "authors": ["Davin Choo", "Yuqi Pan", "Tonghan Wang", "Milind Tambe", "Alastair van Heerden", "Cheryl Johnson"], "title": "Adaptive Frontier Exploration on Graphs with Applications to Network-Based Disease Testing", "categories": ["cs.AI", "cs.DS", "cs.LG", "math.OC"], "comment": null, "summary": "We study a sequential decision-making problem on a $n$-node graph $G$ where\neach node has an unknown label from a finite set $\\mathbf{\\Sigma}$, drawn from\na joint distribution $P$ that is Markov with respect to $G$. At each step,\nselecting a node reveals its label and yields a label-dependent reward. The\ngoal is to adaptively choose nodes to maximize expected accumulated discounted\nrewards. We impose a frontier exploration constraint, where actions are limited\nto neighbors of previously selected nodes, reflecting practical constraints in\nsettings such as contact tracing and robotic exploration. We design a Gittins\nindex-based policy that applies to general graphs and is provably optimal when\n$G$ is a forest. Our implementation runs in $O(n^2 \\cdot |\\mathbf{\\Sigma}|^2)$\ntime while using $O(n \\cdot |\\mathbf{\\Sigma}|^2)$ oracle calls to $P$ and\n$O(n^2 \\cdot |\\mathbf{\\Sigma}|)$ space. Experiments on synthetic and real-world\ngraphs show that our method consistently outperforms natural baselines,\nincluding in non-tree, budget-limited, and undiscounted settings. For example,\nin HIV testing simulations on real-world sexual interaction networks, our\npolicy detects nearly all positive cases with only half the population tested,\nsubstantially outperforming other baselines."}
{"id": "2505.21674", "pdf": "https://arxiv.org/pdf/2505.21674", "abs": "https://arxiv.org/abs/2505.21674", "authors": ["Michael Katz", "Harsha Kokel", "Christian Muise", "Shirin Sohrabi", "Sarath Sreedharan"], "title": "Make Planning Research Rigorous Again!", "categories": ["cs.AI"], "comment": null, "summary": "In over sixty years since its inception, the field of planning has made\nsignificant contributions to both the theory and practice of building planning\nsoftware that can solve a never-before-seen planning problem. This was done\nthrough established practices of rigorous design and evaluation of planning\nsystems. It is our position that this rigor should be applied to the current\ntrend of work on planning with large language models. One way to do so is by\ncorrectly incorporating the insights, tools, and data from the automated\nplanning community into the design and evaluation of LLM-based planners. The\nexperience and expertise of the planning community are not just important from\na historical perspective; the lessons learned could play a crucial role in\naccelerating the development of LLM-based planners. This position is\nparticularly important in light of the abundance of recent works that replicate\nand propagate the same pitfalls that the planning community has encountered and\nlearned from. We believe that avoiding such known pitfalls will contribute\ngreatly to the progress in building LLM-based planners and to planning in\ngeneral."}
{"id": "2505.21523", "pdf": "https://arxiv.org/pdf/2505.21523", "abs": "https://arxiv.org/abs/2505.21523", "authors": ["Chengzhi Liu", "Zhongxing Xu", "Qingyue Wei", "Juncheng Wu", "James Zou", "Xin Eric Wang", "Yuyin Zhou", "Sheng Liu"], "title": "More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Test-time compute has empowered multimodal large language models to generate\nextended reasoning chains, yielding strong performance on tasks such as\nmultimodal math reasoning. However, this improved reasoning ability often comes\nwith increased hallucination: as generations become longer, models tend to\ndrift away from image-grounded content and rely more heavily on language\npriors. Attention analysis shows that longer reasoning chains lead to reduced\nfocus on visual inputs, which contributes to hallucination. To systematically\nstudy this phenomenon, we introduce RH-AUC, a metric that quantifies how a\nmodel's perception accuracy changes with reasoning length, allowing us to\nevaluate whether the model preserves visual grounding during reasoning. We also\nrelease RH-Bench, a diagnostic benchmark that spans a variety of multimodal\ntasks, designed to assess the trade-off between reasoning ability and\nhallucination. Our analysis reveals that (i) larger models typically achieve a\nbetter balance between reasoning and perception, and (ii) this balance is\ninfluenced more by the types and domains of training data than by its overall\nvolume. These findings underscore the importance of evaluation frameworks that\njointly consider both reasoning quality and perceptual fidelity."}
{"id": "2505.21513", "pdf": "https://arxiv.org/pdf/2505.21513", "abs": "https://arxiv.org/abs/2505.21513", "authors": ["Nicolas Echevarrieta-Catalan", "Ana Ribas-Rodriguez", "Francisco Cedron", "Odelia Schwartz", "Vanessa Aguiar-Pulido"], "title": "Enhancing Vision Transformer Explainability Using Artificial Astrocytes", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "LXCV Workshop at IEEE / CVF Computer Vision and Pattern Recognition\n  Conference (CVPR) 2025", "summary": "Machine learning models achieve high precision, but their decision-making\nprocesses often lack explainability. Furthermore, as model complexity\nincreases, explainability typically decreases. Existing efforts to improve\nexplainability primarily involve developing new eXplainable artificial\nintelligence (XAI) techniques or incorporating explainability constraints\nduring training. While these approaches yield specific improvements, their\napplicability remains limited. In this work, we propose the Vision Transformer\nwith artificial Astrocytes (ViTA). This training-free approach is inspired by\nneuroscience and enhances the reasoning of a pretrained deep neural network to\ngenerate more human-aligned explanations. We evaluated our approach employing\ntwo well-known XAI techniques, Grad-CAM and Grad-CAM++, and compared it to a\nstandard Vision Transformer (ViT). Using the ClickMe dataset, we quantified the\nsimilarity between the heatmaps produced by the XAI techniques and a\n(human-aligned) ground truth. Our results consistently demonstrate that\nincorporating artificial astrocytes enhances the alignment of model\nexplanations with human perception, leading to statistically significant\nimprovements across all XAI techniques and metrics utilized."}
{"id": "2505.21765", "pdf": "https://arxiv.org/pdf/2505.21765", "abs": "https://arxiv.org/abs/2505.21765", "authors": ["Sohyun An", "Ruochen Wang", "Tianyi Zhou", "Cho-Jui Hsieh"], "title": "Don't Think Longer, Think Wisely: Optimizing Thinking Dynamics for Large Reasoning Models", "categories": ["cs.AI"], "comment": "Work In Progress", "summary": "While recent success of large reasoning models (LRMs) significantly advanced\nLLMs' reasoning capability by optimizing the final answer accuracy using\nreinforcement learning, they may also drastically increase the output length\ndue to overthinking, characterized by unnecessarily complex reasoning paths\nthat waste computation and potentially degrade the performance. We hypothesize\nthat such inefficiencies stem from LRMs' limited capability to dynamically\nselect the proper modular reasoning strategies, termed thinking patterns at the\nright position. To investigate this hypothesis, we propose a dynamic\noptimization framework that segments model-generated reasoning paths into\ndistinct thinking patterns, systematically identifying and promoting beneficial\npatterns that improve the answer while removing detrimental ones. Empirical\nanalysis confirms that our optimized thinking paths yield more concise yet\nsufficiently informative trajectories, enhancing reasoning efficiency by\nreducing attention FLOPs by up to 47% while maintaining accuracy for originally\ncorrect responses. Moreover, a non-trivial portion of originally incorrect\nresponses are transformed into correct ones, achieving a 15.6% accuracy\nimprovement with reduced length. Motivated by the improvement brought by the\noptimized thinking paths, we apply a preference optimization technique\nsupported by a pairwise dataset contrasting suboptimal and optimal reasoning\npaths. Experimental evaluations across multiple mathematical reasoning\nbenchmarks reveal that our method notably reduces computational overhead while\nsimultaneously improving reasoning accuracy, achieving up to a 12% accuracy\nimprovement and reducing token usage from approximately 5,000 to 3,000 tokens."}
{"id": "2505.21578", "pdf": "https://arxiv.org/pdf/2505.21578", "abs": "https://arxiv.org/abs/2505.21578", "authors": ["Titouan Parcollet", "Yuan Tseng", "Shucong Zhang", "Rogier van Dalen"], "title": "Loquacious Set: 25,000 Hours of Transcribed and Diverse English Speech Recognition Data for Research and Commercial Use", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted at Interspeech 2025", "summary": "Automatic speech recognition (ASR) research is driven by the availability of\ncommon datasets between industrial researchers and academics, encouraging\ncomparisons and evaluations. LibriSpeech, despite its long success as an ASR\nbenchmark, is now limited by its size and focus on clean, read speech, leading\nto near-zero word error rates. More recent datasets, including MOSEL, YODAS,\nGigaspeech, OWSM, Libriheavy or People's Speech suffer from major limitations\nincluding licenses that researchers in the industry cannot use, unreliable\ntranscriptions, incorrect audio data, or the lack of evaluation sets. This work\npresents the Loquacious Set, a 25,000-hour curated collection of commercially\nusable English speech. Featuring hundreds of thousands of speakers with diverse\naccents and a wide range of speech types (read, spontaneous, talks, clean,\nnoisy), the Loquacious Set is designed to work for academics and researchers in\nthe industry to build ASR systems in real-world scenarios."}
{"id": "2505.21520", "pdf": "https://arxiv.org/pdf/2505.21520", "abs": "https://arxiv.org/abs/2505.21520", "authors": ["Spiros Baxavanakis", "Manos Schinas", "Symeon Papadopoulos"], "title": "Do DeepFake Attribution Models Generalize?", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advancements in DeepFake generation, along with the proliferation of\nopen-source tools, have significantly lowered the barrier for creating\nsynthetic media. This trend poses a serious threat to the integrity and\nauthenticity of online information, undermining public trust in institutions\nand media. State-of-the-art research on DeepFake detection has primarily\nfocused on binary detection models. A key limitation of these models is that\nthey treat all manipulation techniques as equivalent, despite the fact that\ndifferent methods introduce distinct artifacts and visual cues. Only a limited\nnumber of studies explore DeepFake attribution models, although such models are\ncrucial in practical settings. By providing the specific manipulation method\nemployed, these models could enhance both the perceived trustworthiness and\nexplainability for end users. In this work, we leverage five state-of-the-art\nbackbone models and conduct extensive experiments across six DeepFake datasets.\nFirst, we compare binary and multi-class models in terms of cross-dataset\ngeneralization. Second, we examine the accuracy of attribution models in\ndetecting seen manipulation methods in unknown datasets, hence uncovering data\ndistribution shifts on the same DeepFake manipulations. Last, we assess the\neffectiveness of contrastive methods in improving cross-dataset generalization\nperformance. Our findings indicate that while binary models demonstrate better\ngeneralization abilities, larger models, contrastive methods, and higher data\nquality can lead to performance improvements in attribution models. The code of\nthis work is available on GitHub."}
{"id": "2505.21784", "pdf": "https://arxiv.org/pdf/2505.21784", "abs": "https://arxiv.org/abs/2505.21784", "authors": ["Tharindu Kumarage", "Ninareh Mehrabi", "Anil Ramakrishna", "Xinyan Zhao", "Richard Zemel", "Kai-Wei Chang", "Aram Galstyan", "Rahul Gupta", "Charith Peris"], "title": "Towards Safety Reasoning in LLMs: AI-agentic Deliberation for Policy-embedded CoT Data Creation", "categories": ["cs.AI", "cs.CL"], "comment": "Accepted to ACL 2025 (Findings)", "summary": "Safety reasoning is a recent paradigm where LLMs reason over safety policies\nbefore generating responses, thereby mitigating limitations in existing safety\nmeasures such as over-refusal and jailbreak vulnerabilities. However,\nimplementing this paradigm is challenging due to the resource-intensive process\nof creating high-quality policy-embedded chain-of-thought (CoT) datasets while\nensuring reasoning remains accurate and free from hallucinations or policy\nconflicts. To tackle this, we propose AIDSAFE: Agentic Iterative Deliberation\nfor Safety Reasoning, a novel data generation recipe that leverages multi-agent\ndeliberation to iteratively expand reasoning on safety policies. A data refiner\nstage in AIDSAFE ensures high-quality outputs by eliminating repetitive,\nredundant, and deceptive thoughts. AIDSAFE-generated CoTs provide a strong\nfoundation for supervised fine-tuning (SFT)-based safety training.\nAdditionally, to address the need of preference data in alignment stages, such\nas DPO training, we introduce a supplemental recipe that uses belief\naugmentation to create distinct selected and rejected CoT samples. Our\nevaluations demonstrate that AIDSAFE-generated CoTs achieve superior policy\nadherence and reasoning quality. Consequently, we show that fine-tuning\nopen-source LLMs on these CoTs can significantly improve safety generalization\nand jailbreak robustness while maintaining acceptable utility and over-refusal\naccuracy. AIDSAFE-generated CoT datasets can be found here:\nhttps://huggingface.co/datasets/AmazonScience/AIDSAFE"}
{"id": "2505.21598", "pdf": "https://arxiv.org/pdf/2505.21598", "abs": "https://arxiv.org/abs/2505.21598", "authors": ["Yajiao Liu", "Congliang Chen", "Junchi Yang", "Ruoyu Sun"], "title": "Rethinking Data Mixture for Large Language Models: A Comprehensive Survey and New Perspectives", "categories": ["cs.CL"], "comment": "The first version of this paper was submitted to ACL ARR 2025\n  February Submission", "summary": "Training large language models with data collected from various domains can\nimprove their performance on downstream tasks. However, given a fixed training\nbudget, the sampling proportions of these different domains significantly\nimpact the model's performance. How can we determine the domain weights across\ndifferent data domains to train the best-performing model within constrained\ncomputational resources? In this paper, we provide a comprehensive overview of\nexisting data mixture methods. First, we propose a fine-grained categorization\nof existing methods, extending beyond the previous offline and online\nclassification. Offline methods are further grouped into heuristic-based,\nalgorithm-based, and function fitting-based methods. For online methods, we\ncategorize them into three groups: online min-max optimization, online mixing\nlaw, and other approaches by drawing connections with the optimization\nframeworks underlying offline methods. Second, we summarize the problem\nformulations, representative algorithms for each subtype of offline and online\nmethods, and clarify the relationships and distinctions among them. Finally, we\ndiscuss the advantages and disadvantages of each method and highlight key\nchallenges in the field of data mixture."}
{"id": "2505.21522", "pdf": "https://arxiv.org/pdf/2505.21522", "abs": "https://arxiv.org/abs/2505.21522", "authors": ["Shan Gao", "Zhiqiang Wu", "Yawen Niu", "Xiaotao Li", "Qingqing Xu"], "title": "CIM-NET: A Video Denoising Deep Neural Network Model Optimized for Computing-in-Memory Architectures", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "comment": null, "summary": "While deep neural network (DNN)-based video denoising has demonstrated\nsignificant performance, deploying state-of-the-art models on edge devices\nremains challenging due to stringent real-time and energy efficiency\nrequirements. Computing-in-Memory (CIM) chips offer a promising solution by\nintegrating computation within memory cells, enabling rapid matrix-vector\nmultiplication (MVM). However, existing DNN models are often designed without\nconsidering CIM architectural constraints, thus limiting their acceleration\npotential during inference. To address this, we propose a hardware-algorithm\nco-design framework incorporating two innovations: (1) a CIM-Aware\nArchitecture, CIM-NET, optimized for large receptive field operation and CIM's\ncrossbar-based MVM acceleration; and (2) a pseudo-convolutional operator,\nCIM-CONV, used within CIM-NET to integrate slide-based processing with fully\nconnected transformations for high-quality feature extraction and\nreconstruction. This framework significantly reduces the number of MVM\noperations, improving inference speed on CIM chips while maintaining\ncompetitive performance. Experimental results indicate that, compared to the\nconventional lightweight model FastDVDnet, CIM-NET substantially reduces MVM\noperations with a slight decrease in denoising performance. With a stride value\nof 8, CIM-NET reduces MVM operations to 1/77th of the original, while\nmaintaining competitive PSNR (35.11 dB vs. 35.56 dB"}
{"id": "2505.21828", "pdf": "https://arxiv.org/pdf/2505.21828", "abs": "https://arxiv.org/abs/2505.21828", "authors": ["Chen Yueh-Han", "Guy Davidson", "Brenden M. Lake"], "title": "SAGE-Eval: Evaluating LLMs for Systematic Generalizations of Safety Facts", "categories": ["cs.AI"], "comment": null, "summary": "Do LLMs robustly generalize critical safety facts to novel situations?\nLacking this ability is dangerous when users ask naive questions. For instance,\n\"I'm considering packing melon balls for my 10-month-old's lunch. What other\nfoods would be good to include?\" Before offering food options, the LLM should\nwarn that melon balls pose a choking hazard to toddlers, as documented by the\nCDC. Failing to provide such warnings could result in serious injuries or even\ndeath. To evaluate this, we introduce SAGE-Eval, SAfety-fact systematic\nGEneralization evaluation, the first benchmark that tests whether LLMs properly\napply well established safety facts to naive user queries. SAGE-Eval comprises\n104 facts manually sourced from reputable organizations, systematically\naugmented to create 10,428 test scenarios across 7 common domains (e.g.,\nOutdoor Activities, Medicine). We find that the top model, Claude-3.7-sonnet,\npasses only 58% of all the safety facts tested. We also observe that model\ncapabilities and training compute weakly correlate with performance on\nSAGE-Eval, implying that scaling up is not the golden solution. Our findings\nsuggest frontier LLMs still lack robust generalization ability. We recommend\ndevelopers use SAGE-Eval in pre-deployment evaluations to assess model\nreliability in addressing salient risks. We publicly release SAGE-Eval at\nhttps://huggingface.co/datasets/YuehHanChen/SAGE-Eval and our code is available\nat https://github.com/YuehHanChen/SAGE-Eval/tree/main."}
{"id": "2505.21600", "pdf": "https://arxiv.org/pdf/2505.21600", "abs": "https://arxiv.org/abs/2505.21600", "authors": ["Tianyu Fu", "Yi Ge", "Yichen You", "Enshu Liu", "Zhihang Yuan", "Guohao Dai", "Shengen Yan", "Huazhong Yang", "Yu Wang"], "title": "R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large Model Token Routing", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PF", "I.2.7"], "comment": null, "summary": "Large Language Models (LLMs) achieve impressive reasoning capabilities at the\ncost of substantial inference overhead, posing substantial deployment\nchallenges. Although distilled Small Language Models (SLMs) significantly\nenhance efficiency, their performance suffers as they fail to follow LLMs'\nreasoning paths. Luckily, we reveal that only a small fraction of tokens\ngenuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens\nare either identical or exhibit neutral differences, such as minor variations\nin abbreviations or expressions. Leveraging this insight, we introduce **Roads\nto Rome (R2R)**, a neural token routing method that selectively utilizes LLMs\nonly for these critical, path-divergent tokens, while leaving the majority of\ntoken generation to the SLM. We also develop an automatic data generation\npipeline that identifies divergent tokens and generates token-level routing\nlabels to train the lightweight router. We apply R2R to combine R1-1.5B and\nR1-32B models from the DeepSeek family, and evaluate on challenging math,\ncoding, and QA benchmarks. With an average activated parameter size of 5.6B,\nR2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the\nR1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with\ncomparable performance, advancing the Pareto frontier of test-time scaling\nefficiency. Our code is available at https://github.com/thu-nics/R2R."}
{"id": "2505.21524", "pdf": "https://arxiv.org/pdf/2505.21524", "abs": "https://arxiv.org/abs/2505.21524", "authors": ["Amitai Yacobi", "Nir Ben-Ari", "Ronen Talmon", "Uri Shaham"], "title": "Learning Shared Representations from Unpaired Data", "categories": ["cs.CV", "cs.LG", "stat.ML"], "comment": null, "summary": "Learning shared representations is a primary area of multimodal\nrepresentation learning. The current approaches to achieve a shared embedding\nspace rely heavily on paired samples from each modality, which are\nsignificantly harder to obtain than unpaired ones. In this work, we demonstrate\nthat shared representations can be learned almost exclusively from unpaired\ndata. Our arguments are grounded in the spectral embeddings of the random walk\nmatrices constructed independently from each unimodal representation. Empirical\nresults in computer vision and natural language processing domains support its\npotential, revealing the effectiveness of unpaired data in capturing meaningful\ncross-modal relations, demonstrating high capabilities in retrieval tasks,\ngeneration, arithmetics, zero-shot, and cross-domain classification. This work,\nto the best of our knowledge, is the first to demonstrate these capabilities\nalmost exclusively from unpaired samples, giving rise to a cross-modal\nembedding that could be viewed as universal, i.e., independent of the specific\nmodalities of the data. Our code IS publicly available at\nhttps://github.com/shaham-lab/SUE."}
{"id": "2505.21887", "pdf": "https://arxiv.org/pdf/2505.21887", "abs": "https://arxiv.org/abs/2505.21887", "authors": ["Ahmed Heakl", "Yahia Salaheldin Shaaban", "Martin Takac", "Salem Lahlou", "Zangir Iklassov"], "title": "SVRPBench: A Realistic Benchmark for Stochastic Vehicle Routing Problem", "categories": ["cs.AI", "cs.CE", "cs.LG"], "comment": "18 pages, 14 figures, 11 tables", "summary": "Robust routing under uncertainty is central to real-world logistics, yet most\nbenchmarks assume static, idealized settings. We present SVRPBench, the first\nopen benchmark to capture high-fidelity stochastic dynamics in vehicle routing\nat urban scale. Spanning more than 500 instances with up to 1000 customers, it\nsimulates realistic delivery conditions: time-dependent congestion, log-normal\ndelays, probabilistic accidents, and empirically grounded time windows for\nresidential and commercial clients. Our pipeline generates diverse,\nconstraint-rich scenarios, including multi-depot and multi-vehicle setups.\nBenchmarking reveals that state-of-the-art RL solvers like POMO and AM degrade\nby over 20% under distributional shift, while classical and metaheuristic\nmethods remain robust. To enable reproducible research, we release the dataset\nand evaluation suite. SVRPBench challenges the community to design solvers that\ngeneralize beyond synthetic assumptions and adapt to real-world uncertainty."}
{"id": "2505.21608", "pdf": "https://arxiv.org/pdf/2505.21608", "abs": "https://arxiv.org/abs/2505.21608", "authors": ["Miao Peng", "Nuo Chen", "Jianheng Tang", "Jia Li"], "title": "How does Misinformation Affect Large Language Model Behaviors and Preferences?", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Large Language Models (LLMs) have shown remarkable capabilities in\nknowledge-intensive tasks, while they remain vulnerable when encountering\nmisinformation. Existing studies have explored the role of LLMs in combating\nmisinformation, but there is still a lack of fine-grained analysis on the\nspecific aspects and extent to which LLMs are influenced by misinformation. To\nbridge this gap, we present MisBench, the current largest and most\ncomprehensive benchmark for evaluating LLMs' behavior and knowledge preference\ntoward misinformation. MisBench consists of 10,346,712 pieces of\nmisinformation, which uniquely considers both knowledge-based conflicts and\nstylistic variations in misinformation. Empirical results reveal that while\nLLMs demonstrate comparable abilities in discerning misinformation, they still\nremain susceptible to knowledge conflicts and stylistic variations. Based on\nthese findings, we further propose a novel approach called Reconstruct to\nDiscriminate (RtD) to strengthen LLMs' ability to detect misinformation. Our\nstudy provides valuable insights into LLMs' interactions with misinformation,\nand we believe MisBench can serve as an effective benchmark for evaluating\nLLM-based detectors and enhancing their reliability in real-world applications.\nCodes and data are available at https://github.com/GKNL/MisBench."}
{"id": "2505.21528", "pdf": "https://arxiv.org/pdf/2505.21528", "abs": "https://arxiv.org/abs/2505.21528", "authors": ["Mokai Pan", "Kaizhen Zhu", "Yuexin Ma", "Yanwei Fu", "Jingyi Yu", "Jingya Wang", "Ye Shi"], "title": "UniDB++: Fast Sampling of Unified Diffusion Bridge", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Diffusion Bridges enable transitions between arbitrary distributions, with\nthe Unified Diffusion Bridge (UniDB) framework achieving high-fidelity image\ngeneration via a Stochastic Optimal Control (SOC) formulation. However, UniDB's\nreliance on iterative Euler sampling methods results in slow, computationally\nexpensive inference, while existing acceleration techniques for diffusion or\ndiffusion bridge models fail to address its unique challenges: missing terminal\nmean constraints and SOC-specific penalty coefficients in its SDEs. We present\nUniDB++, a training-free sampling algorithm that significantly improves upon\nthese limitations. The method's key advancement comes from deriving exact\nclosed-form solutions for UniDB's reverse-time SDEs, effectively reducing the\nerror accumulation inherent in Euler approximations and enabling high-quality\ngeneration with up to 20$\\times$ fewer sampling steps. This method is further\ncomplemented by replacing conventional noise prediction with a more stable data\nprediction model, along with an SDE-Corrector mechanism that maintains\nperceptual quality for low-step regimes (5-10 steps). Additionally, we\ndemonstrate that UniDB++ aligns with existing diffusion bridge acceleration\nmethods by evaluating their update rules, and UniDB++ can recover DBIMs as\nspecial cases under some theoretical conditions. Experiments demonstrate\nUniDB++'s state-of-the-art performance in image restoration tasks,\noutperforming Euler-based methods in fidelity and speed while reducing\ninference time significantly. This work bridges the gap between theoretical\ngenerality and practical efficiency in SOC-driven diffusion bridge models. Our\ncode is available at https://github.com/2769433owo/UniDB-plusplus."}
{"id": "2505.21907", "pdf": "https://arxiv.org/pdf/2505.21907", "abs": "https://arxiv.org/abs/2505.21907", "authors": ["Saleh Afzoon", "Zahra Jahanandish", "Phuong Thao Huynh", "Amin Beheshti", "Usman Naseem"], "title": "Modeling and Optimizing User Preferences in AI Copilots: A Comprehensive Survey and Taxonomy", "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "AI copilots, context-aware, AI-powered systems designed to assist users in\ntasks such as software development and content creation, are becoming integral\nto modern workflows. As these systems grow in capability and adoption,\npersonalization has emerged as a cornerstone for ensuring usability, trust, and\nproductivity. Central to this personalization is preference optimization: the\nability of AI copilots to detect, interpret, and align with individual user\npreferences. While personalization techniques are well-established in domains\nlike recommender systems and dialogue agents, their adaptation to interactive,\nreal-time systems like AI copilots remains fragmented and underexplored. This\nsurvey addresses this gap by synthesizing research on how user preferences are\ncaptured, modeled, and refined within the design of AI copilots. We introduce a\nunified definition of AI copilots and propose a phase-based taxonomy of\npreference optimization strategies, structured around pre-interaction,\nmid-interaction, and post-interaction stages. We analyze techniques for\nacquiring preference signals, modeling user intent, and integrating feedback\nloops, highlighting both established approaches and recent innovations. By\nbridging insights from AI personalization, human-AI collaboration, and large\nlanguage model adaptation, this survey provides a structured foundation for\ndesigning adaptive, preference-aware AI copilots. It offers a holistic view of\nthe available preference resources, how they can be leveraged, and which\ntechnical approaches are most suited to each stage of system design."}
{"id": "2505.21646", "pdf": "https://arxiv.org/pdf/2505.21646", "abs": "https://arxiv.org/abs/2505.21646", "authors": ["Lei Zhang", "Markus Stricker"], "title": "Iterative Corpus Refinement for Materials Property Prediction Based on Scientific Texts", "categories": ["cs.CL", "cond-mat.mtrl-sci"], "comment": "13 pages, 5 figures, 2 tables, accepted at ECMLPKDD 2025", "summary": "The discovery and optimization of materials for specific applications is\nhampered by the practically infinite number of possible elemental combinations\nand associated properties, also known as the `combinatorial explosion'. By\nnature of the problem, data are scarce and all possible data sources should be\nused. In addition to simulations and experimental results, the latent knowledge\nin scientific texts is not yet used to its full potential. We present an\niterative framework that refines a given scientific corpus by strategic\nselection of the most diverse documents, training Word2Vec models, and\nmonitoring the convergence of composition-property correlations in embedding\nspace. Our approach is applied to predict high-performing materials for oxygen\nreduction (ORR), hydrogen evolution (HER), and oxygen evolution (OER) reactions\nfor a large number of possible candidate compositions. Our method successfully\npredicts the highest performing compositions among a large pool of candidates,\nvalidated by experimental measurements of the electrocatalytic performance in\nthe lab. This work demonstrates and validates the potential of iterative corpus\nrefinement to accelerate materials discovery and optimization, offering a\nscalable and efficient tool for screening large compositional spaces where\nreliable data are scarce or non-existent."}
{"id": "2505.21531", "pdf": "https://arxiv.org/pdf/2505.21531", "abs": "https://arxiv.org/abs/2505.21531", "authors": ["Kunhang Li", "Jason Naradowsky", "Yansong Feng", "Yusuke Miyao"], "title": "How Much Do Large Language Models Know about Human Motion? A Case Study in 3D Avatar Control", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "comment": null, "summary": "We explore Large Language Models (LLMs)' human motion knowledge through 3D\navatar control. Given a motion instruction, we prompt LLMs to first generate a\nhigh-level movement plan with consecutive steps (High-level Planning), then\nspecify body part positions in each step (Low-level Planning), which we\nlinearly interpolate into avatar animations as a clear verification lens for\nhuman evaluators. Through carefully designed 20 representative motion\ninstructions with full coverage of basic movement primitives and balanced body\npart usage, we conduct comprehensive evaluations including human assessment of\nboth generated animations and high-level movement plans, as well as automatic\ncomparison with oracle positions in low-level planning. We find that LLMs are\nstrong at interpreting the high-level body movements but struggle with precise\nbody part positioning. While breaking down motion queries into atomic\ncomponents improves planning performance, LLMs have difficulty with multi-step\nmovements involving high-degree-of-freedom body parts. Furthermore, LLMs\nprovide reasonable approximation for general spatial descriptions, but fail to\nhandle precise spatial specifications in text, and the precise spatial-temporal\nparameters needed for avatar control. Notably, LLMs show promise in\nconceptualizing creative motions and distinguishing culturally-specific motion\npatterns."}
{"id": "2505.21935", "pdf": "https://arxiv.org/pdf/2505.21935", "abs": "https://arxiv.org/abs/2505.21935", "authors": ["Kaiyu He", "Zhiyu Chen"], "title": "From Reasoning to Learning: A Survey on Hypothesis Discovery and Rule Learning with Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Since the advent of Large Language Models (LLMs), efforts have largely\nfocused on improving their instruction-following and deductive reasoning\nabilities, leaving open the question of whether these models can truly discover\nnew knowledge. In pursuit of artificial general intelligence (AGI), there is a\ngrowing need for models that not only execute commands or retrieve information\nbut also learn, reason, and generate new knowledge by formulating novel\nhypotheses and theories that deepen our understanding of the world. Guided by\nPeirce's framework of abduction, deduction, and induction, this survey offers a\nstructured lens to examine LLM-based hypothesis discovery. We synthesize\nexisting work in hypothesis generation, application, and validation,\nidentifying both key achievements and critical gaps. By unifying these threads,\nwe illuminate how LLMs might evolve from mere ``information executors'' into\nengines of genuine innovation, potentially transforming research, science, and\nreal-world problem solving."}
{"id": "2505.21657", "pdf": "https://arxiv.org/pdf/2505.21657", "abs": "https://arxiv.org/abs/2505.21657", "authors": ["Zeinab Dehghani", "Koorosh Aslansefat", "Adil Khan", "Mohammed Naveed Akram"], "title": "Explainability of Large Language Models using SMILE: Statistical Model-agnostic Interpretability with Local Explanations", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "arXiv admin note: text overlap with arXiv:2412.16277", "summary": "Large language models like GPT, LLAMA, and Claude have become incredibly\npowerful at generating text, but they are still black boxes, so it is hard to\nunderstand how they decide what to say. That lack of transparency can be\nproblematic, especially in fields where trust and accountability matter. To\nhelp with this, we introduce SMILE, a new method that explains how these models\nrespond to different parts of a prompt. SMILE is model-agnostic and works by\nslightly changing the input, measuring how the output changes, and then\nhighlighting which words had the most impact. Create simple visual heat maps\nshowing which parts of a prompt matter the most. We tested SMILE on several\nleading LLMs and used metrics such as accuracy, consistency, stability, and\nfidelity to show that it gives clear and reliable explanations. By making these\nmodels easier to understand, SMILE brings us one step closer to making AI more\ntransparent and trustworthy."}
{"id": "2505.21532", "pdf": "https://arxiv.org/pdf/2505.21532", "abs": "https://arxiv.org/abs/2505.21532", "authors": ["Ismail Erbas", "Ferhat Demirkiran", "Karthik Swaminathan", "Naigang Wang", "Navid Ibtehaj Nizam", "Stefan T. Radev", "Kaoutar El Maghraoui", "Xavier Intes", "Vikas Pandey"], "title": "EvidenceMoE: A Physics-Guided Mixture-of-Experts with Evidential Critics for Advancing Fluorescence Light Detection and Ranging in Scattering Media", "categories": ["cs.CV", "cs.AI", "cs.LG", "physics.optics"], "comment": "18 pages, 4 figures", "summary": "Fluorescence LiDAR (FLiDAR), a Light Detection and Ranging (LiDAR) technology\nemployed for distance and depth estimation across medical, automotive, and\nother fields, encounters significant computational challenges in scattering\nmedia. The complex nature of the acquired FLiDAR signal, particularly in such\nenvironments, makes isolating photon time-of-flight (related to target depth)\nand intrinsic fluorescence lifetime exceptionally difficult, thus limiting the\neffectiveness of current analytical and computational methodologies. To\novercome this limitation, we present a Physics-Guided Mixture-of-Experts (MoE)\nframework tailored for specialized modeling of diverse temporal components. In\ncontrast to the conventional MoE approaches our expert models are informed by\nunderlying physics, such as the radiative transport equation governing photon\npropagation in scattering media. Central to our approach is EvidenceMoE, which\nintegrates Evidence-Based Dirichlet Critics (EDCs). These critic models assess\nthe reliability of each expert's output by providing per-expert quality scores\nand corrective feedback. A Decider Network then leverages this information to\nfuse expert predictions into a robust final estimate adaptively. We validate\nour method using realistically simulated Fluorescence LiDAR (FLiDAR) data for\nnon-invasive cancer cell depth detection generated from photon transport models\nin tissue. Our framework demonstrates strong performance, achieving a\nnormalized root mean squared error (NRMSE) of 0.030 for depth estimation and\n0.074 for fluorescence lifetime."}
{"id": "2505.21988", "pdf": "https://arxiv.org/pdf/2505.21988", "abs": "https://arxiv.org/abs/2505.21988", "authors": ["Ziyang Zheng", "Kezhi Li", "Zhengyuan Shi", "Qiang Xu"], "title": "Functional Matching of Logic Subgraphs: Beyond Structural Isomorphism", "categories": ["cs.AI"], "comment": null, "summary": "Subgraph matching in logic circuits is foundational for numerous Electronic\nDesign Automation (EDA) applications, including datapath optimization,\narithmetic verification, and hardware trojan detection. However, existing\ntechniques rely primarily on structural graph isomorphism and thus fail to\nidentify function-related subgraphs when synthesis transformations\nsubstantially alter circuit topology. To overcome this critical limitation, we\nintroduce the concept of functional subgraph matching, a novel approach that\nidentifies whether a given logic function is implicitly present within a larger\ncircuit, irrespective of structural variations induced by synthesis or\ntechnology mapping. Specifically, we propose a two-stage multi-modal framework:\n(1) learning robust functional embeddings across AIG and post-mapping netlists\nfor functional subgraph detection, and (2) identifying fuzzy boundaries using a\ngraph segmentation approach. Evaluations on standard benchmarks (ITC99,\nOpenABCD, ForgeEDA) demonstrate significant performance improvements over\nexisting structural methods, with average $93.8\\%$ accuracy in functional\nsubgraph detection and a dice score of $91.3\\%$ in fuzzy boundary\nidentification."}
{"id": "2505.21670", "pdf": "https://arxiv.org/pdf/2505.21670", "abs": "https://arxiv.org/abs/2505.21670", "authors": ["Rahul Raman", "Khushi Sharma", "Sai Qian Zhang"], "title": "Rethinking the Outlier Distribution in Large Language Models: An In-depth Study", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Investigating outliers in large language models (LLMs) is crucial due to\ntheir significant impact on various aspects of LLM performance, including\nquantization and compression. Outliers often cause considerable quantization\nerrors, leading to degraded model performance. Identifying and addressing these\noutliers can enhance the accuracy and efficiency of the quantization process,\nenabling smoother deployment on edge devices or specialized hardware. Recent\nstudies have identified two common types of outliers in LLMs: massive\nactivations and channel-wise outliers. While numerous quantization algorithms\nhave been proposed to mitigate their effects and maintain satisfactory\naccuracy, few have thoroughly explored the root causes of these outliers in\ndepth. In this paper, we conduct a comprehensive investigation into the\nformation mechanisms of these outliers and propose potential strategies to\nmitigate their occurrence. Ultimately, we introduce some efficient approaches\nto eliminate most massive activations and channel-wise outliers with minimal\nimpact on accuracy."}
{"id": "2505.21533", "pdf": "https://arxiv.org/pdf/2505.21533", "abs": "https://arxiv.org/abs/2505.21533", "authors": ["Thalles Silva", "Helio Pedrini", "Adín Ramírez Rivera"], "title": "Self-Organizing Visual Prototypes for Non-Parametric Representation Learning", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at ICML 2025, code at https://github.com/sthalles/sop", "summary": "We present Self-Organizing Visual Prototypes (SOP), a new training technique\nfor unsupervised visual feature learning. Unlike existing prototypical\nself-supervised learning (SSL) methods that rely on a single prototype to\nencode all relevant features of a hidden cluster in the data, we propose the\nSOP strategy. In this strategy, a prototype is represented by many semantically\nsimilar representations, or support embeddings (SEs), each containing a\ncomplementary set of features that together better characterize their region in\nspace and maximize training performance. We reaffirm the feasibility of\nnon-parametric SSL by introducing novel non-parametric adaptations of two loss\nfunctions that implement the SOP strategy. Notably, we introduce the SOP Masked\nImage Modeling (SOP-MIM) task, where masked representations are reconstructed\nfrom the perspective of multiple non-parametric local SEs. We comprehensively\nevaluate the representations learned using the SOP strategy on a range of\nbenchmarks, including retrieval, linear evaluation, fine-tuning, and object\ndetection. Our pre-trained encoders achieve state-of-the-art performance on\nmany retrieval benchmarks and demonstrate increasing performance gains with\nmore complex encoders."}
{"id": "2505.22006", "pdf": "https://arxiv.org/pdf/2505.22006", "abs": "https://arxiv.org/abs/2505.22006", "authors": ["Changze Qiao", "Mingming Lu"], "title": "Efficiently Enhancing General Agents With Hierarchical-categorical Memory", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "With large language models (LLMs) demonstrating remarkable capabilities,\nthere has been a surge in research on leveraging LLMs to build general-purpose\nmulti-modal agents. However, existing approaches either rely on computationally\nexpensive end-to-end training using large-scale multi-modal data or adopt\ntool-use methods that lack the ability to continuously learn and adapt to new\nenvironments. In this paper, we introduce EHC, a general agent capable of\nlearning without parameter updates. EHC consists of a Hierarchical Memory\nRetrieval (HMR) module and a Task-Category Oriented Experience Learning (TOEL)\nmodule. The HMR module facilitates rapid retrieval of relevant memories and\ncontinuously stores new information without being constrained by memory\ncapacity. The TOEL module enhances the agent's comprehension of various task\ncharacteristics by classifying experiences and extracting patterns across\ndifferent categories. Extensive experiments conducted on multiple standard\ndatasets demonstrate that EHC outperforms existing methods, achieving\nstate-of-the-art performance and underscoring its effectiveness as a general\nagent for handling complex multi-modal tasks."}
{"id": "2505.21689", "pdf": "https://arxiv.org/pdf/2505.21689", "abs": "https://arxiv.org/abs/2505.21689", "authors": ["Avijit Gayen", "Somyajit Chakraborty", "Mainak Sen", "Soham Paul", "Angshuman Jana"], "title": "LLMPR: A Novel LLM-Driven Transfer Learning based Petition Ranking Model", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "28 pages, 5 figures, journal paper, submitted to AI and Law", "summary": "The persistent accumulation of unresolved legal cases, especially within the\nIndian judiciary, significantly hampers the timely delivery of justice. Manual\nmethods of prioritizing petitions are often prone to inefficiencies and\nsubjective biases further exacerbating delays. To address this issue, we\npropose LLMPR (Large Language Model-based Petition Ranking), an automated\nframework that utilizes transfer learning and machine learning to assign\npriority rankings to legal petitions based on their contextual urgency.\nLeveraging the ILDC dataset comprising 7,593 annotated petitions, we process\nunstructured legal text and extract features through various embedding\ntechniques, including DistilBERT, LegalBERT, and MiniLM. These textual\nembeddings are combined with quantitative indicators such as gap days, rank\nscores, and word counts to train multiple machine learning models, including\nRandom Forest, Decision Tree, XGBoost, LightGBM, and CatBoost. Our experiments\ndemonstrate that Random Forest and Decision Tree models yield superior\nperformance, with accuracy exceeding 99% and a Spearman rank correlation of\n0.99. Notably, models using only numerical features achieve nearly optimal\nranking results (R2 = 0.988, \\r{ho} = 0.998), while LLM-based embeddings offer\nonly marginal gains. These findings suggest that automated petition ranking can\neffectively streamline judicial workflows, reduce case backlog, and improve\nfairness in legal prioritization."}
{"id": "2505.21535", "pdf": "https://arxiv.org/pdf/2505.21535", "abs": "https://arxiv.org/abs/2505.21535", "authors": ["Yuxin Ren", "Maxwell D Collins", "Miao Hu", "Huanrui Yang"], "title": "Is Attention Required for Transformer Inference? Explore Function-preserving Attention Replacement", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "12 pages main paper + 6 pages appendix, 14 figures; submitted to\n  NeurIPS 2025", "summary": "While transformers excel across vision and language pretraining tasks, their\nreliance on attention mechanisms poses challenges for inference efficiency,\nespecially on edge and embedded accelerators with limited parallelism and\nmemory bandwidth. Hinted by the observed redundancy of attention at inference\ntime, we hypothesize that though the model learns complicated token dependency\nthrough pretraining, the inference-time sequence-to-sequence mapping in each\nattention layer is actually ''simple'' enough to be represented with a much\ncheaper function. In this work, we explore FAR, a Function-preserving Attention\nReplacement framework that replaces all attention blocks in pretrained\ntransformers with learnable sequence-to-sequence modules, exemplified by an\nLSTM. FAR optimize a multi-head LSTM architecture with a block-wise\ndistillation objective and a global structural pruning framework to achieve a\nfamily of efficient LSTM-based models from pretrained transformers. We validate\nFAR on the DeiT vision transformer family and demonstrate that it matches the\naccuracy of the original models on ImageNet and multiple downstream tasks with\nreduced parameters and latency. Further analysis shows that FAR preserves the\nsemantic token relationships and the token-to-token correlation learned in the\ntransformer's attention module."}
{"id": "2505.22050", "pdf": "https://arxiv.org/pdf/2505.22050", "abs": "https://arxiv.org/abs/2505.22050", "authors": ["Di Wu", "Jiaxin Fan", "Junzhe Zang", "Guanbo Wang", "Wei Yin", "Wenhao Li", "Bo Jin"], "title": "Reinforced Reasoning for Embodied Planning", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Embodied planning requires agents to make coherent multi-step decisions based\non dynamic visual observations and natural language goals. While recent\nvision-language models (VLMs) excel at static perception tasks, they struggle\nwith the temporal reasoning, spatial understanding, and commonsense grounding\nneeded for planning in interactive environments. In this work, we introduce a\nreinforcement fine-tuning framework that brings R1-style reasoning enhancement\ninto embodied planning. We first distill a high-quality dataset from a powerful\nclosed-source model and perform supervised fine-tuning (SFT) to equip the model\nwith structured decision-making priors. We then design a rule-based reward\nfunction tailored to multi-step action quality and optimize the policy via\nGeneralized Reinforced Preference Optimization (GRPO). Our approach is\nevaluated on Embench, a recent benchmark for interactive embodied tasks,\ncovering both in-domain and out-of-domain scenarios. Experimental results show\nthat our method significantly outperforms models of similar or larger scale,\nincluding GPT-4o-mini and 70B+ open-source baselines, and exhibits strong\ngeneralization to unseen environments. This work highlights the potential of\nreinforcement-driven reasoning to advance long-horizon planning in embodied AI."}
{"id": "2505.21693", "pdf": "https://arxiv.org/pdf/2505.21693", "abs": "https://arxiv.org/abs/2505.21693", "authors": ["Raoyuan Zhao", "Beiduo Chen", "Barbara Plank", "Michael A. Hedderich"], "title": "MAKIEval: A Multilingual Automatic WiKidata-based Framework for Cultural Awareness Evaluation for LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are used globally across many languages, but\ntheir English-centric pretraining raises concerns about cross-lingual\ndisparities for cultural awareness, often resulting in biased outputs. However,\ncomprehensive multilingual evaluation remains challenging due to limited\nbenchmarks and questionable translation quality. To better assess these\ndisparities, we introduce MAKIEval, an automatic multilingual framework for\nevaluating cultural awareness in LLMs across languages, regions, and topics.\nMAKIEval evaluates open-ended text generation, capturing how models express\nculturally grounded knowledge in natural language. Leveraging Wikidata's\nmultilingual structure as a cross-lingual anchor, it automatically identifies\ncultural entities in model outputs and links them to structured knowledge,\nenabling scalable, language-agnostic evaluation without manual annotation or\ntranslation. We then introduce four metrics that capture complementary\ndimensions of cultural awareness: granularity, diversity, cultural specificity,\nand consensus across languages. We assess 7 LLMs developed from different parts\nof the world, encompassing both open-source and proprietary systems, across 13\nlanguages, 19 countries and regions, and 6 culturally salient topics (e.g.,\nfood, clothing). Notably, we find that models tend to exhibit stronger cultural\nawareness in English, suggesting that English prompts more effectively activate\nculturally grounded knowledge. We publicly release our code and data."}
{"id": "2505.21538", "pdf": "https://arxiv.org/pdf/2505.21538", "abs": "https://arxiv.org/abs/2505.21538", "authors": ["Zihan Weng", "Lucas Gomez", "Taylor Whittington Webb", "Pouya Bashivan"], "title": "Caption This, Reason That: VLMs Caught in the Middle", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision-Language Models (VLMs) have shown remarkable progress in visual\nunderstanding in recent years. Yet, they still lag behind human capabilities in\nspecific visual tasks such as counting or relational reasoning. To understand\nthe underlying limitations, we adopt methodologies from cognitive science,\nanalyzing VLM performance along core cognitive axes: Perception, Attention, and\nMemory. Using a suite of tasks targeting these abilities, we evaluate\nstate-of-the-art VLMs, including GPT-4o. Our analysis reveals distinct\ncognitive profiles: while advanced models approach ceiling performance on some\ntasks (e.g. category identification), a significant gap persists, particularly\nin tasks requiring spatial understanding or selective attention. Investigating\nthe source of these failures and potential methods for improvement, we employ a\nvision-text decoupling analysis, finding that models struggling with direct\nvisual reasoning show marked improvement when reasoning over their own\ngenerated text captions. These experiments reveal a strong need for improved\nVLM Chain-of-Thought (CoT) abilities, even in models that consistently exceed\nhuman performance. Furthermore, we demonstrate the potential of targeted\nfine-tuning on composite visual reasoning tasks and show that fine-tuning\nsmaller VLMs substantially improves core cognitive abilities. While this\nimprovement does not translate to large enhancements on challenging,\nout-of-distribution benchmarks, we show broadly that VLM performance on our\ndatasets strongly correlates with performance on these other benchmarks. Our\nwork provides a detailed analysis of VLM cognitive strengths and weaknesses and\nidentifies key bottlenecks in simultaneous perception and reasoning while also\nproviding an effective and simple solution."}
{"id": "2505.22087", "pdf": "https://arxiv.org/pdf/2505.22087", "abs": "https://arxiv.org/abs/2505.22087", "authors": ["Ruxiao Chen", "Dezheng Han", "Wenjie Han", "Shuaishuai Guo"], "title": "Cognitively-Inspired Emergent Communication via Knowledge Graphs for Assisting the Visually Impaired", "categories": ["cs.AI"], "comment": null, "summary": "Assistive systems for visually impaired individuals must deliver rapid,\ninterpretable, and adaptive feedback to facilitate real-time navigation.\nCurrent approaches face a trade-off between latency and semantic richness:\nnatural language-based systems provide detailed guidance but are too slow for\ndynamic scenarios, while emergent communication frameworks offer low-latency\nsymbolic languages but lack semantic depth, limiting their utility in tactile\nmodalities like vibration. To address these limitations, we introduce a novel\nframework, Cognitively-Inspired Emergent Communication via Knowledge Graphs\n(VAG-EC), which emulates human visual perception and cognitive mapping. Our\nmethod constructs knowledge graphs to represent objects and their\nrelationships, incorporating attention mechanisms to prioritize task-relevant\nentities, thereby mirroring human selective attention. This structured approach\nenables the emergence of compact, interpretable, and context-sensitive symbolic\nlanguages. Extensive experiments across varying vocabulary sizes and message\nlengths demonstrate that VAG-EC outperforms traditional emergent communication\nmethods in Topographic Similarity (TopSim) and Context Independence (CI). These\nfindings underscore the potential of cognitively grounded emergent\ncommunication as a fast, adaptive, and human-aligned solution for real-time\nassistive technologies. Code is available at\nhttps://github.com/Anonymous-NLPcode/Anonymous_submission/tree/main."}
{"id": "2505.21701", "pdf": "https://arxiv.org/pdf/2505.21701", "abs": "https://arxiv.org/abs/2505.21701", "authors": ["Raoyuan Zhao", "Abdullatif Köksal", "Ali Modarressi", "Michael A. Hedderich", "Hinrich Schütze"], "title": "Do We Know What LLMs Don't Know? A Study of Consistency in Knowledge Probing", "categories": ["cs.CL"], "comment": null, "summary": "The reliability of large language models (LLMs) is greatly compromised by\ntheir tendency to hallucinate, underscoring the need for precise identification\nof knowledge gaps within LLMs. Various methods for probing such gaps exist,\nranging from calibration-based to prompting-based methods. To evaluate these\nprobing methods, in this paper, we propose a new process based on using input\nvariations and quantitative metrics. Through this, we expose two dimensions of\ninconsistency in knowledge gap probing. (1) Intra-method inconsistency: Minimal\nnon-semantic perturbations in prompts lead to considerable variance in detected\nknowledge gaps within the same probing method; e.g., the simple variation of\nshuffling answer options can decrease agreement to around 40%. (2) Cross-method\ninconsistency: Probing methods contradict each other on whether a model knows\nthe answer. Methods are highly inconsistent -- with decision consistency across\nmethods being as low as 7% -- even though the model, dataset, and prompt are\nall the same. These findings challenge existing probing methods and highlight\nthe urgent need for perturbation-robust probing frameworks."}
{"id": "2505.21539", "pdf": "https://arxiv.org/pdf/2505.21539", "abs": "https://arxiv.org/abs/2505.21539", "authors": ["Ziming Wang", "Nan Xue", "Rebecka Jörnsten"], "title": "Equivariant Flow Matching for Point Cloud Assembly", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The goal of point cloud assembly is to reconstruct a complete 3D shape by\naligning multiple point cloud pieces. This work presents a novel equivariant\nsolver for assembly tasks based on flow matching models. We first theoretically\nshow that the key to learning equivariant distributions via flow matching is to\nlearn related vector fields. Based on this result, we propose an assembly\nmodel, called equivariant diffusion assembly (Eda), which learns related vector\nfields conditioned on the input pieces. We further construct an equivariant\npath for Eda, which guarantees high data efficiency of the training process.\nOur numerical results show that Eda is highly competitive on practical\ndatasets, and it can even handle the challenging situation where the input\npieces are non-overlapped."}
{"id": "2505.22092", "pdf": "https://arxiv.org/pdf/2505.22092", "abs": "https://arxiv.org/abs/2505.22092", "authors": ["Valentin Cuzin-Rambaud", "Emilien Komlenovic", "Alexandre Faure", "Bruno Yun"], "title": "VIRAL: Vision-grounded Integration for Reward design And Learning", "categories": ["cs.AI"], "comment": null, "summary": "The alignment between humans and machines is a critical challenge in\nartificial intelligence today. Reinforcement learning, which aims to maximize a\nreward function, is particularly vulnerable to the risks associated with poorly\ndesigned reward functions. Recent advancements has shown that Large Language\nModels (LLMs) for reward generation can outperform human performance in this\ncontext. We introduce VIRAL, a pipeline for generating and refining reward\nfunctions through the use of multi-modal LLMs. VIRAL autonomously creates and\ninteractively improves reward functions based on a given environment and a goal\nprompt or annotated image. The refinement process can incorporate human\nfeedback or be guided by a description generated by a video LLM, which explains\nthe agent's policy in video form. We evaluated VIRAL in five Gymnasium\nenvironments, demonstrating that it accelerates the learning of new behaviors\nwhile ensuring improved alignment with user intent. The source-code and demo\nvideo are available at: https://github.com/VIRAL-UCBL1/VIRAL and\nhttps://youtu.be/t4_BXugBm9Q."}
{"id": "2505.21710", "pdf": "https://arxiv.org/pdf/2505.21710", "abs": "https://arxiv.org/abs/2505.21710", "authors": ["Barbarestani Baran", "Maks Isa", "Vossen Piek"], "title": "Assessing and Refining ChatGPT's Performance in Identifying Targeting and Inappropriate Language: A Comparative Study", "categories": ["cs.CL"], "comment": null, "summary": "This study evaluates the effectiveness of ChatGPT, an advanced AI model for\nnatural language processing, in identifying targeting and inappropriate\nlanguage in online comments. With the increasing challenge of moderating vast\nvolumes of user-generated content on social network sites, the role of AI in\ncontent moderation has gained prominence. We compared ChatGPT's performance\nagainst crowd-sourced annotations and expert evaluations to assess its\naccuracy, scope of detection, and consistency. Our findings highlight that\nChatGPT performs well in detecting inappropriate content, showing notable\nimprovements in accuracy through iterative refinements, particularly in Version\n6. However, its performance in targeting language detection showed variability,\nwith higher false positive rates compared to expert judgments. This study\ncontributes to the field by demonstrating the potential of AI models like\nChatGPT to enhance automated content moderation systems while also identifying\nareas for further improvement. The results underscore the importance of\ncontinuous model refinement and contextual understanding to better support\nautomated moderation and mitigate harmful online behavior."}
{"id": "2505.21541", "pdf": "https://arxiv.org/pdf/2505.21541", "abs": "https://arxiv.org/abs/2505.21541", "authors": ["Zitong Wang", "Hang Zhao", "Qianyu Zhou", "Xuequan Lu", "Xiangtai Li", "Yiren Song"], "title": "DiffDecompose: Layer-Wise Decomposition of Alpha-Composited Images via Diffusion Transformers", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Diffusion models have recently motivated great success in many generation\ntasks like object removal. Nevertheless, existing image decomposition methods\nstruggle to disentangle semi-transparent or transparent layer occlusions due to\nmask prior dependencies, static object assumptions, and the lack of datasets.\nIn this paper, we delve into a novel task: Layer-Wise Decomposition of\nAlpha-Composited Images, aiming to recover constituent layers from single\noverlapped images under the condition of semi-transparent/transparent alpha\nlayer non-linear occlusion. To address challenges in layer ambiguity,\ngeneralization, and data scarcity, we first introduce AlphaBlend, the first\nlarge-scale and high-quality dataset for transparent and semi-transparent layer\ndecomposition, supporting six real-world subtasks (e.g., translucent flare\nremoval, semi-transparent cell decomposition, glassware decomposition).\nBuilding on this dataset, we present DiffDecompose, a diffusion\nTransformer-based framework that learns the posterior over possible layer\ndecompositions conditioned on the input image, semantic prompts, and blending\ntype. Rather than regressing alpha mattes directly, DiffDecompose performs\nIn-Context Decomposition, enabling the model to predict one or multiple layers\nwithout per-layer supervision, and introduces Layer Position Encoding Cloning\nto maintain pixel-level correspondence across layers. Extensive experiments on\nthe proposed AlphaBlend dataset and public LOGO dataset verify the\neffectiveness of DiffDecompose. The code and dataset will be available upon\npaper acceptance. Our code will be available at:\nhttps://github.com/Wangzt1121/DiffDecompose."}
{"id": "2505.22104", "pdf": "https://arxiv.org/pdf/2505.22104", "abs": "https://arxiv.org/abs/2505.22104", "authors": ["Davide Corsi", "Kaushik Mallik", "Andoni Rodriguez", "Cesar Sanchez"], "title": "Efficient Dynamic Shielding for Parametric Safety Specifications", "categories": ["cs.AI", "cs.LG", "cs.LO", "cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Shielding has emerged as a promising approach for ensuring safety of\nAI-controlled autonomous systems. The algorithmic goal is to compute a shield,\nwhich is a runtime safety enforcement tool that needs to monitor and intervene\nthe AI controller's actions if safety could be compromised otherwise.\nTraditional shields are designed statically for a specific safety requirement.\nTherefore, if the safety requirement changes at runtime due to changing\noperating conditions, the shield needs to be recomputed from scratch, causing\ndelays that could be fatal. We introduce dynamic shields for parametric safety\nspecifications, which are succinctly represented sets of all possible safety\nspecifications that may be encountered at runtime. Our dynamic shields are\nstatically designed for a given safety parameter set, and are able to\ndynamically adapt as the true safety specification (permissible by the\nparameters) is revealed at runtime. The main algorithmic novelty lies in the\ndynamic adaptation procedure, which is a simple and fast algorithm that\nutilizes known features of standard safety shields, like maximal\npermissiveness. We report experimental results for a robot navigation problem\nin unknown territories, where the safety specification evolves as new obstacles\nare discovered at runtime. In our experiments, the dynamic shields took a few\nminutes for their offline design, and took between a fraction of a second and a\nfew seconds for online adaptation at each step, whereas the brute-force online\nrecomputation approach was up to 5 times slower."}
{"id": "2505.21740", "pdf": "https://arxiv.org/pdf/2505.21740", "abs": "https://arxiv.org/abs/2505.21740", "authors": ["Marvin Limpijankit", "Yanda Chen", "Melanie Subbiah", "Nicholas Deas", "Kathleen McKeown"], "title": "Counterfactual Simulatability of LLM Explanations for Generation Tasks", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "LLMs can be unpredictable, as even slight alterations to the prompt can cause\nthe output to change in unexpected ways. Thus, the ability of models to\naccurately explain their behavior is critical, especially in high-stakes\nsettings. One approach for evaluating explanations is counterfactual\nsimulatability, how well an explanation allows users to infer the model's\noutput on related counterfactuals. Counterfactual simulatability has been\npreviously studied for yes/no question answering tasks. We provide a general\nframework for extending this method to generation tasks, using news\nsummarization and medical suggestion as example use cases. We find that while\nLLM explanations do enable users to better predict LLM outputs on\ncounterfactuals in the summarization setting, there is significant room for\nimprovement for medical suggestion. Furthermore, our results suggest that the\nevaluation for counterfactual simulatability may be more appropriate for\nskill-based tasks as opposed to knowledge-based tasks."}
{"id": "2505.21544", "pdf": "https://arxiv.org/pdf/2505.21544", "abs": "https://arxiv.org/abs/2505.21544", "authors": ["Semanto Mondal"], "title": "Vision Meets Language: A RAG-Augmented YOLOv8 Framework for Coffee Disease Diagnosis and Farmer Assistance", "categories": ["cs.CV", "cs.CL"], "comment": "There are 14 pages, 8 figures", "summary": "As a social being, we have an intimate bond with the environment. A plethora\nof things in human life, such as lifestyle, health, and food are dependent on\nthe environment and agriculture. It comes under our responsibility to support\nthe environment as well as agriculture. However, traditional farming practices\noften result in inefficient resource use and environmental challenges. To\naddress these issues, precision agriculture has emerged as a promising approach\nthat leverages advanced technologies to optimise agricultural processes. In\nthis work, a hybrid approach is proposed that combines the three different\npotential fields of model AI: object detection, large language model (LLM), and\nRetrieval-Augmented Generation (RAG). In this novel framework, we have tried to\ncombine the vision and language models to work together to identify potential\ndiseases in the tree leaf. This study introduces a novel AI-based precision\nagriculture system that uses Retrieval Augmented Generation (RAG) to provide\ncontext-aware diagnoses and natural language processing (NLP) and YOLOv8 for\ncrop disease detection. The system aims to tackle major issues with large\nlanguage models (LLMs), especially hallucinations and allows for adaptive\ntreatment plans and real-time disease detection. The system provides an\neasy-to-use interface to the farmers, which they can use to detect the\ndifferent diseases related to coffee leaves by just submitting the image of the\naffected leaf the model will detect the diseases as well as suggest potential\nremediation methodologies which aim to lower the use of pesticides, preserving\nlivelihoods, and encouraging environmentally friendly methods. With an emphasis\non scalability, dependability, and user-friendliness, the project intends to\nimprove RAG-integrated object detection systems for wider agricultural\napplications in the future."}
{"id": "2505.22112", "pdf": "https://arxiv.org/pdf/2505.22112", "abs": "https://arxiv.org/abs/2505.22112", "authors": ["Guangfu Hao", "Frederic Alexandre", "Shan Yu"], "title": "Visual Large Language Models Exhibit Human-Level Cognitive Flexibility in the Wisconsin Card Sorting Test", "categories": ["cs.AI", "q-bio.NC"], "comment": null, "summary": "Cognitive flexibility has been extensively studied in human cognition but\nremains relatively unexplored in the context of Visual Large Language Models\n(VLLMs). This study assesses the cognitive flexibility of state-of-the-art\nVLLMs (GPT-4o, Gemini-1.5 Pro, and Claude-3.5 Sonnet) using the Wisconsin Card\nSorting Test (WCST), a classic measure of set-shifting ability. Our results\nreveal that VLLMs achieve or surpass human-level set-shifting capabilities\nunder chain-of-thought prompting with text-based inputs. However, their\nabilities are highly influenced by both input modality and prompting strategy.\nIn addition, we find that through role-playing, VLLMs can simulate various\nfunctional deficits aligned with patients having impairments in cognitive\nflexibility, suggesting that VLLMs may possess a cognitive architecture, at\nleast regarding the ability of set-shifting, similar to the brain. This study\nreveals the fact that VLLMs have already approached the human level on a key\ncomponent underlying our higher cognition, and highlights the potential to use\nthem to emulate complex brain processes."}
{"id": "2505.21757", "pdf": "https://arxiv.org/pdf/2505.21757", "abs": "https://arxiv.org/abs/2505.21757", "authors": ["Yubin Kim", "Zhiyuan Hu", "Hyewon Jeong", "Eugene Park", "Shuyue Stella Li", "Chanwoo Park", "Shiyun Xiong", "MingYu Lu", "Hyeonhoon Lee", "Xin Liu", "Daniel McDuff", "Cynthia Breazeal", "Samir Tulebaev", "Hae Won Park"], "title": "BehaviorSFT: Behavioral Token Conditioning for Clinical Agents Across the Proactivity Spectrum", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) as clinical agents require careful behavioral\nadaptation. While adept at reactive tasks (e.g., diagnosis reasoning), LLMs\noften struggle with proactive engagement, like unprompted identification of\ncritical missing information or risks. We introduce BehaviorBench, a\ncomprehensive dataset to evaluate agent behaviors across a clinical assistance\nspectrum, ranging from reactive query responses to proactive interventions\n(e.g., clarifying ambiguities, flagging overlooked critical data). Our\nBehaviorBench experiments reveal LLMs' inconsistent proactivity. To address\nthis, we propose BehaviorSFT, a novel training strategy using behavioral tokens\nto explicitly condition LLMs for dynamic behavioral selection along this\nspectrum. BehaviorSFT boosts performance, achieving up to 97.3% overall Macro\nF1 on BehaviorBench and improving proactive task scores (e.g., from 95.0% to\n96.5% for Qwen2.5-7B-Ins). Crucially, blind clinician evaluations confirmed\nBehaviorSFT-trained agents exhibit more realistic clinical behavior, striking a\nsuperior balance between helpful proactivity (e.g., timely, relevant\nsuggestions) and necessary restraint (e.g., avoiding over-intervention) versus\nstandard fine-tuning or explicit instructed agents."}
{"id": "2505.21545", "pdf": "https://arxiv.org/pdf/2505.21545", "abs": "https://arxiv.org/abs/2505.21545", "authors": ["Chika Maduabuchi", "Hao Chen", "Yujin Han", "Jindong Wang"], "title": "Corruption-Aware Training of Latent Video Diffusion Models for Robust Text-to-Video Generation", "categories": ["cs.CV", "cs.LG"], "comment": "Code: https://github.com/chikap421/catlvdm Models:\n  https://huggingface.co/Chikap421/catlvdm-checkpoints/tree/main", "summary": "Latent Video Diffusion Models (LVDMs) achieve high-quality generation but are\nsensitive to imperfect conditioning, which causes semantic drift and temporal\nincoherence on noisy, web-scale video-text datasets. We introduce CAT-LVDM, the\nfirst corruption-aware training framework for LVDMs that improves robustness\nthrough structured, data-aligned noise injection. Our method includes\nBatch-Centered Noise Injection (BCNI), which perturbs embeddings along\nintra-batch semantic directions to preserve temporal consistency. BCNI is\nespecially effective on caption-rich datasets like WebVid-2M, MSR-VTT, and\nMSVD. We also propose Spectrum-Aware Contextual Noise (SACN), which injects\nnoise along dominant spectral directions to improve low-frequency smoothness,\nshowing strong results on UCF-101. On average, BCNI reduces FVD by 31.9% across\nWebVid-2M, MSR-VTT, and MSVD, while SACN yields a 12.3% improvement on UCF-101.\nAblation studies confirm the benefit of low-rank, data-aligned noise. Our\ntheoretical analysis further explains how such perturbations tighten entropy,\nWasserstein, score-drift, mixing-time, and generalization bounds. CAT-LVDM\nestablishes a principled, scalable training approach for robust video diffusion\nunder multimodal noise. Code and models: https://github.com/chikap421/catlvdm"}
{"id": "2505.22147", "pdf": "https://arxiv.org/pdf/2505.22147", "abs": "https://arxiv.org/abs/2505.22147", "authors": ["Florian Andreas Marwitz", "Tanya Braun", "Ralf Möller", "Marcel Gehrke"], "title": "Lifted Forward Planning in Relational Factored Markov Decision Processes with Concurrent Actions", "categories": ["cs.AI"], "comment": null, "summary": "Decision making is a central problem in AI that can be formalized using a\nMarkov Decision Process. A problem is that, with increasing numbers of\n(indistinguishable) objects, the state space grows exponentially. To compute\npolicies, the state space has to be enumerated. Even more possibilities have to\nbe enumerated if the size of the action space depends on the size of the state\nspace, especially if we allow concurrent actions. To tackle the exponential\nblow-up in the action and state space, we present a first-order representation\nto store the spaces in polynomial instead of exponential size in the number of\nobjects and introduce Foreplan, a relational forward planner, which uses this\nrepresentation to efficiently compute policies for numerous indistinguishable\nobjects and actions. Additionally, we introduce an even faster approximate\nversion of Foreplan. Moreover, Foreplan identifies how many objects an agent\nshould act on to achieve a certain task given restrictions. Further, we provide\na theoretical analysis and an empirical evaluation of Foreplan, demonstrating a\nspeedup of at least four orders of magnitude."}
{"id": "2505.21772", "pdf": "https://arxiv.org/pdf/2505.21772", "abs": "https://arxiv.org/abs/2505.21772", "authors": ["Reza Khanmohammadi", "Erfan Miahi", "Mehrsa Mardikoraem", "Simerjot Kaur", "Ivan Brugere", "Charese H. Smiley", "Kundan Thind", "Mohammad M. Ghassemi"], "title": "Calibrating LLM Confidence by Probing Perturbed Representation Stability", "categories": ["cs.CL"], "comment": null, "summary": "Miscalibration in Large Language Models (LLMs) undermines their reliability,\nhighlighting the need for accurate confidence estimation. We introduce CCPS\n(Calibrating LLM Confidence by Probing Perturbed Representation Stability), a\nnovel method analyzing internal representational stability in LLMs. CCPS\napplies targeted adversarial perturbations to final hidden states, extracts\nfeatures reflecting the model's response to these perturbations, and uses a\nlightweight classifier to predict answer correctness. CCPS was evaluated on\nLLMs from 8B to 32B parameters (covering Llama, Qwen, and Mistral\narchitectures) using MMLU and MMLU-Pro benchmarks in both multiple-choice and\nopen-ended formats. Our results show that CCPS significantly outperforms\ncurrent approaches. Across four LLMs and three MMLU variants, CCPS reduces\nExpected Calibration Error by approximately 55% and Brier score by 21%, while\nincreasing accuracy by 5 percentage points, Area Under the Precision-Recall\nCurve by 4 percentage points, and Area Under the Receiver Operating\nCharacteristic Curve by 6 percentage points, all relative to the strongest\nprior method. CCPS delivers an efficient, broadly applicable, and more accurate\nsolution for estimating LLM confidence, thereby improving their\ntrustworthiness."}
{"id": "2505.21547", "pdf": "https://arxiv.org/pdf/2505.21547", "abs": "https://arxiv.org/abs/2505.21547", "authors": ["Weixing Wang", "Zifeng Ding", "Jindong Gu", "Rui Cao", "Christoph Meinel", "Gerard de Melo", "Haojin Yang"], "title": "Image Tokens Matter: Mitigating Hallucination in Discrete Tokenizer-based Large Vision-Language Models via Latent Editing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) with discrete image tokenizers unify\nmultimodal representations by encoding visual inputs into a finite set of\ntokens. Despite their effectiveness, we find that these models still\nhallucinate non-existent objects. We hypothesize that this may be due to visual\npriors induced during training: When certain image tokens frequently co-occur\nin the same spatial regions and represent shared objects, they become strongly\nassociated with the verbalizations of those objects. As a result, the model may\nhallucinate by evoking visually absent tokens that often co-occur with present\nones. To test this assumption, we construct a co-occurrence graph of image\ntokens using a segmentation dataset and employ a Graph Neural Network (GNN)\nwith contrastive learning followed by a clustering method to group tokens that\nfrequently co-occur in similar visual contexts. We find that hallucinations\npredominantly correspond to clusters whose tokens dominate the input, and more\nspecifically, that the visually absent tokens in those clusters show much\nhigher correlation with hallucinated objects compared to tokens present in the\nimage. Based on this observation, we propose a hallucination mitigation method\nthat suppresses the influence of visually absent tokens by modifying latent\nimage embeddings during generation. Experiments show our method reduces\nhallucinations while preserving expressivity. Code is available at\nhttps://github.com/weixingW/CGC-VTD/tree/main"}
{"id": "2505.22148", "pdf": "https://arxiv.org/pdf/2505.22148", "abs": "https://arxiv.org/abs/2505.22148", "authors": ["Gangwei Jiang", "Yahui Liu", "Zhaoyi Li", "Qi Wang", "Fuzheng Zhang", "Linqi Song", "Ying Wei", "Defu Lian"], "title": "What Makes a Good Reasoning Chain? Uncovering Structural Patterns in Long Chain-of-Thought Reasoning", "categories": ["cs.AI"], "comment": null, "summary": "Recent advances in reasoning with large language models (LLMs) have\npopularized Long Chain-of-Thought (LCoT), a strategy that encourages deliberate\nand step-by-step reasoning before producing a final answer. While LCoTs have\nenabled expert-level performance in complex tasks, how the internal structures\nof their reasoning chains drive, or even predict, the correctness of final\nanswers remains a critical yet underexplored question. In this work, we present\nLCoT2Tree, an automated framework that converts sequential LCoTs into\nhierarchical tree structures and thus enables deeper structural analysis of LLM\nreasoning. Using graph neural networks (GNNs), we reveal that structural\npatterns extracted by LCoT2Tree, including exploration, backtracking, and\nverification, serve as stronger predictors of final performance across a wide\nrange of tasks and models. Leveraging an explainability technique, we further\nidentify critical thought patterns such as over-branching that account for\nfailures. Beyond diagnostic insights, the structural patterns by LCoT2Tree\nsupport practical applications, including improving Best-of-N decoding\neffectiveness. Overall, our results underscore the critical role of internal\nstructures of reasoning chains, positioning LCoT2Tree as a powerful tool for\ndiagnosing, interpreting, and improving reasoning in LLMs."}
{"id": "2505.21781", "pdf": "https://arxiv.org/pdf/2505.21781", "abs": "https://arxiv.org/abs/2505.21781", "authors": ["Chutong Meng", "Antonios Anastasopoulos"], "title": "GMU Systems for the IWSLT 2025 Low-Resource Speech Translation Shared Task", "categories": ["cs.CL"], "comment": "IWSLT 2025", "summary": "This paper describes the GMU systems for the IWSLT 2025 low-resource speech\ntranslation shared task. We trained systems for all language pairs, except for\nLevantine Arabic. We fine-tuned SeamlessM4T-v2 for automatic speech recognition\n(ASR), machine translation (MT), and end-to-end speech translation (E2E ST).\nThe ASR and MT models are also used to form cascaded ST systems. Additionally,\nwe explored various training paradigms for E2E ST fine-tuning, including direct\nE2E fine-tuning, multi-task training, and parameter initialization using\ncomponents from fine-tuned ASR and/or MT models. Our results show that (1)\ndirect E2E fine-tuning yields strong results; (2) initializing with a\nfine-tuned ASR encoder improves ST performance on languages SeamlessM4T-v2 has\nnot been trained on; (3) multi-task training can be slightly helpful."}
{"id": "2505.21549", "pdf": "https://arxiv.org/pdf/2505.21549", "abs": "https://arxiv.org/abs/2505.21549", "authors": ["Daniel Csizmadia", "Andrei Codreanu", "Victor Sim", "Vighnesh Prabeau", "Michael Lu", "Kevin Zhu", "Sean O'Brien", "Vasu Sharma"], "title": "Distill CLIP (DCLIP): Enhancing Image-Text Retrieval via Cross-Modal Transformer Distillation", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "We present Distill CLIP (DCLIP), a fine-tuned variant of the CLIP model that\nenhances multimodal image-text retrieval while preserving the original model's\nstrong zero-shot classification capabilities. CLIP models are typically\nconstrained by fixed image resolutions and limited context, which can hinder\ntheir effectiveness in retrieval tasks that require fine-grained cross-modal\nunderstanding. DCLIP addresses these challenges through a meta teacher-student\ndistillation framework, where a cross-modal transformer teacher is fine-tuned\nto produce enriched embeddings via bidirectional cross-attention between\nYOLO-extracted image regions and corresponding textual spans. These\nsemantically and spatially aligned global representations guide the training of\na lightweight student model using a hybrid loss that combines contrastive\nlearning and cosine similarity objectives. Despite being trained on only\n~67,500 samples curated from MSCOCO, Flickr30k, and Conceptual Captions-just a\nfraction of CLIP's original dataset-DCLIP significantly improves image-text\nretrieval metrics (Recall@K, MAP), while retaining approximately 94% of CLIP's\nzero-shot classification performance. These results demonstrate that DCLIP\neffectively mitigates the trade-off between task specialization and\ngeneralization, offering a resource-efficient, domain-adaptive, and\ndetail-sensitive solution for advanced vision-language tasks. Code available at\nhttps://anonymous.4open.science/r/DCLIP-B772/README.md."}
{"id": "2505.22244", "pdf": "https://arxiv.org/pdf/2505.22244", "abs": "https://arxiv.org/abs/2505.22244", "authors": ["Yaron Halle", "Ariel Felner", "Sven Koenig", "Oren Salzman"], "title": "A Preprocessing Framework for Efficient Approximate Bi-Objective Shortest-Path Computation in the Presence of Correlated Objectives", "categories": ["cs.AI"], "comment": null, "summary": "The bi-objective shortest-path (BOSP) problem seeks to find paths between\nstart and target vertices of a graph while optimizing two conflicting objective\nfunctions. We consider the BOSP problem in the presence of correlated\nobjectives. Such correlations often occur in real-world settings such as road\nnetworks, where optimizing two positively correlated objectives, such as travel\ntime and fuel consumption, is common. BOSP is generally computationally\nchallenging as the size of the search space is exponential in the number of\nobjective functions and the graph size. Bounded sub-optimal BOSP solvers such\nas A*pex alleviate this complexity by approximating the Pareto-optimal solution\nset rather than computing it exactly (given a user-provided approximation\nfactor). As the correlation between objective functions increases, smaller\napproximation factors are sufficient for collapsing the entire Pareto-optimal\nset into a single solution. We leverage this insight to propose an efficient\nalgorithm that reduces the search effort in the presence of correlated\nobjectives. Our approach for computing approximations of the entire\nPareto-optimal set is inspired by graph-clustering algorithms. It uses a\npreprocessing phase to identify correlated clusters within a graph and to\ngenerate a new graph representation. This allows a natural generalization of\nA*pex to run up to five times faster on DIMACS dataset instances, a standard\nbenchmark in the field. To the best of our knowledge, this is the first\nalgorithm proposed that efficiently and effectively exploits correlations in\nthe context of bi-objective search while providing theoretical guarantees on\nsolution quality."}
{"id": "2505.21786", "pdf": "https://arxiv.org/pdf/2505.21786", "abs": "https://arxiv.org/abs/2505.21786", "authors": ["Dasha Metropolitansky", "Jonathan Larson"], "title": "VeriTrail: Closed-Domain Hallucination Detection with Traceability", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Even when instructed to adhere to source material, Language Models often\ngenerate unsubstantiated content - a phenomenon known as \"closed-domain\nhallucination.\" This risk is amplified in processes with multiple generative\nsteps (MGS), compared to processes with a single generative step (SGS).\nHowever, due to the greater complexity of MGS processes, we argue that\ndetecting hallucinations in their final outputs is necessary but not\nsufficient: it is equally important to trace where hallucinated content was\nlikely introduced and how faithful content may have been derived from the\nsource through intermediate outputs. To address this need, we present\nVeriTrail, the first closed-domain hallucination detection method designed to\nprovide traceability for both MGS and SGS processes. We also introduce the\nfirst datasets to include all intermediate outputs as well as human annotations\nof final outputs' faithfulness for their respective MGS processes. We\ndemonstrate that VeriTrail outperforms baseline methods on both datasets."}
{"id": "2505.21556", "pdf": "https://arxiv.org/pdf/2505.21556", "abs": "https://arxiv.org/abs/2505.21556", "authors": ["Hee-Seon Kim", "Minbeom Kim", "Wonjun Lee", "Kihyun Kim", "Changick Kim"], "title": "Benign-to-Toxic Jailbreaking: Inducing Harmful Responses from Harmless Prompts", "categories": ["cs.CV", "cs.AI"], "comment": "LVLM, Jailbreak", "summary": "Optimization-based jailbreaks typically adopt the Toxic-Continuation setting\nin large vision-language models (LVLMs), following the standard next-token\nprediction objective. In this setting, an adversarial image is optimized to\nmake the model predict the next token of a toxic prompt. However, we find that\nthe Toxic-Continuation paradigm is effective at continuing already-toxic\ninputs, but struggles to induce safety misalignment when explicit toxic signals\nare absent. We propose a new paradigm: Benign-to-Toxic (B2T) jailbreak. Unlike\nprior work, we optimize adversarial images to induce toxic outputs from benign\nconditioning. Since benign conditioning contains no safety violations, the\nimage alone must break the model's safety mechanisms. Our method outperforms\nprior approaches, transfers in black-box settings, and complements text-based\njailbreaks. These results reveal an underexplored vulnerability in multimodal\nalignment and introduce a fundamentally new direction for jailbreak approaches."}
{"id": "2505.22288", "pdf": "https://arxiv.org/pdf/2505.22288", "abs": "https://arxiv.org/abs/2505.22288", "authors": ["Jan Speller", "Malte Luttermann", "Marcel Gehrke", "Tanya Braun"], "title": "Compression versus Accuracy: A Hierarchy of Lifted Models", "categories": ["cs.AI"], "comment": null, "summary": "Probabilistic graphical models that encode indistinguishable objects and\nrelations among them use first-order logic constructs to compress a\npropositional factorised model for more efficient (lifted) inference. To obtain\na lifted representation, the state-of-the-art algorithm Advanced Colour Passing\n(ACP) groups factors that represent matching distributions. In an approximate\nversion using $\\varepsilon$ as a hyperparameter, factors are grouped that\ndiffer by a factor of at most $(1\\pm \\varepsilon)$. However, finding a suitable\n$\\varepsilon$ is not obvious and may need a lot of exploration, possibly\nrequiring many ACP runs with different $\\varepsilon$ values. Additionally,\nvarying $\\varepsilon$ can yield wildly different models, leading to decreased\ninterpretability. Therefore, this paper presents a hierarchical approach to\nlifted model construction that is hyperparameter-free. It efficiently computes\na hierarchy of $\\varepsilon$ values that ensures a hierarchy of models, meaning\nthat once factors are grouped together given some $\\varepsilon$, these factors\nwill be grouped together for larger $\\varepsilon$ as well. The hierarchy of\n$\\varepsilon$ values also leads to a hierarchy of error bounds. This allows for\nexplicitly weighing compression versus accuracy when choosing specific\n$\\varepsilon$ values to run ACP with and enables interpretability between the\ndifferent models."}
{"id": "2505.21816", "pdf": "https://arxiv.org/pdf/2505.21816", "abs": "https://arxiv.org/abs/2505.21816", "authors": ["Amr Keleg", "Sharon Goldwater", "Walid Magdy"], "title": "Revisiting Common Assumptions about Arabic Dialects in NLP", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025", "summary": "Arabic has diverse dialects, where one dialect can be substantially different\nfrom the others. In the NLP literature, some assumptions about these dialects\nare widely adopted (e.g., ``Arabic dialects can be grouped into distinguishable\nregional dialects\") and are manifested in different computational tasks such as\nArabic Dialect Identification (ADI). However, these assumptions are not\nquantitatively verified. We identify four of these assumptions and examine them\nby extending and analyzing a multi-label dataset, where the validity of each\nsentence in 11 different country-level dialects is manually assessed by\nspeakers of these dialects. Our analysis indicates that the four assumptions\noversimplify reality, and some of them are not always accurate. This in turn\nmight be hindering further progress in different Arabic NLP tasks."}
{"id": "2505.21557", "pdf": "https://arxiv.org/pdf/2505.21557", "abs": "https://arxiv.org/abs/2505.21557", "authors": ["Polad Geidarov"], "title": "Analytical Calculation of Weights Convolutional Neural Network", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper presents an algorithm for analytically calculating the weights and\nthresholds of convolutional neural networks (CNNs) without using standard\ntraining procedures. The algorithm enables the determination of CNN parameters\nbased on just 10 selected images from the MNIST dataset, each representing a\ndigit from 0 to 9. As part of the method, the number of channels in CNN layers\nis also derived analytically. A software module was implemented in C++ Builder,\nand a series of experiments were conducted using the MNIST dataset. Results\ndemonstrate that the analytically computed CNN can recognize over half of 1000\nhandwritten digit images without any training, achieving inference in fractions\nof a second. These findings suggest that CNNs can be constructed and applied\ndirectly for classification tasks without training, using purely analytical\ncomputation of weights."}
{"id": "2505.22290", "pdf": "https://arxiv.org/pdf/2505.22290", "abs": "https://arxiv.org/abs/2505.22290", "authors": ["Fanzeng Xia", "Yidong Luo", "Tinko Sebastian Bartels", "Yaqi Xu", "Tongxin Li"], "title": "Rethinking the Unsolvable: When In-Context Search Meets Test-Time Scaling", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Recent research has highlighted that Large Language Models (LLMs), even when\ntrained to generate extended long reasoning steps, still face significant\nchallenges on hard reasoning problems. However, much of the existing literature\nrelies on direct prompting with simple in-context learning examples for\nevaluation, which largely overlooks advanced techniques to elicit LLMs'\ndeliberate reasoning before drawing conclusions that LLMs hit a performance\nceiling. In this paper, we systematically explore the combined potential of\nin-context search and test-time scaling on super hard reasoning tasks. We find\nthat by employing advanced in-context search prompting to LLMs augmented with\ninternal scaling, one can achieve transformative performance breakthroughs on\ntasks previously deemed \"unsolvable\" (e.g., reported success rates below 5%).\nWe provide both empirical results and theoretical analysis of how this\ncombination can unleash LLM reasoning capabilities: i) Empirically, on\ncontrolled NP-hard tasks and complex real-world planning benchmarks, our\napproach achieves up to a 30x improvement in success rates compared to\npreviously reported results without any external mechanisms; ii) Theoretically,\nwe show that in-context search prompting, when combined with internal scaling,\nsignificantly extends the complexity class of solvable reasoning problems.\nThese findings challenge prevailing assumptions about the limitations of LLMs\non complex tasks, indicating that current evaluation paradigms systematically\nunderestimate their true potential. Our work calls for a critical reassessment\nof how LLM reasoning is benchmarked and a more robust evaluation strategy that\nfully captures the true capabilities of contemporary LLMs, which can lead to a\nbetter understanding of their operational reasoning boundaries in real-world\ndeployments."}
{"id": "2505.21819", "pdf": "https://arxiv.org/pdf/2505.21819", "abs": "https://arxiv.org/abs/2505.21819", "authors": ["Charlotte Peale", "Vinod Raman", "Omer Reingold"], "title": "Representative Language Generation", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to ICML 2025", "summary": "We introduce \"representative generation,\" extending the theoretical framework\nfor generation proposed by Kleinberg et al. (2024) and formalized by Li et al.\n(2024), to additionally address diversity and bias concerns in generative\nmodels. Our notion requires outputs of a generative model to proportionally\nrepresent groups of interest from the training data. We characterize\nrepresentative uniform and non-uniform generation, introducing the \"group\nclosure dimension\" as a key combinatorial quantity. For representative\ngeneration in the limit, we analyze both information-theoretic and\ncomputational aspects, demonstrating feasibility for countably infinite\nhypothesis classes and collections of groups under certain conditions, but\nproving a negative result for computability using only membership queries. This\ncontrasts with Kleinberg et al.'s (2024) positive results for standard\ngeneration in the limit. Our findings provide a rigorous foundation for\ndeveloping more diverse and representative generative models."}
{"id": "2505.21558", "pdf": "https://arxiv.org/pdf/2505.21558", "abs": "https://arxiv.org/abs/2505.21558", "authors": ["Elhoucine Elfatimia", "Recep Eryigitb", "Lahcen Elfatimi"], "title": "A Novel Convolutional Neural Network-Based Framework for Complex Multiclass Brassica Seed Classification", "categories": ["cs.CV", "cs.AI", "cs.LG", "na"], "comment": "11 Figure", "summary": "Agricultural research has accelerated in recent years, yet farmers often lack\nthe time and resources for on-farm research due to the demands of crop\nproduction and farm operations. Seed classification offers valuable insights\ninto quality control, production efficiency, and impurity detection. Early\nidentification of seed types is critical to reducing the cost and risk\nassociated with field emergence, which can lead to yield losses or disruptions\nin downstream processes like harvesting. Seed sampling supports growers in\nmonitoring and managing seed quality, improving precision in determining seed\npurity levels, guiding management adjustments, and enhancing yield estimations.\nThis study proposes a novel convolutional neural network (CNN)-based framework\nfor the efficient classification of ten common Brassica seed types. The\napproach addresses the inherent challenge of texture similarity in seed images\nusing a custom-designed CNN architecture. The model's performance was evaluated\nagainst several pre-trained state-of-the-art architectures, with adjustments to\nlayer configurations for optimized classification. Experimental results using\nour collected Brassica seed dataset demonstrate that the proposed model\nachieved a high accuracy rate of 93 percent."}
{"id": "2505.22311", "pdf": "https://arxiv.org/pdf/2505.22311", "abs": "https://arxiv.org/abs/2505.22311", "authors": ["Feibo Jiang", "Cunhua Pan", "Li Dong", "Kezhi Wang", "Octavia A. Dobre", "Merouane Debbah"], "title": "From Large AI Models to Agentic AI: A Tutorial on Future Intelligent Communications", "categories": ["cs.AI", "cs.CY", "cs.NI", "eess.SP"], "comment": null, "summary": "With the advent of 6G communications, intelligent communication systems face\nmultiple challenges, including constrained perception and response\ncapabilities, limited scalability, and low adaptability in dynamic\nenvironments. This tutorial provides a systematic introduction to the\nprinciples, design, and applications of Large Artificial Intelligence Models\n(LAMs) and Agentic AI technologies in intelligent communication systems, aiming\nto offer researchers a comprehensive overview of cutting-edge technologies and\npractical guidance. First, we outline the background of 6G communications,\nreview the technological evolution from LAMs to Agentic AI, and clarify the\ntutorial's motivation and main contributions. Subsequently, we present a\ncomprehensive review of the key components required for constructing LAMs. We\nfurther categorize LAMs and analyze their applicability, covering Large\nLanguage Models (LLMs), Large Vision Models (LVMs), Large Multimodal Models\n(LMMs), Large Reasoning Models (LRMs), and lightweight LAMs. Next, we propose a\nLAM-centric design paradigm tailored for communications, encompassing dataset\nconstruction and both internal and external learning approaches. Building upon\nthis, we develop an LAM-based Agentic AI system for intelligent communications,\nclarifying its core components such as planners, knowledge bases, tools, and\nmemory modules, as well as its interaction mechanisms. We also introduce a\nmulti-agent framework with data retrieval, collaborative planning, and\nreflective evaluation for 6G. Subsequently, we provide a detailed overview of\nthe applications of LAMs and Agentic AI in communication scenarios. Finally, we\nsummarize the research challenges and future directions in current studies,\naiming to support the development of efficient, secure, and sustainable\nnext-generation intelligent communication systems."}
{"id": "2505.21859", "pdf": "https://arxiv.org/pdf/2505.21859", "abs": "https://arxiv.org/abs/2505.21859", "authors": ["Vishakh Padmakumar", "Zichao Wang", "David Arbour", "Jennifer Healey"], "title": "Principled Content Selection to Generate Diverse and Personalized Multi-Document Summaries", "categories": ["cs.CL"], "comment": "To appear at ACL 2025 - Main Conference", "summary": "While large language models (LLMs) are increasingly capable of handling\nlonger contexts, recent work has demonstrated that they exhibit the \"lost in\nthe middle\" phenomenon (Liu et al., 2024) of unevenly attending to different\nparts of the provided context. This hinders their ability to cover diverse\nsource material in multi-document summarization, as noted in the DiverseSumm\nbenchmark (Huang et al., 2024). In this work, we contend that principled\ncontent selection is a simple way to increase source coverage on this task. As\nopposed to prompting an LLM to perform the summarization in a single step, we\nexplicitly divide the task into three steps -- (1) reducing document\ncollections to atomic key points, (2) using determinantal point processes (DPP)\nto perform select key points that prioritize diverse content, and (3) rewriting\nto the final summary. By combining prompting steps, for extraction and\nrewriting, with principled techniques, for content selection, we consistently\nimprove source coverage on the DiverseSumm benchmark across various LLMs.\nFinally, we also show that by incorporating relevance to a provided user intent\ninto the DPP kernel, we can generate personalized summaries that cover relevant\nsource information while retaining coverage."}
{"id": "2505.21561", "pdf": "https://arxiv.org/pdf/2505.21561", "abs": "https://arxiv.org/abs/2505.21561", "authors": ["Omid Halimi Milani", "Amanda Nikho", "Marouane Tliba", "Lauren Mills", "Ahmet Enis Cetin", "Mohammed H Elnagar"], "title": "Knowledge Distillation Approach for SOS Fusion Staging: Towards Fully Automated Skeletal Maturity Assessment", "categories": ["cs.CV", "cs.LG"], "comment": "This paper has been accepted to the CVPR Workshop 2025, to be held in\n  Nashville, Tennessee", "summary": "We introduce a novel deep learning framework for the automated staging of\nspheno-occipital synchondrosis (SOS) fusion, a critical diagnostic marker in\nboth orthodontics and forensic anthropology. Our approach leverages a\ndual-model architecture wherein a teacher model, trained on manually cropped\nimages, transfers its precise spatial understanding to a student model that\noperates on full, uncropped images. This knowledge distillation is facilitated\nby a newly formulated loss function that aligns spatial logits as well as\nincorporates gradient-based attention spatial mapping, ensuring that the\nstudent model internalizes the anatomically relevant features without relying\non external cropping or YOLO-based segmentation. By leveraging expert-curated\ndata and feedback at each step, our framework attains robust diagnostic\naccuracy, culminating in a clinically viable end-to-end pipeline. This\nstreamlined approach obviates the need for additional pre-processing tools and\naccelerates deployment, thereby enhancing both the efficiency and consistency\nof skeletal maturation assessment in diverse clinical settings."}
{"id": "2505.22368", "pdf": "https://arxiv.org/pdf/2505.22368", "abs": "https://arxiv.org/abs/2505.22368", "authors": ["Enfang Cui", "Yujun Cheng", "Rui She", "Dan Liu", "Zhiyuan Liang", "Minxin Guo", "Tianzheng Li", "Qian Wei", "Wenjuan Xing", "Zhijie Zhong"], "title": "AgentDNS: A Root Domain Naming System for LLM Agents", "categories": ["cs.AI"], "comment": "7 pages, 6 figures", "summary": "The rapid evolution of Large Language Model (LLM) agents has highlighted\ncritical challenges in cross-vendor service discovery, interoperability, and\ncommunication. Existing protocols like model context protocol and\nagent-to-agent protocol have made significant strides in standardizing\ninteroperability between agents and tools, as well as communication among\nmulti-agents. However, there remains a lack of standardized protocols and\nsolutions for service discovery across different agent and tool vendors. In\nthis paper, we propose AgentDNS, a root domain naming and service discovery\nsystem designed to enable LLM agents to autonomously discover, resolve, and\nsecurely invoke third-party agent and tool services across organizational and\ntechnological boundaries. Inspired by the principles of the traditional DNS,\nAgentDNS introduces a structured mechanism for service registration, semantic\nservice discovery, secure invocation, and unified billing. We detail the\narchitecture, core functionalities, and use cases of AgentDNS, demonstrating\nits potential to streamline multi-agent collaboration in real-world scenarios.\nThe source code will be published on https://github.com/agentdns."}
{"id": "2505.21870", "pdf": "https://arxiv.org/pdf/2505.21870", "abs": "https://arxiv.org/abs/2505.21870", "authors": ["Shuyang Cao", "Karthik Radhakrishnan", "David Rosenberg", "Steven Lu", "Pengxiang Cheng", "Lu Wang", "Shiyue Zhang"], "title": "Evaluating the Retrieval Robustness of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "19 pages", "summary": "Retrieval-augmented generation (RAG) generally enhances large language\nmodels' (LLMs) ability to solve knowledge-intensive tasks. But RAG may also\nlead to performance degradation due to imperfect retrieval and the model's\nlimited ability to leverage retrieved content. In this work, we evaluate the\nrobustness of LLMs in practical RAG setups (henceforth retrieval robustness).\nWe focus on three research questions: (1) whether RAG is always better than\nnon-RAG; (2) whether more retrieved documents always lead to better\nperformance; (3) and whether document orders impact results. To facilitate this\nstudy, we establish a benchmark of 1500 open-domain questions, each with\nretrieved documents from Wikipedia. We introduce three robustness metrics, each\ncorresponds to one research question. Our comprehensive experiments, involving\n11 LLMs and 3 prompting strategies, reveal that all of these LLMs exhibit\nsurprisingly high retrieval robustness; nonetheless, different degrees of\nimperfect robustness hinders them from fully utilizing the benefits of RAG."}
{"id": "2505.21564", "pdf": "https://arxiv.org/pdf/2505.21564", "abs": "https://arxiv.org/abs/2505.21564", "authors": ["Koki Matsuishi", "Tsuyoshi Okita"], "title": "Multi-instance Learning as Downstream Task of Self-Supervised Learning-based Pre-trained Model", "categories": ["cs.CV", "cs.LG"], "comment": "8 pages, 6 figures", "summary": "In deep multi-instance learning, the number of applicable instances depends\non the data set. In histopathology images, deep learning multi-instance\nlearners usually assume there are hundreds to thousands instances in a bag.\nHowever, when the number of instances in a bag increases to 256 in brain\nhematoma CT, learning becomes extremely difficult. In this paper, we address\nthis drawback. To overcome this problem, we propose using a pre-trained model\nwith self-supervised learning for the multi-instance learner as a downstream\ntask. With this method, even when the original target task suffers from the\nspurious correlation problem, we show improvements of 5% to 13% in accuracy and\n40% to 55% in the F1 measure for the hypodensity marker classification of brain\nhematoma CT."}
{"id": "2505.22451", "pdf": "https://arxiv.org/pdf/2505.22451", "abs": "https://arxiv.org/abs/2505.22451", "authors": ["Yuanhang Liu", "Yanxing Huang", "Yanqiao Wang", "Peng Li", "Yang Liu"], "title": "AI Mathematician: Towards Fully Automated Frontier Mathematical Research", "categories": ["cs.AI"], "comment": "95 pages, 1 figure", "summary": "Large Reasoning Models (LRMs) have made significant progress in mathematical\ncapabilities in recent times. However, these successes have been primarily\nconfined to competition-level problems. In this work, we propose AI\nMathematician (AIM) framework, which harnesses the reasoning strength of LRMs\nto support frontier mathematical research. We have identified two critical\nchallenges of mathematical research compared to competition, {\\it the intrinsic\ncomplexity of research problems} and {\\it the requirement of procedural rigor}.\nTo address these challenges, AIM incorporates two core strategies: an\nexploration mechanism to foster longer solution paths, and the pessimistic\nreasonable verification method to ensure reliability.\n  This early version of AIM already exhibits strong capability in tackling\nresearch-level tasks. We conducted extensive experiments across several\nreal-world mathematical topics and obtained promising results. AIM is able to\nautonomously construct substantial portions of proofs and uncover non-trivial\ninsights within each research area. These findings highlight the potential of\nLRMs in mathematical discovery and suggest that LRM-based agent systems could\nsignificantly accelerate mathematical research in the future."}
{"id": "2505.21889", "pdf": "https://arxiv.org/pdf/2505.21889", "abs": "https://arxiv.org/abs/2505.21889", "authors": ["Tianyu Guo", "Hande Dong", "Yichong Leng", "Feng Liu", "Cheater Lin", "Nong Xiao", "Xianwei Zhang"], "title": "EFIM: Efficient Serving of LLMs for Infilling Tasks with Improved KV Cache Reuse", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are often used for infilling tasks, which\ninvolve predicting or generating missing information in a given text. These\ntasks typically require multiple interactions with similar context. To reduce\nthe computation of repeated historical tokens, cross-request key-value (KV)\ncache reuse, a technique that stores and reuses intermediate computations, has\nbecome a crucial method in multi-round interactive services. However, in\ninfilling tasks, the KV cache reuse is often hindered by the structure of the\nprompt format, which typically consists of a prefix and suffix relative to the\ninsertion point. Specifically, the KV cache of the prefix or suffix part is\nfrequently invalidated as the other part (suffix or prefix) is incrementally\ngenerated. To address the issue, we propose EFIM, a transformed prompt format\nof FIM to unleash the performance potential of KV cache reuse. Although the\ntransformed prompt can solve the inefficiency, it exposes subtoken generation\nproblems in current LLMs, where they have difficulty generating partial words\naccurately. Therefore, we introduce a fragment tokenization training method\nwhich splits text into multiple fragments before tokenization during data\nprocessing. Experiments on two representative LLMs show that LLM serving with\nEFIM can lower the latency by 52% and improve the throughput by 98% while\nmaintaining the original infilling capability.EFIM's source code is publicly\navailable at https://github.com/gty111/EFIM."}
{"id": "2505.21566", "pdf": "https://arxiv.org/pdf/2505.21566", "abs": "https://arxiv.org/abs/2505.21566", "authors": ["Gao Huayu", "Huang Tengjiu", "Ye Xiaolong", "Tsuyoshi Okita"], "title": "Diffusion Model-based Activity Completion for AI Motion Capture from Videos", "categories": ["cs.CV", "cs.LG"], "comment": "32 pages, 16 figures", "summary": "AI-based motion capture is an emerging technology that offers a\ncost-effective alternative to traditional motion capture systems. However,\ncurrent AI motion capture methods rely entirely on observed video sequences,\nsimilar to conventional motion capture. This means that all human actions must\nbe predefined, and movements outside the observed sequences are not possible.\nTo address this limitation, we aim to apply AI motion capture to virtual\nhumans, where flexible actions beyond the observed sequences are required. We\nassume that while many action fragments exist in the training data, the\ntransitions between them may be missing. To bridge these gaps, we propose a\ndiffusion-model-based action completion technique that generates complementary\nhuman motion sequences, ensuring smooth and continuous movements. By\nintroducing a gate module and a position-time embedding module, our approach\nachieves competitive results on the Human3.6M dataset. Our experimental results\nshow that (1) MDC-Net outperforms existing methods in ADE, FDE, and MMADE but\nis slightly less accurate in MMFDE, (2) MDC-Net has a smaller model size\n(16.84M) compared to HumanMAC (28.40M), and (3) MDC-Net generates more natural\nand coherent motion sequences. Additionally, we propose a method for extracting\nsensor data, including acceleration and angular velocity, from human motion\nsequences."}
{"id": "2505.22597", "pdf": "https://arxiv.org/pdf/2505.22597", "abs": "https://arxiv.org/abs/2505.22597", "authors": ["Ngoc La", "Ruaridh Mon-Williams", "Julie A. Shah"], "title": "HDDLGym: A Tool for Studying Multi-Agent Hierarchical Problems Defined in HDDL with OpenAI Gym", "categories": ["cs.AI", "cs.LG", "cs.MA"], "comment": "Accepted to Proceedings of ICAPS 2025", "summary": "In recent years, reinforcement learning (RL) methods have been widely tested\nusing tools like OpenAI Gym, though many tasks in these environments could also\nbenefit from hierarchical planning. However, there is a lack of a tool that\nenables seamless integration of hierarchical planning with RL. Hierarchical\nDomain Definition Language (HDDL), used in classical planning, introduces a\nstructured approach well-suited for model-based RL to address this gap. To\nbridge this integration, we introduce HDDLGym, a Python-based tool that\nautomatically generates OpenAI Gym environments from HDDL domains and problems.\nHDDLGym serves as a link between RL and hierarchical planning, supporting\nmulti-agent scenarios and enabling collaborative planning among agents. This\npaper provides an overview of HDDLGym's design and implementation, highlighting\nthe challenges and design choices involved in integrating HDDL with the Gym\ninterface, and applying RL policies to support hierarchical planning. We also\nprovide detailed instructions and demonstrations for using the HDDLGym\nframework, including how to work with existing HDDL domains and problems from\nInternational Planning Competitions, exemplified by the Transport domain.\nAdditionally, we offer guidance on creating new HDDL domains for multi-agent\nscenarios and demonstrate the practical use of HDDLGym in the Overcooked\ndomain. By leveraging the advantages of HDDL and Gym, HDDLGym aims to be a\nvaluable tool for studying RL in hierarchical planning, particularly in\nmulti-agent contexts."}
{"id": "2505.21898", "pdf": "https://arxiv.org/pdf/2505.21898", "abs": "https://arxiv.org/abs/2505.21898", "authors": ["Rennai Qiu", "Chen Qian", "Ran Li", "Yufan Dang", "Weize Chen", "Cheng Yang", "Yingli Zhang", "Ye Tian", "Xuantang Xiong", "Lei Han", "Zhiyuan Liu", "Maosong Sun"], "title": "Co-Saving: Resource Aware Multi-Agent Collaboration for Software Development", "categories": ["cs.CL", "cs.AI", "cs.MA", "cs.SE"], "comment": "Work in Progress", "summary": "Recent advancements in Large Language Models (LLMs) and autonomous agents\nhave demonstrated remarkable capabilities across various domains. However,\nstandalone agents frequently encounter limitations when handling complex tasks\nthat demand extensive interactions and substantial computational resources.\nAlthough Multi-Agent Systems (MAS) alleviate some of these limitations through\ncollaborative mechanisms like task decomposition, iterative communication, and\nrole specialization, they typically remain resource-unaware, incurring\nsignificant inefficiencies due to high token consumption and excessive\nexecution time. To address these limitations, we propose a resource-aware\nmulti-agent system -- Co-Saving (meaning that multiple agents collaboratively\nengage in resource-saving activities), which leverages experiential knowledge\nto enhance operational efficiency and solution quality. Our key innovation is\nthe introduction of \"shortcuts\" -- instructional transitions learned from\nhistorically successful trajectories -- which allows to bypass redundant\nreasoning agents and expedite the collective problem-solving process.\nExperiments for software development tasks demonstrate significant advantages\nover existing methods. Specifically, compared to the state-of-the-art MAS\nChatDev, our method achieves an average reduction of 50.85% in token usage, and\nimproves the overall code quality by 10.06%."}
{"id": "2505.21567", "pdf": "https://arxiv.org/pdf/2505.21567", "abs": "https://arxiv.org/abs/2505.21567", "authors": ["Feng Jiang", "Zihao Zheng", "Xiuping Cui", "Maoliang Li", "JIayu Chen", "Xiang Chen"], "title": "EaqVLA: Encoding-aligned Quantization for Vision-Language-Action Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "With the development of Embodied Artificial intelligence, the end-to-end\ncontrol policy such as Vision-Language-Action (VLA) model has become the\nmainstream. Existing VLA models faces expensive computing/storage cost, which\nneed to be optimized. Quantization is considered as the most effective method\nwhich can not only reduce the memory cost but also achieve computation\nacceleration. However, we find the token alignment of VLA models hinders the\napplication of existing quantization methods. To address this, we proposed an\noptimized framework called EaqVLA, which apply encoding-aligned quantization to\nVLA models. Specifically, we propose an complete analysis method to find the\nmisalignment in various granularity. Based on the analysis results, we propose\na mixed precision quantization with the awareness of encoding alignment.\nExperiments shows that the porposed EaqVLA achieves better quantization\nperformance (with the minimal quantization loss for end-to-end action control\nand xxx times acceleration) than existing quantization methods."}
{"id": "2404.11045", "pdf": "https://arxiv.org/pdf/2404.11045", "abs": "https://arxiv.org/abs/2404.11045", "authors": ["James Y. Huang", "Wenxuan Zhou", "Fei Wang", "Fred Morstatter", "Sheng Zhang", "Hoifung Poon", "Muhao Chen"], "title": "Offset Unlearning for Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Published in TMLR. https://openreview.net/pdf?id=A4RLpHPXCu", "summary": "Despite the strong capabilities of Large Language Models (LLMs) to acquire\nknowledge from their training corpora, the memorization of sensitive\ninformation in the corpora such as copyrighted, biased, and private content has\nled to ethical and legal concerns. In response to these challenges, unlearning\nhas emerged as a potential remedy for LLMs affected by problematic training\ndata. However, previous unlearning techniques are either not applicable to\nblack-box LLMs due to required access to model internal weights, or violate\ndata protection principles by retaining sensitive data for inference-time\ncorrection. We propose {\\delta}-Unlearning, an offset unlearning framework for\nblack-box LLMs. Instead of tuning the black-box LLM itself, {\\delta}-Unlearning\nlearns the logit offset needed for unlearning by contrasting the logits from a\npair of smaller models. Experiments demonstrate that {\\delta}- Unlearning can\neffectively unlearn target data while maintaining similar or even stronger\nperformance on general out-of-forget-scope tasks. {\\delta}-Unlearning also\neffectively incorporates different unlearning algorithms, making our approach a\nversatile solution to adapting various existing unlearning algorithms to\nblack-box LLMs."}
{"id": "2505.21926", "pdf": "https://arxiv.org/pdf/2505.21926", "abs": "https://arxiv.org/abs/2505.21926", "authors": ["Yin Hua", "Zhiqiang Liu", "Mingyang Chen", "Zheng Fang", "Chi Man Wong", "Lingxiao Li", "Chi Man Vong", "Huajun Chen", "Wen Zhang"], "title": "Beyond Completion: A Foundation Model for General Knowledge Graph Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings", "summary": "In natural language processing (NLP) and computer vision (CV), the successful\napplication of foundation models across diverse tasks has demonstrated their\nremarkable potential. However, despite the rich structural and textual\ninformation embedded in knowledge graphs (KGs), existing research of foundation\nmodel for KG has primarily focused on their structural aspects, with most\nefforts restricted to in-KG tasks (e.g., knowledge graph completion, KGC). This\nlimitation has hindered progress in addressing more challenging out-of-KG\ntasks. In this paper, we introduce MERRY, a foundation model for general\nknowledge graph reasoning, and investigate its performance across two task\ncategories: in-KG reasoning tasks (e.g., KGC) and out-of-KG tasks (e.g., KG\nquestion answering, KGQA). We not only utilize the structural information, but\nalso the textual information in KGs. Specifically, we propose a\nmulti-perspective Conditional Message Passing (CMP) encoding architecture to\nbridge the gap between textual and structural modalities, enabling their\nseamless integration. Additionally, we introduce a dynamic residual fusion\nmodule to selectively retain relevant textual information and a flexible edge\nscoring mechanism to adapt to diverse downstream tasks. Comprehensive\nevaluations on 28 datasets demonstrate that MERRY outperforms existing\nbaselines in most scenarios, showcasing strong reasoning capabilities within\nKGs and excellent generalization to out-of-KG tasks such as KGQA."}
{"id": "2505.21572", "pdf": "https://arxiv.org/pdf/2505.21572", "abs": "https://arxiv.org/abs/2505.21572", "authors": ["Sungwon Kim", "Namkyeong Lee", "Yunyoung Doh", "Seungmin Shin", "Guimok Cho", "Seung-Won Jeon", "Sangkook Kim", "Chanyoung Park"], "title": "Thickness-aware E(3)-Equivariant 3D Mesh Neural Networks", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "ICML 2025", "summary": "Mesh-based 3D static analysis methods have recently emerged as efficient\nalternatives to traditional computational numerical solvers, significantly\nreducing computational costs and runtime for various physics-based analyses.\nHowever, these methods primarily focus on surface topology and geometry, often\noverlooking the inherent thickness of real-world 3D objects, which exhibits\nhigh correlations and similar behavior between opposing surfaces. This\nlimitation arises from the disconnected nature of these surfaces and the\nabsence of internal edge connections within the mesh. In this work, we propose\na novel framework, the Thickness-aware E(3)-Equivariant 3D Mesh Neural Network\n(T-EMNN), that effectively integrates the thickness of 3D objects while\nmaintaining the computational efficiency of surface meshes. Additionally, we\nintroduce data-driven coordinates that encode spatial information while\npreserving E(3)-equivariance or invariance properties, ensuring consistent and\nrobust analysis. Evaluations on a real-world industrial dataset demonstrate the\nsuperior performance of T-EMNN in accurately predicting node-level 3D\ndeformations, effectively capturing thickness effects while maintaining\ncomputational efficiency."}
{"id": "2505.21506", "pdf": "https://arxiv.org/pdf/2505.21506", "abs": "https://arxiv.org/abs/2505.21506", "authors": ["Eli Bogdanov", "Izack Cohen", "Avigdor Gal"], "title": "Conformance Checking for Less: Efficient Conformance Checking for Long Event Sequences", "categories": ["cs.DB", "cs.AI", "cs.PL"], "comment": "17 pages, 4 figures", "summary": "Long event sequences (termed traces) and large data logs that originate from\nsensors and prediction models are becoming increasingly common in our data-rich\nworld. In such scenarios, conformance checking-validating a data log against an\nexpected system behavior (the process model) can become computationally\ninfeasible due to the exponential complexity of finding an optimal alignment.\nTo alleviate scalability challenges for this task, we propose ConLES, a\nsliding-window conformance checking approach for long event sequences that\npreserves the interpretability of alignment-based methods. ConLES partitions\ntraces into manageable subtraces and iteratively aligns each against the\nexpected behavior, leading to significant reduction of the search space while\nmaintaining overall accuracy. We use global information that captures\nstructural properties of both the trace and the process model, enabling\ninformed alignment decisions and discarding unpromising alignments, even if\nthey appear locally optimal. Performance evaluations across multiple datasets\nhighlight that ConLES outperforms the leading optimal and heuristic algorithms\nfor long traces, consistently achieving the optimal or near-optimal solution.\nUnlike other conformance methods that struggle with long event sequences,\nConLES significantly reduces the search space, scales efficiently, and uniquely\nsupports both predefined and discovered process models, making it a viable and\nleading option for conformance checking of long event sequences."}
{"id": "2505.21936", "pdf": "https://arxiv.org/pdf/2505.21936", "abs": "https://arxiv.org/abs/2505.21936", "authors": ["Zeyi Liao", "Jaylen Jones", "Linxi Jiang", "Eric Fosler-Lussier", "Yu Su", "Zhiqiang Lin", "Huan Sun"], "title": "RedTeamCUA: Realistic Adversarial Testing of Computer-Use Agents in Hybrid Web-OS Environments", "categories": ["cs.CL"], "comment": null, "summary": "Computer-use agents (CUAs) promise to automate complex tasks across operating\nsystems (OS) and the web, but remain vulnerable to indirect prompt injection.\nCurrent evaluations of this threat either lack support realistic but controlled\nenvironments or ignore hybrid web-OS attack scenarios involving both\ninterfaces. To address this, we propose RedTeamCUA, an adversarial testing\nframework featuring a novel hybrid sandbox that integrates a VM-based OS\nenvironment with Docker-based web platforms. Our sandbox supports key features\ntailored for red teaming, such as flexible adversarial scenario configuration,\nand a setting that decouples adversarial evaluation from navigational\nlimitations of CUAs by initializing tests directly at the point of an\nadversarial injection. Using RedTeamCUA, we develop RTC-Bench, a comprehensive\nbenchmark with 864 examples that investigate realistic, hybrid web-OS attack\nscenarios and fundamental security vulnerabilities. Benchmarking current\nfrontier CUAs identifies significant vulnerabilities: Claude 3.7 Sonnet | CUA\ndemonstrates an ASR of 42.9%, while Operator, the most secure CUA evaluated,\nstill exhibits an ASR of 7.6%. Notably, CUAs often attempt to execute\nadversarial tasks with an Attempt Rate as high as 92.5%, although failing to\ncomplete them due to capability limitations. Nevertheless, we observe\nconcerning ASRs of up to 50% in realistic end-to-end settings, with the\nrecently released frontier Claude 4 Opus | CUA showing an alarming ASR of 48%,\ndemonstrating that indirect prompt injection presents tangible risks for even\nadvanced CUAs despite their capabilities and safeguards. Overall, RedTeamCUA\nprovides an essential framework for advancing realistic, controlled, and\nsystematic analysis of CUA vulnerabilities, highlighting the urgent need for\nrobust defenses to indirect prompt injection prior to real-world deployment."}
{"id": "2505.21574", "pdf": "https://arxiv.org/pdf/2505.21574", "abs": "https://arxiv.org/abs/2505.21574", "authors": ["Dang Nguyen", "Jiping Li", "Jinghao Zheng", "Baharan Mirzasoleiman"], "title": "Do We Need All the Synthetic Data? Towards Targeted Synthetic Image Augmentation via Diffusion Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Synthetically augmenting training datasets with diffusion models has been an\neffective strategy for improving generalization of image classifiers. However,\nexisting techniques struggle to ensure the diversity of generation and increase\nthe size of the data by up to 10-30x to improve the in-distribution\nperformance. In this work, we show that synthetically augmenting part of the\ndata that is not learned early in training outperforms augmenting the entire\ndataset. By analyzing a two-layer CNN, we prove that this strategy improves\ngeneralization by promoting homogeneity in feature learning speed without\namplifying noise. Our extensive experiments show that by augmenting only\n30%-40% of the data, our method boosts the performance by up to 2.8% in a\nvariety of scenarios, including training ResNet, ViT and DenseNet on CIFAR-10,\nCIFAR-100, and TinyImageNet, with a range of optimizers including SGD and SAM.\nNotably, our method applied with SGD outperforms the SOTA optimizer, SAM, on\nCIFAR-100 and TinyImageNet. It can also easily stack with existing weak and\nstrong augmentation strategies to further boost the performance."}
{"id": "2505.21513", "pdf": "https://arxiv.org/pdf/2505.21513", "abs": "https://arxiv.org/abs/2505.21513", "authors": ["Nicolas Echevarrieta-Catalan", "Ana Ribas-Rodriguez", "Francisco Cedron", "Odelia Schwartz", "Vanessa Aguiar-Pulido"], "title": "Enhancing Vision Transformer Explainability Using Artificial Astrocytes", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "LXCV Workshop at IEEE / CVF Computer Vision and Pattern Recognition\n  Conference (CVPR) 2025", "summary": "Machine learning models achieve high precision, but their decision-making\nprocesses often lack explainability. Furthermore, as model complexity\nincreases, explainability typically decreases. Existing efforts to improve\nexplainability primarily involve developing new eXplainable artificial\nintelligence (XAI) techniques or incorporating explainability constraints\nduring training. While these approaches yield specific improvements, their\napplicability remains limited. In this work, we propose the Vision Transformer\nwith artificial Astrocytes (ViTA). This training-free approach is inspired by\nneuroscience and enhances the reasoning of a pretrained deep neural network to\ngenerate more human-aligned explanations. We evaluated our approach employing\ntwo well-known XAI techniques, Grad-CAM and Grad-CAM++, and compared it to a\nstandard Vision Transformer (ViT). Using the ClickMe dataset, we quantified the\nsimilarity between the heatmaps produced by the XAI techniques and a\n(human-aligned) ground truth. Our results consistently demonstrate that\nincorporating artificial astrocytes enhances the alignment of model\nexplanations with human perception, leading to statistically significant\nimprovements across all XAI techniques and metrics utilized."}
{"id": "2505.21937", "pdf": "https://arxiv.org/pdf/2505.21937", "abs": "https://arxiv.org/abs/2505.21937", "authors": ["Pratik Rakesh Singh", "Kritarth Prasad", "Mohammadi Zaki", "Pankaj Wasnik"], "title": "Graph-Assisted Culturally Adaptable Idiomatic Translation for Indic Languages", "categories": ["cs.CL"], "comment": null, "summary": "Translating multi-word expressions (MWEs) and idioms requires a deep\nunderstanding of the cultural nuances of both the source and target languages.\nThis challenge is further amplified by the one-to-many nature of idiomatic\ntranslations, where a single source idiom can have multiple target-language\nequivalents depending on cultural references and contextual variations.\nTraditional static knowledge graphs (KGs) and prompt-based approaches struggle\nto capture these complex relationships, often leading to suboptimal\ntranslations. To address this, we propose IdiomCE, an adaptive graph neural\nnetwork (GNN) based methodology that learns intricate mappings between\nidiomatic expressions, effectively generalizing to both seen and unseen nodes\nduring training. Our proposed method enhances translation quality even in\nresource-constrained settings, facilitating improved idiomatic translation in\nsmaller models. We evaluate our approach on multiple idiomatic translation\ndatasets using reference-less metrics, demonstrating significant improvements\nin translating idioms from English to various Indian languages."}
{"id": "2505.21589", "pdf": "https://arxiv.org/pdf/2505.21589", "abs": "https://arxiv.org/abs/2505.21589", "authors": ["Carina Newen", "Luca Hinkamp", "Maria Ntonti", "Emmanuel Müller"], "title": "Do you see what I see? An Ambiguous Optical Illusion Dataset exposing limitations of Explainable AI", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "19 pages, 18 figures", "summary": "From uncertainty quantification to real-world object detection, we recognize\nthe importance of machine learning algorithms, particularly in safety-critical\ndomains such as autonomous driving or medical diagnostics. In machine learning,\nambiguous data plays an important role in various machine learning domains.\nOptical illusions present a compelling area of study in this context, as they\noffer insight into the limitations of both human and machine perception.\nDespite this relevance, optical illusion datasets remain scarce. In this work,\nwe introduce a novel dataset of optical illusions featuring intermingled animal\npairs designed to evoke perceptual ambiguity. We identify generalizable visual\nconcepts, particularly gaze direction and eye cues, as subtle yet impactful\nfeatures that significantly influence model accuracy. By confronting models\nwith perceptual ambiguity, our findings underscore the importance of concepts\nin visual learning and provide a foundation for studying bias and alignment\nbetween human and machine vision. To make this dataset useful for general\npurposes, we generate optical illusions systematically with different concepts\ndiscussed in our bias mitigation section. The dataset is accessible in Kaggle\nvia\nhttps://kaggle.com/datasets/693bf7c6dd2cb45c8a863f9177350c8f9849a9508e9d50526e2ffcc5559a8333.\nOur source code can be found at\nhttps://github.com/KDD-OpenSource/Ambivision.git."}
{"id": "2505.21520", "pdf": "https://arxiv.org/pdf/2505.21520", "abs": "https://arxiv.org/abs/2505.21520", "authors": ["Spiros Baxavanakis", "Manos Schinas", "Symeon Papadopoulos"], "title": "Do DeepFake Attribution Models Generalize?", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advancements in DeepFake generation, along with the proliferation of\nopen-source tools, have significantly lowered the barrier for creating\nsynthetic media. This trend poses a serious threat to the integrity and\nauthenticity of online information, undermining public trust in institutions\nand media. State-of-the-art research on DeepFake detection has primarily\nfocused on binary detection models. A key limitation of these models is that\nthey treat all manipulation techniques as equivalent, despite the fact that\ndifferent methods introduce distinct artifacts and visual cues. Only a limited\nnumber of studies explore DeepFake attribution models, although such models are\ncrucial in practical settings. By providing the specific manipulation method\nemployed, these models could enhance both the perceived trustworthiness and\nexplainability for end users. In this work, we leverage five state-of-the-art\nbackbone models and conduct extensive experiments across six DeepFake datasets.\nFirst, we compare binary and multi-class models in terms of cross-dataset\ngeneralization. Second, we examine the accuracy of attribution models in\ndetecting seen manipulation methods in unknown datasets, hence uncovering data\ndistribution shifts on the same DeepFake manipulations. Last, we assess the\neffectiveness of contrastive methods in improving cross-dataset generalization\nperformance. Our findings indicate that while binary models demonstrate better\ngeneralization abilities, larger models, contrastive methods, and higher data\nquality can lead to performance improvements in attribution models. The code of\nthis work is available on GitHub."}
{"id": "2505.21940", "pdf": "https://arxiv.org/pdf/2505.21940", "abs": "https://arxiv.org/abs/2505.21940", "authors": ["Bolei He", "Xinran He", "Mengke Chen", "Xianwei Xue", "Ying Zhu", "Zhenhua Ling"], "title": "RISE: Reasoning Enhancement via Iterative Self-Exploration in Multi-hop Question Answering", "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Large Language Models (LLMs) excel in many areas but continue to face\nchallenges with complex reasoning tasks, such as Multi-Hop Question Answering\n(MHQA). MHQA requires integrating evidence from diverse sources while managing\nintricate logical dependencies, often leads to errors in reasoning.\nRetrieval-Augmented Generation (RAG), widely employed in MHQA tasks, faces\nchallenges in effectively filtering noisy data and retrieving all necessary\nevidence, thereby limiting its effectiveness in addressing MHQA challenges. To\naddress these challenges, we propose RISE:Reasoning Enhancement via Iterative\nSelf-Exploration, a novel framework designed to enhance models' reasoning\ncapability through iterative self-exploration. Specifically, RISE involves\nthree key steps in addressing MHQA tasks: question decomposition,\nretrieve-then-read, and self-critique. By leveraging continuous\nself-exploration, RISE identifies accurate reasoning paths, iteratively\nself-improving the model's capability to integrate evidence, maintain logical\nconsistency, and enhance performance in MHQA tasks. Extensive experiments on\nmultiple MHQA benchmarks demonstrate that RISE significantly improves reasoning\naccuracy and task performance."}
{"id": "2505.21593", "pdf": "https://arxiv.org/pdf/2505.21593", "abs": "https://arxiv.org/abs/2505.21593", "authors": ["Yang Yang", "Siming Zheng", "Jinwei Chen", "Boxi Wu", "Xiaofei He", "Deng Cai", "Bo Li", "Peng-Tao Jiang"], "title": "Any-to-Bokeh: One-Step Video Bokeh via Multi-Plane Image Guided Diffusion", "categories": ["cs.CV", "cs.AI"], "comment": "project page: https://vivocameraresearch.github.io/any2bokeh/", "summary": "Recent advances in diffusion based editing models have enabled realistic\ncamera simulation and image-based bokeh, but video bokeh remains largely\nunexplored. Existing video editing models cannot explicitly control focus\nplanes or adjust bokeh intensity, limiting their applicability for controllable\noptical effects. Moreover, naively extending image-based bokeh methods to video\noften results in temporal flickering and unsatisfactory edge blur transitions\ndue to the lack of temporal modeling and generalization capability. To address\nthese challenges, we propose a novel one-step video bokeh framework that\nconverts arbitrary input videos into temporally coherent, depth-aware bokeh\neffects. Our method leverages a multi-plane image (MPI) representation\nconstructed through a progressively widening depth sampling function, providing\nexplicit geometric guidance for depth-dependent blur synthesis. By conditioning\na single-step video diffusion model on MPI layers and utilizing the strong 3D\npriors from pre-trained models such as Stable Video Diffusion, our approach\nachieves realistic and consistent bokeh effects across diverse scenes.\nAdditionally, we introduce a progressive training strategy to enhance temporal\nconsistency, depth robustness, and detail preservation. Extensive experiments\ndemonstrate that our method produces high-quality, controllable bokeh effects\nand achieves state-of-the-art performance on multiple evaluation benchmarks."}
{"id": "2505.21522", "pdf": "https://arxiv.org/pdf/2505.21522", "abs": "https://arxiv.org/abs/2505.21522", "authors": ["Shan Gao", "Zhiqiang Wu", "Yawen Niu", "Xiaotao Li", "Qingqing Xu"], "title": "CIM-NET: A Video Denoising Deep Neural Network Model Optimized for Computing-in-Memory Architectures", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "comment": null, "summary": "While deep neural network (DNN)-based video denoising has demonstrated\nsignificant performance, deploying state-of-the-art models on edge devices\nremains challenging due to stringent real-time and energy efficiency\nrequirements. Computing-in-Memory (CIM) chips offer a promising solution by\nintegrating computation within memory cells, enabling rapid matrix-vector\nmultiplication (MVM). However, existing DNN models are often designed without\nconsidering CIM architectural constraints, thus limiting their acceleration\npotential during inference. To address this, we propose a hardware-algorithm\nco-design framework incorporating two innovations: (1) a CIM-Aware\nArchitecture, CIM-NET, optimized for large receptive field operation and CIM's\ncrossbar-based MVM acceleration; and (2) a pseudo-convolutional operator,\nCIM-CONV, used within CIM-NET to integrate slide-based processing with fully\nconnected transformations for high-quality feature extraction and\nreconstruction. This framework significantly reduces the number of MVM\noperations, improving inference speed on CIM chips while maintaining\ncompetitive performance. Experimental results indicate that, compared to the\nconventional lightweight model FastDVDnet, CIM-NET substantially reduces MVM\noperations with a slight decrease in denoising performance. With a stride value\nof 8, CIM-NET reduces MVM operations to 1/77th of the original, while\nmaintaining competitive PSNR (35.11 dB vs. 35.56 dB"}
{"id": "2505.21941", "pdf": "https://arxiv.org/pdf/2505.21941", "abs": "https://arxiv.org/abs/2505.21941", "authors": ["Ashim Gupta", "Vivek Srikumar"], "title": "Test-Time Scaling with Repeated Sampling Improves Multilingual Text Generation", "categories": ["cs.CL"], "comment": null, "summary": "Inference-time scaling via repeated sampling has shown promise in reasoning\ntasks, but its effectiveness in multilingual generation remains underexplored.\nWe evaluate this approach using perplexity- and reward-based verifiers on two\nmultilingual benchmarks: the Aya Evaluation Suite and m-ArenaHard. Our results\nshow consistent quality improvements, with gains exceeding 35% in some cases.\nWhile perplexity-based scoring is effective for open-ended prompts, only\nreward-based verifiers improve performance on tasks requiring reasoning (e.g.,\nmath, code). Our results demonstrate the broader utility of repeated sampling\nfor multilingual text generation and underscore the importance of selecting\nright verifiers for the task."}
{"id": "2505.21635", "pdf": "https://arxiv.org/pdf/2505.21635", "abs": "https://arxiv.org/abs/2505.21635", "authors": ["Haoqian Liang", "Xiaohui Wang", "Zhichao Li", "Ya Yang", "Naiyan Wang"], "title": "Object Concepts Emerge from Motion", "categories": ["cs.CV"], "comment": null, "summary": "Object concepts play a foundational role in human visual cognition, enabling\nperception, memory, and interaction in the physical world. Inspired by findings\nin developmental neuroscience - where infants are shown to acquire object\nunderstanding through observation of motion - we propose a biologically\ninspired framework for learning object-centric visual representations in an\nunsupervised manner. Our key insight is that motion boundary serves as a strong\nsignal for object-level grouping, which can be used to derive pseudo instance\nsupervision from raw videos. Concretely, we generate motion-based instance\nmasks using off-the-shelf optical flow and clustering algorithms, and use them\nto train visual encoders via contrastive learning. Our framework is fully\nlabel-free and does not rely on camera calibration, making it scalable to\nlarge-scale unstructured video data. We evaluate our approach on three\ndownstream tasks spanning both low-level (monocular depth estimation) and\nhigh-level (3D object detection and occupancy prediction) vision. Our models\noutperform previous supervised and self-supervised baselines and demonstrate\nstrong generalization to unseen scenes. These results suggest that\nmotion-induced object representations offer a compelling alternative to\nexisting vision foundation models, capturing a crucial but overlooked level of\nabstraction: the visual instance. The corresponding code will be released upon\npaper acceptance."}
{"id": "2505.21523", "pdf": "https://arxiv.org/pdf/2505.21523", "abs": "https://arxiv.org/abs/2505.21523", "authors": ["Chengzhi Liu", "Zhongxing Xu", "Qingyue Wei", "Juncheng Wu", "James Zou", "Xin Eric Wang", "Yuyin Zhou", "Sheng Liu"], "title": "More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Test-time compute has empowered multimodal large language models to generate\nextended reasoning chains, yielding strong performance on tasks such as\nmultimodal math reasoning. However, this improved reasoning ability often comes\nwith increased hallucination: as generations become longer, models tend to\ndrift away from image-grounded content and rely more heavily on language\npriors. Attention analysis shows that longer reasoning chains lead to reduced\nfocus on visual inputs, which contributes to hallucination. To systematically\nstudy this phenomenon, we introduce RH-AUC, a metric that quantifies how a\nmodel's perception accuracy changes with reasoning length, allowing us to\nevaluate whether the model preserves visual grounding during reasoning. We also\nrelease RH-Bench, a diagnostic benchmark that spans a variety of multimodal\ntasks, designed to assess the trade-off between reasoning ability and\nhallucination. Our analysis reveals that (i) larger models typically achieve a\nbetter balance between reasoning and perception, and (ii) this balance is\ninfluenced more by the types and domains of training data than by its overall\nvolume. These findings underscore the importance of evaluation frameworks that\njointly consider both reasoning quality and perceptual fidelity."}
{"id": "2505.21958", "pdf": "https://arxiv.org/pdf/2505.21958", "abs": "https://arxiv.org/abs/2505.21958", "authors": ["Qihuang Zhong", "Liang Ding", "Fei Liao", "Juhua Liu", "Bo Du", "Dacheng Tao"], "title": "Resolving Knowledge Conflicts in Domain-specific Data Selection: A Case Study on Medical Instruction-tuning", "categories": ["cs.CL"], "comment": null, "summary": "Domain-specific instruction-tuning has become the defacto standard for\nimproving the performance of large language models (LLMs) in specialized\napplications, e.g., medical question answering. Since the instruction-tuning\ndataset might contain redundant or low-quality data, data selection (DS) is\nusually required to maximize the data efficiency. Despite the successes in the\ngeneral domain, current DS methods often struggle to select the desired data\nfor domain-specific instruction-tuning. One of the main reasons is that they\nneglect the impact of knowledge conflicts, i.e., the discrepancy between LLMs'\npretrained knowledge and context knowledge of instruction data, which could\ndamage LLMs' prior abilities and lead to hallucination. To this end, we propose\na simple-yet-effective Knowledge-aware Data Selection (namely KDS) framework to\nselect the domain-specific instruction-tuning data that meets LLMs' actual\nneeds. The core of KDS is to leverage two knowledge-aware metrics for\nquantitatively measuring knowledge conflicts from two aspects: context-memory\nknowledge alignment and intra-memory knowledge consistency. By filtering the\ndata with large knowledge conflicts and sampling the high-quality and diverse\ndata, KDS can effectively stimulate the LLMs' abilities and achieve better\ndomain-specific performance. Taking the medical domain as the testbed, we\nconduct extensive experiments and empirically prove that KDS surpasses the\nother baselines and brings significant and consistent performance gains among\nall LLMs. More encouragingly, KDS effectively improves the model generalization\nand alleviates the hallucination problem."}
{"id": "2505.21637", "pdf": "https://arxiv.org/pdf/2505.21637", "abs": "https://arxiv.org/abs/2505.21637", "authors": ["Xiaole Tang", "Xiaoyi He", "Xiang Gu", "Jian Sun"], "title": "BaryIR: Learning Multi-Source Unified Representation in Continuous Barycenter Space for Generalizable All-in-One Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "Despite remarkable advances made in all-in-one image restoration (AIR) for\nhandling different types of degradations simultaneously, existing methods\nremain vulnerable to out-of-distribution degradations and images, limiting\ntheir real-world applicability. In this paper, we propose a multi-source\nrepresentation learning framework BaryIR, which decomposes the latent space of\nmulti-source degraded images into a continuous barycenter space for unified\nfeature encoding and source-specific subspaces for specific semantic encoding.\nSpecifically, we seek the multi-source unified representation by introducing a\nmulti-source latent optimal transport barycenter problem, in which a continuous\nbarycenter map is learned to transport the latent representations to the\nbarycenter space. The transport cost is designed such that the representations\nfrom source-specific subspaces are contrasted with each other while maintaining\northogonality to those from the barycenter space. This enables BaryIR to learn\ncompact representations with unified degradation-agnostic information from the\nbarycenter space, as well as degradation-specific semantics from\nsource-specific subspaces, capturing the inherent geometry of multi-source data\nmanifold for generalizable AIR. Extensive experiments demonstrate that BaryIR\nachieves competitive performance compared to state-of-the-art all-in-one\nmethods. Particularly, BaryIR exhibits superior generalization ability to\nreal-world data and unseen degradations. The code will be publicly available at\nhttps://github.com/xl-tang3/BaryIR."}
{"id": "2505.21525", "pdf": "https://arxiv.org/pdf/2505.21525", "abs": "https://arxiv.org/abs/2505.21525", "authors": ["Peiliang Gong", "Yucheng Wang", "Min Wu", "Zhenghua Chen", "Xiaoli Li", "Daoqiang Zhang"], "title": "Temporal Restoration and Spatial Rewiring for Source-Free Multivariate Time Series Domain Adaptation", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained model from\nan annotated source domain to an unlabelled target domain without accessing the\nsource data, thereby preserving data privacy. While existing SFDA methods have\nproven effective in reducing reliance on source data, they struggle to perform\nwell on multivariate time series (MTS) due to their failure to consider the\nintrinsic spatial correlations inherent in MTS data. These spatial correlations\nare crucial for accurately representing MTS data and preserving invariant\ninformation across domains. To address this challenge, we propose Temporal\nRestoration and Spatial Rewiring (TERSE), a novel and concise SFDA method\ntailored for MTS data. Specifically, TERSE comprises a customized\nspatial-temporal feature encoder designed to capture the underlying\nspatial-temporal characteristics, coupled with both temporal restoration and\nspatial rewiring tasks to reinstate latent representations of the temporally\nmasked time series and the spatially masked correlated structures. During the\ntarget adaptation phase, the target encoder is guided to produce spatially and\ntemporally consistent features with the source domain by leveraging the source\npre-trained temporal restoration and spatial rewiring networks. Therefore,\nTERSE can effectively model and transfer spatial-temporal dependencies across\ndomains, facilitating implicit feature alignment. In addition, as the first\napproach to simultaneously consider spatial-temporal consistency in MTS-SFDA,\nTERSE can also be integrated as a versatile plug-and-play module into\nestablished SFDA methods. Extensive experiments on three real-world time series\ndatasets demonstrate the effectiveness and versatility of our approach."}
{"id": "2505.21963", "pdf": "https://arxiv.org/pdf/2505.21963", "abs": "https://arxiv.org/abs/2505.21963", "authors": ["Taro Yano", "Yoichi Ishibashi", "Masafumi Oyamada"], "title": "LaMDAgent: An Autonomous Framework for Post-Training Pipeline Optimization via LLM Agents", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated exceptional performance across\na wide range of tasks. To further tailor LLMs to specific domains or\napplications, post-training techniques such as Supervised Fine-Tuning (SFT),\nPreference Learning, and model merging are commonly employed. While each of\nthese methods has been extensively studied in isolation, the automated\nconstruction of complete post-training pipelines remains an underexplored area.\nExisting approaches typically rely on manual design or focus narrowly on\noptimizing individual components, such as data ordering or merging strategies.\nIn this work, we introduce LaMDAgent (short for Language Model Developing\nAgent), a novel framework that autonomously constructs and optimizes full\npost-training pipelines through the use of LLM-based agents. LaMDAgent\nsystematically explores diverse model generation techniques, datasets, and\nhyperparameter configurations, leveraging task-based feedback to discover\nhigh-performing pipelines with minimal human intervention. Our experiments show\nthat LaMDAgent improves tool-use accuracy by 9.0 points while preserving\ninstruction-following capabilities. Moreover, it uncovers effective\npost-training strategies that are often overlooked by conventional human-driven\nexploration. We further analyze the impact of data and model size scaling to\nreduce computational costs on the exploration, finding that model size scalings\nintroduces new challenges, whereas scaling data size enables cost-effective\npipeline discovery."}
{"id": "2505.21644", "pdf": "https://arxiv.org/pdf/2505.21644", "abs": "https://arxiv.org/abs/2505.21644", "authors": ["Kenneth Ball", "Erin Taylor", "Nirav Patel", "Andrew Bartels", "Gary Koplik", "James Polly", "Jay Hineman"], "title": "Geometric Feature Prompting of Image Segmentation Models", "categories": ["cs.CV"], "comment": null, "summary": "Advances in machine learning, especially the introduction of transformer\narchitectures and vision transformers, have led to the development of highly\ncapable computer vision foundation models. The segment anything model (known\ncolloquially as SAM and more recently SAM 2), is a highly capable foundation\nmodel for segmentation of natural images and has been further applied to\nmedical and scientific image segmentation tasks. SAM relies on prompts --\npoints or regions of interest in an image -- to generate associated\nsegmentations.\n  In this manuscript we propose the use of a geometrically motivated prompt\ngenerator to produce prompt points that are colocated with particular features\nof interest. Focused prompting enables the automatic generation of sensitive\nand specific segmentations in a scientific image analysis task using SAM with\nrelatively few point prompts. The image analysis task examined is the\nsegmentation of plant roots in rhizotron or minirhizotron images, which has\nhistorically been a difficult task to automate. Hand annotation of rhizotron\nimages is laborious and often subjective; SAM, initialized with GeomPrompt\nlocal ridge prompts has the potential to dramatically improve rhizotron image\nprocessing.\n  The authors have concurrently released an open source software suite called\ngeomprompt https://pypi.org/project/geomprompt/ that can produce point prompts\nin a format that enables direct integration with the segment-anything package."}
{"id": "2505.21527", "pdf": "https://arxiv.org/pdf/2505.21527", "abs": "https://arxiv.org/abs/2505.21527", "authors": ["Jianheng Zhuo", "Yifan Yang", "Yiwen Shao", "Yong Xu", "Dong Yu", "Kai Yu", "Xie Chen"], "title": "VietASR: Achieving Industry-level Vietnamese ASR with 50-hour labeled data and Large-Scale Speech Pretraining", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": null, "summary": "Automatic speech recognition (ASR) has made remarkable progress but heavily\nrelies on large-scale labeled data, which is scarce for low-resource languages\nlike Vietnamese. While existing systems such as Whisper, USM, and MMS achieve\npromising performance, their efficacy remains inadequate in terms of training\ncosts, latency, and accessibility. To address these issues, we propose VietASR,\na novel ASR training pipeline that leverages vast amounts of unlabeled data and\na small set of labeled data. Through multi-iteration ASR-biased self-supervised\nlearning on a large-scale unlabeled dataset, VietASR offers a cost-effective\nand practical solution for enhancing ASR performance. Experiments demonstrate\nthat pre-training on 70,000-hour unlabeled data and fine-tuning on merely\n50-hour labeled data yield a lightweight but powerful ASR model. It outperforms\nWhisper Large-v3 and commercial ASR systems on real-world data. Our code and\nmodels will be open-sourced to facilitate research in low-resource ASR."}
{"id": "2505.21967", "pdf": "https://arxiv.org/pdf/2505.21967", "abs": "https://arxiv.org/abs/2505.21967", "authors": ["Juan Ren", "Mark Dras", "Usman Naseem"], "title": "Seeing the Threat: Vulnerabilities in Vision-Language Models to Adversarial Attack", "categories": ["cs.CL"], "comment": "Preprint", "summary": "Large Vision-Language Models (LVLMs) have shown remarkable capabilities\nacross a wide range of multimodal tasks. However, their integration of visual\ninputs introduces expanded attack surfaces, thereby exposing them to novel\nsecurity vulnerabilities. In this work, we conduct a systematic\nrepresentational analysis to uncover why conventional adversarial attacks can\ncircumvent the safety mechanisms embedded in LVLMs. We further propose a novel\ntwo stage evaluation framework for adversarial attacks on LVLMs. The first\nstage differentiates among instruction non compliance, outright refusal, and\nsuccessful adversarial exploitation. The second stage quantifies the degree to\nwhich the model's output fulfills the harmful intent of the adversarial prompt,\nwhile categorizing refusal behavior into direct refusals, soft refusals, and\npartial refusals that remain inadvertently helpful. Finally, we introduce a\nnormative schema that defines idealized model behavior when confronted with\nharmful prompts, offering a principled target for safety alignment in\nmultimodal systems."}
{"id": "2505.21647", "pdf": "https://arxiv.org/pdf/2505.21647", "abs": "https://arxiv.org/abs/2505.21647", "authors": ["Eric Xing", "Abby Stylianou", "Robert Pless", "Nathan Jacobs"], "title": "QuARI: Query Adaptive Retrieval Improvement", "categories": ["cs.CV", "cs.LG"], "comment": "13 pages, 4 figures, 4 tables", "summary": "Massive-scale pretraining has made vision-language models increasingly\npopular for image-to-image and text-to-image retrieval across a broad\ncollection of domains. However, these models do not perform well when used for\nchallenging retrieval tasks, such as instance retrieval in very large-scale\nimage collections. Recent work has shown that linear transformations of VLM\nfeatures trained for instance retrieval can improve performance by emphasizing\nsubspaces that relate to the domain of interest. In this paper, we explore a\nmore extreme version of this specialization by learning to map a given query to\na query-specific feature space transformation. Because this transformation is\nlinear, it can be applied with minimal computational cost to millions of image\nembeddings, making it effective for large-scale retrieval or re-ranking.\nResults show that this method consistently outperforms state-of-the-art\nalternatives, including those that require many orders of magnitude more\ncomputation at query time."}
{"id": "2505.21528", "pdf": "https://arxiv.org/pdf/2505.21528", "abs": "https://arxiv.org/abs/2505.21528", "authors": ["Mokai Pan", "Kaizhen Zhu", "Yuexin Ma", "Yanwei Fu", "Jingyi Yu", "Jingya Wang", "Ye Shi"], "title": "UniDB++: Fast Sampling of Unified Diffusion Bridge", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Diffusion Bridges enable transitions between arbitrary distributions, with\nthe Unified Diffusion Bridge (UniDB) framework achieving high-fidelity image\ngeneration via a Stochastic Optimal Control (SOC) formulation. However, UniDB's\nreliance on iterative Euler sampling methods results in slow, computationally\nexpensive inference, while existing acceleration techniques for diffusion or\ndiffusion bridge models fail to address its unique challenges: missing terminal\nmean constraints and SOC-specific penalty coefficients in its SDEs. We present\nUniDB++, a training-free sampling algorithm that significantly improves upon\nthese limitations. The method's key advancement comes from deriving exact\nclosed-form solutions for UniDB's reverse-time SDEs, effectively reducing the\nerror accumulation inherent in Euler approximations and enabling high-quality\ngeneration with up to 20$\\times$ fewer sampling steps. This method is further\ncomplemented by replacing conventional noise prediction with a more stable data\nprediction model, along with an SDE-Corrector mechanism that maintains\nperceptual quality for low-step regimes (5-10 steps). Additionally, we\ndemonstrate that UniDB++ aligns with existing diffusion bridge acceleration\nmethods by evaluating their update rules, and UniDB++ can recover DBIMs as\nspecial cases under some theoretical conditions. Experiments demonstrate\nUniDB++'s state-of-the-art performance in image restoration tasks,\noutperforming Euler-based methods in fidelity and speed while reducing\ninference time significantly. This work bridges the gap between theoretical\ngenerality and practical efficiency in SOC-driven diffusion bridge models. Our\ncode is available at https://github.com/2769433owo/UniDB-plusplus."}
{"id": "2505.21979", "pdf": "https://arxiv.org/pdf/2505.21979", "abs": "https://arxiv.org/abs/2505.21979", "authors": ["Fakhraddin Alwajih", "Samar Mohamed Magdy", "Abdellah El Mekki", "Omer Nacar", "Youssef Nafea", "Safaa Taher Abdelfadil", "Abdulfattah Mohammed Yahya", "Hamzah Luqman", "Nada Almarwani", "Samah Aloufi", "Baraah Qawasmeh", "Houdaifa Atou", "Serry Sibaee", "Hamzah A. Alsayadi", "Walid Al-Dhabyani", "Maged S. Al-shaibani", "Aya El aatar", "Nour Qandos", "Rahaf Alhamouri", "Samar Ahmad", "Razan Khassib", "Lina Hamad", "Mohammed Anwar AL-Ghrawi", "Fatimah Alshamari", "Cheikh Malainine", "Doaa Qawasmeh", "Aminetou Yacoub", "Tfeil moilid", "Ruwa AbuHweidi", "Ahmed Aboeitta", "Vatimetou Mohamed Lemin", "Reem Abdel-Salam", "Ahlam Bashiti", "Adel Ammar", "Aisha Alansari", "Ahmed Ashraf", "Nora Alturayeif", "Sara Shatnawi", "Alcides Alcoba Inciarte", "AbdelRahim A. Elmadany", "Mohamedou cheikh tourad", "Ismail Berrada", "Mustafa Jarrar", "Shady Shehata", "Muhammad Abdul-Mageed"], "title": "Pearl: A Multimodal Culturally-Aware Arabic Instruction Dataset", "categories": ["cs.CL"], "comment": "https://github.com/UBC-NLP/pearl", "summary": "Mainstream large vision-language models (LVLMs) inherently encode cultural\nbiases, highlighting the need for diverse multimodal datasets. To address this\ngap, we introduce Pearl, a large-scale Arabic multimodal dataset and benchmark\nexplicitly designed for cultural understanding. Constructed through advanced\nagentic workflows and extensive human-in-the-loop annotations by 45 annotators\nfrom across the Arab world, Pearl comprises over K multimodal examples spanning\nten culturally significant domains covering all Arab countries. We further\nprovide two robust evaluation benchmarks Pearl and Pearl-Lite along with a\nspecialized subset Pearl-X explicitly developed to assess nuanced cultural\nvariations. Comprehensive evaluations on state-of-the-art open and proprietary\nLVLMs demonstrate that reasoning-centric instruction alignment substantially\nimproves models' cultural grounding compared to conventional scaling methods.\nPearl establishes a foundational resource for advancing culturally-informed\nmultimodal modeling research. All datasets and benchmarks are publicly\navailable."}
{"id": "2505.21649", "pdf": "https://arxiv.org/pdf/2505.21649", "abs": "https://arxiv.org/abs/2505.21649", "authors": ["Keanu Nichols", "Nazia Tasnim", "Yan Yuting", "Nicholas Ikechukwu", "Elva Zou", "Deepti Ghadiyaram", "Bryan Plummer"], "title": "Right Side Up? Disentangling Orientation Understanding in MLLMs with Fine-grained Multi-axis Perception Tasks", "categories": ["cs.CV"], "comment": null, "summary": "Object orientation understanding represents a fundamental challenge in visual\nperception critical for applications like robotic manipulation and augmented\nreality. Current vision-language benchmarks fail to isolate this capability,\noften conflating it with positional relationships and general scene\nunderstanding. We introduce DORI (Discriminative Orientation Reasoning\nIntelligence), a comprehensive benchmark establishing object orientation\nperception as a primary evaluation target. DORI assesses four dimensions of\norientation comprehension: frontal alignment, rotational transformations,\nrelative directional relationships, and canonical orientation understanding.\nThrough carefully curated tasks from 11 datasets spanning 67 object categories\nacross synthetic and real-world scenarios, DORI provides insights on how\nmulti-modal systems understand object orientations. Our evaluation of 15\nstate-of-the-art vision-language models reveals critical limitations: even the\nbest models achieve only 54.2% accuracy on coarse tasks and 33.0% on granular\norientation judgments, with performance deteriorating for tasks requiring\nreference frame shifts or compound rotations. These findings demonstrate the\nneed for dedicated orientation representation mechanisms, as models show\nsystematic inability to perform precise angular estimations, track orientation\nchanges across viewpoints, and understand compound rotations - suggesting\nlimitations in their internal 3D spatial representations. As the first\ndiagnostic framework specifically designed for orientation awareness in\nmultimodal systems, DORI offers implications for improving robotic control, 3D\nscene reconstruction, and human-AI interaction in physical environments. DORI\ndata: https://huggingface.co/datasets/appledora/DORI-Benchmark"}
{"id": "2505.21530", "pdf": "https://arxiv.org/pdf/2505.21530", "abs": "https://arxiv.org/abs/2505.21530", "authors": ["Xuhang Chen", "Zhuo Li", "Yanyan Shen", "Mufti Mahmud", "Hieu Pham", "Chi-Man Pun", "Shuqiang Wang"], "title": "High-Fidelity Functional Ultrasound Reconstruction via A Visual Auto-Regressive Framework", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Functional ultrasound (fUS) imaging provides exceptional spatiotemporal\nresolution for neurovascular mapping, yet its practical application is\nsignificantly hampered by critical challenges. Foremost among these are data\nscarcity, arising from ethical considerations and signal degradation through\nthe cranium, which collectively limit dataset diversity and compromise the\nfairness of downstream machine learning models."}
{"id": "2505.21997", "pdf": "https://arxiv.org/pdf/2505.21997", "abs": "https://arxiv.org/abs/2505.21997", "authors": ["Jihong Zhang", "Xinya Liang", "Anqi Deng", "Nicole Bonge", "Lin Tan", "Ling Zhang", "Nicole Zarrett"], "title": "Leveraging Interview-Informed LLMs to Model Survey Responses: Comparative Insights from AI-Generated and Human Data", "categories": ["cs.CL"], "comment": null, "summary": "Mixed methods research integrates quantitative and qualitative data but faces\nchallenges in aligning their distinct structures, particularly in examining\nmeasurement characteristics and individual response patterns. Advances in large\nlanguage models (LLMs) offer promising solutions by generating synthetic survey\nresponses informed by qualitative data. This study investigates whether LLMs,\nguided by personal interviews, can reliably predict human survey responses,\nusing the Behavioral Regulations in Exercise Questionnaire (BREQ) and\ninterviews from after-school program staff as a case study. Results indicate\nthat LLMs capture overall response patterns but exhibit lower variability than\nhumans. Incorporating interview data improves response diversity for some\nmodels (e.g., Claude, GPT), while well-crafted prompts and low-temperature\nsettings enhance alignment between LLM and human responses. Demographic\ninformation had less impact than interview content on alignment accuracy. These\nfindings underscore the potential of interview-informed LLMs to bridge\nqualitative and quantitative methodologies while revealing limitations in\nresponse variability, emotional interpretation, and psychometric fidelity.\nFuture research should refine prompt design, explore bias mitigation, and\noptimize model settings to enhance the validity of LLM-generated survey data in\nsocial science research."}
{"id": "2505.21653", "pdf": "https://arxiv.org/pdf/2505.21653", "abs": "https://arxiv.org/abs/2505.21653", "authors": ["Ke Zhang", "Cihan Xiao", "Yiqun Mei", "Jiacong Xu", "Vishal M. Patel"], "title": "Think Before You Diffuse: LLMs-Guided Physics-Aware Video Generation", "categories": ["cs.CV"], "comment": "19 pages, 8 figures", "summary": "Recent video diffusion models have demonstrated their great capability in\ngenerating visually-pleasing results, while synthesizing the correct physical\neffects in generated videos remains challenging. The complexity of real-world\nmotions, interactions, and dynamics introduce great difficulties when learning\nphysics from data. In this work, we propose DiffPhy, a generic framework that\nenables physically-correct and photo-realistic video generation by fine-tuning\na pre-trained video diffusion model. Our method leverages large language models\n(LLMs) to explicitly reason a comprehensive physical context from the text\nprompt and use it to guide the generation. To incorporate physical context into\nthe diffusion model, we leverage a Multimodal large language model (MLLM) as a\nsupervisory signal and introduce a set of novel training objectives that\njointly enforce physical correctness and semantic consistency with the input\ntext. We also establish a high-quality physical video dataset containing\ndiverse phyiscal actions and events to facilitate effective finetuning.\nExtensive experiments on public benchmarks demonstrate that DiffPhy is able to\nproduce state-of-the-art results across diverse physics-related scenarios. Our\nproject page is available at https://bwgzk-keke.github.io/DiffPhy/"}
{"id": "2505.21531", "pdf": "https://arxiv.org/pdf/2505.21531", "abs": "https://arxiv.org/abs/2505.21531", "authors": ["Kunhang Li", "Jason Naradowsky", "Yansong Feng", "Yusuke Miyao"], "title": "How Much Do Large Language Models Know about Human Motion? A Case Study in 3D Avatar Control", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "comment": null, "summary": "We explore Large Language Models (LLMs)' human motion knowledge through 3D\navatar control. Given a motion instruction, we prompt LLMs to first generate a\nhigh-level movement plan with consecutive steps (High-level Planning), then\nspecify body part positions in each step (Low-level Planning), which we\nlinearly interpolate into avatar animations as a clear verification lens for\nhuman evaluators. Through carefully designed 20 representative motion\ninstructions with full coverage of basic movement primitives and balanced body\npart usage, we conduct comprehensive evaluations including human assessment of\nboth generated animations and high-level movement plans, as well as automatic\ncomparison with oracle positions in low-level planning. We find that LLMs are\nstrong at interpreting the high-level body movements but struggle with precise\nbody part positioning. While breaking down motion queries into atomic\ncomponents improves planning performance, LLMs have difficulty with multi-step\nmovements involving high-degree-of-freedom body parts. Furthermore, LLMs\nprovide reasonable approximation for general spatial descriptions, but fail to\nhandle precise spatial specifications in text, and the precise spatial-temporal\nparameters needed for avatar control. Notably, LLMs show promise in\nconceptualizing creative motions and distinguishing culturally-specific motion\npatterns."}
{"id": "2505.21999", "pdf": "https://arxiv.org/pdf/2505.21999", "abs": "https://arxiv.org/abs/2505.21999", "authors": ["Ashim Gupta", "Maitrey Mehta", "Zhichao Xu", "Vivek Srikumar"], "title": "Found in Translation: Measuring Multilingual LLM Consistency as Simple as Translate then Evaluate", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) provide detailed and impressive responses to\nqueries in English. However, are they really consistent at responding to the\nsame query in other languages? The popular way of evaluating for multilingual\nperformance of LLMs requires expensive-to-collect annotated datasets. Further,\nevaluating for tasks like open-ended generation, where multiple correct answers\nmay exist, is nontrivial. Instead, we propose to evaluate the predictability of\nmodel response across different languages. In this work, we propose a framework\nto evaluate LLM's cross-lingual consistency based on a simple Translate then\nEvaluate strategy. We instantiate this evaluation framework along two\ndimensions of consistency: information and empathy. Our results reveal\npronounced inconsistencies in popular LLM responses across thirty languages,\nwith severe performance deficits in certain language families and scripts,\nunderscoring critical weaknesses in their multilingual capabilities. These\nfindings necessitate cross-lingual evaluations that are consistent along\nmultiple dimensions. We invite practitioners to use our framework for future\nmultilingual LLM benchmarking."}
{"id": "2505.21697", "pdf": "https://arxiv.org/pdf/2505.21697", "abs": "https://arxiv.org/abs/2505.21697", "authors": ["Xiaoling Hu", "Peirong Liu", "Dina Zemlyanker", "Jonathan Williams Ramirez", "Oula Puonti", "Juan Eugenio Iglesias"], "title": "Scalable Segmentation for Ultra-High-Resolution Brain MR Images", "categories": ["cs.CV"], "comment": "15 pages, 4 figures", "summary": "Although deep learning has shown great success in 3D brain MRI segmentation,\nachieving accurate and efficient segmentation of ultra-high-resolution brain\nimages remains challenging due to the lack of labeled training data for\nfine-scale anatomical structures and high computational demands. In this work,\nwe propose a novel framework that leverages easily accessible, low-resolution\ncoarse labels as spatial references and guidance, without incurring additional\nannotation cost. Instead of directly predicting discrete segmentation maps, our\napproach regresses per-class signed distance transform maps, enabling smooth,\nboundary-aware supervision. Furthermore, to enhance scalability,\ngeneralizability, and efficiency, we introduce a scalable class-conditional\nsegmentation strategy, where the model learns to segment one class at a time\nconditioned on a class-specific input. This novel design not only reduces\nmemory consumption during both training and testing, but also allows the model\nto generalize to unseen anatomical classes. We validate our method through\ncomprehensive experiments on both synthetic and real-world datasets,\ndemonstrating its superior performance and scalability compared to conventional\nsegmentation approaches."}
{"id": "2505.21532", "pdf": "https://arxiv.org/pdf/2505.21532", "abs": "https://arxiv.org/abs/2505.21532", "authors": ["Ismail Erbas", "Ferhat Demirkiran", "Karthik Swaminathan", "Naigang Wang", "Navid Ibtehaj Nizam", "Stefan T. Radev", "Kaoutar El Maghraoui", "Xavier Intes", "Vikas Pandey"], "title": "EvidenceMoE: A Physics-Guided Mixture-of-Experts with Evidential Critics for Advancing Fluorescence Light Detection and Ranging in Scattering Media", "categories": ["cs.CV", "cs.AI", "cs.LG", "physics.optics"], "comment": "18 pages, 4 figures", "summary": "Fluorescence LiDAR (FLiDAR), a Light Detection and Ranging (LiDAR) technology\nemployed for distance and depth estimation across medical, automotive, and\nother fields, encounters significant computational challenges in scattering\nmedia. The complex nature of the acquired FLiDAR signal, particularly in such\nenvironments, makes isolating photon time-of-flight (related to target depth)\nand intrinsic fluorescence lifetime exceptionally difficult, thus limiting the\neffectiveness of current analytical and computational methodologies. To\novercome this limitation, we present a Physics-Guided Mixture-of-Experts (MoE)\nframework tailored for specialized modeling of diverse temporal components. In\ncontrast to the conventional MoE approaches our expert models are informed by\nunderlying physics, such as the radiative transport equation governing photon\npropagation in scattering media. Central to our approach is EvidenceMoE, which\nintegrates Evidence-Based Dirichlet Critics (EDCs). These critic models assess\nthe reliability of each expert's output by providing per-expert quality scores\nand corrective feedback. A Decider Network then leverages this information to\nfuse expert predictions into a robust final estimate adaptively. We validate\nour method using realistically simulated Fluorescence LiDAR (FLiDAR) data for\nnon-invasive cancer cell depth detection generated from photon transport models\nin tissue. Our framework demonstrates strong performance, achieving a\nnormalized root mean squared error (NRMSE) of 0.030 for depth estimation and\n0.074 for fluorescence lifetime."}
{"id": "2505.22003", "pdf": "https://arxiv.org/pdf/2505.22003", "abs": "https://arxiv.org/abs/2505.22003", "authors": ["Jatin Gupta", "Akhil Sharma", "Saransh Singhania", "Ali Imam Abidi"], "title": "Legal Assist AI: Leveraging Transformer-Based Model for Effective Legal Assistance", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 5 tables, 4 figures. This is a revised version of a preprint\n  previously available at this URL: https://doi.org/10.21203/rs.3.rs-5351879/v1", "summary": "Pursuit of accessible legal assistance in India faces a critical gap, as many\ncitizens struggle to leverage their legal rights due to limited awareness and\naccess to relevant legal information. This paper introduces Legal Assist AI, a\ntransformer-based model designed to bridge this gap by offering effective legal\nassistance through large language models (LLMs). The system retrieves relevant\nlegal information from a curated database and generates accurate responses,\nenabling effective assistance for diverse users, including legal professionals,\nscholars, and the general public. The model was fine-tuned on extensive\ndatasets from the Indian legal domain, including Indian Constitution, Bharatiya\nNyaya Sanhita (BNS), Bharatiya Nagarik Suraksha Sanhita (BNSS) and so forth,\nproviding a robust understanding of the complexities of Indian law. By\nincorporating domain-specific legal datasets, the proposed model demonstrated\nremarkable efficiency and specialization in legal Question-Answering. The model\nwas evaluated against state-of-the-art models such as GPT-3.5 Turbo and Mistral\n7B, achieving a 60.08% score on the AIBE, outperforming its competitors in\nlegal reasoning and accuracy. Unlike other models, Legal Assist AI avoided\ncommon issues such as hallucinations, making it highly reliable for practical\nlegal applications. It showcases the model's applicability in real-world legal\nscenarios, with future iterations aiming to enhance performance and expand its\ndataset to cover a broader range of multilingual and case-specific queries as\nwell."}
{"id": "2505.21698", "pdf": "https://arxiv.org/pdf/2505.21698", "abs": "https://arxiv.org/abs/2505.21698", "authors": ["Yitong Li", "Morteza Ghahremani", "Christian Wachinger"], "title": "MedBridge: Bridging Foundation Vision-Language Models to Medical Image Diagnosis", "categories": ["cs.CV"], "comment": null, "summary": "Recent vision-language foundation models deliver state-of-the-art results on\nnatural image classification but falter on medical images due to pronounced\ndomain shifts. At the same time, training a medical foundation model requires\nsubstantial resources, including extensive annotated data and high\ncomputational capacity. To bridge this gap with minimal overhead, we introduce\nMedBridge, a lightweight multimodal adaptation framework that re-purposes\npretrained VLMs for accurate medical image diagnosis. MedBridge comprises three\nkey components. First, a Focal Sampling module that extracts high-resolution\nlocal regions to capture subtle pathological features and compensate for the\nlimited input resolution of general-purpose VLMs. Second, a Query Encoder\n(QEncoder) injects a small set of learnable queries that attend to the frozen\nfeature maps of VLM, aligning them with medical semantics without retraining\nthe entire backbone. Third, a Mixture of Experts mechanism, driven by learnable\nqueries, harnesses the complementary strength of diverse VLMs to maximize\ndiagnostic performance. We evaluate MedBridge on five medical imaging\nbenchmarks across three key adaptation tasks, demonstrating its superior\nperformance in both cross-domain and in-domain adaptation settings, even under\nvarying levels of training data availability. Notably, MedBridge achieved over\n6-15% improvement in AUC compared to state-of-the-art VLM adaptation methods in\nmulti-label thoracic disease diagnosis, underscoring its effectiveness in\nleveraging foundation models for accurate and data-efficient medical diagnosis.\nOur code is available at https://github.com/ai-med/MedBridge."}
{"id": "2505.21534", "pdf": "https://arxiv.org/pdf/2505.21534", "abs": "https://arxiv.org/abs/2505.21534", "authors": ["Yao Fehlis"], "title": "Uncovering Bottlenecks and Optimizing Scientific Lab Workflows with Cycle Time Reduction Agents", "categories": ["cs.MA", "cs.AI"], "comment": null, "summary": "Scientific laboratories, particularly those in pharmaceutical and\nbiotechnology companies, encounter significant challenges in optimizing\nworkflows due to the complexity and volume of tasks such as compound screening\nand assay execution. We introduce Cycle Time Reduction Agents (CTRA), a\nLangGraph-based agentic workflow designed to automate the analysis of lab\noperational metrics. CTRA comprises three main components: the Question\nCreation Agent for initiating analysis, Operational Metrics Agents for data\nextraction and validation, and Insights Agents for reporting and visualization,\nidentifying bottlenecks in lab processes. This paper details CTRA's\narchitecture, evaluates its performance on a lab dataset, and discusses its\npotential to accelerate pharmaceutical and biotechnological development. CTRA\noffers a scalable framework for reducing cycle times in scientific labs."}
{"id": "2505.22017", "pdf": "https://arxiv.org/pdf/2505.22017", "abs": "https://arxiv.org/abs/2505.22017", "authors": ["Siqi Fan", "Peng Han", "Shuo Shang", "Yequan Wang", "Aixin Sun"], "title": "CoThink: Token-Efficient Reasoning via Instruct Models Guiding Reasoning Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) benefit from increased test-time compute, a\nphenomenon known as test-time scaling. However, reasoning-optimized models\noften overthink even simple problems, producing excessively verbose outputs and\nleading to low token efficiency. By comparing these models with equally sized\ninstruct models, we identify two key causes of this verbosity: (1)\nreinforcement learning reduces the information density of forward reasoning,\nand (2) backward chain-of thought training encourages redundant and often\nunnecessary verification steps. Since LLMs cannot assess the difficulty of a\ngiven problem, they tend to apply the same cautious reasoning strategy across\nall tasks, resulting in inefficient overthinking. To address this, we propose\nCoThink, an embarrassingly simple pipeline: an instruct model first drafts a\nhigh-level solution outline; a reasoning model then works out the solution. We\nobserve that CoThink enables dynamic adjustment of reasoning depth based on\ninput difficulty. Evaluated with three reasoning models DAPO, DeepSeek-R1, and\nQwQ on three datasets GSM8K, MATH500, and AIME24, CoThink reduces total token\ngeneration by 22.3% while maintaining pass@1 accuracy within a 0.42% margin on\naverage. With reference to the instruct model, we formally define reasoning\nefficiency and observe a potential reasoning efficiency scaling law in LLMs."}
{"id": "2505.21724", "pdf": "https://arxiv.org/pdf/2505.21724", "abs": "https://arxiv.org/abs/2505.21724", "authors": ["Cheng Luo", "Jianghui Wang", "Bing Li", "Siyang Song", "Bernard Ghanem"], "title": "OmniResponse: Online Multimodal Conversational Response Generation in Dyadic Interactions", "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": "23 pages, 9 figures", "summary": "In this paper, we introduce Online Multimodal Conversational Response\nGeneration (OMCRG), a novel task that aims to online generate synchronized\nverbal and non-verbal listener feedback, conditioned on the speaker's\nmultimodal input. OMCRG reflects natural dyadic interactions and poses new\nchallenges in achieving synchronization between the generated audio and facial\nresponses of the listener. To address these challenges, we innovatively\nintroduce text as an intermediate modality to bridge the audio and facial\nresponses. We hence propose OmniResponse, a Multimodal Large Language Model\n(MLLM) that autoregressively generates high-quality multi-modal listener\nresponses. OmniResponse leverages a pretrained LLM enhanced with two novel\ncomponents: Chrono-Text, which temporally anchors generated text tokens, and\nTempoVoice, a controllable online TTS module that produces speech synchronized\nwith facial reactions. To support further OMCRG research, we present\nResponseNet, a new dataset comprising 696 high-quality dyadic interactions\nfeaturing synchronized split-screen videos, multichannel audio, transcripts,\nand facial behavior annotations. Comprehensive evaluations conducted on\nResponseNet demonstrate that OmniResponse significantly outperforms baseline\nmodels in terms of semantic speech content, audio-visual synchronization, and\ngeneration quality."}
{"id": "2505.21535", "pdf": "https://arxiv.org/pdf/2505.21535", "abs": "https://arxiv.org/abs/2505.21535", "authors": ["Yuxin Ren", "Maxwell D Collins", "Miao Hu", "Huanrui Yang"], "title": "Is Attention Required for Transformer Inference? Explore Function-preserving Attention Replacement", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "12 pages main paper + 6 pages appendix, 14 figures; submitted to\n  NeurIPS 2025", "summary": "While transformers excel across vision and language pretraining tasks, their\nreliance on attention mechanisms poses challenges for inference efficiency,\nespecially on edge and embedded accelerators with limited parallelism and\nmemory bandwidth. Hinted by the observed redundancy of attention at inference\ntime, we hypothesize that though the model learns complicated token dependency\nthrough pretraining, the inference-time sequence-to-sequence mapping in each\nattention layer is actually ''simple'' enough to be represented with a much\ncheaper function. In this work, we explore FAR, a Function-preserving Attention\nReplacement framework that replaces all attention blocks in pretrained\ntransformers with learnable sequence-to-sequence modules, exemplified by an\nLSTM. FAR optimize a multi-head LSTM architecture with a block-wise\ndistillation objective and a global structural pruning framework to achieve a\nfamily of efficient LSTM-based models from pretrained transformers. We validate\nFAR on the DeiT vision transformer family and demonstrate that it matches the\naccuracy of the original models on ImageNet and multiple downstream tasks with\nreduced parameters and latency. Further analysis shows that FAR preserves the\nsemantic token relationships and the token-to-token correlation learned in the\ntransformer's attention module."}
{"id": "2505.22018", "pdf": "https://arxiv.org/pdf/2505.22018", "abs": "https://arxiv.org/abs/2505.22018", "authors": ["Ruicheng Yin", "Xuan Gao", "Changze Lv", "Xiaohua Wang", "Xiaoqing Zheng", "Xuanjing Huang"], "title": "Improving Continual Pre-training Through Seamless Data Packing", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Findings", "summary": "Continual pre-training has demonstrated significant potential in enhancing\nmodel performance, particularly in domain-specific scenarios. The most common\napproach for packing data before continual pre-training involves concatenating\ninput texts and splitting them into fixed-length sequences. While\nstraightforward and efficient, this method often leads to excessive truncation\nand context discontinuity, which can hinder model performance. To address these\nissues, we explore the potential of data engineering to enhance continual\npre-training, particularly its impact on model performance and efficiency. We\npropose Seamless Packing (SP), a novel data packing strategy aimed at\npreserving contextual information more effectively and enhancing model\nperformance. Our approach employs a sliding window technique in the first stage\nthat synchronizes overlapping tokens across consecutive sequences, ensuring\nbetter continuity and contextual coherence. In the second stage, we adopt a\nFirst-Fit-Decreasing algorithm to pack shorter texts into bins slightly larger\nthan the target sequence length, thereby minimizing padding and truncation.\nEmpirical evaluations across various model architectures and corpus domains\ndemonstrate the effectiveness of our method, outperforming baseline method in\n99% of all settings. Code is available at\nhttps://github.com/Infernus-WIND/Seamless-Packing."}
{"id": "2505.21736", "pdf": "https://arxiv.org/pdf/2505.21736", "abs": "https://arxiv.org/abs/2505.21736", "authors": ["Zachary Schlamowitz", "Andrew Bennecke", "Daniel J. Tward"], "title": "Moment kernels: a simple and scalable approach for equivariance to rotations and reflections in deep convolutional networks", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The principle of translation equivariance (if an input image is translated an\noutput image should be translated by the same amount), led to the development\nof convolutional neural networks that revolutionized machine vision. Other\nsymmetries, like rotations and reflections, play a similarly critical role,\nespecially in biomedical image analysis, but exploiting these symmetries has\nnot seen wide adoption. We hypothesize that this is partially due to the\nmathematical complexity of methods used to exploit these symmetries, which\noften rely on representation theory, a bespoke concept in differential geometry\nand group theory. In this work, we show that the same equivariance can be\nachieved using a simple form of convolution kernels that we call ``moment\nkernels,'' and prove that all equivariant kernels must take this form. These\nare a set of radially symmetric functions of a spatial position $x$, multiplied\nby powers of the components of $x$ or the identity matrix. We implement\nequivariant neural networks using standard convolution modules, and provide\narchitectures to execute several biomedical image analysis tasks that depend on\nequivariance principles: classification (outputs are invariant under orthogonal\ntransforms), 3D image registration (outputs transform like a vector), and cell\nsegmentation (quadratic forms defining ellipses transform like a matrix)."}
{"id": "2505.21537", "pdf": "https://arxiv.org/pdf/2505.21537", "abs": "https://arxiv.org/abs/2505.21537", "authors": ["Hao Sun", "Yunyi Shen", "Mihaela van der Schaar"], "title": "OpenReview Should be Protected and Leveraged as a Community Asset for Research in the Era of Large Language Models", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "In the era of large language models (LLMs), high-quality, domain-rich, and\ncontinuously evolving datasets capturing expert-level knowledge, core human\nvalues, and reasoning are increasingly valuable. This position paper argues\nthat OpenReview -- the continually evolving repository of research papers, peer\nreviews, author rebuttals, meta-reviews, and decision outcomes -- should be\nleveraged more broadly as a core community asset for advancing research in the\nera of LLMs. We highlight three promising areas in which OpenReview can\nuniquely contribute: enhancing the quality, scalability, and accountability of\npeer review processes; enabling meaningful, open-ended benchmarks rooted in\ngenuine expert deliberation; and supporting alignment research through\nreal-world interactions reflecting expert assessment, intentions, and\nscientific values. To better realize these opportunities, we suggest the\ncommunity collaboratively explore standardized benchmarks and usage guidelines\naround OpenReview, inviting broader dialogue on responsible data use, ethical\nconsiderations, and collective stewardship."}
{"id": "2505.22019", "pdf": "https://arxiv.org/pdf/2505.22019", "abs": "https://arxiv.org/abs/2505.22019", "authors": ["Qiuchen Wang", "Ruixue Ding", "Yu Zeng", "Zehui Chen", "Lin Chen", "Shihang Wang", "Pengjun Xie", "Fei Huang", "Feng Zhao"], "title": "VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Effectively retrieving, reasoning and understanding visually rich information\nremains a challenge for RAG methods. Traditional text-based methods cannot\nhandle visual-related information. On the other hand, current vision-based RAG\napproaches are often limited by fixed pipelines and frequently struggle to\nreason effectively due to the insufficient activation of the fundamental\ncapabilities of models. As RL has been proven to be beneficial for model\nreasoning, we introduce VRAG-RL, a novel RL framework tailored for complex\nreasoning across visually rich information. With this framework, VLMs interact\nwith search engines, autonomously sampling single-turn or multi-turn reasoning\ntrajectories with the help of visual perception tokens and undergoing continual\noptimization based on these samples. Our approach highlights key limitations of\nRL in RAG domains: (i) Prior Multi-modal RAG approaches tend to merely\nincorporate images into the context, leading to insufficient reasoning token\nallocation and neglecting visual-specific perception; and (ii) When models\ninteract with search engines, their queries often fail to retrieve relevant\ninformation due to the inability to articulate requirements, thereby leading to\nsuboptimal performance. To address these challenges, we define an action space\ntailored for visually rich inputs, with actions including cropping and scaling,\nallowing the model to gather information from a coarse-to-fine perspective.\nFurthermore, to bridge the gap between users' original inquiries and the\nretriever, we employ a simple yet effective reward that integrates query\nrewriting and retrieval performance with a model-based reward. Our VRAG-RL\noptimizes VLMs for RAG tasks using specially designed RL strategies, aligning\nthe model with real-world applications. The code is available at\n\\hyperlink{https://github.com/Alibaba-NLP/VRAG}{https://github.com/Alibaba-NLP/VRAG}."}
{"id": "2505.21742", "pdf": "https://arxiv.org/pdf/2505.21742", "abs": "https://arxiv.org/abs/2505.21742", "authors": ["Briglia Maria Rosaria", "Mujtaba Hussain Mirza", "Giuseppe Lisanti", "Iacopo Masi"], "title": "What is Adversarial Training for Diffusion Models?", "categories": ["cs.CV", "cs.LG"], "comment": "40 pages", "summary": "We answer the question in the title, showing that adversarial training (AT)\nfor diffusion models (DMs) fundamentally differs from classifiers: while AT in\nclassifiers enforces output invariance, AT in DMs requires equivariance to keep\nthe diffusion process aligned with the data distribution. AT is a way to\nenforce smoothness in the diffusion flow, improving robustness to outliers and\ncorrupted data. Unlike prior art, our method makes no assumptions about the\nnoise model and integrates seamlessly into diffusion training by adding random\nnoise, similar to randomized smoothing, or adversarial noise, akin to AT. This\nenables intrinsic capabilities such as handling noisy data, dealing with\nextreme variability such as outliers, preventing memorization, and improving\nrobustness. We rigorously evaluate our approach with proof-of-concept datasets\nwith known distributions in low- and high-dimensional space, thereby taking a\nperfect measure of errors; we further evaluate on standard benchmarks such as\nCIFAR-10, CelebA and LSUN Bedroom, showing strong performance under severe\nnoise, data corruption, and iterative adversarial attacks."}
{"id": "2505.21538", "pdf": "https://arxiv.org/pdf/2505.21538", "abs": "https://arxiv.org/abs/2505.21538", "authors": ["Zihan Weng", "Lucas Gomez", "Taylor Whittington Webb", "Pouya Bashivan"], "title": "Caption This, Reason That: VLMs Caught in the Middle", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision-Language Models (VLMs) have shown remarkable progress in visual\nunderstanding in recent years. Yet, they still lag behind human capabilities in\nspecific visual tasks such as counting or relational reasoning. To understand\nthe underlying limitations, we adopt methodologies from cognitive science,\nanalyzing VLM performance along core cognitive axes: Perception, Attention, and\nMemory. Using a suite of tasks targeting these abilities, we evaluate\nstate-of-the-art VLMs, including GPT-4o. Our analysis reveals distinct\ncognitive profiles: while advanced models approach ceiling performance on some\ntasks (e.g. category identification), a significant gap persists, particularly\nin tasks requiring spatial understanding or selective attention. Investigating\nthe source of these failures and potential methods for improvement, we employ a\nvision-text decoupling analysis, finding that models struggling with direct\nvisual reasoning show marked improvement when reasoning over their own\ngenerated text captions. These experiments reveal a strong need for improved\nVLM Chain-of-Thought (CoT) abilities, even in models that consistently exceed\nhuman performance. Furthermore, we demonstrate the potential of targeted\nfine-tuning on composite visual reasoning tasks and show that fine-tuning\nsmaller VLMs substantially improves core cognitive abilities. While this\nimprovement does not translate to large enhancements on challenging,\nout-of-distribution benchmarks, we show broadly that VLM performance on our\ndatasets strongly correlates with performance on these other benchmarks. Our\nwork provides a detailed analysis of VLM cognitive strengths and weaknesses and\nidentifies key bottlenecks in simultaneous perception and reasoning while also\nproviding an effective and simple solution."}
{"id": "2505.22037", "pdf": "https://arxiv.org/pdf/2505.22037", "abs": "https://arxiv.org/abs/2505.22037", "authors": ["Jingyu Zhang", "Ahmed Elgohary", "Xiawei Wang", "A S M Iftekhar", "Ahmed Magooda", "Benjamin Van Durme", "Daniel Khashabi", "Kyle Jackson"], "title": "Jailbreak Distillation: Renewable Safety Benchmarking", "categories": ["cs.CL", "cs.CR", "cs.SE"], "comment": "Project page: https://aka.ms/jailbreak-distillation", "summary": "Large language models (LLMs) are rapidly deployed in critical applications,\nraising urgent needs for robust safety benchmarking. We propose Jailbreak\nDistillation (JBDistill), a novel benchmark construction framework that\n\"distills\" jailbreak attacks into high-quality and easily-updatable safety\nbenchmarks. JBDistill utilizes a small set of development models and existing\njailbreak attack algorithms to create a candidate prompt pool, then employs\nprompt selection algorithms to identify an effective subset of prompts as\nsafety benchmarks. JBDistill addresses challenges in existing safety\nevaluation: the use of consistent evaluation prompts across models ensures fair\ncomparisons and reproducibility. It requires minimal human effort to rerun the\nJBDistill pipeline and produce updated benchmarks, alleviating concerns on\nsaturation and contamination. Extensive experiments demonstrate our benchmarks\ngeneralize robustly to 13 diverse evaluation models held out from benchmark\nconstruction, including proprietary, specialized, and newer-generation LLMs,\nsignificantly outperforming existing safety benchmarks in effectiveness while\nmaintaining high separability and diversity. Our framework thus provides an\neffective, sustainable, and adaptable solution for streamlining safety\nevaluation."}
{"id": "2505.21746", "pdf": "https://arxiv.org/pdf/2505.21746", "abs": "https://arxiv.org/abs/2505.21746", "authors": ["Arif Masrur", "Peder A. Olsen", "Paul R. Adler", "Carlan Jackson", "Matthew W. Myers", "Nathan Sedghi", "Ray R. Weil"], "title": "Learning to See More: UAS-Guided Super-Resolution of Satellite Imagery for Precision Agriculture", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Unmanned Aircraft Systems (UAS) and satellites are key data sources for\nprecision agriculture, yet each presents trade-offs. Satellite data offer broad\nspatial, temporal, and spectral coverage but lack the resolution needed for\nmany precision farming applications, while UAS provide high spatial detail but\nare limited by coverage and cost, especially for hyperspectral data. This study\npresents a novel framework that fuses satellite and UAS imagery using\nsuper-resolution methods. By integrating data across spatial, spectral, and\ntemporal domains, we leverage the strengths of both platforms cost-effectively.\nWe use estimation of cover crop biomass and nitrogen (N) as a case study to\nevaluate our approach. By spectrally extending UAS RGB data to the vegetation\nred edge and near-infrared regions, we generate high-resolution Sentinel-2\nimagery and improve biomass and N estimation accuracy by 18% and 31%,\nrespectively. Our results show that UAS data need only be collected from a\nsubset of fields and time points. Farmers can then 1) enhance the spectral\ndetail of UAS RGB imagery; 2) increase the spatial resolution by using\nsatellite data; and 3) extend these enhancements spatially and across the\ngrowing season at the frequency of the satellite flights. Our SRCNN-based\nspectral extension model shows considerable promise for model transferability\nover other cropping systems in the Upper and Lower Chesapeake Bay regions.\nAdditionally, it remains effective even when cloud-free satellite data are\nunavailable, relying solely on the UAS RGB input. The spatial extension model\nproduces better biomass and N predictions than models built on raw UAS RGB\nimages. Once trained with targeted UAS RGB data, the spatial extension model\nallows farmers to stop repeated UAS flights. While we introduce\nsuper-resolution advances, the core contribution is a lightweight and scalable\nsystem for affordable on-farm use."}
{"id": "2505.21539", "pdf": "https://arxiv.org/pdf/2505.21539", "abs": "https://arxiv.org/abs/2505.21539", "authors": ["Ziming Wang", "Nan Xue", "Rebecka Jörnsten"], "title": "Equivariant Flow Matching for Point Cloud Assembly", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The goal of point cloud assembly is to reconstruct a complete 3D shape by\naligning multiple point cloud pieces. This work presents a novel equivariant\nsolver for assembly tasks based on flow matching models. We first theoretically\nshow that the key to learning equivariant distributions via flow matching is to\nlearn related vector fields. Based on this result, we propose an assembly\nmodel, called equivariant diffusion assembly (Eda), which learns related vector\nfields conditioned on the input pieces. We further construct an equivariant\npath for Eda, which guarantees high data efficiency of the training process.\nOur numerical results show that Eda is highly competitive on practical\ndatasets, and it can even handle the challenging situation where the input\npieces are non-overlapped."}
{"id": "2505.22054", "pdf": "https://arxiv.org/pdf/2505.22054", "abs": "https://arxiv.org/abs/2505.22054", "authors": ["Samuel Stucki", "Jan Deriu", "Mark Cieliebak"], "title": "Voice Adaptation for Swiss German", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Submitted to Interspeech", "summary": "This work investigates the performance of Voice Adaptation models for Swiss\nGerman dialects, i.e., translating Standard German text to Swiss German dialect\nspeech. For this, we preprocess a large dataset of Swiss podcasts, which we\nautomatically transcribe and annotate with dialect classes, yielding\napproximately 5000 hours of weakly labeled training material. We fine-tune the\nXTTSv2 model on this dataset and show that it achieves good scores in human and\nautomated evaluations and can correctly render the desired dialect. Our work\nshows a step towards adapting Voice Cloning technology to underrepresented\nlanguages. The resulting model achieves CMOS scores of up to -0.28 and SMOS\nscores of 3.8."}
{"id": "2505.21754", "pdf": "https://arxiv.org/pdf/2505.21754", "abs": "https://arxiv.org/abs/2505.21754", "authors": ["Martin Büchner", "Liza Dahiya", "Simon Dorer", "Vipul Ramtekkar", "Kenji Nishimiya", "Daniele Cattaneo", "Abhinav Valada"], "title": "Visual Loop Closure Detection Through Deep Graph Consensus", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Visual loop closure detection traditionally relies on place recognition\nmethods to retrieve candidate loops that are validated using computationally\nexpensive RANSAC-based geometric verification. As false positive loop closures\nsignificantly degrade downstream pose graph estimates, verifying a large number\nof candidates in online simultaneous localization and mapping scenarios is\nconstrained by limited time and compute resources. While most deep loop closure\ndetection approaches only operate on pairs of keyframes, we relax this\nconstraint by considering neighborhoods of multiple keyframes when detecting\nloops. In this work, we introduce LoopGNN, a graph neural network architecture\nthat estimates loop closure consensus by leveraging cliques of visually similar\nkeyframes retrieved through place recognition. By propagating deep feature\nencodings among nodes of the clique, our method yields high-precision estimates\nwhile maintaining high recall. Extensive experimental evaluations on the\nTartanDrive 2.0 and NCLT datasets demonstrate that LoopGNN outperforms\ntraditional baselines. Additionally, an ablation study across various keypoint\nextractors demonstrates that our method is robust, regardless of the type of\ndeep feature encodings used, and exhibits higher computational efficiency\ncompared to classical geometric verification baselines. We release our code,\nsupplementary material, and keyframe data at\nhttps://loopgnn.cs.uni-freiburg.de."}
{"id": "2505.21541", "pdf": "https://arxiv.org/pdf/2505.21541", "abs": "https://arxiv.org/abs/2505.21541", "authors": ["Zitong Wang", "Hang Zhao", "Qianyu Zhou", "Xuequan Lu", "Xiangtai Li", "Yiren Song"], "title": "DiffDecompose: Layer-Wise Decomposition of Alpha-Composited Images via Diffusion Transformers", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Diffusion models have recently motivated great success in many generation\ntasks like object removal. Nevertheless, existing image decomposition methods\nstruggle to disentangle semi-transparent or transparent layer occlusions due to\nmask prior dependencies, static object assumptions, and the lack of datasets.\nIn this paper, we delve into a novel task: Layer-Wise Decomposition of\nAlpha-Composited Images, aiming to recover constituent layers from single\noverlapped images under the condition of semi-transparent/transparent alpha\nlayer non-linear occlusion. To address challenges in layer ambiguity,\ngeneralization, and data scarcity, we first introduce AlphaBlend, the first\nlarge-scale and high-quality dataset for transparent and semi-transparent layer\ndecomposition, supporting six real-world subtasks (e.g., translucent flare\nremoval, semi-transparent cell decomposition, glassware decomposition).\nBuilding on this dataset, we present DiffDecompose, a diffusion\nTransformer-based framework that learns the posterior over possible layer\ndecompositions conditioned on the input image, semantic prompts, and blending\ntype. Rather than regressing alpha mattes directly, DiffDecompose performs\nIn-Context Decomposition, enabling the model to predict one or multiple layers\nwithout per-layer supervision, and introduces Layer Position Encoding Cloning\nto maintain pixel-level correspondence across layers. Extensive experiments on\nthe proposed AlphaBlend dataset and public LOGO dataset verify the\neffectiveness of DiffDecompose. The code and dataset will be available upon\npaper acceptance. Our code will be available at:\nhttps://github.com/Wangzt1121/DiffDecompose."}
{"id": "2505.22061", "pdf": "https://arxiv.org/pdf/2505.22061", "abs": "https://arxiv.org/abs/2505.22061", "authors": ["Yujin Choi", "Youngjoo Park", "Junyoung Byun", "Jaewook Lee", "Jinseong Park"], "title": "Safeguarding Privacy of Retrieval Data against Membership Inference Attacks: Is This Query Too Close to Home?", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-augmented generation (RAG) mitigates the hallucination problem in\nlarge language models (LLMs) and has proven effective for specific,\npersonalized applications. However, passing private retrieved documents\ndirectly to LLMs introduces vulnerability to membership inference attacks\n(MIAs), which try to determine whether the target datum exists in the private\nexternal database or not. Based on the insight that MIA queries typically\nexhibit high similarity to only one target document, we introduce Mirabel, a\nsimilarity-based MIA detection framework designed for the RAG system. With the\nproposed Mirabel, we show that simple detect-and-hide strategies can\nsuccessfully obfuscate attackers, maintain data utility, and remain\nsystem-agnostic. We experimentally prove its detection and defense against\nvarious state-of-the-art MIA methods and its adaptability to existing private\nRAG systems."}
{"id": "2505.21755", "pdf": "https://arxiv.org/pdf/2505.21755", "abs": "https://arxiv.org/abs/2505.21755", "authors": ["Chengyue Huang", "Brisa Maneechotesuwan", "Shivang Chopra", "Zsolt Kira"], "title": "FRAMES-VQA: Benchmarking Fine-Tuning Robustness across Multi-Modal Shifts in Visual Question Answering", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted to CVPR 2025", "summary": "Visual question answering (VQA) systems face significant challenges when\nadapting to real-world data shifts, especially in multi-modal contexts. While\nrobust fine-tuning strategies are essential for maintaining performance across\nin-distribution (ID) and out-of-distribution (OOD) scenarios, current\nevaluation settings are primarily unimodal or particular to some types of OOD,\noffering limited insight into the complexities of multi-modal contexts. In this\nwork, we propose a new benchmark FRAMES-VQA (Fine-Tuning Robustness across\nMulti-Modal Shifts in VQA) for evaluating robust fine-tuning for VQA tasks. We\nutilize ten existing VQA benchmarks, including VQAv2, IV-VQA, VQA-CP, OK-VQA\nand others, and categorize them into ID, near and far OOD datasets covering\nuni-modal, multi-modal and adversarial distribution shifts. We first conduct a\ncomprehensive comparison of existing robust fine-tuning methods. We then\nquantify the distribution shifts by calculating the Mahalanobis distance using\nuni-modal and multi-modal embeddings extracted from various models. Further, we\nperform an extensive analysis to explore the interactions between uni- and\nmulti-modal shifts as well as modality importance for ID and OOD samples. These\nanalyses offer valuable guidance on developing more robust fine-tuning methods\nto handle multi-modal distribution shifts. The code is available at\nhttps://github.com/chengyuehuang511/FRAMES-VQA ."}
{"id": "2505.21547", "pdf": "https://arxiv.org/pdf/2505.21547", "abs": "https://arxiv.org/abs/2505.21547", "authors": ["Weixing Wang", "Zifeng Ding", "Jindong Gu", "Rui Cao", "Christoph Meinel", "Gerard de Melo", "Haojin Yang"], "title": "Image Tokens Matter: Mitigating Hallucination in Discrete Tokenizer-based Large Vision-Language Models via Latent Editing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) with discrete image tokenizers unify\nmultimodal representations by encoding visual inputs into a finite set of\ntokens. Despite their effectiveness, we find that these models still\nhallucinate non-existent objects. We hypothesize that this may be due to visual\npriors induced during training: When certain image tokens frequently co-occur\nin the same spatial regions and represent shared objects, they become strongly\nassociated with the verbalizations of those objects. As a result, the model may\nhallucinate by evoking visually absent tokens that often co-occur with present\nones. To test this assumption, we construct a co-occurrence graph of image\ntokens using a segmentation dataset and employ a Graph Neural Network (GNN)\nwith contrastive learning followed by a clustering method to group tokens that\nfrequently co-occur in similar visual contexts. We find that hallucinations\npredominantly correspond to clusters whose tokens dominate the input, and more\nspecifically, that the visually absent tokens in those clusters show much\nhigher correlation with hallucinated objects compared to tokens present in the\nimage. Based on this observation, we propose a hallucination mitigation method\nthat suppresses the influence of visually absent tokens by modifying latent\nimage embeddings during generation. Experiments show our method reduces\nhallucinations while preserving expressivity. Code is available at\nhttps://github.com/weixingW/CGC-VTD/tree/main"}
{"id": "2505.22068", "pdf": "https://arxiv.org/pdf/2505.22068", "abs": "https://arxiv.org/abs/2505.22068", "authors": ["Ran Li", "Shimin Di", "Yuchen Liu", "Chen Jing", "Yu Qiu", "Lei Chen"], "title": "Beyond path selection: Better LLMs for Scientific Information Extraction with MimicSFT and Relevance and Rule-induced(R$^2$)GRPO", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Previous study suggest that powerful Large Language Models (LLMs) trained\nwith Reinforcement Learning with Verifiable Rewards (RLVR) only refines\nreasoning path without improving the reasoning capacity in math tasks while\nsupervised-finetuning(SFT) with distillation can. We study this from the view\nof Scientific information extraction (SciIE) where LLMs and reasoning LLMs\nunderperforms small Bert-based models. SciIE require both the reasoning and\nmemorization. We argue that both SFT and RLVR can refine the reasoning path and\nimprove reasoning capacity in a simple way based on SciIE. We propose two-stage\ntraining with 1. MimicSFT, using structured reasoning templates without needing\nhigh-quality chain-of-thought data, 2. R$^2$GRPO with relevance and\nrule-induced rewards. Experiments on scientific IE benchmarks show that both\nmethods can improve the reasoning capacity. R$^2$GRPO with mimicSFT surpasses\nbaseline LLMs and specialized supervised models in relation extraction. Our\ncode is available at https://github.com/ranlislz/R2GRPO."}
{"id": "2505.21771", "pdf": "https://arxiv.org/pdf/2505.21771", "abs": "https://arxiv.org/abs/2505.21771", "authors": ["Prasham Yatinkumar Titiya", "Jainil Trivedi", "Chitta Baral", "Vivek Gupta"], "title": "MMTBENCH: A Unified Benchmark for Complex Multimodal Table Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multimodal tables those that integrate semi structured data with visual\nelements such as charts and maps are ubiquitous across real world domains, yet\nthey pose a formidable challenge to current vision language models (VLMs).\nWhile Large Language models (LLMs) and VLMs have demonstrated strong\ncapabilities in text and image understanding, their performance on complex,\nreal world multimodal table reasoning remains unexplored. To bridge this gap,\nwe introduce MMTBENCH (Multimodal Table Benchmark), a benchmark consisting of\n500 real world multimodal tables drawn from diverse real world sources, with a\ntotal of 4021 question answer pairs. MMTBENCH questions cover four question\ntypes (Explicit, Implicit, Answer Mention, and Visual Based), five reasoning\ntypes (Mathematical, Extrema Identification, Fact Verification, Vision Based,\nand Others), and eight table types (Single/Multiple Entity, Maps and Charts\nwith Entities, Single/Multiple Charts, Maps, and Visualizations). Extensive\nevaluation of state of the art models on all types reveals substantial\nperformance gaps, particularly on questions requiring visual-based reasoning\nand multi-step inference. These findings show the urgent need for improved\narchitectures that more tightly integrate vision and language processing. By\nproviding a challenging, high-quality resource that mirrors the complexity of\nreal-world tasks, MMTBENCH underscores its value as a resource for future\nresearch on multimodal tables."}
{"id": "2505.21548", "pdf": "https://arxiv.org/pdf/2505.21548", "abs": "https://arxiv.org/abs/2505.21548", "authors": ["Dhruv Agarwal", "Anya Shukla", "Sunayana Sitaram", "Aditya Vashistha"], "title": "Fluent but Culturally Distant: Can Regional Training Teach Cultural Understanding?", "categories": ["physics.soc-ph", "cs.AI", "cs.CL", "cs.CY"], "comment": "Under review", "summary": "Large language models (LLMs) are used around the world but exhibit Western\ncultural tendencies. To address this cultural misalignment, many countries have\nbegun developing \"regional\" LLMs tailored to local communities. Yet it remains\nunclear whether these models merely speak the language of their users or also\nreflect their cultural values and practices. Using India as a case study, we\nevaluate five Indic and five global LLMs along two key dimensions: values (via\nthe Inglehart-Welzel map and GlobalOpinionQA) and practices (via CulturalBench\nand NormAd). Across all four tasks, we find that Indic models do not align more\nclosely with Indian cultural norms than global models. In fact, an average\nAmerican person is a better proxy for Indian cultural values than any Indic\nmodel. Even prompting strategies fail to meaningfully improve alignment.\nAblations show that regional fine-tuning does not enhance cultural competence\nand may in fact hurt it by impeding recall of existing knowledge. We trace this\nfailure to the scarcity of high-quality, untranslated, and culturally grounded\npretraining and fine-tuning data. Our study positions cultural evaluation as a\nfirst-class requirement alongside multilingual benchmarks and offers a reusable\nmethodology for developers. We call for deeper investments in culturally\nrepresentative data to build and evaluate truly sovereign LLMs."}
{"id": "2505.22076", "pdf": "https://arxiv.org/pdf/2505.22076", "abs": "https://arxiv.org/abs/2505.22076", "authors": ["Maja Stahl", "Timon Ziegenbein", "Joonsuk Park", "Henning Wachsmuth"], "title": "ArgInstruct: Specialized Instruction Fine-Tuning for Computational Argumentation", "categories": ["cs.CL"], "comment": null, "summary": "Training large language models (LLMs) to follow instructions has\nsignificantly enhanced their ability to tackle unseen tasks. However, despite\ntheir strong generalization capabilities, instruction-following LLMs encounter\ndifficulties when dealing with tasks that require domain knowledge. This work\nintroduces a specialized instruction fine-tuning for the domain of\ncomputational argumentation (CA). The goal is to enable an LLM to effectively\ntackle any unseen CA tasks while preserving its generalization capabilities.\nReviewing existing CA research, we crafted natural language instructions for\n105 CA tasks to this end. On this basis, we developed a CA-specific benchmark\nfor LLMs that allows for a comprehensive evaluation of LLMs' capabilities in\nsolving various CA tasks. We synthesized 52k CA-related instructions, adapting\nthe self-instruct process to train a CA-specialized instruction-following LLM.\nOur experiments suggest that CA-specialized instruction fine-tuning\nsignificantly enhances the LLM on both seen and unseen CA tasks. At the same\ntime, performance on the general NLP tasks of the SuperNI benchmark remains\nstable."}
{"id": "2505.21780", "pdf": "https://arxiv.org/pdf/2505.21780", "abs": "https://arxiv.org/abs/2505.21780", "authors": ["Yanbo Wang", "Justin Dauwels", "Yilun Du"], "title": "Compositional Scene Understanding through Inverse Generative Modeling", "categories": ["cs.CV"], "comment": "ICML 2025, Webpage:\n  https://energy-based-model.github.io/compositional-inference/", "summary": "Generative models have demonstrated remarkable abilities in generating\nhigh-fidelity visual content. In this work, we explore how generative models\ncan further be used not only to synthesize visual content but also to\nunderstand the properties of a scene given a natural image. We formulate scene\nunderstanding as an inverse generative modeling problem, where we seek to find\nconditional parameters of a visual generative model to best fit a given natural\nimage. To enable this procedure to infer scene structure from images\nsubstantially different than those seen during training, we further propose to\nbuild this visual generative model compositionally from smaller models over\npieces of a scene. We illustrate how this procedure enables us to infer the set\nof objects in a scene, enabling robust generalization to new test scenes with\nan increased number of objects of new shapes. We further illustrate how this\nenables us to infer global scene factors, likewise enabling robust\ngeneralization to new scenes. Finally, we illustrate how this approach can be\ndirectly applied to existing pretrained text-to-image generative models for\nzero-shot multi-object perception. Code and visualizations are at\n\\href{https://energy-based-model.github.io/compositional-inference}{https://energy-based-model.github.io/compositional-inference}."}
{"id": "2505.21550", "pdf": "https://arxiv.org/pdf/2505.21550", "abs": "https://arxiv.org/abs/2505.21550", "authors": ["Rishi Sharma", "Martijn de Vos", "Pradyumna Chari", "Ramesh Raskar", "Anne-Marie Kermarrec"], "title": "Collaborative Agentic AI Needs Interoperability Across Ecosystems", "categories": ["cs.NI", "cs.AI", "cs.MA"], "comment": null, "summary": "Collaborative agentic AI is projected to transform entire industries by\nenabling AI-powered agents to autonomously perceive, plan, and act within\ndigital environments. Yet, current solutions in this field are all built in\nisolation, and we are rapidly heading toward a landscape of fragmented,\nincompatible ecosystems. In this position paper, we argue that\ninteroperability, achieved by the adoption of minimal standards, is essential\nto ensure open, secure, web-scale, and widely-adopted agentic ecosystems. To\nthis end, we devise a minimal architectural foundation for collaborative\nagentic AI, named Web of Agents, which is composed of four components:\nagent-to-agent messaging, interaction interoperability, state management, and\nagent discovery. Web of Agents adopts existing standards and reuses existing\ninfrastructure where possible. With Web of Agents, we take the first but\ncritical step toward interoperable agentic systems and offer a pragmatic path\nforward before ecosystem fragmentation becomes the norm."}
{"id": "2505.22095", "pdf": "https://arxiv.org/pdf/2505.22095", "abs": "https://arxiv.org/abs/2505.22095", "authors": ["Chunyi Peng", "Zhipeng Xu", "Zhenghao Liu", "Yishan Li", "Yukun Yan", "Shuo Wang", "Zhiyuan Liu", "Yu Gu", "Minghe Yu", "Ge Yu", "Maosong Sun"], "title": "Learning to Route Queries Across Knowledge Bases for Step-wise Retrieval-Augmented Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Multimodal Retrieval-Augmented Generation (MRAG) has shown promise in\nmitigating hallucinations in Multimodal Large Language Models (MLLMs) by\nincorporating external knowledge during generation. Existing MRAG methods\ntypically adopt a static retrieval pipeline that fetches relevant information\nfrom multiple Knowledge Bases (KBs), followed by a refinement step. However,\nthese approaches overlook the reasoning and planning capabilities of MLLMs to\ndynamically determine how to interact with different KBs during the reasoning\nprocess. To address this limitation, we propose R1-Router, a novel MRAG\nframework that learns to decide when and where to retrieve knowledge based on\nthe evolving reasoning state. Specifically, R1-Router can generate follow-up\nqueries according to the current reasoning step, routing these intermediate\nqueries to the most suitable KB, and integrating external knowledge into a\ncoherent reasoning trajectory to answer the original query. Furthermore, we\nintroduce Step-wise Group Relative Policy Optimization (Step-GRPO), a tailored\nreinforcement learning algorithm that assigns step-specific rewards to optimize\nthe reasoning behavior of MLLMs. Experimental results on various open-domain QA\nbenchmarks across multiple modalities demonstrate that R1-Router outperforms\nbaseline models by over 7%. Further analysis shows that R1-Router can\nadaptively and effectively leverage diverse KBs, reducing unnecessary\nretrievals and improving both efficiency and accuracy."}
{"id": "2505.21795", "pdf": "https://arxiv.org/pdf/2505.21795", "abs": "https://arxiv.org/abs/2505.21795", "authors": ["Claudia Cuttano", "Gabriele Trivigno", "Giuseppe Averta", "Carlo Masone"], "title": "SANSA: Unleashing the Hidden Semantics in SAM2 for Few-Shot Segmentation", "categories": ["cs.CV"], "comment": "Code: https://github.com/ClaudiaCuttano/SANSA", "summary": "Few-shot segmentation aims to segment unseen object categories from just a\nhandful of annotated examples. This requires mechanisms that can both identify\nsemantically related objects across images and accurately produce segmentation\nmasks. We note that Segment Anything 2 (SAM2), with its prompt-and-propagate\nmechanism, offers both strong segmentation capabilities and a built-in feature\nmatching process. However, we show that its representations are entangled with\ntask-specific cues optimized for object tracking, which impairs its use for\ntasks requiring higher level semantic understanding. Our key insight is that,\ndespite its class-agnostic pretraining, SAM2 already encodes rich semantic\nstructure in its features. We propose SANSA (Semantically AligNed Segment\nAnything 2), a framework that makes this latent structure explicit, and\nrepurposes SAM2 for few-shot segmentation through minimal task-specific\nmodifications. SANSA achieves state-of-the-art performance on few-shot\nsegmentation benchmarks specifically designed to assess generalization,\noutperforms generalist methods in the popular in-context setting, supports\nvarious prompts flexible interaction via points, boxes, or scribbles, and\nremains significantly faster and more compact than prior approaches. Code is\navailable at https://github.com/ClaudiaCuttano/SANSA."}
{"id": "2505.21551", "pdf": "https://arxiv.org/pdf/2505.21551", "abs": "https://arxiv.org/abs/2505.21551", "authors": ["Emmanuel Akinrintoyo", "Nadine Abdelhalim", "Nicole Salomons"], "title": "WhisperD: Dementia Speech Recognition and Filler Word Detection with Whisper", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "comment": "Submitted to Interspeech 2025 (Accepted)", "summary": "Whisper fails to correctly transcribe dementia speech because persons with\ndementia (PwDs) often exhibit irregular speech patterns and disfluencies such\nas pauses, repetitions, and fragmented sentences. It was trained on standard\nspeech and may have had little or no exposure to dementia-affected speech.\nHowever, correct transcription is vital for dementia speech for cost-effective\ndiagnosis and the development of assistive technology. In this work, we\nfine-tune Whisper with the open-source dementia speech dataset (DementiaBank)\nand our in-house dataset to improve its word error rate (WER). The fine-tuning\nalso includes filler words to ascertain the filler inclusion rate (FIR) and F1\nscore. The fine-tuned models significantly outperformed the off-the-shelf\nmodels. The medium-sized model achieved a WER of 0.24, outperforming previous\nwork. Similarly, there was a notable generalisability to unseen data and speech\npatterns."}
{"id": "2505.22096", "pdf": "https://arxiv.org/pdf/2505.22096", "abs": "https://arxiv.org/abs/2505.22096", "authors": ["Jinheon Baek", "Horst Samulowitz", "Oktie Hassanzadeh", "Dharmashankar Subramanian", "Sola Shirai", "Alfio Gliozzo", "Debarun Bhattacharjya"], "title": "Knowledge Base Construction for Knowledge-Augmented Text-to-SQL", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL Findings 2025", "summary": "Text-to-SQL aims to translate natural language queries into SQL statements,\nwhich is practical as it enables anyone to easily retrieve the desired\ninformation from databases. Recently, many existing approaches tackle this\nproblem with Large Language Models (LLMs), leveraging their strong capability\nin understanding user queries and generating corresponding SQL code. Yet, the\nparametric knowledge in LLMs might be limited to covering all the diverse and\ndomain-specific queries that require grounding in various database schemas,\nwhich makes generated SQLs less accurate oftentimes. To tackle this, we propose\nconstructing the knowledge base for text-to-SQL, a foundational source of\nknowledge, from which we retrieve and generate the necessary knowledge for\ngiven queries. In particular, unlike existing approaches that either manually\nannotate knowledge or generate only a few pieces of knowledge for each query,\nour knowledge base is comprehensive, which is constructed based on a\ncombination of all the available questions and their associated database\nschemas along with their relevant knowledge, and can be reused for unseen\ndatabases from different datasets and domains. We validate our approach on\nmultiple text-to-SQL datasets, considering both the overlapping and\nnon-overlapping database scenarios, where it outperforms relevant baselines\nsubstantially."}
{"id": "2505.21817", "pdf": "https://arxiv.org/pdf/2505.21817", "abs": "https://arxiv.org/abs/2505.21817", "authors": ["Xiaomeng Yang", "Lei Lu", "Qihui Fan", "Changdi Yang", "Juyi Lin", "Yanzhi Wang", "Xuan Zhang", "Shangqian Gao"], "title": "ALTER: All-in-One Layer Pruning and Temporal Expert Routing for Efficient Diffusion Generation", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have demonstrated exceptional capabilities in generating\nhigh-fidelity images. However, their iterative denoising process results in\nsignificant computational overhead during inference, limiting their practical\ndeployment in resource-constrained environments. Existing acceleration methods\noften adopt uniform strategies that fail to capture the temporal variations\nduring diffusion generation, while the commonly adopted sequential\npruning-then-fine-tuning strategy suffers from sub-optimality due to the\nmisalignment between pruning decisions made on pretrained weights and the\nmodel's final parameters. To address these limitations, we introduce ALTER:\nAll-in-One Layer Pruning and Temporal Expert Routing, a unified framework that\ntransforms diffusion models into a mixture of efficient temporal experts. ALTER\nachieves a single-stage optimization that unifies layer pruning, expert\nrouting, and model fine-tuning by employing a trainable hypernetwork, which\ndynamically generates layer pruning decisions and manages timestep routing to\nspecialized, pruned expert sub-networks throughout the ongoing fine-tuning of\nthe UNet. This unified co-optimization strategy enables significant efficiency\ngains while preserving high generative quality. Specifically, ALTER achieves\nsame-level visual fidelity to the original 50-step Stable Diffusion v2.1 model\nwhile utilizing only 25.9% of its total MACs with just 20 inference steps and\ndelivering a 3.64x speedup through 35% sparsity."}
{"id": "2505.21553", "pdf": "https://arxiv.org/pdf/2505.21553", "abs": "https://arxiv.org/abs/2505.21553", "authors": ["Hui Ma", "Kai Yang"], "title": "MetaSTNet: Multimodal Meta-learning for Cellular Traffic Conformal Prediction", "categories": ["cs.NI", "cs.AI", "cs.LG"], "comment": null, "summary": "Network traffic prediction techniques have attracted much attention since\nthey are valuable for network congestion control and user experience\nimprovement. While existing prediction techniques can achieve favorable\nperformance when there is sufficient training data, it remains a great\nchallenge to make accurate predictions when only a small amount of training\ndata is available. To tackle this problem, we propose a deep learning model,\nentitled MetaSTNet, based on a multimodal meta-learning framework. It is an\nend-to-end network architecture that trains the model in a simulator and\ntransfers the meta-knowledge to a real-world environment, which can quickly\nadapt and obtain accurate predictions on a new task with only a small amount of\nreal-world training data. In addition, we further employ cross conformal\nprediction to assess the calibrated prediction intervals. Extensive experiments\nhave been conducted on real-world datasets to illustrate the efficiency and\neffectiveness of MetaSTNet."}
{"id": "2505.22101", "pdf": "https://arxiv.org/pdf/2505.22101", "abs": "https://arxiv.org/abs/2505.22101", "authors": ["Zhiyu Li", "Shichao Song", "Hanyu Wang", "Simin Niu", "Ding Chen", "Jiawei Yang", "Chenyang Xi", "Huayi Lai", "Jihao Zhao", "Yezhaohui Wang", "Junpeng Ren", "Zehao Lin", "Jiahao Huo", "Tianyi Chen", "Kai Chen", "Kehang Li", "Zhiqiang Yin", "Qingchen Yu", "Bo Tang", "Hongkang Yang", "Zhi-Qin John Xu", "Feiyu Xiong"], "title": "MemOS: An Operating System for Memory-Augmented Generation (MAG) in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have emerged as foundational infrastructure in\nthe pursuit of Artificial General Intelligence (AGI). Despite their remarkable\ncapabilities in language perception and generation, current LLMs fundamentally\nlack a unified and structured architecture for handling memory. They primarily\nrely on parametric memory (knowledge encoded in model weights) and ephemeral\nactivation memory (context-limited runtime states). While emerging methods like\nRetrieval-Augmented Generation (RAG) incorporate plaintext memory, they lack\nlifecycle management and multi-modal integration, limiting their capacity for\nlong-term knowledge evolution. To address this, we introduce MemOS, a memory\noperating system designed for LLMs that, for the first time, elevates memory to\na first-class operational resource. It builds unified mechanisms for\nrepresentation, organization, and governance across three core memory types:\nparametric, activation, and plaintext. At its core is the MemCube, a\nstandardized memory abstraction that enables tracking, fusion, and migration of\nheterogeneous memory, while offering structured, traceable access across tasks\nand contexts. MemOS establishes a memory-centric execution framework with\nstrong controllability, adaptability, and evolvability. It fills a critical gap\nin current LLM infrastructure and lays the groundwork for continual adaptation,\npersonalized intelligence, and cross-platform coordination in next-generation\nintelligent systems."}
{"id": "2505.21831", "pdf": "https://arxiv.org/pdf/2505.21831", "abs": "https://arxiv.org/abs/2505.21831", "authors": ["Bowen Chen", "Cheng-han Lee", "Yixu Chen", "Zaixi Shang", "Hai Wei", "Alan C. Bovik"], "title": "HDRSDR-VQA: A Subjective Video Quality Dataset for HDR and SDR Comparative Evaluation", "categories": ["cs.CV"], "comment": null, "summary": "We introduce HDRSDR-VQA, a large-scale video quality assessment dataset\ndesigned to facilitate comparative analysis between High Dynamic Range (HDR)\nand Standard Dynamic Range (SDR) content under realistic viewing conditions.\nThe dataset comprises 960 videos generated from 54 diverse source sequences,\neach presented in both HDR and SDR formats across nine distortion levels. To\nobtain reliable perceptual quality scores, we conducted a comprehensive\nsubjective study involving 145 participants and six consumer-grade HDR-capable\ntelevisions. A total of over 22,000 pairwise comparisons were collected and\nscaled into Just-Objectionable-Difference (JOD) scores. Unlike prior datasets\nthat focus on a single dynamic range format or use limited evaluation\nprotocols, HDRSDR-VQA enables direct content-level comparison between HDR and\nSDR versions, supporting detailed investigations into when and why one format\nis preferred over the other. The open-sourced part of the dataset is publicly\navailable to support further research in video quality assessment,\ncontent-adaptive streaming, and perceptual model development."}
{"id": "2505.21556", "pdf": "https://arxiv.org/pdf/2505.21556", "abs": "https://arxiv.org/abs/2505.21556", "authors": ["Hee-Seon Kim", "Minbeom Kim", "Wonjun Lee", "Kihyun Kim", "Changick Kim"], "title": "Benign-to-Toxic Jailbreaking: Inducing Harmful Responses from Harmless Prompts", "categories": ["cs.CV", "cs.AI"], "comment": "LVLM, Jailbreak", "summary": "Optimization-based jailbreaks typically adopt the Toxic-Continuation setting\nin large vision-language models (LVLMs), following the standard next-token\nprediction objective. In this setting, an adversarial image is optimized to\nmake the model predict the next token of a toxic prompt. However, we find that\nthe Toxic-Continuation paradigm is effective at continuing already-toxic\ninputs, but struggles to induce safety misalignment when explicit toxic signals\nare absent. We propose a new paradigm: Benign-to-Toxic (B2T) jailbreak. Unlike\nprior work, we optimize adversarial images to induce toxic outputs from benign\nconditioning. Since benign conditioning contains no safety violations, the\nimage alone must break the model's safety mechanisms. Our method outperforms\nprior approaches, transfers in black-box settings, and complements text-based\njailbreaks. These results reveal an underexplored vulnerability in multimodal\nalignment and introduce a fundamentally new direction for jailbreak approaches."}
{"id": "2505.22107", "pdf": "https://arxiv.org/pdf/2505.22107", "abs": "https://arxiv.org/abs/2505.22107", "authors": ["Shuhai Zhang", "Zeng You", "Yaofo Chen", "Zhiquan Wen", "Qianyue Wang", "Zhijie Qiu", "Yuanqing Li", "Mingkui Tan"], "title": "Curse of High Dimensionality Issue in Transformer for Long-context Modeling", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted at ICML 2025", "summary": "Transformer-based large language models (LLMs) excel in natural language\nprocessing tasks by capturing long-range dependencies through self-attention\nmechanisms. However, long-context modeling faces significant computational\ninefficiencies due to \\textit{redundant} attention computations: while\nattention weights are often \\textit{sparse}, all tokens consume \\textit{equal}\ncomputational resources. In this paper, we reformulate traditional\nprobabilistic sequence modeling as a \\textit{supervised learning task},\nenabling the separation of relevant and irrelevant tokens and providing a\nclearer understanding of redundancy. Based on this reformulation, we\ntheoretically analyze attention sparsity, revealing that only a few tokens\nsignificantly contribute to predictions. Building on this, we formulate\nattention optimization as a linear coding problem and propose a \\textit{group\ncoding strategy}, theoretically showing its ability to improve robustness\nagainst random noise and enhance learning efficiency. Motivated by this, we\npropose \\textit{Dynamic Group Attention} (DGA), which leverages the group\ncoding to explicitly reduce redundancy by aggregating less important tokens\nduring attention computation. Empirical results show that our DGA significantly\nreduces computational costs while maintaining competitive performance.Code is\navailable at https://github.com/bolixinyu/DynamicGroupAttention."}
{"id": "2505.21837", "pdf": "https://arxiv.org/pdf/2505.21837", "abs": "https://arxiv.org/abs/2505.21837", "authors": ["Aliasghar Khani", "Arianna Rampini", "Evan Atherton", "Bruno Roy"], "title": "UniMoGen: Universal Motion Generation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Motion generation is a cornerstone of computer graphics, animation, gaming,\nand robotics, enabling the creation of realistic and varied character\nmovements. A significant limitation of existing methods is their reliance on\nspecific skeletal structures, which restricts their versatility across\ndifferent characters. To overcome this, we introduce UniMoGen, a novel\nUNet-based diffusion model designed for skeleton-agnostic motion generation.\nUniMoGen can be trained on motion data from diverse characters, such as humans\nand animals, without the need for a predefined maximum number of joints. By\ndynamically processing only the necessary joints for each character, our model\nachieves both skeleton agnosticism and computational efficiency. Key features\nof UniMoGen include controllability via style and trajectory inputs, and the\nability to continue motions from past frames. We demonstrate UniMoGen's\neffectiveness on the 100style dataset, where it outperforms state-of-the-art\nmethods in diverse character motion generation. Furthermore, when trained on\nboth the 100style and LAFAN1 datasets, which use different skeletons, UniMoGen\nachieves high performance and improved efficiency across both skeletons. These\nresults highlight UniMoGen's potential to advance motion generation by\nproviding a flexible, efficient, and controllable solution for a wide range of\ncharacter animations."}
{"id": "2505.21557", "pdf": "https://arxiv.org/pdf/2505.21557", "abs": "https://arxiv.org/abs/2505.21557", "authors": ["Polad Geidarov"], "title": "Analytical Calculation of Weights Convolutional Neural Network", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper presents an algorithm for analytically calculating the weights and\nthresholds of convolutional neural networks (CNNs) without using standard\ntraining procedures. The algorithm enables the determination of CNN parameters\nbased on just 10 selected images from the MNIST dataset, each representing a\ndigit from 0 to 9. As part of the method, the number of channels in CNN layers\nis also derived analytically. A software module was implemented in C++ Builder,\nand a series of experiments were conducted using the MNIST dataset. Results\ndemonstrate that the analytically computed CNN can recognize over half of 1000\nhandwritten digit images without any training, achieving inference in fractions\nof a second. These findings suggest that CNNs can be constructed and applied\ndirectly for classification tasks without training, using purely analytical\ncomputation of weights."}
{"id": "2505.22113", "pdf": "https://arxiv.org/pdf/2505.22113", "abs": "https://arxiv.org/abs/2505.22113", "authors": ["Zhiyuan Li", "Yi Chang", "Yuan Wu"], "title": "THINK-Bench: Evaluating Thinking Efficiency and Chain-of-Thought Quality of Large Reasoning Models", "categories": ["cs.CL"], "comment": "20 pages, 8 figures, 6 tables", "summary": "Large reasoning models (LRMs) have achieved impressive performance in complex\ntasks, often outperforming conventional large language models (LLMs). However,\nthe prevalent issue of overthinking severely limits their computational\nefficiency. Overthinking occurs when models generate excessive and redundant\ntokens that contribute little to accurate outcomes, especially in simple tasks,\nresulting in a significant waste of computational resources. To systematically\ninvestigate this issue, we introduce Think-Bench, a benchmark designed to\nevaluate the reasoning efficiency of LRMs. We also propose novel efficiency\nmetrics and conduct a comprehensive evaluation of various LRMs across multiple\ndimensions, including the reasoning process, outcome quality, and\nchain-of-thought (CoT) characteristics. Our analysis reveals that most LRMs\nexhibit overthinking in handling easy questions, generating unnecessarily\nlengthy reasoning chains. While many LRMs demonstrate high CoT quality, several\nsuffer from low efficiency. We hope that Think-Bench can serve as a robust\nfoundation for advancing research into LRMs."}
{"id": "2505.21844", "pdf": "https://arxiv.org/pdf/2505.21844", "abs": "https://arxiv.org/abs/2505.21844", "authors": ["Mehrdad Noori", "David Osowiechi", "Gustavo Adolfo Vargas Hakim", "Ali Bahri", "Moslem Yazdanpanah", "Sahar Dastani", "Farzad Beizaee", "Ismail Ben Ayed", "Christian Desrosiers"], "title": "Test-Time Adaptation of Vision-Language Models for Open-Vocabulary Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Recently, test-time adaptation has attracted wide interest in the context of\nvision-language models for image classification. However, to the best of our\nknowledge, the problem is completely overlooked in dense prediction tasks such\nas Open-Vocabulary Semantic Segmentation (OVSS). In response, we propose a\nnovel TTA method tailored to adapting VLMs for segmentation during test time.\nUnlike TTA methods for image classification, our Multi-Level and Multi-Prompt\n(MLMP) entropy minimization integrates features from intermediate\nvision-encoder layers and is performed with different text-prompt templates at\nboth the global CLS token and local pixel-wise levels. Our approach could be\nused as plug-and-play for any segmentation network, does not require additional\ntraining data or labels, and remains effective even with a single test sample.\nFurthermore, we introduce a comprehensive OVSS TTA benchmark suite, which\nintegrates a rigorous evaluation protocol, seven segmentation datasets, and 15\ncommon corruptions, with a total of 82 distinct test scenarios, establishing a\nstandardized and comprehensive testbed for future TTA research in\nopen-vocabulary segmentation. Our experiments on this suite demonstrate that\nour segmentation-tailored method consistently delivers significant gains over\ndirect adoption of TTA classification baselines."}
{"id": "2505.21558", "pdf": "https://arxiv.org/pdf/2505.21558", "abs": "https://arxiv.org/abs/2505.21558", "authors": ["Elhoucine Elfatimia", "Recep Eryigitb", "Lahcen Elfatimi"], "title": "A Novel Convolutional Neural Network-Based Framework for Complex Multiclass Brassica Seed Classification", "categories": ["cs.CV", "cs.AI", "cs.LG", "na"], "comment": "11 Figure", "summary": "Agricultural research has accelerated in recent years, yet farmers often lack\nthe time and resources for on-farm research due to the demands of crop\nproduction and farm operations. Seed classification offers valuable insights\ninto quality control, production efficiency, and impurity detection. Early\nidentification of seed types is critical to reducing the cost and risk\nassociated with field emergence, which can lead to yield losses or disruptions\nin downstream processes like harvesting. Seed sampling supports growers in\nmonitoring and managing seed quality, improving precision in determining seed\npurity levels, guiding management adjustments, and enhancing yield estimations.\nThis study proposes a novel convolutional neural network (CNN)-based framework\nfor the efficient classification of ten common Brassica seed types. The\napproach addresses the inherent challenge of texture similarity in seed images\nusing a custom-designed CNN architecture. The model's performance was evaluated\nagainst several pre-trained state-of-the-art architectures, with adjustments to\nlayer configurations for optimized classification. Experimental results using\nour collected Brassica seed dataset demonstrate that the proposed model\nachieved a high accuracy rate of 93 percent."}
{"id": "2505.22116", "pdf": "https://arxiv.org/pdf/2505.22116", "abs": "https://arxiv.org/abs/2505.22116", "authors": ["Jintao Zhang", "Zirui Liu", "Mingyue Cheng", "Shilong Zhang", "Tingyue Pan", "Qi Liu", "Yanhu Xie"], "title": "Multimodal Forecasting of Sparse Intraoperative Hypotension Events Powered by Language Model", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Intraoperative hypotension (IOH) frequently occurs under general anesthesia\nand is strongly linked to adverse outcomes such as myocardial injury and\nincreased mortality. Despite its significance, IOH prediction is hindered by\nevent sparsity and the challenge of integrating static and dynamic data across\ndiverse patients. In this paper, we propose \\textbf{IOHFuseLM}, a multimodal\nlanguage model framework. To accurately identify and differentiate sparse\nhypotensive events, we leverage a two-stage training strategy. The first stage\ninvolves domain adaptive pretraining on IOH physiological time series augmented\nthrough diffusion methods, thereby enhancing the model sensitivity to patterns\nassociated with hypotension. Subsequently, task fine-tuning is performed on the\noriginal clinical dataset to further enhance the ability to distinguish\nnormotensive from hypotensive states. To enable multimodal fusion for each\npatient, we align structured clinical descriptions with the corresponding\nphysiological time series at the token level. Such alignment enables the model\nto capture individualized temporal patterns alongside their corresponding\nclinical semantics. In addition, we convert static patient attributes into\nstructured text to enrich personalized information. Experimental evaluations on\ntwo intraoperative datasets demonstrate that IOHFuseLM outperforms established\nbaselines in accurately identifying IOH events, highlighting its applicability\nin clinical decision support scenarios. Our code is publicly available to\npromote reproducibility at https://github.com/zjt-gpu/IOHFuseLM."}
{"id": "2505.21847", "pdf": "https://arxiv.org/pdf/2505.21847", "abs": "https://arxiv.org/abs/2505.21847", "authors": ["Xuwei Xu", "Yang Li", "Yudong Chen", "Jiajun Liu", "Sen Wang"], "title": "RePaViT: Scalable Vision Transformer Acceleration via Structural Reparameterization on Feedforward Network Layers", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to ICML2025", "summary": "We reveal that feedforward network (FFN) layers, rather than attention\nlayers, are the primary contributors to Vision Transformer (ViT) inference\nlatency, with their impact signifying as model size increases. This finding\nhighlights a critical opportunity for optimizing the efficiency of large-scale\nViTs by focusing on FFN layers. In this work, we propose a novel channel idle\nmechanism that facilitates post-training structural reparameterization for\nefficient FFN layers during testing. Specifically, a set of feature channels\nremains idle and bypasses the nonlinear activation function in each FFN layer,\nthereby forming a linear pathway that enables structural reparameterization\nduring inference. This mechanism results in a family of ReParameterizable\nVision Transformers (RePaViTs), which achieve remarkable latency reductions\nwith acceptable sacrifices (sometimes gains) in accuracy across various ViTs.\nThe benefits of our method scale consistently with model sizes, demonstrating\ngreater speed improvements and progressively narrowing accuracy gaps or even\nhigher accuracies on larger models. In particular, RePa-ViT-Large and\nRePa-ViT-Huge enjoy 66.8% and 68.7% speed-ups with +1.7% and +1.1% higher top-1\naccuracies under the same training strategy, respectively. RePaViT is the first\nto employ structural reparameterization on FFN layers to expedite ViTs to our\nbest knowledge, and we believe that it represents an auspicious direction for\nefficient ViTs. Source code is available at\nhttps://github.com/Ackesnal/RePaViT."}
{"id": "2505.21559", "pdf": "https://arxiv.org/pdf/2505.21559", "abs": "https://arxiv.org/abs/2505.21559", "authors": ["Julien Soulé", "Jean-Paul Jamont", "Michel Occello", "Louis-Marie Traonouez", "Paul Théron"], "title": "Streamlining Resilient Kubernetes Autoscaling with Multi-Agent Systems via an Automated Online Design Framework", "categories": ["cs.MA", "cs.AI"], "comment": null, "summary": "In cloud-native systems, Kubernetes clusters with interdependent services\noften face challenges to their operational resilience due to poor workload\nmanagement issues such as resource blocking, bottlenecks, or continuous pod\ncrashes. These vulnerabilities are further amplified in adversarial scenarios,\nsuch as Distributed Denial-of-Service attacks (DDoS). Conventional Horizontal\nPod Autoscaling (HPA) approaches struggle to address such dynamic conditions,\nwhile reinforcement learning-based methods, though more adaptable, typically\noptimize single goals like latency or resource usage, neglecting broader\nfailure scenarios. We propose decomposing the overarching goal of maintaining\noperational resilience into failure-specific sub-goals delegated to\ncollaborative agents, collectively forming an HPA Multi-Agent System (MAS). We\nintroduce an automated, four-phase online framework for HPA MAS design: 1)\nmodeling a digital twin built from cluster traces; 2) training agents in\nsimulation using roles and missions tailored to failure contexts; 3) analyzing\nagent behaviors for explainability; and 4) transferring learned policies to the\nreal cluster. Experimental results demonstrate that the generated HPA MASs\noutperform three state-of-the-art HPA systems in sustaining operational\nresilience under various adversarial conditions in a proposed complex cluster."}
{"id": "2505.22118", "pdf": "https://arxiv.org/pdf/2505.22118", "abs": "https://arxiv.org/abs/2505.22118", "authors": ["Alan Ramponi", "Marco Rovera", "Robert Moro", "Sara Tonelli"], "title": "Multilingual vs Crosslingual Retrieval of Fact-Checked Claims: A Tale of Two Approaches", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval of previously fact-checked claims is a well-established task, whose\nautomation can assist professional fact-checkers in the initial steps of\ninformation verification. Previous works have mostly tackled the task\nmonolingually, i.e., having both the input and the retrieved claims in the same\nlanguage. However, especially for languages with a limited availability of\nfact-checks and in case of global narratives, such as pandemics, wars, or\ninternational politics, it is crucial to be able to retrieve claims across\nlanguages. In this work, we examine strategies to improve the multilingual and\ncrosslingual performance, namely selection of negative examples (in the\nsupervised) and re-ranking (in the unsupervised setting). We evaluate all\napproaches on a dataset containing posts and claims in 47 languages (283\nlanguage combinations). We observe that the best results are obtained by using\nLLM-based re-ranking, followed by fine-tuning with negative examples sampled\nusing a sentence similarity-based strategy. Most importantly, we show that\ncrosslinguality is a setup with its own unique characteristics compared to the\nmultilingual setup."}
{"id": "2505.21848", "pdf": "https://arxiv.org/pdf/2505.21848", "abs": "https://arxiv.org/abs/2505.21848", "authors": ["Jingqi Xu", "Chenghao Li", "Yuke Zhang", "Peter A. Beerel"], "title": "FPAN: Mitigating Replication in Diffusion Models through the Fine-Grained Probabilistic Addition of Noise to Token Embeddings", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have demonstrated remarkable potential in generating\nhigh-quality images. However, their tendency to replicate training data raises\nserious privacy concerns, particularly when the training datasets contain\nsensitive or private information. Existing mitigation strategies primarily\nfocus on reducing image duplication, modifying the cross-attention mechanism,\nand altering the denoising backbone architecture of diffusion models. Moreover,\nrecent work has shown that adding a consistent small amount of noise to text\nembeddings can reduce replication to some degree. In this work, we begin by\nanalyzing the impact of adding varying amounts of noise. Based on our analysis,\nwe propose a fine-grained noise injection technique that probabilistically adds\na larger amount of noise to token embeddings. We refer to our method as\nFine-grained Probabilistic Addition of Noise (FPAN). Through our extensive\nexperiments, we show that our proposed FPAN can reduce replication by an\naverage of 28.78% compared to the baseline diffusion model without\nsignificantly impacting image quality, and outperforms the prior\nconsistent-magnitude-noise-addition approach by 26.51%. Moreover, when combined\nwith other existing mitigation methods, our FPAN approach can further reduce\nreplication by up to 16.82% with similar, if not improved, image quality."}
{"id": "2505.21562", "pdf": "https://arxiv.org/pdf/2505.21562", "abs": "https://arxiv.org/abs/2505.21562", "authors": ["Jennifer Turliuk", "Alejandro Sevilla", "Daniela Gorza", "Tod Hynes"], "title": "Enhancing Selection of Climate Tech Startups with AI -- A Case Study on Integrating Human and AI Evaluations in the ClimaTech Great Global Innovation Challenge", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": null, "summary": "This case study examines the ClimaTech Great Global Innovation Challenge's\napproach to selecting climate tech startups by integrating human and AI\nevaluations. The competition aimed to identify top startups and enhance the\naccuracy and efficiency of the selection process through a hybrid model.\nResearch shows data-driven approaches help VC firms reduce bias and improve\ndecision-making. Machine learning models have outperformed human investors in\ndeal screening, helping identify high-potential startups. Incorporating AI\naimed to ensure more equitable and objective evaluations.\n  The methodology included three phases: initial AI review, semi-finals judged\nby humans, and finals using a hybrid weighting. In phase one, 57 applications\nwere scored by an AI tool built with StackAI and OpenAI's GPT-4o, and the top\n36 advanced. In the semi-finals, human judges, unaware of AI scores, evaluated\nstartups on team quality, market potential, and technological innovation. Each\nscore - human or AI - was weighted equally, resulting in 75 percent human and\n25 percent AI influence. In the finals, with five human judges, weighting\nshifted to 83.3 percent human and 16.7 percent AI. There was a moderate\npositive correlation between AI and human scores - Spearman's = 0.47 -\nindicating general alignment with key differences. Notably, the final four\nstartups, selected mainly by humans, were among those rated highest by the AI.\nThis highlights the complementary nature of AI and human judgment. The study\nshows that hybrid models can streamline and improve startup assessments. The\nClimaTech approach offers a strong framework for future competitions by\ncombining human expertise with AI capabilities."}
{"id": "2505.22120", "pdf": "https://arxiv.org/pdf/2505.22120", "abs": "https://arxiv.org/abs/2505.22120", "authors": ["Runyu Wang", "Peng Ping", "Zhengyu Guo", "Xiaoye Zhang", "Quan Shi", "Liting Zhou", "Tianbo Ji"], "title": "LoKI: Low-damage Knowledge Implanting of Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Fine-tuning adapts pretrained models for specific tasks but poses the risk of\ncatastrophic forgetting (CF), where critical knowledge from pre-training is\noverwritten. Current Parameter-Efficient Fine-Tuning (PEFT) methods for Large\nLanguage Models (LLMs), while efficient, often sacrifice general capabilities.\nTo address the issue of CF in a general-purpose PEFT framework, we propose\n\\textbf{Lo}w-damage \\textbf{K}nowledge \\textbf{I}mplanting (\\textbf{LoKI}), a\nPEFT technique that is based on a mechanistic understanding of how knowledge is\nstored in transformer architectures. In two real-world scenarios, LoKI\ndemonstrates task-specific performance that is comparable to or even surpasses\nthat of full fine-tuning and LoRA-based methods across various model types,\nwhile significantly better preserving general capabilities. Our work connects\nmechanistic insights into LLM knowledge storage with practical fine-tuning\nobjectives, achieving state-of-the-art trade-offs between task specialization\nand the preservation of general capabilities. Our implementation is publicly\navailable as ready-to-use code\\footnote{https://github.com/Nexround/LoKI}."}
{"id": "2505.21850", "pdf": "https://arxiv.org/pdf/2505.21850", "abs": "https://arxiv.org/abs/2505.21850", "authors": ["Yanbei Jiang", "Yihao Ding", "Chao Lei", "Jiayang Ao", "Jey Han Lau", "Krista A. Ehinger"], "title": "Beyond Perception: Evaluating Abstract Visual Reasoning through Multi-Stage Task", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at ACL Findings", "summary": "Current Multimodal Large Language Models (MLLMs) excel in general visual\nreasoning but remain underexplored in Abstract Visual Reasoning (AVR), which\ndemands higher-order reasoning to identify abstract rules beyond simple\nperception. Existing AVR benchmarks focus on single-step reasoning, emphasizing\nthe end result but neglecting the multi-stage nature of reasoning process. Past\nstudies found MLLMs struggle with these benchmarks, but it doesn't explain how\nthey fail. To address this gap, we introduce MultiStAR, a Multi-Stage AVR\nbenchmark, based on RAVEN, designed to assess reasoning across varying levels\nof complexity. Additionally, existing metrics like accuracy only focus on the\nfinal outcomes while do not account for the correctness of intermediate steps.\nTherefore, we propose a novel metric, MSEval, which considers the correctness\nof intermediate steps in addition to the final outcomes. We conduct\ncomprehensive experiments on MultiStAR using 17 representative close-source and\nopen-source MLLMs. The results reveal that while existing MLLMs perform\nadequately on basic perception tasks, they continue to face challenges in more\ncomplex rule detection stages."}
{"id": "2505.21563", "pdf": "https://arxiv.org/pdf/2505.21563", "abs": "https://arxiv.org/abs/2505.21563", "authors": ["Kai Yang", "Hui Ma", "Shaoyu Dou"], "title": "Fog Intelligence for Network Anomaly Detection", "categories": ["cs.NI", "cs.AI"], "comment": "published in IEEE Network", "summary": "Anomalies are common in network system monitoring. When manifested as network\nthreats to be mitigated, service outages to be prevented, and security risks to\nbe ameliorated, detecting such anomalous network behaviors becomes of great\nimportance. However, the growing scale and complexity of the mobile\ncommunication networks, as well as the ever-increasing amount and\ndimensionality of the network surveillance data, make it extremely difficult to\nmonitor a mobile network and discover abnormal network behaviors. Recent\nadvances in machine learning allow for obtaining near-optimal solutions to\ncomplicated decision-making problems with many sources of uncertainty that\ncannot be accurately characterized by traditional mathematical models. However,\nmost machine learning algorithms are centralized, which renders them\ninapplicable to a large-scale distributed wireless networks with tens of\nmillions of mobile devices. In this article, we present fog intelligence, a\ndistributed machine learning architecture that enables intelligent wireless\nnetwork management. It preserves the advantage of both edge processing and\ncentralized cloud computing. In addition, the proposed architecture is\nscalable, privacy-preserving, and well suited for intelligent management of a\ndistributed wireless network."}
{"id": "2505.22131", "pdf": "https://arxiv.org/pdf/2505.22131", "abs": "https://arxiv.org/abs/2505.22131", "authors": ["Zhuoyang Wu", "Xinze Li", "Zhenghao Liu", "Yukun Yan", "Zhiyuan Liu", "Minghe Yu", "Cheng Yang", "Yu Gu", "Ge Yu", "Maosong Sun"], "title": "EULER: Enhancing the Reasoning Ability of Large Language Models through Error-Induced Learning", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated strong reasoning capabilities\nand achieved promising results in mathematical problem-solving tasks. Learning\nfrom errors offers the potential to further enhance the performance of LLMs\nduring Supervised Fine-Tuning (SFT). However, the errors in synthesized\nsolutions are typically gathered from sampling trails, making it challenging to\ngenerate solution errors for each mathematical problem. This paper introduces\nthe Error-IndUced LEaRning (EULER) model, which aims to develop an error\nexposure model that generates high-quality solution errors to enhance the\nmathematical reasoning capabilities of LLMs. Specifically, EULER optimizes the\nerror exposure model to increase the generation probability of self-made\nsolution errors while utilizing solutions produced by a superior LLM to\nregularize the generation quality. Our experiments across various mathematical\nproblem datasets demonstrate the effectiveness of the EULER model, achieving an\nimprovement of over 4% compared to all baseline models. Further analysis\nreveals that EULER is capable of synthesizing more challenging and educational\nsolution errors, which facilitate both the training and inference processes of\nLLMs. All codes are available at https://github.com/NEUIR/EULER."}
{"id": "2505.21854", "pdf": "https://arxiv.org/pdf/2505.21854", "abs": "https://arxiv.org/abs/2505.21854", "authors": ["Jun Chen", "Xinke Li", "Mingyue Xu", "Tianrui Li", "Chongshou Li"], "title": "Rethinking Gradient-based Adversarial Attacks on Point Cloud Classification", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Gradient-based adversarial attacks have become a dominant approach for\nevaluating the robustness of point cloud classification models. However,\nexisting methods often rely on uniform update rules that fail to consider the\nheterogeneous nature of point clouds, resulting in excessive and perceptible\nperturbations. In this paper, we rethink the design of gradient-based attacks\nby analyzing the limitations of conventional gradient update mechanisms and\npropose two new strategies to improve both attack effectiveness and\nimperceptibility. First, we introduce WAAttack, a novel framework that\nincorporates weighted gradients and an adaptive step-size strategy to account\nfor the non-uniform contribution of points during optimization. This approach\nenables more targeted and subtle perturbations by dynamically adjusting updates\naccording to the local structure and sensitivity of each point. Second, we\npropose SubAttack, a complementary strategy that decomposes the point cloud\ninto subsets and focuses perturbation efforts on structurally critical regions.\nTogether, these methods represent a principled rethinking of gradient-based\nadversarial attacks for 3D point cloud classification. Extensive experiments\ndemonstrate that our approach outperforms state-of-the-art baselines in\ngenerating highly imperceptible adversarial examples. Code will be released\nupon paper acceptance."}
{"id": "2505.21565", "pdf": "https://arxiv.org/pdf/2505.21565", "abs": "https://arxiv.org/abs/2505.21565", "authors": ["Haicheng Liao", "Zhenning Li", "Guohui Zhang", "Keqiang Li", "Chengzhong Xu"], "title": "Towards Human-Like Trajectory Prediction for Autonomous Driving: A Behavior-Centric Approach", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Predicting the trajectories of vehicles is crucial for the development of\nautonomous driving (AD) systems, particularly in complex and dynamic traffic\nenvironments. In this study, we introduce HiT (Human-like Trajectory\nPrediction), a novel model designed to enhance trajectory prediction by\nincorporating behavior-aware modules and dynamic centrality measures. Unlike\ntraditional methods that primarily rely on static graph structures, HiT\nleverages a dynamic framework that accounts for both direct and indirect\ninteractions among traffic participants. This allows the model to capture the\nsubtle yet significant influences of surrounding vehicles, enabling more\naccurate and human-like predictions. To evaluate HiT's performance, we\nconducted extensive experiments using diverse and challenging real-world\ndatasets, including NGSIM, HighD, RounD, ApolloScape, and MoCAD++. The results\ndemonstrate that HiT consistently outperforms other top models across multiple\nmetrics, particularly excelling in scenarios involving aggressive driving\nbehaviors. This research presents a significant step forward in trajectory\nprediction, offering a more reliable and interpretable approach for enhancing\nthe safety and efficiency of fully autonomous driving systems."}
{"id": "2505.22135", "pdf": "https://arxiv.org/pdf/2505.22135", "abs": "https://arxiv.org/abs/2505.22135", "authors": ["Yuichiro Hoshino", "Hideyuki Tachibana", "Muneyoshi Inahara", "Hiroto Takegawa"], "title": "RAD: Redundancy-Aware Distillation for Hybrid Models via Self-Speculative Decoding", "categories": ["cs.CL", "cs.LG"], "comment": "26 pages", "summary": "Hybrid models combining Transformers and State Space Models (SSMs) are\npromising for balancing performance and efficiency. However, optimizing these\nhybrid models, particularly by addressing the potential redundancy inherent\nwithin the Transformer components, remains a significant challenge. In this\npaper, we propose RAD (Redundancy-Aware Distillation), a novel framework that\nuses self-speculative decoding as a diagnostic tool to identify redundant\nattention layers within the model. These identified layers are then selectively\nreplaced with SSM components, followed by targeted (self-)distillation.\nSpecifically, RAD focuses knowledge transfer on the components identified as\nredundant, considering architectural changes and specific weight initialization\nstrategies. We experimentally demonstrate that self-distillation using RAD\nsignificantly surpasses the performance of the original base model on\nmathematical and coding tasks. Furthermore, RAD is also effective in standard\nknowledge distillation settings, achieving up to approximately 2x faster\nconvergence compared to baseline methods. Notably, while a baseline model\ndistilled from a Llama-3.1 70B teacher achieves scores of 46.17 on GSM8K and\n22.75 on CRUX, RAD achieves significantly higher scores of 71.27 on GSM8K and\n28.25 on CRUX, even when using a much smaller Llama-3.1 8B teacher. RAD offers\na new pathway for efficient optimization and performance enhancement in the\ndistillation of hybrid models."}
{"id": "2505.21862", "pdf": "https://arxiv.org/pdf/2505.21862", "abs": "https://arxiv.org/abs/2505.21862", "authors": ["Chenhui Zhao", "Yiwei Lyu", "Asadur Chowdury", "Edward Harake", "Akhil Kondepudi", "Akshay Rao", "Xinhai Hou", "Honglak Lee", "Todd Hollon"], "title": "Towards Scalable Language-Image Pre-training for 3D Medical Imaging", "categories": ["cs.CV"], "comment": null, "summary": "Language-image pre-training has demonstrated strong performance in 2D medical\nimaging, but its success in 3D modalities such as CT and MRI remains limited\ndue to the high computational demands of volumetric data, which pose a\nsignificant barrier to training on large-scale, uncurated clinical studies. In\nthis study, we introduce Hierarchical attention for Language-Image Pre-training\n(HLIP), a scalable pre-training framework for 3D medical imaging. HLIP adopts a\nlightweight hierarchical attention mechanism inspired by the natural hierarchy\nof radiology data: slice, scan, and study. This mechanism exhibits strong\ngeneralizability, e.g., +4.3% macro AUC on the Rad-ChestCT benchmark when\npre-trained on CT-RATE. Moreover, the computational efficiency of HLIP enables\ndirect training on uncurated datasets. Trained on 220K patients with 3.13\nmillion scans for brain MRI and 240K patients with 1.44 million scans for head\nCT, HLIP achieves state-of-the-art performance, e.g., +32.4% balanced ACC on\nthe proposed publicly available brain MRI benchmark Pub-Brain-5; +1.4% and\n+6.9% macro AUC on head CT benchmarks RSNA and CQ500, respectively. These\nresults demonstrate that, with HLIP, directly pre-training on uncurated\nclinical datasets is a scalable and effective direction for language-image\npre-training in 3D medical imaging. The code is available at\nhttps://github.com/Zch0414/hlip"}
{"id": "2505.21568", "pdf": "https://arxiv.org/pdf/2505.21568", "abs": "https://arxiv.org/abs/2505.21568", "authors": ["Haiyun Li", "Zhiyong Wu", "Xiaofeng Xie", "Jingran Xie", "Yaoxun Xu", "Hanyang Peng"], "title": "VoiceMark: Zero-Shot Voice Cloning-Resistant Watermarking Approach Leveraging Speaker-Specific Latents", "categories": ["cs.SD", "cs.AI", "cs.CR", "eess.AS"], "comment": "Accepted by Interspeech 2025", "summary": "Voice cloning (VC)-resistant watermarking is an emerging technique for\ntracing and preventing unauthorized cloning. Existing methods effectively trace\ntraditional VC models by training them on watermarked audio but fail in\nzero-shot VC scenarios, where models synthesize audio from an audio prompt\nwithout training. To address this, we propose VoiceMark, the first zero-shot\nVC-resistant watermarking method that leverages speaker-specific latents as the\nwatermark carrier, allowing the watermark to transfer through the zero-shot VC\nprocess into the synthesized audio. Additionally, we introduce VC-simulated\naugmentations and VAD-based loss to enhance robustness against distortions.\nExperiments on multiple zero-shot VC models demonstrate that VoiceMark achieves\nover 95% accuracy in watermark detection after zero-shot VC synthesis,\nsignificantly outperforming existing methods, which only reach around 50%. See\nour code and demos at: https://huggingface.co/spaces/haiyunli/VoiceMark"}
{"id": "2505.22137", "pdf": "https://arxiv.org/pdf/2505.22137", "abs": "https://arxiv.org/abs/2505.22137", "authors": ["Marc Feger", "Katarina Boland", "Stefan Dietze"], "title": "Limited Generalizability in Argument Mining: State-Of-The-Art Models Learn Datasets, Not Arguments", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "This paper has been accepted to ACL 2025 and will be published after\n  27.07.2025", "summary": "Identifying arguments is a necessary prerequisite for various tasks in\nautomated discourse analysis, particularly within contexts such as political\ndebates, online discussions, and scientific reasoning. In addition to\ntheoretical advances in understanding the constitution of arguments, a\nsignificant body of research has emerged around practical argument mining,\nsupported by a growing number of publicly available datasets. On these\nbenchmarks, BERT-like transformers have consistently performed best,\nreinforcing the belief that such models are broadly applicable across diverse\ncontexts of debate. This study offers the first large-scale re-evaluation of\nsuch state-of-the-art models, with a specific focus on their ability to\ngeneralize in identifying arguments. We evaluate four transformers, three\nstandard and one enhanced with contrastive pre-training for better\ngeneralization, on 17 English sentence-level datasets as most relevant to the\ntask. Our findings show that, to varying degrees, these models tend to rely on\nlexical shortcuts tied to content words, suggesting that apparent progress may\noften be driven by dataset-specific cues rather than true task alignment. While\nthe models achieve strong results on familiar benchmarks, their performance\ndrops markedly when applied to unseen datasets. Nonetheless, incorporating both\ntask-specific pre-training and joint benchmark training proves effective in\nenhancing both robustness and generalization."}
{"id": "2505.21863", "pdf": "https://arxiv.org/pdf/2505.21863", "abs": "https://arxiv.org/abs/2505.21863", "authors": ["Shikhhar Siingh", "Abhinav Rawat", "Vivek Gupta", "Chitta Baral"], "title": "GETReason: Enhancing Image Context Extraction through Hierarchical Multi-Agent Reasoning", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Publicly significant images from events hold valuable contextual information,\ncrucial for journalism and education. However, existing methods often struggle\nto extract this relevance accurately. To address this, we introduce GETReason\n(Geospatial Event Temporal Reasoning), a framework that moves beyond\nsurface-level image descriptions to infer deeper contextual meaning. We propose\nthat extracting global event, temporal, and geospatial information enhances\nunderstanding of an image's significance. Additionally, we introduce GREAT\n(Geospatial Reasoning and Event Accuracy with Temporal Alignment), a new metric\nfor evaluating reasoning-based image understanding. Our layered multi-agent\napproach, assessed using a reasoning-weighted metric, demonstrates that\nmeaningful insights can be inferred, effectively linking images to their\nbroader event context."}
{"id": "2505.21569", "pdf": "https://arxiv.org/pdf/2505.21569", "abs": "https://arxiv.org/abs/2505.21569", "authors": ["Zhucong Li", "Bowei Zhang", "Jin Xiao", "Zhijian Zhou", "Fenglei Cao", "Jiaqing Liang", "Yuan Qi"], "title": "ChemHAS: Hierarchical Agent Stacking for Enhancing Chemistry Tools", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "9 pages", "summary": "Large Language Model (LLM)-based agents have demonstrated the ability to\nimprove performance in chemistry-related tasks by selecting appropriate tools.\nHowever, their effectiveness remains limited by the inherent prediction errors\nof chemistry tools. In this paper, we take a step further by exploring how\nLLMbased agents can, in turn, be leveraged to reduce prediction errors of the\ntools. To this end, we propose ChemHAS (Chemical Hierarchical Agent Stacking),\na simple yet effective method that enhances chemistry tools through optimizing\nagent-stacking structures from limited data. ChemHAS achieves state-of-the-art\nperformance across four fundamental chemistry tasks, demonstrating that our\nmethod can effectively compensate for prediction errors of the tools.\nFurthermore, we identify and characterize four distinct agent-stacking\nbehaviors, potentially improving interpretability and revealing new\npossibilities for AI agent applications in scientific research. Our code and\ndataset are publicly available at https:\n//anonymous.4open.science/r/ChemHAS-01E4/README.md."}
{"id": "2505.22156", "pdf": "https://arxiv.org/pdf/2505.22156", "abs": "https://arxiv.org/abs/2505.22156", "authors": ["Shuaiyi Li", "Zhisong Zhang", "Yang Deng", "Chenlong Deng", "Tianqing Fang", "Hongming Zhang", "Haitao Mi", "Dong Yu", "Wai Lam"], "title": "InComeS: Integrating Compression and Selection Mechanisms into LLMs for Efficient Model Editing", "categories": ["cs.CL"], "comment": "Under review", "summary": "Although existing model editing methods perform well in recalling exact edit\nfacts, they often struggle in complex scenarios that require deeper semantic\nunderstanding rather than mere knowledge regurgitation. Leveraging the strong\ncontextual reasoning abilities of large language models (LLMs), in-context\nlearning (ICL) becomes a promising editing method by comprehending edit\ninformation through context encoding. However, this method is constrained by\nthe limited context window of LLMs, leading to degraded performance and\nefficiency as the number of edits increases. To overcome this limitation, we\npropose InComeS, a flexible framework that enhances LLMs' ability to process\nediting contexts through explicit compression and selection mechanisms.\nSpecifically, InComeS compresses each editing context into the key-value (KV)\ncache of a special gist token, enabling efficient handling of multiple edits\nwithout being restricted by the model's context window. Furthermore,\nspecialized cross-attention modules are added to dynamically select the most\nrelevant information from the gist pools, enabling adaptive and effective\nutilization of edit information. We conduct experiments on diverse model\nediting benchmarks with various editing formats, and the results demonstrate\nthe effectiveness and efficiency of our method."}
{"id": "2505.21868", "pdf": "https://arxiv.org/pdf/2505.21868", "abs": "https://arxiv.org/abs/2505.21868", "authors": ["Guiping Cao", "Wenjian Huang", "Xiangyuan Lan", "Jianguo Zhang", "Dongmei Jiang", "Yaowei Wang"], "title": "Cross-DINO: Cross the Deep MLP and Transformer for Small Object Detection", "categories": ["cs.CV"], "comment": "IEEE TRANSACTIONS ON MULTIMEDIA", "summary": "Small Object Detection (SOD) poses significant challenges due to limited\ninformation and the model's low class prediction score. While Transformer-based\ndetectors have shown promising performance, their potential for SOD remains\nlargely unexplored. In typical DETR-like frameworks, the CNN backbone network,\nspecialized in aggregating local information, struggles to capture the\nnecessary contextual information for SOD. The multiple attention layers in the\nTransformer Encoder face difficulties in effectively attending to small objects\nand can also lead to blurring of features. Furthermore, the model's lower class\nprediction score of small objects compared to large objects further increases\nthe difficulty of SOD. To address these challenges, we introduce a novel\napproach called Cross-DINO. This approach incorporates the deep MLP network to\naggregate initial feature representations with both short and long range\ninformation for SOD. Then, a new Cross Coding Twice Module (CCTM) is applied to\nintegrate these initial representations to the Transformer Encoder feature,\nenhancing the details of small objects. Additionally, we introduce a new kind\nof soft label named Category-Size (CS), integrating the Category and Size of\nobjects. By treating CS as new ground truth, we propose a new loss function\ncalled Boost Loss to improve the class prediction score of the model. Extensive\nexperimental results on COCO, WiderPerson, VisDrone, AI-TOD, and SODA-D\ndatasets demonstrate that Cross-DINO efficiently improves the performance of\nDETR-like models on SOD. Specifically, our model achieves 36.4% APs on COCO for\nSOD with only 45M parameters, outperforming the DINO by +4.4% APS (36.4% vs.\n32.0%) with fewer parameters and FLOPs, under 12 epochs training setting. The\nsource codes will be available at https://github.com/Med-Process/Cross-DINO."}
{"id": "2505.21570", "pdf": "https://arxiv.org/pdf/2505.21570", "abs": "https://arxiv.org/abs/2505.21570", "authors": ["Dalit Ken-Dror Feldman", "Daniel Benoliel"], "title": "Beyond Explainability: The Case for AI Validation", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Artificial Knowledge (AK) systems are transforming decision-making across\ncritical domains such as healthcare, finance, and criminal justice. However,\ntheir growing opacity presents governance challenges that current regulatory\napproaches, focused predominantly on explainability, fail to address\nadequately. This article argues for a shift toward validation as a central\nregulatory pillar. Validation, ensuring the reliability, consistency, and\nrobustness of AI outputs, offers a more practical, scalable, and risk-sensitive\nalternative to explainability, particularly in high-stakes contexts where\ninterpretability may be technically or economically unfeasible. We introduce a\ntypology based on two axes, validity and explainability, classifying AK systems\ninto four categories and exposing the trade-offs between interpretability and\noutput reliability. Drawing on comparative analysis of regulatory approaches in\nthe EU, US, UK, and China, we show how validation can enhance societal trust,\nfairness, and safety even where explainability is limited. We propose a\nforward-looking policy framework centered on pre- and post-deployment\nvalidation, third-party auditing, harmonized standards, and liability\nincentives. This framework balances innovation with accountability and provides\na governance roadmap for responsibly integrating opaque, high-performing AK\nsystems into society."}
{"id": "2505.22157", "pdf": "https://arxiv.org/pdf/2505.22157", "abs": "https://arxiv.org/abs/2505.22157", "authors": ["Paramita Mirza", "Lucas Weber", "Fabian Küch"], "title": "Stratified Selective Sampling for Instruction Tuning with Dedicated Scoring Strategy", "categories": ["cs.CL"], "comment": null, "summary": "Recent work shows that post-training datasets for LLMs can be substantially\ndownsampled without noticeably deteriorating performance. However, data\nselection often incurs high computational costs or is limited to narrow\ndomains. In this paper, we demonstrate that data selection can be both --\nefficient and universal -- by using a multi-step pipeline in which we\nefficiently bin data points into groups, estimate quality using specialized\nmodels, and score difficulty with a robust, lightweight method. Task-based\ncategorization allows us to control the composition of our final data --\ncrucial for finetuning multi-purpose models. To guarantee diversity, we improve\nupon previous work using embedding models and a clustering algorithm. This\nintegrated strategy enables high-performance fine-tuning with minimal overhead."}
{"id": "2505.21876", "pdf": "https://arxiv.org/pdf/2505.21876", "abs": "https://arxiv.org/abs/2505.21876", "authors": ["Zun Wang", "Jaemin Cho", "Jialu Li", "Han Lin", "Jaehong Yoon", "Yue Zhang", "Mohit Bansal"], "title": "EPiC: Efficient Video Camera Control Learning with Precise Anchor-Video Guidance", "categories": ["cs.CV", "cs.AI"], "comment": "Project website: https://zunwang1.github.io/Epic", "summary": "Recent approaches on 3D camera control in video diffusion models (VDMs) often\ncreate anchor videos to guide diffusion models as a structured prior by\nrendering from estimated point clouds following annotated camera trajectories.\nHowever, errors inherent in point cloud estimation often lead to inaccurate\nanchor videos. Moreover, the requirement for extensive camera trajectory\nannotations further increases resource demands. To address these limitations,\nwe introduce EPiC, an efficient and precise camera control learning framework\nthat automatically constructs high-quality anchor videos without expensive\ncamera trajectory annotations. Concretely, we create highly precise anchor\nvideos for training by masking source videos based on first-frame visibility.\nThis approach ensures high alignment, eliminates the need for camera trajectory\nannotations, and thus can be readily applied to any in-the-wild video to\ngenerate image-to-video (I2V) training pairs. Furthermore, we introduce\nAnchor-ControlNet, a lightweight conditioning module that integrates anchor\nvideo guidance in visible regions to pretrained VDMs, with less than 1% of\nbackbone model parameters. By combining the proposed anchor video data and\nControlNet module, EPiC achieves efficient training with substantially fewer\nparameters, training steps, and less data, without requiring modifications to\nthe diffusion model backbone typically needed to mitigate rendering\nmisalignments. Although being trained on masking-based anchor videos, our\nmethod generalizes robustly to anchor videos made with point clouds during\ninference, enabling precise 3D-informed camera control. EPiC achieves SOTA\nperformance on RealEstate10K and MiraData for I2V camera control task,\ndemonstrating precise and robust camera control ability both quantitatively and\nqualitatively. Notably, EPiC also exhibits strong zero-shot generalization to\nvideo-to-video scenarios."}
{"id": "2505.21571", "pdf": "https://arxiv.org/pdf/2505.21571", "abs": "https://arxiv.org/abs/2505.21571", "authors": ["Yao Lu", "Tengfei Ma", "Zeyu Wang", "Zhuangzhi Chen", "Dongwei Xu", "Yun Lin", "Qi Xuan", "Guan Gui"], "title": "FCOS: A Two-Stage Recoverable Model Pruning Framework for Automatic Modulation Recognition", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "With the rapid development of wireless communications and the growing\ncomplexity of digital modulation schemes, traditional manual modulation\nrecognition methods struggle to extract reliable signal features and meet\nreal-time requirements in modern scenarios. Recently, deep learning based\nAutomatic Modulation Recognition (AMR) approaches have greatly improved\nclassification accuracy. However, their large model sizes and high\ncomputational demands hinder deployment on resource-constrained devices. Model\npruning provides a general approach to reduce model complexity, but existing\nweight, channel, and layer pruning techniques each present a trade-off between\ncompression rate, hardware acceleration, and accuracy preservation. To this\nend, in this paper, we introduce FCOS, a novel Fine-to-COarse two-Stage pruning\nframework that combines channel-level pruning with layer-level collapse\ndiagnosis to achieve extreme compression, high performance and efficient\ninference. In the first stage of FCOS, hierarchical clustering and parameter\nfusion are applied to channel weights to achieve channel-level pruning. Then a\nLayer Collapse Diagnosis (LaCD) module uses linear probing to identify layer\ncollapse and removes the collapsed layers due to high channel compression\nratio. Experiments on multiple AMR benchmarks demonstrate that FCOS outperforms\nexisting channel and layer pruning methods. Specifically, FCOS achieves 95.51%\nFLOPs reduction and 95.31% parameter reduction while still maintaining\nperformance close to the original ResNet56, with only a 0.46% drop in accuracy\non Sig2019-12. Code is available at https://github.com/yaolu-zjut/FCOS."}
{"id": "2505.22165", "pdf": "https://arxiv.org/pdf/2505.22165", "abs": "https://arxiv.org/abs/2505.22165", "authors": ["Bocheng Li", "Zhujin Gao", "Linli Xu"], "title": "Unifying Continuous and Discrete Text Diffusion with Non-simultaneous Diffusion Processes", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Diffusion models have emerged as a promising approach for text generation,\nwith recent works falling into two main categories: discrete and continuous\ndiffusion models. Discrete diffusion models apply token corruption\nindependently using categorical distributions, allowing for different diffusion\nprogress across tokens but lacking fine-grained control. Continuous diffusion\nmodels map tokens to continuous spaces and apply fine-grained noise, but the\ndiffusion progress is uniform across tokens, limiting their ability to capture\nsemantic nuances. To address these limitations, we propose\n\\textbf{\\underline{N}}on-simultan\\textbf{\\underline{e}}ous\nC\\textbf{\\underline{o}}ntinuous \\textbf{\\underline{Diff}}usion Models\n(NeoDiff), a novel diffusion model that integrates the strengths of both\ndiscrete and continuous approaches. NeoDiff introduces a Poisson diffusion\nprocess for the forward process, enabling a flexible and fine-grained noising\nparadigm, and employs a time predictor for the reverse process to adaptively\nmodulate the denoising progress based on token semantics. Furthermore, NeoDiff\nutilizes an optimized schedule for inference to ensure more precise noise\ncontrol and improved performance. Our approach unifies the theories of discrete\nand continuous diffusion models, offering a more principled and effective\nframework for text generation. Experimental results on several text generation\ntasks demonstrate NeoDiff's superior performance compared to baselines of\nnon-autoregressive continuous and discrete diffusion models, iterative-based\nmethods and autoregressive diffusion-based methods. These results highlight\nNeoDiff's potential as a powerful tool for generating high-quality text and\nadvancing the field of diffusion-based text generation."}
{"id": "2505.21890", "pdf": "https://arxiv.org/pdf/2505.21890", "abs": "https://arxiv.org/abs/2505.21890", "authors": ["Sunil Kumar Narayanan", "Lingjun Zhao", "Lu Gan", "Yongsheng Chen"], "title": "Hyperspectral Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Hyperspectral imaging (HSI) has been widely used in agricultural applications\nfor non-destructive estimation of plant nutrient composition and precise\ndetermination of nutritional elements in samples. Recently, 3D reconstruction\nmethods have been used to create implicit neural representations of HSI scenes,\nwhich can help localize the target object's nutrient composition spatially and\nspectrally. Neural Radiance Field (NeRF) is a cutting-edge implicit\nrepresentation that can render hyperspectral channel compositions of each\nspatial location from any viewing direction. However, it faces limitations in\ntraining time and rendering speed. In this paper, we propose Hyperspectral\nGaussian Splatting (HS-GS), which combines the state-of-the-art 3D Gaussian\nSplatting (3DGS) with a diffusion model to enable 3D explicit reconstruction of\nthe hyperspectral scenes and novel view synthesis for the entire spectral\nrange. To enhance the model's ability to capture fine-grained reflectance\nvariations across the light spectrum and leverage correlations between adjacent\nwavelengths for denoising, we introduce a wavelength encoder to generate\nwavelength-specific spherical harmonics offsets. We also introduce a novel\nKullback--Leibler divergence-based loss to mitigate the spectral distribution\ngap between the rendered image and the ground truth. A diffusion model is\nfurther applied for denoising the rendered images and generating photorealistic\nhyperspectral images. We present extensive evaluations on five diverse\nhyperspectral scenes from the Hyper-NeRF dataset to show the effectiveness of\nour proposed HS-GS framework. The results demonstrate that HS-GS achieves new\nstate-of-the-art performance among all previously published methods. Code will\nbe released upon publication."}
{"id": "2505.21572", "pdf": "https://arxiv.org/pdf/2505.21572", "abs": "https://arxiv.org/abs/2505.21572", "authors": ["Sungwon Kim", "Namkyeong Lee", "Yunyoung Doh", "Seungmin Shin", "Guimok Cho", "Seung-Won Jeon", "Sangkook Kim", "Chanyoung Park"], "title": "Thickness-aware E(3)-Equivariant 3D Mesh Neural Networks", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "ICML 2025", "summary": "Mesh-based 3D static analysis methods have recently emerged as efficient\nalternatives to traditional computational numerical solvers, significantly\nreducing computational costs and runtime for various physics-based analyses.\nHowever, these methods primarily focus on surface topology and geometry, often\noverlooking the inherent thickness of real-world 3D objects, which exhibits\nhigh correlations and similar behavior between opposing surfaces. This\nlimitation arises from the disconnected nature of these surfaces and the\nabsence of internal edge connections within the mesh. In this work, we propose\na novel framework, the Thickness-aware E(3)-Equivariant 3D Mesh Neural Network\n(T-EMNN), that effectively integrates the thickness of 3D objects while\nmaintaining the computational efficiency of surface meshes. Additionally, we\nintroduce data-driven coordinates that encode spatial information while\npreserving E(3)-equivariance or invariance properties, ensuring consistent and\nrobust analysis. Evaluations on a real-world industrial dataset demonstrate the\nsuperior performance of T-EMNN in accurately predicting node-level 3D\ndeformations, effectively capturing thickness effects while maintaining\ncomputational efficiency."}
{"id": "2505.22169", "pdf": "https://arxiv.org/pdf/2505.22169", "abs": "https://arxiv.org/abs/2505.22169", "authors": ["Gili Lior", "Eliya Habba", "Shahar Levy", "Avi Caciularu", "Gabriel Stanovsky"], "title": "ReliableEval: A Recipe for Stochastic LLM Evaluation via Method of Moments", "categories": ["cs.CL"], "comment": null, "summary": "LLMs are highly sensitive to prompt phrasing, yet standard benchmarks\ntypically report performance using a single prompt, raising concerns about the\nreliability of such evaluations. In this work, we argue for a stochastic method\nof moments evaluation over the space of meaning-preserving prompt\nperturbations. We introduce a formal definition of reliable evaluation that\naccounts for prompt sensitivity, and suggest ReliableEval - a method for\nestimating the number of prompt resamplings needed to obtain meaningful\nresults. Using our framework, we stochastically evaluate five frontier LLMs and\nfind that even top-performing models like GPT-4o and Claude-3.7-Sonnet exhibit\nsubstantial prompt sensitivity. Our approach is model-, task-, and\nmetric-agnostic, offering a recipe for meaningful and robust LLM evaluation."}
{"id": "2505.21897", "pdf": "https://arxiv.org/pdf/2505.21897", "abs": "https://arxiv.org/abs/2505.21897", "authors": ["Jianchao Jiang", "Haofeng Zhang"], "title": "Concentrate on Weakness: Mining Hard Prototypes for Few-Shot Medical Image Segmentation", "categories": ["cs.CV"], "comment": "12 pages, 9 figures, 9 tables, accepted by IJCAI 2025", "summary": "Few-Shot Medical Image Segmentation (FSMIS) has been widely used to train a\nmodel that can perform segmentation from only a few annotated images. However,\nmost existing prototype-based FSMIS methods generate multiple prototypes from\nthe support image solely by random sampling or local averaging, which can cause\nparticularly severe boundary blurring due to the tendency for normal features\naccounting for the majority of features of a specific category. Consequently,\nwe propose to focus more attention to those weaker features that are crucial\nfor clear segmentation boundary. Specifically, we design a Support\nSelf-Prediction (SSP) module to identify such weak features by comparing true\nsupport mask with one predicted by global support prototype. Then, a Hard\nPrototypes Generation (HPG) module is employed to generate multiple hard\nprototypes based on these weak features. Subsequently, a Multiple Similarity\nMaps Fusion (MSMF) module is devised to generate final segmenting mask in a\ndual-path fashion to mitigate the imbalance between foreground and background\nin medical images. Furthermore, we introduce a boundary loss to further\nconstraint the edge of segmentation. Extensive experiments on three publicly\navailable medical image datasets demonstrate that our method achieves\nstate-of-the-art performance. Code is available at\nhttps://github.com/jcjiang99/CoW."}
{"id": "2505.21573", "pdf": "https://arxiv.org/pdf/2505.21573", "abs": "https://arxiv.org/abs/2505.21573", "authors": ["Han Wan", "Rui Zhang", "Hao Sun"], "title": "Spectral-inspired Neural Operator for Data-efficient PDE Simulation in Physics-agnostic Regimes", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Partial differential equations (PDEs) govern the spatiotemporal evolution of\nvarious physical systems. Classical numerical solvers, while accurate, require\nfine discretization and full knowledge of the governing PDEs, limiting their\napplicability when the physics is unknown or fast inference is required.\nData-driven neural PDE solvers alleviate these constraints by learning from\ndata but demand large training datasets and perform poorly in data-scarce\nregimes. Physics-aware methods mitigate data requirements by incorporating\nphysical knowledge yet rely on known PDE terms or local numerical schemes,\nrestricting their ability to handle unknown or globally coupled systems. In\nthis work, we propose the Spectral-inspired Neural Operator (SINO), a novel\nframework that learns PDE operators from limited trajectories (as few as 2-5),\nwithout any known PDE terms. SINO operates in the frequency domain and\nintroduces a Frequency-to-Vector module to learn spectral representations\nanalogous to derivative multipliers. To model nonlinear physical interactions,\nwe design a nonlinear operator block that includes a $\\Pi$-Block with low-pass\nfiltering to prevent aliasing. Finally, we introduce an operator distillation\ntechnique to distill the trained model for efficient inference. SINO achieves\nstate-of-the-art results across multiple PDE benchmarks, demonstrating strong\ndiscretization invariance and robust generalization to out-of-distribution\ninitial conditions. To our knowledge, SINO is the first physics-aware method\ncapable of accurately simulating globally coupled systems (e.g., the\nNavier-Stokes equations) from limited data without any explicit PDE terms."}
{"id": "2505.22172", "pdf": "https://arxiv.org/pdf/2505.22172", "abs": "https://arxiv.org/abs/2505.22172", "authors": ["Xiang Huang", "Ting-En Lin", "Feiteng Fang", "Yuchuan Wu", "Hangyu Li", "Yuzhong Qu", "Fei Huang", "Yongbin Li"], "title": "Reverse Preference Optimization for Complex Instruction Following", "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Instruction following (IF) is a critical capability for large language models\n(LLMs). However, handling complex instructions with multiple constraints\nremains challenging. Previous methods typically select preference pairs based\non the number of constraints they satisfy, introducing noise where chosen\nexamples may fail to follow some constraints and rejected examples may excel in\ncertain respects over the chosen ones. To address the challenge of aligning\nwith multiple preferences, we propose a simple yet effective method called\nReverse Preference Optimization (RPO). It mitigates noise in preference pairs\nby dynamically reversing the constraints within the instruction to ensure the\nchosen response is perfect, alleviating the burden of extensive sampling and\nfiltering to collect perfect responses. Besides, reversal also enlarges the gap\nbetween chosen and rejected responses, thereby clarifying the optimization\ndirection and making it more robust to noise. We evaluate RPO on two multi-turn\nIF benchmarks, Sysbench and Multi-IF, demonstrating average improvements over\nthe DPO baseline of 4.6 and 2.5 points (on Llama-3.1 8B), respectively.\nMoreover, RPO scales effectively across model sizes (8B to 70B parameters),\nwith the 70B RPO model surpassing GPT-4o."}
{"id": "2505.21904", "pdf": "https://arxiv.org/pdf/2505.21904", "abs": "https://arxiv.org/abs/2505.21904", "authors": ["Pardis Taghavi", "Tian Liu", "Renjie Li", "Reza Langari", "Zhengzhong Tu"], "title": "CAST: Contrastive Adaptation and Distillation for Semi-Supervised Instance Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Instance segmentation demands costly per-pixel annotations and large models.\nWe introduce CAST, a semi-supervised knowledge distillation (SSKD) framework\nthat compresses pretrained vision foundation models (VFM) into compact experts\nusing limited labeled and abundant unlabeled data. CAST unfolds in three\nstages: (1) domain adaptation of the VFM teacher(s) via self-training with\ncontrastive pixel calibration, (2) distillation into a compact student via a\nunified multi-objective loss that couples standard supervision and\npseudo-labels with our instance-aware pixel-wise contrastive term, and (3)\nfine-tuning on labeled data to remove residual pseudo-label bias. Central to\nCAST is an \\emph{instance-aware pixel-wise contrastive loss} that fuses mask\nand class scores to mine informative negatives and enforce clear inter-instance\nmargins. By maintaining this contrastive signal across both adaptation and\ndistillation, we align teacher and student embeddings and fully leverage\nunlabeled images. On Cityscapes and ADE20K, our ~11X smaller student surpasses\nits adapted VFM teacher(s) by +3.4 AP (33.9 vs. 30.5) and +1.5 AP (16.7 vs.\n15.2) and outperforms state-of-the-art semi-supervised approaches."}
{"id": "2505.21575", "pdf": "https://arxiv.org/pdf/2505.21575", "abs": "https://arxiv.org/abs/2505.21575", "authors": ["Dawei Feng", "Di Mei", "Huiri Tan", "Lei Ren", "Xianying Lou", "Zhangxi Tan"], "title": "StreamLink: Large-Language-Model Driven Distributed Data Engineering System", "categories": ["cs.DB", "cs.AI"], "comment": "Accepted by CIKM Workshop 2024,\n  https://sites.google.com/view/cikm2024-rag/papers?authuser=0#h.ddm5fg2z885t", "summary": "Large Language Models (LLMs) have shown remarkable proficiency in natural\nlanguage understanding (NLU), opening doors for innovative applications. We\nintroduce StreamLink - an LLM-driven distributed data system designed to\nimprove the efficiency and accessibility of data engineering tasks. We build\nStreamLink on top of distributed frameworks such as Apache Spark and Hadoop to\nhandle large data at scale. One of the important design philosophies of\nStreamLink is to respect user data privacy by utilizing local fine-tuned LLMs\ninstead of a public AI service like ChatGPT. With help from domain-adapted\nLLMs, we can improve our system's understanding of natural language queries\nfrom users in various scenarios and simplify the procedure of generating\ndatabase queries like the Structured Query Language (SQL) for information\nprocessing. We also incorporate LLM-based syntax and security checkers to\nguarantee the reliability and safety of each generated query. StreamLink\nillustrates the potential of merging generative LLMs with distributed data\nprocessing for comprehensive and user-centric data engineering. With this\narchitecture, we allow users to interact with complex database systems at\ndifferent scales in a user-friendly and security-ensured manner, where the SQL\ngeneration reaches over 10\\% of execution accuracy compared to baseline\nmethods, and allow users to find the most concerned item from hundreds of\nmillions of items within a few seconds using natural language."}
{"id": "2505.22176", "pdf": "https://arxiv.org/pdf/2505.22176", "abs": "https://arxiv.org/abs/2505.22176", "authors": ["Vihang Pancholi", "Jainit Bafna", "Tejas Anvekar", "Manish Shrivastava", "Vivek Gupta"], "title": "TabXEval: Why this is a Bad Table? An eXhaustive Rubric for Table Evaluation", "categories": ["cs.CL"], "comment": "Accepeted for Findings at ACL 2025", "summary": "Evaluating tables qualitatively & quantitatively presents a significant\nchallenge, as traditional metrics often fail to capture nuanced structural and\ncontent discrepancies. To address this, we introduce a novel, methodical rubric\nintegrating multi-level structural descriptors with fine-grained contextual\nquantification, thereby establishing a robust foundation for comprehensive\ntable comparison. Building on this foundation, we propose TabXEval, an\neXhaustive and eXplainable two-phase evaluation framework. TabXEval initially\naligns reference tables structurally via TabAlign & subsequently conducts a\nsystematic semantic and syntactic comparison using TabCompare; this approach\nclarifies the evaluation process and pinpoints subtle discrepancies overlooked\nby conventional methods. The efficacy of this framework is assessed using\nTabXBench, a novel, diverse, multi-domain benchmark we developed, featuring\nrealistic table perturbations and human-annotated assessments. Finally, a\nsystematic analysis of existing evaluation methods through\nsensitivity-specificity trade-offs demonstrates the qualitative and\nquantitative effectiveness of TabXEval across diverse table-related tasks and\ndomains, paving the way for future innovations in explainable table evaluation."}
{"id": "2505.21905", "pdf": "https://arxiv.org/pdf/2505.21905", "abs": "https://arxiv.org/abs/2505.21905", "authors": ["Mo Zhou", "Keren Ye", "Viraj Shah", "Kangfu Mei", "Mauricio Delbracio", "Peyman Milanfar", "Vishal M. Patel", "Hossein Talebi"], "title": "Reference-Guided Identity Preserving Face Restoration", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Preserving face identity is a critical yet persistent challenge in\ndiffusion-based image restoration. While reference faces offer a path forward,\nexisting reference-based methods often fail to fully exploit their potential.\nThis paper introduces a novel approach that maximizes reference face utility\nfor improved face restoration and identity preservation. Our method makes three\nkey contributions: 1) Composite Context, a comprehensive representation that\nfuses multi-level (high- and low-level) information from the reference face,\noffering richer guidance than prior singular representations. 2) Hard Example\nIdentity Loss, a novel loss function that leverages the reference face to\naddress the identity learning inefficiencies found in the existing identity\nloss. 3) A training-free method to adapt the model to multi-reference inputs\nduring inference. The proposed method demonstrably restores high-quality faces\nand achieves state-of-the-art identity preserving restoration on benchmarks\nsuch as FFHQ-Ref and CelebA-Ref-Test, consistently outperforming previous work."}
{"id": "2505.21576", "pdf": "https://arxiv.org/pdf/2505.21576", "abs": "https://arxiv.org/abs/2505.21576", "authors": ["Jiawei Tang", "Yuheng Jia"], "title": "Concentration Distribution Learning from Label Distributions", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Label distribution learning (LDL) is an effective method to predict the\nrelative label description degree (a.k.a. label distribution) of a sample.\nHowever, the label distribution is not a complete representation of an instance\nbecause it overlooks the absolute intensity of each label. Specifically, it's\nimpossible to obtain the total description degree of hidden labels that not in\nthe label space, which leads to the loss of information and confusion in\ninstances. To solve the above problem, we come up with a new concept named\nbackground concentration to serve as the absolute description degree term of\nthe label distribution and introduce it into the LDL process, forming the\nimproved paradigm of concentration distribution learning. Moreover, we propose\na novel model by probabilistic methods and neural networks to learn label\ndistributions and background concentrations from existing LDL datasets.\nExtensive experiments prove that the proposed approach is able to extract\nbackground concentrations from label distributions while producing more\naccurate prediction results than the state-of-the-art LDL methods. The code is\navailable in https://github.com/seutjw/CDL-LD."}
{"id": "2505.22179", "pdf": "https://arxiv.org/pdf/2505.22179", "abs": "https://arxiv.org/abs/2505.22179", "authors": ["Yudi Zhang", "Weilin Zhao", "Xu Han", "Tiejun Zhao", "Wang Xu", "Hailong Cao", "Conghui Zhu"], "title": "Speculative Decoding Meets Quantization: Compatibility Evaluation and Hierarchical Framework Design", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "12 pages, 5 figures", "summary": "Speculative decoding and quantization effectively accelerate memory-bound\ninference of large language models. Speculative decoding mitigates the memory\nbandwidth bottleneck by verifying multiple tokens within a single forward pass,\nwhich increases computational effort. Quantization achieves this optimization\nby compressing weights and activations into lower bit-widths and also reduces\ncomputations via low-bit matrix multiplications. To further leverage their\nstrengths, we investigate the integration of these two techniques.\nSurprisingly, experiments applying the advanced speculative decoding method\nEAGLE-2 to various quantized models reveal that the memory benefits from 4-bit\nweight quantization are diminished by the computational load from speculative\ndecoding. Specifically, verifying a tree-style draft incurs significantly more\ntime overhead than a single-token forward pass on 4-bit weight quantized\nmodels. This finding led to our new speculative decoding design: a hierarchical\nframework that employs a small model as an intermediate stage to turn\ntree-style drafts into sequence drafts, leveraging the memory access benefits\nof the target quantized model. Experimental results show that our hierarchical\napproach achieves a 2.78$\\times$ speedup across various tasks for the 4-bit\nweight Llama-3-70B model on an A100 GPU, outperforming EAGLE-2 by 1.31$\\times$.\nCode available at https://github.com/AI9Stars/SpecMQuant."}
{"id": "2505.21911", "pdf": "https://arxiv.org/pdf/2505.21911", "abs": "https://arxiv.org/abs/2505.21911", "authors": ["Yiheng Lin", "Shifang Zhao", "Ting Liu", "Xiaochao Qu", "Luoqi Liu", "Yao Zhao", "Yunchao Wei"], "title": "AlignGen: Boosting Personalized Image Generation with Cross-Modality Prior Alignment", "categories": ["cs.CV"], "comment": null, "summary": "Personalized image generation aims to integrate user-provided concepts into\ntext-to-image models, enabling the generation of customized content based on a\ngiven prompt. Recent zero-shot approaches, particularly those leveraging\ndiffusion transformers, incorporate reference image information through\nmulti-modal attention mechanism. This integration allows the generated output\nto be influenced by both the textual prior from the prompt and the visual prior\nfrom the reference image. However, we observe that when the prompt and\nreference image are misaligned, the generated results exhibit a stronger bias\ntoward the textual prior, leading to a significant loss of reference content.\nTo address this issue, we propose AlignGen, a Cross-Modality Prior Alignment\nmechanism that enhances personalized image generation by: 1) introducing a\nlearnable token to bridge the gap between the textual and visual priors, 2)\nincorporating a robust training strategy to ensure proper prior alignment, and\n3) employing a selective cross-modal attention mask within the multi-modal\nattention mechanism to further align the priors. Experimental results\ndemonstrate that AlignGen outperforms existing zero-shot methods and even\nsurpasses popular test-time optimization approaches."}
{"id": "2505.21577", "pdf": "https://arxiv.org/pdf/2505.21577", "abs": "https://arxiv.org/abs/2505.21577", "authors": ["Huacan Wang", "Ziyi Ni", "Shuo Zhang", "Shuo Lu", "Sen Hu", "Ziyang He", "Chen Hu", "Jiaye Lin", "Yifu Guo", "Yuntao Du", "Pin Lyu"], "title": "RepoMaster: Autonomous Exploration and Understanding of GitHub Repositories for Complex Task Solving", "categories": ["cs.SE", "cs.AI"], "comment": "A novel approach; Very practical", "summary": "The ultimate goal of code agents is to solve complex tasks autonomously.\nAlthough large language models (LLMs) have made substantial progress in code\ngeneration, real-world tasks typically demand full-fledged code repositories\nrather than simple scripts. Building such repositories from scratch remains a\nmajor challenge. Fortunately, GitHub hosts a vast, evolving collection of\nopen-source repositories, which developers frequently reuse as modular\ncomponents for complex tasks. Yet, existing frameworks like OpenHands and\nSWE-Agent still struggle to effectively leverage these valuable resources.\nRelying solely on README files provides insufficient guidance, and deeper\nexploration reveals two core obstacles: overwhelming information and tangled\ndependencies of repositories, both constrained by the limited context windows\nof current LLMs. To tackle these issues, we propose RepoMaster, an autonomous\nagent framework designed to explore and reuse GitHub repositories for solving\ncomplex tasks. For efficient understanding, RepoMaster constructs function-call\ngraphs, module-dependency graphs, and hierarchical code trees to identify\nessential components, providing only identified core elements to the LLMs\nrather than the entire repository. During autonomous execution, it\nprogressively explores related components using our exploration tools and\nprunes information to optimize context usage. Evaluated on the adjusted\nMLE-bench, RepoMaster achieves a 110% relative boost in valid submissions over\nthe strongest baseline OpenHands. On our newly released GitTaskBench,\nRepoMaster lifts the task-pass rate from 24.1% to 62.9% while reducing token\nusage by 95%. Our code and demonstration materials are publicly available at\nhttps://github.com/wanghuacan/RepoMaster."}
{"id": "2505.22184", "pdf": "https://arxiv.org/pdf/2505.22184", "abs": "https://arxiv.org/abs/2505.22184", "authors": ["Xuchen Ma", "Jianxiang Yu", "Wenming Shao", "Bo Pang", "Xiang Li"], "title": "Breaking the Cloak! Unveiling Chinese Cloaked Toxicity with Homophone Graph and Toxic Lexicon", "categories": ["cs.CL", "cs.AI"], "comment": "25 pages, 5 figures, 9 tables", "summary": "Social media platforms have experienced a significant rise in toxic content,\nincluding abusive language and discriminatory remarks, presenting growing\nchallenges for content moderation. Some users evade censorship by deliberately\ndisguising toxic words through homophonic cloak, which necessitates the task of\nunveiling cloaked toxicity. Existing methods are mostly designed for English\ntexts, while Chinese cloaked toxicity unveiling has not been solved yet. To\ntackle the issue, we propose C$^2$TU, a novel training-free and prompt-free\nmethod for Chinese cloaked toxic content unveiling. It first employs substring\nmatching to identify candidate toxic words based on Chinese homo-graph and\ntoxic lexicon. Then it filters those candidates that are non-toxic and corrects\ncloaks to be their corresponding toxicities. Specifically, we develop two model\nvariants for filtering, which are based on BERT and LLMs, respectively. For\nLLMs, we address the auto-regressive limitation in computing word occurrence\nprobability and utilize the full semantic contexts of a text sequence to reveal\ncloaked toxic words. Extensive experiments demonstrate that C$^2$TU can achieve\nsuperior performance on two Chinese toxic datasets. In particular, our method\noutperforms the best competitor by up to 71% on the F1 score and 35% on\naccuracy, respectively."}
{"id": "2505.21914", "pdf": "https://arxiv.org/pdf/2505.21914", "abs": "https://arxiv.org/abs/2505.21914", "authors": ["Chenfeng Wei", "Qi Wu", "Si Zuo", "Jiahua Xu", "Boyang Zhao", "Zeyu Yang", "Guotao Xie", "Shenhong Wang"], "title": "LiDARDustX: A LiDAR Dataset for Dusty Unstructured Road Environments", "categories": ["cs.CV"], "comment": null, "summary": "Autonomous driving datasets are essential for validating the progress of\nintelligent vehicle algorithms, which include localization, perception, and\nprediction. However, existing datasets are predominantly focused on structured\nurban environments, which limits the exploration of unstructured and\nspecialized scenarios, particularly those characterized by significant dust\nlevels. This paper introduces the LiDARDustX dataset, which is specifically\ndesigned for perception tasks under high-dust conditions, such as those\nencountered in mining areas. The LiDARDustX dataset consists of 30,000 LiDAR\nframes captured by six different LiDAR sensors, each accompanied by 3D bounding\nbox annotations and point cloud semantic segmentation. Notably, over 80% of the\ndataset comprises dust-affected scenes. By utilizing this dataset, we have\nestablished a benchmark for evaluating the performance of state-of-the-art 3D\ndetection and segmentation algorithms. Additionally, we have analyzed the\nimpact of dust on perception accuracy and delved into the causes of these\neffects. The data and further information can be accessed at:\nhttps://github.com/vincentweikey/LiDARDustX."}
{"id": "2505.21582", "pdf": "https://arxiv.org/pdf/2505.21582", "abs": "https://arxiv.org/abs/2505.21582", "authors": ["Christopher Knievel", "Alexander Bernhardt", "Christian Bernhardt"], "title": "AITEE -- Agentic Tutor for Electrical Engineering", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "12 pages, 11 figures, 6 tables", "summary": "Intelligent tutoring systems combined with large language models offer a\npromising approach to address students' diverse needs and promote\nself-efficacious learning. While large language models possess good\nfoundational knowledge of electrical engineering basics, they remain\ninsufficiently capable of addressing specific questions about electrical\ncircuits. In this paper, we present AITEE, an agent-based tutoring system for\nelectrical engineering designed to accompany students throughout their learning\nprocess, offer individualized support, and promote self-directed learning.\nAITEE supports both hand-drawn and digital circuits through an adapted circuit\nreconstruction process, enabling natural interaction with students. Our novel\ngraph-based similarity measure identifies relevant context from lecture\nmaterials through a retrieval augmented generation approach, while parallel\nSpice simulation further enhances accuracy in applying solution methodologies.\nThe system implements a Socratic dialogue to foster learner autonomy through\nguided questioning. Experimental evaluations demonstrate that AITEE\nsignificantly outperforms baseline approaches in domain-specific knowledge\napplication, with even medium-sized LLM models showing acceptable performance.\nOur results highlight the potential of agentic tutors to deliver scalable,\npersonalized, and effective learning environments for electrical engineering\neducation."}
{"id": "2505.22202", "pdf": "https://arxiv.org/pdf/2505.22202", "abs": "https://arxiv.org/abs/2505.22202", "authors": ["Hyeonbin Hwang", "Byeongguk Jeon", "Seungone Kim", "Jiyeon Kim", "Hoyeon Chang", "Sohee Yang", "Seungpil Won", "Dohaeng Lee", "Youbin Ahn", "Minjoon Seo"], "title": "Let's Predict Sentence by Sentence", "categories": ["cs.CL", "cs.AI"], "comment": "Work In Progress", "summary": "Autoregressive language models (LMs) generate one token at a time, yet human\nreasoning operates over higher-level abstractions - sentences, propositions,\nand concepts. This contrast raises a central question- Can LMs likewise learn\nto reason over structured semantic units rather than raw token sequences? In\nthis work, we investigate whether pretrained LMs can be lifted into such\nabstract reasoning spaces by building on their learned representations. We\npresent a framework that adapts a pretrained token-level LM to operate in\nsentence space by autoregressively predicting continuous embeddings of next\nsentences. We explore two embedding paradigms inspired by classical\nrepresentation learning: 1) semantic embeddings, learned via autoencoding to\npreserve surface meaning; and 2) contextual embeddings, trained via\nnext-sentence prediction to encode anticipatory structure. We evaluate both\nunder two inference regimes: Discretized, which decodes each predicted\nembedding into text before re-encoding; and Continuous, which reasons entirely\nin embedding space for improved efficiency. Across four domains - mathematics,\nlogic, commonsense, and planning - contextual embeddings under continuous\ninference show competitive performance with Chain-of-Thought (CoT) while\nreducing inference-time FLOPs on average by half. We also present early signs\nof scalability and modular adaptation. Finally, to visualize latent\ntrajectories, we introduce SentenceLens, a diagnostic tool that decodes\nintermediate model states into interpretable sentences. Together, our results\nindicate that pretrained LMs can effectively transition to abstract, structured\nreasoning within latent embedding spaces."}
{"id": "2505.21915", "pdf": "https://arxiv.org/pdf/2505.21915", "abs": "https://arxiv.org/abs/2505.21915", "authors": ["Mir Sazzat Hossain", "Ovi Paul", "Md Akil Raihan Iftee", "Rakibul Hasan Rajib", "Abu Bakar Siddik Nayem", "Anis Sarker", "Arshad Momen", "Md. Ashraful Amin", "Amin Ahsan Ali", "AKM Mahbubur Rahman"], "title": "BD Open LULC Map: High-resolution land use land cover mapping & benchmarking for urban development in Dhaka, Bangladesh", "categories": ["cs.CV"], "comment": "6 pages, 5 figures, 3 tables, Accepted In ICIP 2025", "summary": "Land Use Land Cover (LULC) mapping using deep learning significantly enhances\nthe reliability of LULC classification, aiding in understanding geography,\nsocioeconomic conditions, poverty levels, and urban sprawl. However, the\nscarcity of annotated satellite data, especially in South/East Asian developing\ncountries, poses a major challenge due to limited funding, diverse\ninfrastructures, and dense populations. In this work, we introduce the BD Open\nLULC Map (BOLM), providing pixel-wise LULC annotations across eleven classes\n(e.g., Farmland, Water, Forest, Urban Structure, Rural Built-Up) for Dhaka\nmetropolitan city and its surroundings using high-resolution Bing satellite\nimagery (2.22 m/pixel). BOLM spans 4,392 sq km (891 million pixels), with\nground truth validated through a three-stage process involving GIS experts. We\nbenchmark LULC segmentation using DeepLab V3+ across five major classes and\ncompare performance on Bing and Sentinel-2A imagery. BOLM aims to support\nreliable deep models and domain adaptation tasks, addressing critical LULC\ndataset gaps in South/East Asia."}
{"id": "2505.21584", "pdf": "https://arxiv.org/pdf/2505.21584", "abs": "https://arxiv.org/abs/2505.21584", "authors": ["Afaf Taik", "Khaoula Chehbouni", "Golnoosh Farnadi"], "title": "Fairness in Federated Learning: Fairness for Whom?", "categories": ["cs.LG", "cs.AI", "cs.CY"], "comment": null, "summary": "Fairness in federated learning has emerged as a rapidly growing area of\nresearch, with numerous works proposing formal definitions and algorithmic\ninterventions. Yet, despite this technical progress, fairness in FL is often\ndefined and evaluated in ways that abstract away from the sociotechnical\ncontexts in which these systems are deployed. In this paper, we argue that\nexisting approaches tend to optimize narrow system level metrics, such as\nperformance parity or contribution-based rewards, while overlooking how harms\narise throughout the FL lifecycle and how they impact diverse stakeholders. We\nsupport this claim through a critical analysis of the literature, based on a\nsystematic annotation of papers for their fairness definitions, design\ndecisions, evaluation practices, and motivating use cases. Our analysis reveals\nfive recurring pitfalls: 1) fairness framed solely through the lens of server\nclient architecture, 2) a mismatch between simulations and motivating use-cases\nand contexts, 3) definitions that conflate protecting the system with\nprotecting its users, 4) interventions that target isolated stages of the\nlifecycle while neglecting upstream and downstream effects, 5) and a lack of\nmulti-stakeholder alignment where multiple fairness definitions can be relevant\nat once. Building on these insights, we propose a harm centered framework that\nlinks fairness definitions to concrete risks and stakeholder vulnerabilities.\nWe conclude with recommendations for more holistic, context-aware, and\naccountable fairness research in FL."}
{"id": "2505.22232", "pdf": "https://arxiv.org/pdf/2505.22232", "abs": "https://arxiv.org/abs/2505.22232", "authors": ["Mehdi Ali", "Manuel Brack", "Max Lübbering", "Elias Wendt", "Abbas Goher Khan", "Richard Rutmann", "Alex Jude", "Maurice Kraus", "Alexander Arno Weber", "Felix Stollenwerk", "David Kaczér", "Florian Mai", "Lucie Flek", "Rafet Sifa", "Nicolas Flores-Herr", "Joachim Köhler", "Patrick Schramowski", "Michael Fromm", "Kristian Kersting"], "title": "Judging Quality Across Languages: A Multilingual Approach to Pretraining Data Filtering with Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Project page available at https://huggingface.co/spaces/Jackal-AI/JQL", "summary": "High-quality multilingual training data is essential for effectively\npretraining large language models (LLMs). Yet, the availability of suitable\nopen-source multilingual datasets remains limited. Existing state-of-the-art\ndatasets mostly rely on heuristic filtering methods, restricting both their\ncross-lingual transferability and scalability. Here, we introduce JQL, a\nsystematic approach that efficiently curates diverse and high-quality\nmultilingual data at scale while significantly reducing computational demands.\nJQL distills LLMs' annotation capabilities into lightweight annotators based on\npretrained multilingual embeddings. These models exhibit robust multilingual\nand cross-lingual performance, even for languages and scripts unseen during\ntraining. Evaluated empirically across 35 languages, the resulting annotation\npipeline substantially outperforms current heuristic filtering methods like\nFineweb2. JQL notably enhances downstream model training quality and increases\ndata retention rates. Our research provides practical insights and valuable\nresources for multilingual data curation, raising the standards of multilingual\ndataset development."}
{"id": "2505.21920", "pdf": "https://arxiv.org/pdf/2505.21920", "abs": "https://arxiv.org/abs/2505.21920", "authors": ["Yuanhong Zhang", "Muyao Yuan", "Weizhan Zhang", "Tieliang Gong", "Wen Wen", "Jiangyong Ying", "Weijie Shi"], "title": "InfoSAM: Fine-Tuning the Segment Anything Model from An Information-Theoretic Perspective", "categories": ["cs.CV"], "comment": "Accepted by ICML 2025 (Highlight)", "summary": "The Segment Anything Model (SAM), a vision foundation model, exhibits\nimpressive zero-shot capabilities in general tasks but struggles in specialized\ndomains. Parameter-efficient fine-tuning (PEFT) is a promising approach to\nunleash the potential of SAM in novel scenarios. However, existing PEFT methods\nfor SAM neglect the domain-invariant relations encoded in the pre-trained\nmodel. To bridge this gap, we propose InfoSAM, an information-theoretic\napproach that enhances SAM fine-tuning by distilling and preserving its\npre-trained segmentation knowledge. Specifically, we formulate the knowledge\ntransfer process as two novel mutual information-based objectives: (i) to\ncompress the domain-invariant relation extracted from pre-trained SAM,\nexcluding pseudo-invariant information as possible, and (ii) to maximize mutual\ninformation between the relational knowledge learned by the teacher\n(pre-trained SAM) and the student (fine-tuned model). The proposed InfoSAM\nestablishes a robust distillation framework for PEFT of SAM. Extensive\nexperiments across diverse benchmarks validate InfoSAM's effectiveness in\nimproving SAM family's performance on real-world tasks, demonstrating its\nadaptability and superiority in handling specialized scenarios."}
{"id": "2505.21587", "pdf": "https://arxiv.org/pdf/2505.21587", "abs": "https://arxiv.org/abs/2505.21587", "authors": ["Bin Qin", "Qirui Ji", "Jiangmeng Li", "Yupeng Wang", "Xuesong Wu", "Jianwen Cao", "Fanjiang Xu"], "title": "CellCLAT: Preserving Topology and Trimming Redundancy in Self-Supervised Cellular Contrastive Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Self-supervised topological deep learning (TDL) represents a nascent but\nunderexplored area with significant potential for modeling higher-order\ninteractions in simplicial complexes and cellular complexes to derive\nrepresentations of unlabeled graphs. Compared to simplicial complexes, cellular\ncomplexes exhibit greater expressive power. However, the advancement in\nself-supervised learning for cellular TDL is largely hindered by two core\nchallenges: \\textit{extrinsic structural constraints} inherent to cellular\ncomplexes, and intrinsic semantic redundancy in cellular representations. The\nfirst challenge highlights that traditional graph augmentation techniques may\ncompromise the integrity of higher-order cellular interactions, while the\nsecond underscores that topological redundancy in cellular complexes\npotentially diminish task-relevant information. To address these issues, we\nintroduce Cellular Complex Contrastive Learning with Adaptive Trimming\n(CellCLAT), a twofold framework designed to adhere to the combinatorial\nconstraints of cellular complexes while mitigating informational redundancy.\nSpecifically, we propose a parameter perturbation-based augmentation method\nthat injects controlled noise into cellular interactions without altering the\nunderlying cellular structures, thereby preserving cellular topology during\ncontrastive learning. Additionally, a cellular trimming scheduler is employed\nto mask gradient contributions from task-irrelevant cells through a bi-level\nmeta-learning approach, effectively removing redundant topological elements\nwhile maintaining critical higher-order semantics. We provide theoretical\njustification and empirical validation to demonstrate that CellCLAT achieves\nsubstantial improvements over existing self-supervised graph learning methods,\nmarking a significant attempt in this domain."}
{"id": "2505.22236", "pdf": "https://arxiv.org/pdf/2505.22236", "abs": "https://arxiv.org/abs/2505.22236", "authors": ["Charlotte Pouw", "Afra Alishahi", "Willem Zuidema"], "title": "A Linguistically Motivated Analysis of Intonational Phrasing in Text-to-Speech Systems: Revealing Gaps in Syntactic Sensitivity", "categories": ["cs.CL"], "comment": "Accepted to CoNLL 2025", "summary": "We analyze the syntactic sensitivity of Text-to-Speech (TTS) systems using\nmethods inspired by psycholinguistic research. Specifically, we focus on the\ngeneration of intonational phrase boundaries, which can often be predicted by\nidentifying syntactic boundaries within a sentence. We find that TTS systems\nstruggle to accurately generate intonational phrase boundaries in sentences\nwhere syntactic boundaries are ambiguous (e.g., garden path sentences or\nsentences with attachment ambiguity). In these cases, systems need superficial\ncues such as commas to place boundaries at the correct positions. In contrast,\nfor sentences with simpler syntactic structures, we find that systems do\nincorporate syntactic cues beyond surface markers. Finally, we finetune models\non sentences without commas at the syntactic boundary positions, encouraging\nthem to focus on more subtle linguistic cues. Our findings indicate that this\nleads to more distinct intonation patterns that better reflect the underlying\nstructure."}
{"id": "2505.21943", "pdf": "https://arxiv.org/pdf/2505.21943", "abs": "https://arxiv.org/abs/2505.21943", "authors": ["Wei Lin", "Chenyang Zhao", "Antoni B. Chan"], "title": "Point-to-Region Loss for Semi-Supervised Point-Based Crowd Counting", "categories": ["cs.CV"], "comment": "accepted by CVPR-2025(highlight)", "summary": "Point detection has been developed to locate pedestrians in crowded scenes by\ntraining a counter through a point-to-point (P2P) supervision scheme. Despite\nits excellent localization and counting performance, training a point-based\ncounter still faces challenges concerning annotation labor: hundreds to\nthousands of points are required to annotate a single sample capturing a dense\ncrowd. In this paper, we integrate point-based methods into a semi-supervised\ncounting framework based on pseudo-labeling, enabling the training of a counter\nwith only a few annotated samples supplemented by a large volume of\npseudo-labeled data. However, during implementation, the training encounters\nissues as the confidence for pseudo-labels fails to be propagated to background\npixels via the P2P. To tackle this challenge, we devise a point-specific\nactivation map (PSAM) to visually interpret the phenomena occurring during the\nill-posed training. Observations from the PSAM suggest that the feature map is\nexcessively activated by the loss for unlabeled data, causing the decoder to\nmisinterpret these over-activations as pedestrians. To mitigate this issue, we\npropose a point-to-region (P2R) scheme to substitute P2P, which segments out\nlocal regions rather than detects a point corresponding to a pedestrian for\nsupervision. Consequently, pixels in the local region can share the same\nconfidence with the corresponding pseudo points. Experimental results in both\nsemi-supervised counting and unsupervised domain adaptation highlight the\nadvantages of our method, illustrating P2R can resolve issues identified in\nPSAM. The code is available at https://github.com/Elin24/P2RLoss."}
{"id": "2505.21588", "pdf": "https://arxiv.org/pdf/2505.21588", "abs": "https://arxiv.org/abs/2505.21588", "authors": ["Young-Min Cho", "Sharath Chandra Guntuku", "Lyle Ungar"], "title": "Herd Behavior: Investigating Peer Influence in LLM-based Multi-Agent Systems", "categories": ["cs.MA", "cs.AI"], "comment": "Preprint", "summary": "Recent advancements in Large Language Models (LLMs) have enabled the\nemergence of multi-agent systems where LLMs interact, collaborate, and make\ndecisions in shared environments. While individual model behavior has been\nextensively studied, the dynamics of peer influence in such systems remain\nunderexplored. In this paper, we investigate herd behavior, the tendency of\nagents to align their outputs with those of their peers, within LLM-based\nmulti-agent interactions. We present a series of controlled experiments that\nreveal how herd behaviors are shaped by multiple factors. First, we show that\nthe gap between self-confidence and perceived confidence in peers significantly\nimpacts an agent's likelihood to conform. Second, we find that the format in\nwhich peer information is presented plays a critical role in modulating the\nstrength of herd behavior. Finally, we demonstrate that the degree of herd\nbehavior can be systematically controlled, and that appropriately calibrated\nherd tendencies can enhance collaborative outcomes. These findings offer new\ninsights into the social dynamics of LLM-based systems and open pathways for\ndesigning more effective and adaptive multi-agent collaboration frameworks."}
{"id": "2505.22240", "pdf": "https://arxiv.org/pdf/2505.22240", "abs": "https://arxiv.org/abs/2505.22240", "authors": ["Yunsoo Kim", "Yusuf Abdulle", "Honghan Wu"], "title": "BioHopR: A Benchmark for Multi-Hop, Multi-Answer Reasoning in Biomedical Domain", "categories": ["cs.CL"], "comment": null, "summary": "Biomedical reasoning often requires traversing interconnected relationships\nacross entities such as drugs, diseases, and proteins. Despite the increasing\nprominence of large language models (LLMs), existing benchmarks lack the\nability to evaluate multi-hop reasoning in the biomedical domain, particularly\nfor queries involving one-to-many and many-to-many relationships. This gap\nleaves the critical challenges of biomedical multi-hop reasoning underexplored.\nTo address this, we introduce BioHopR, a novel benchmark designed to evaluate\nmulti-hop, multi-answer reasoning in structured biomedical knowledge graphs.\nBuilt from the comprehensive PrimeKG, BioHopR includes 1-hop and 2-hop\nreasoning tasks that reflect real-world biomedical complexities.\n  Evaluations of state-of-the-art models reveal that O3-mini, a proprietary\nreasoning-focused model, achieves 37.93% precision on 1-hop tasks and 14.57% on\n2-hop tasks, outperforming proprietary models such as GPT4O and open-source\nbiomedical models including HuatuoGPT-o1-70B and Llama-3.3-70B. However, all\nmodels exhibit significant declines in multi-hop performance, underscoring the\nchallenges of resolving implicit reasoning steps in the biomedical domain. By\naddressing the lack of benchmarks for multi-hop reasoning in biomedical domain,\nBioHopR sets a new standard for evaluating reasoning capabilities and\nhighlights critical gaps between proprietary and open-source models while\npaving the way for future advancements in biomedical LLMs."}
{"id": "2505.21954", "pdf": "https://arxiv.org/pdf/2505.21954", "abs": "https://arxiv.org/abs/2505.21954", "authors": ["Le Thien Phuc Nguyen", "Zhuoran Yu", "Khoa Quang Nhat Cao", "Yuwei Guo", "Tu Ho Manh Pham", "Tuan Tai Nguyen", "Toan Ngo Duc Vo", "Lucas Poon", "Soochahn Lee", "Yong Jae Lee"], "title": "UniTalk: Towards Universal Active Speaker Detection in Real World Scenarios", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We present UniTalk, a novel dataset specifically designed for the task of\nactive speaker detection, emphasizing challenging scenarios to enhance model\ngeneralization. Unlike previously established benchmarks such as AVA, which\npredominantly features old movies and thus exhibits significant domain gaps,\nUniTalk focuses explicitly on diverse and difficult real-world conditions.\nThese include underrepresented languages, noisy backgrounds, and crowded scenes\n- such as multiple visible speakers speaking concurrently or in overlapping\nturns. It contains over 44.5 hours of video with frame-level active speaker\nannotations across 48,693 speaking identities, and spans a broad range of video\ntypes that reflect real-world conditions. Through rigorous evaluation, we show\nthat state-of-the-art models, while achieving nearly perfect scores on AVA,\nfail to reach saturation on UniTalk, suggesting that the ASD task remains far\nfrom solved under realistic conditions. Nevertheless, models trained on UniTalk\ndemonstrate stronger generalization to modern \"in-the-wild\" datasets like\nTalkies and ASW, as well as to AVA. UniTalk thus establishes a new benchmark\nfor active speaker detection, providing researchers with a valuable resource\nfor developing and evaluating versatile and resilient models.\n  Dataset: https://huggingface.co/datasets/plnguyen2908/UniTalk-ASD\n  Code: https://github.com/plnguyen2908/UniTalk-ASD-code"}
{"id": "2505.21589", "pdf": "https://arxiv.org/pdf/2505.21589", "abs": "https://arxiv.org/abs/2505.21589", "authors": ["Carina Newen", "Luca Hinkamp", "Maria Ntonti", "Emmanuel Müller"], "title": "Do you see what I see? An Ambiguous Optical Illusion Dataset exposing limitations of Explainable AI", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "19 pages, 18 figures", "summary": "From uncertainty quantification to real-world object detection, we recognize\nthe importance of machine learning algorithms, particularly in safety-critical\ndomains such as autonomous driving or medical diagnostics. In machine learning,\nambiguous data plays an important role in various machine learning domains.\nOptical illusions present a compelling area of study in this context, as they\noffer insight into the limitations of both human and machine perception.\nDespite this relevance, optical illusion datasets remain scarce. In this work,\nwe introduce a novel dataset of optical illusions featuring intermingled animal\npairs designed to evoke perceptual ambiguity. We identify generalizable visual\nconcepts, particularly gaze direction and eye cues, as subtle yet impactful\nfeatures that significantly influence model accuracy. By confronting models\nwith perceptual ambiguity, our findings underscore the importance of concepts\nin visual learning and provide a foundation for studying bias and alignment\nbetween human and machine vision. To make this dataset useful for general\npurposes, we generate optical illusions systematically with different concepts\ndiscussed in our bias mitigation section. The dataset is accessible in Kaggle\nvia\nhttps://kaggle.com/datasets/693bf7c6dd2cb45c8a863f9177350c8f9849a9508e9d50526e2ffcc5559a8333.\nOur source code can be found at\nhttps://github.com/KDD-OpenSource/Ambivision.git."}
{"id": "2505.22264", "pdf": "https://arxiv.org/pdf/2505.22264", "abs": "https://arxiv.org/abs/2505.22264", "authors": ["Maximiliano Hormazábal Lagos", "Álvaro Bueno Saez", "Héctor Cerezo-Costas", "Pedro Alonso Doval", "Jorge Alcalde Vesteiro"], "title": "MRT at SemEval-2025 Task 8: Maximizing Recovery from Tables with Multiple Steps", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "7 pages, 6 tables", "summary": "In this paper we expose our approach to solve the \\textit{SemEval 2025 Task\n8: Question-Answering over Tabular Data} challenge. Our strategy leverages\nPython code generation with LLMs to interact with the table and get the answer\nto the questions. The process is composed of multiple steps: understanding the\ncontent of the table, generating natural language instructions in the form of\nsteps to follow in order to get the answer, translating these instructions to\ncode, running it and handling potential errors or exceptions. These steps use\nopen source LLMs and fine grained optimized prompts for each task (step). With\nthis approach, we achieved a score of $70.50\\%$ for subtask 1."}
{"id": "2505.21955", "pdf": "https://arxiv.org/pdf/2505.21955", "abs": "https://arxiv.org/abs/2505.21955", "authors": ["Insu Lee", "Wooje Park", "Jaeyun Jang", "Minyoung Noh", "Kyuhong Shim", "Byonghyo Shim"], "title": "Towards Comprehensive Scene Understanding: Integrating First and Third-Person Views for LVLMs", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large vision-language models (LVLMs) are increasingly deployed in interactive\napplications such as virtual and augmented reality, where first-person\n(egocentric) view captured by head-mounted cameras serves as key input. While\nthis view offers fine-grained cues about user attention and hand-object\ninteractions, their narrow field of view and lack of global context often lead\nto failures on spatially or contextually demanding queries. To address this, we\nintroduce a framework that augments egocentric inputs with third-person\n(exocentric) views, providing complementary information such as global scene\nlayout and object visibility to LVLMs. We present E3VQA, the first benchmark\nfor multi-view question answering with 4K high-quality question-answer pairs\ngrounded in synchronized ego-exo image pairs. Additionally, we propose M3CoT, a\ntraining-free prompting technique that constructs a unified scene\nrepresentation by integrating scene graphs from three complementary\nperspectives. M3CoT enables LVLMs to reason more effectively across views,\nyielding consistent performance gains (4.84% for GPT-4o and 5.94% for Gemini\n2.0 Flash) over a recent CoT baseline. Our extensive evaluation reveals key\nstrengths and limitations of LVLMs in multi-view reasoning and highlights the\nvalue of leveraging both egocentric and exocentric inputs."}
{"id": "2505.21591", "pdf": "https://arxiv.org/pdf/2505.21591", "abs": "https://arxiv.org/abs/2505.21591", "authors": ["Maosen Zhao", "Pengtao Chen", "Chong Yu", "Yan Wen", "Xudong Tan", "Tao Chen"], "title": "Pioneering 4-Bit FP Quantization for Diffusion Models: Mixup-Sign Quantization and Timestep-Aware Fine-Tuning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Model quantization reduces the bit-width of weights and activations,\nimproving memory efficiency and inference speed in diffusion models. However,\nachieving 4-bit quantization remains challenging. Existing methods, primarily\nbased on integer quantization and post-training quantization fine-tuning,\nstruggle with inconsistent performance. Inspired by the success of\nfloating-point (FP) quantization in large language models, we explore low-bit\nFP quantization for diffusion models and identify key challenges: the failure\nof signed FP quantization to handle asymmetric activation distributions, the\ninsufficient consideration of temporal complexity in the denoising process\nduring fine-tuning, and the misalignment between fine-tuning loss and\nquantization error. To address these challenges, we propose the mixup-sign\nfloating-point quantization (MSFP) framework, first introducing unsigned FP\nquantization in model quantization, along with timestep-aware LoRA (TALoRA) and\ndenoising-factor loss alignment (DFA), which ensure precise and stable\nfine-tuning. Extensive experiments show that we are the first to achieve\nsuperior performance in 4-bit FP quantization for diffusion models,\noutperforming existing PTQ fine-tuning methods in 4-bit INT quantization."}
{"id": "2505.22273", "pdf": "https://arxiv.org/pdf/2505.22273", "abs": "https://arxiv.org/abs/2505.22273", "authors": ["Shohei Higashiyama", "Masao Utiyama"], "title": "Comprehensive Evaluation on Lexical Normalization: Boundary-Aware Approaches for Unsegmented Languages", "categories": ["cs.CL"], "comment": "23 pages", "summary": "Lexical normalization research has sought to tackle the challenge of\nprocessing informal expressions in user-generated text, yet the absence of\ncomprehensive evaluations leaves it unclear which methods excel across multiple\nperspectives. Focusing on unsegmented languages, we make three key\ncontributions: (1) creating a large-scale, multi-domain Japanese normalization\ndataset, (2) developing normalization methods based on state-of-the-art\npretrained models, and (3) conducting experiments across multiple evaluation\nperspectives. Our experiments show that both encoder-only and decoder-only\napproaches achieve promising results in both accuracy and efficiency."}
{"id": "2505.21956", "pdf": "https://arxiv.org/pdf/2505.21956", "abs": "https://arxiv.org/abs/2505.21956", "authors": ["Mengdan Zhu", "Senhao Cheng", "Guangji Bai", "Yifei Zhang", "Liang Zhao"], "title": "Cross-modal RAG: Sub-dimensional Retrieval-Augmented Text-to-Image Generation", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Text-to-image generation increasingly demands access to domain-specific,\nfine-grained, and rapidly evolving knowledge that pretrained models cannot\nfully capture. Existing Retrieval-Augmented Generation (RAG) methods attempt to\naddress this by retrieving globally relevant images, but they fail when no\nsingle image contains all desired elements from a complex user query. We\npropose Cross-modal RAG, a novel framework that decomposes both queries and\nimages into sub-dimensional components, enabling subquery-aware retrieval and\ngeneration. Our method introduces a hybrid retrieval strategy - combining a\nsub-dimensional sparse retriever with a dense retriever - to identify a\nPareto-optimal set of images, each contributing complementary aspects of the\nquery. During generation, a multimodal large language model is guided to\nselectively condition on relevant visual features aligned to specific\nsubqueries, ensuring subquery-aware image synthesis. Extensive experiments on\nMS-COCO, Flickr30K, WikiArt, CUB, and ImageNet-LT demonstrate that Cross-modal\nRAG significantly outperforms existing baselines in both retrieval and\ngeneration quality, while maintaining high efficiency."}
{"id": "2505.21593", "pdf": "https://arxiv.org/pdf/2505.21593", "abs": "https://arxiv.org/abs/2505.21593", "authors": ["Yang Yang", "Siming Zheng", "Jinwei Chen", "Boxi Wu", "Xiaofei He", "Deng Cai", "Bo Li", "Peng-Tao Jiang"], "title": "Any-to-Bokeh: One-Step Video Bokeh via Multi-Plane Image Guided Diffusion", "categories": ["cs.CV", "cs.AI"], "comment": "project page: https://vivocameraresearch.github.io/any2bokeh/", "summary": "Recent advances in diffusion based editing models have enabled realistic\ncamera simulation and image-based bokeh, but video bokeh remains largely\nunexplored. Existing video editing models cannot explicitly control focus\nplanes or adjust bokeh intensity, limiting their applicability for controllable\noptical effects. Moreover, naively extending image-based bokeh methods to video\noften results in temporal flickering and unsatisfactory edge blur transitions\ndue to the lack of temporal modeling and generalization capability. To address\nthese challenges, we propose a novel one-step video bokeh framework that\nconverts arbitrary input videos into temporally coherent, depth-aware bokeh\neffects. Our method leverages a multi-plane image (MPI) representation\nconstructed through a progressively widening depth sampling function, providing\nexplicit geometric guidance for depth-dependent blur synthesis. By conditioning\na single-step video diffusion model on MPI layers and utilizing the strong 3D\npriors from pre-trained models such as Stable Video Diffusion, our approach\nachieves realistic and consistent bokeh effects across diverse scenes.\nAdditionally, we introduce a progressive training strategy to enhance temporal\nconsistency, depth robustness, and detail preservation. Extensive experiments\ndemonstrate that our method produces high-quality, controllable bokeh effects\nand achieves state-of-the-art performance on multiple evaluation benchmarks."}
{"id": "2505.22280", "pdf": "https://arxiv.org/pdf/2505.22280", "abs": "https://arxiv.org/abs/2505.22280", "authors": ["Zihan Xu", "Haotian Ma", "Gongbo Zhang", "Yihao Ding", "Chunhua Weng", "Yifan Peng"], "title": "Natural Language Processing in Support of Evidence-based Medicine: A Scoping Review", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025 Findings", "summary": "Evidence-based medicine (EBM) is at the forefront of modern healthcare,\nemphasizing the use of the best available scientific evidence to guide clinical\ndecisions. Due to the sheer volume and rapid growth of medical literature and\nthe high cost of curation, there is a critical need to investigate Natural\nLanguage Processing (NLP) methods to identify, appraise, synthesize, summarize,\nand disseminate evidence in EBM. This survey presents an in-depth review of 129\nresearch studies on leveraging NLP for EBM, illustrating its pivotal role in\nenhancing clinical decision-making processes. The paper systematically explores\nhow NLP supports the five fundamental steps of EBM -- Ask, Acquire, Appraise,\nApply, and Assess. The review not only identifies current limitations within\nthe field but also proposes directions for future research, emphasizing the\npotential for NLP to revolutionize EBM by refining evidence extraction,\nevidence synthesis, appraisal, summarization, enhancing data comprehensibility,\nand facilitating a more efficient clinical workflow."}
{"id": "2505.21960", "pdf": "https://arxiv.org/pdf/2505.21960", "abs": "https://arxiv.org/abs/2505.21960", "authors": ["Senmao Li", "Lei Wang", "Kai Wang", "Tao Liu", "Jiehang Xie", "Joost van de Weijer", "Fahad Shahbaz Khan", "Shiqi Yang", "Yaxing Wang", "Jian Yang"], "title": "One-Way Ticket:Time-Independent Unified Encoder for Distilling Text-to-Image Diffusion Models", "categories": ["cs.CV"], "comment": "Accepted at CVPR2025, Code: https://github.com/sen-mao/Loopfree", "summary": "Text-to-Image (T2I) diffusion models have made remarkable advancements in\ngenerative modeling; however, they face a trade-off between inference speed and\nimage quality, posing challenges for efficient deployment. Existing distilled\nT2I models can generate high-fidelity images with fewer sampling steps, but\noften struggle with diversity and quality, especially in one-step models. From\nour analysis, we observe redundant computations in the UNet encoders. Our\nfindings suggest that, for T2I diffusion models, decoders are more adept at\ncapturing richer and more explicit semantic information, while encoders can be\neffectively shared across decoders from diverse time steps. Based on these\nobservations, we introduce the first Time-independent Unified Encoder TiUE for\nthe student model UNet architecture, which is a loop-free image generation\napproach for distilling T2I diffusion models. Using a one-pass scheme, TiUE\nshares encoder features across multiple decoder time steps, enabling parallel\nsampling and significantly reducing inference time complexity. In addition, we\nincorporate a KL divergence term to regularize noise prediction, which enhances\nthe perceptual realism and diversity of the generated images. Experimental\nresults demonstrate that TiUE outperforms state-of-the-art methods, including\nLCM, SD-Turbo, and SwiftBrushv2, producing more diverse and realistic results\nwhile maintaining the computational efficiency."}
{"id": "2505.21594", "pdf": "https://arxiv.org/pdf/2505.21594", "abs": "https://arxiv.org/abs/2505.21594", "authors": ["Yeshwanth Venkatesha", "Souvik Kundu", "Priyadarshini Panda"], "title": "Fast and Cost-effective Speculative Edge-Cloud Decoding with Early Exits", "categories": ["cs.RO", "cs.AI", "cs.DC"], "comment": null, "summary": "Large Language Models (LLMs) enable various applications on edge devices such\nas smartphones, wearables, and embodied robots. However, their deployment often\ndepends on expensive cloud-based APIs, creating high operational costs, which\nlimit access for smaller organizations and raise sustainability concerns.\nCertain LLMs can be deployed on-device, offering a cost-effective solution with\nreduced latency and improved privacy. Yet, limited computing resources\nconstrain the size and accuracy of models that can be deployed, necessitating a\ncollaborative design between edge and cloud. We propose a fast and\ncost-effective speculative edge-cloud decoding framework with a large target\nmodel on the server and a small draft model on the device. By introducing early\nexits in the target model, tokens are generated mid-verification, allowing the\nclient to preemptively draft subsequent tokens before final verification, thus\nutilizing idle time and enhancing parallelism between edge and cloud. Using an\nNVIDIA Jetson Nano (client) and an A100 GPU (server) with Vicuna-68M (draft)\nand Llama2-7B (target) models, our method achieves up to a 35% reduction in\nlatency compared to cloud-based autoregressive decoding, with an additional 11%\nimprovement from preemptive drafting. To demonstrate real-world applicability,\nwe deploy our method on the Unitree Go2 quadruped robot using Vision-Language\nModel (VLM) based control, achieving a 21% speedup over traditional cloud-based\nautoregressive decoding. These results demonstrate the potential of our\nframework for real-time LLM and VLM applications on resource-constrained edge\ndevices."}
{"id": "2505.22293", "pdf": "https://arxiv.org/pdf/2505.22293", "abs": "https://arxiv.org/abs/2505.22293", "authors": ["Samuel Frontull", "Thomas Ströhle"], "title": "Compensating for Data with Reasoning: Low-Resource Machine Translation with LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in\nmultilingual machine translation, sometimes even outperforming traditional\nneural systems. However, previous research has highlighted the challenges of\nusing LLMs, particularly with prompt engineering, for low-resource languages.\nIn this work, we introduce Fragment-Shot Prompting, a novel in-context learning\nmethod that segments input and retrieves translation examples based on\nsyntactic coverage, along with Pivoted Fragment-Shot, an extension that enables\ntranslation without direct parallel data. We evaluate these methods using\nGPT-3.5, GPT-4o, o1-mini, LLaMA-3.3, and DeepSeek-R1 for translation between\nItalian and two Ladin variants, revealing three key findings: (1) Fragment-Shot\nPrompting is effective for translating into and between the studied\nlow-resource languages, with syntactic coverage positively correlating with\ntranslation quality; (2) Models with stronger reasoning abilities make more\neffective use of retrieved knowledge, generally produce better translations,\nand enable Pivoted Fragment-Shot to significantly improve translation quality\nbetween the Ladin variants; and (3) prompt engineering offers limited, if any,\nimprovements when translating from a low-resource to a high-resource language,\nwhere zero-shot prompting already yields satisfactory results. We publicly\nrelease our code and the retrieval corpora."}
{"id": "2505.21962", "pdf": "https://arxiv.org/pdf/2505.21962", "abs": "https://arxiv.org/abs/2505.21962", "authors": ["Mengjingcheng Mo", "Xinyang Tong", "Jiaxu Leng", "Mingpi Tan", "Jiankang Zheng", "Yiran Liu", "Haosheng Chen", "Ji Gan", "Weisheng Li", "Xinbo Gao"], "title": "A2Seek: Towards Reasoning-Centric Benchmark for Aerial Anomaly Understanding", "categories": ["cs.CV"], "comment": null, "summary": "While unmanned aerial vehicles (UAVs) offer wide-area, high-altitude coverage\nfor anomaly detection, they face challenges such as dynamic viewpoints, scale\nvariations, and complex scenes. Existing datasets and methods, mainly designed\nfor fixed ground-level views, struggle to adapt to these conditions, leading to\nsignificant performance drops in drone-view scenarios. To bridge this gap, we\nintroduce A2Seek (Aerial Anomaly Seek), a large-scale, reasoning-centric\nbenchmark dataset for aerial anomaly understanding. This dataset covers various\nscenarios and environmental conditions, providing high-resolution real-world\naerial videos with detailed annotations, including anomaly categories,\nframe-level timestamps, region-level bounding boxes, and natural language\nexplanations for causal reasoning. Building on this dataset, we propose\nA2Seek-R1, a novel reasoning framework that generalizes R1-style strategies to\naerial anomaly understanding, enabling a deeper understanding of \"Where\"\nanomalies occur and \"Why\" they happen in aerial frames. To this end, A2Seek-R1\nfirst employs a graph-of-thought (GoT)-guided supervised fine-tuning approach\nto activate the model's latent reasoning capabilities on A2Seek. Then, we\nintroduce Aerial Group Relative Policy Optimization (A-GRPO) to design\nrule-based reward functions tailored to aerial scenarios. Furthermore, we\npropose a novel \"seeking\" mechanism that simulates UAV flight behavior by\ndirecting the model's attention to informative regions. Extensive experiments\ndemonstrate that A2Seek-R1 achieves up to a 22.04% improvement in AP for\nprediction accuracy and a 13.9% gain in mIoU for anomaly localization,\nexhibiting strong generalization across complex environments and\nout-of-distribution scenarios. Our dataset and code will be released at\nhttps://hayneyday.github.io/A2Seek/."}
{"id": "2505.21595", "pdf": "https://arxiv.org/pdf/2505.21595", "abs": "https://arxiv.org/abs/2505.21595", "authors": ["Shreyas Gururaj", "Lars Grüne", "Wojciech Samek", "Sebastian Lapuschkin", "Leander Weber"], "title": "Relevance-driven Input Dropout: an Explanation-guided Regularization Technique", "categories": ["cs.LG", "cs.AI", "I.2.6"], "comment": null, "summary": "Overfitting is a well-known issue extending even to state-of-the-art (SOTA)\nMachine Learning (ML) models, resulting in reduced generalization, and a\nsignificant train-test performance gap. Mitigation measures include a\ncombination of dropout, data augmentation, weight decay, and other\nregularization techniques. Among the various data augmentation strategies,\nocclusion is a prominent technique that typically focuses on randomly masking\nregions of the input during training. Most of the existing literature\nemphasizes randomness in selecting and modifying the input features instead of\nregions that strongly influence model decisions. We propose Relevance-driven\nInput Dropout (RelDrop), a novel data augmentation method which selectively\noccludes the most relevant regions of the input, nudging the model to use other\nimportant features in the prediction process, thus improving model\ngeneralization through informed regularization. We further conduct qualitative\nand quantitative analyses to study how Relevance-driven Input Dropout (RelDrop)\naffects model decision-making. Through a series of experiments on benchmark\ndatasets, we demonstrate that our approach improves robustness towards\nocclusion, results in models utilizing more features within the region of\ninterest, and boosts inference time generalization performance. Our code is\navailable at https://github.com/Shreyas-Gururaj/LRP_Relevance_Dropout."}
{"id": "2505.22296", "pdf": "https://arxiv.org/pdf/2505.22296", "abs": "https://arxiv.org/abs/2505.22296", "authors": ["Haosheng Zou", "Xiaowei Lv", "Shousheng Jia", "Xiangzheng Zhang"], "title": "360-LLaMA-Factory: Plug & Play Sequence Parallelism for Long Post-Training", "categories": ["cs.CL", "cs.LG"], "comment": "code at https://github.com/Qihoo360/360-LLaMA-Factory", "summary": "Adding sequence parallelism into LLaMA-Factory, we open-sourced\n360-LLaMA-Factory at https://github.com/Qihoo360/360-LLaMA-Factory.\n360-LLaMA-Factory has received wide recognition and used in models such as\nLight-R1 arXiv:2503.10460, TinyR1 arXiv:2503.04872, Kaggle AIMO math models and\nalso in large companies' training frameworks. This technical report delves\ndeeper into the different sequence parallel modes behind 360-LLaMA-Factory and\ndiscusses our implementation insights."}
{"id": "2505.21975", "pdf": "https://arxiv.org/pdf/2505.21975", "abs": "https://arxiv.org/abs/2505.21975", "authors": ["Weiguang Zhang", "Huangcheng Lu", "Maizhen Ning", "Xiaowei Huang", "Wei Wang", "Kaizhu Huang", "Qiufeng Wang"], "title": "DvD: Unleashing a Generative Paradigm for Document Dewarping via Coordinates-based Diffusion Model", "categories": ["cs.CV"], "comment": null, "summary": "Document dewarping aims to rectify deformations in photographic document\nimages, thus improving text readability, which has attracted much attention and\nmade great progress, but it is still challenging to preserve document\nstructures. Given recent advances in diffusion models, it is natural for us to\nconsider their potential applicability to document dewarping. However, it is\nfar from straightforward to adopt diffusion models in document dewarping due to\ntheir unfaithful control on highly complex document images (e.g.,\n2000$\\times$3000 resolution). In this paper, we propose DvD, the first\ngenerative model to tackle document \\textbf{D}ewarping \\textbf{v}ia a\n\\textbf{D}iffusion framework. To be specific, DvD introduces a coordinate-level\ndenoising instead of typical pixel-level denoising, generating a mapping for\ndeformation rectification. In addition, we further propose a time-variant\ncondition refinement mechanism to enhance the preservation of document\nstructures. In experiments, we find that current document dewarping benchmarks\ncan not evaluate dewarping models comprehensively. To this end, we present\nAnyPhotoDoc6300, a rigorously designed large-scale document dewarping benchmark\ncomprising 6,300 real image pairs across three distinct domains, enabling\nfine-grained evaluation of dewarping models. Comprehensive experiments\ndemonstrate that our proposed DvD can achieve state-of-the-art performance with\nacceptable computational efficiency on multiple metrics across various\nbenchmarks including DocUNet, DIR300, and AnyPhotoDoc6300. The new benchmark\nand code will be publicly available."}
{"id": "2505.21596", "pdf": "https://arxiv.org/pdf/2505.21596", "abs": "https://arxiv.org/abs/2505.21596", "authors": ["Esra Adiyeke", "Tianqi Liu", "Venkata Sai Dheeraj Naganaboina", "Han Li", "Tyler J. Loftus", "Yuanfang Ren", "Benjamin Shickel", "Matthew M. Ruppert", "Karandeep Singh", "Ruogu Fang", "Parisa Rashidi", "Azra Bihorac", "Tezcan Ozrazgat-Baslanti"], "title": "Learning optimal treatment strategies for intraoperative hypotension using deep reinforcement learning", "categories": ["q-bio.QM", "cs.AI", "cs.LG"], "comment": "41 pages, 1 table, 5 figures, 5 supplemental tables, 6 supplemental\n  figures", "summary": "Traditional methods of surgical decision making heavily rely on human\nexperience and prompt actions, which are variable. A data-driven system\ngenerating treatment recommendations based on patient states can be a\nsubstantial asset in perioperative decision-making, as in cases of\nintraoperative hypotension, for which suboptimal management is associated with\nacute kidney injury (AKI), a common and morbid postoperative complication. We\ndeveloped a Reinforcement Learning (RL) model to recommend optimum dose of\nintravenous (IV) fluid and vasopressors during surgery to avoid intraoperative\nhypotension and postoperative AKI. We retrospectively analyzed 50,021 surgeries\nfrom 42,547 adult patients who underwent major surgery at a quaternary care\nhospital between June 2014 and September 2020. Of these, 34,186 surgeries were\nused for model training and 15,835 surgeries were reserved for testing. We\ndeveloped a Deep Q-Networks based RL model using 16 variables including\nintraoperative physiologic time series, total dose of IV fluid and vasopressors\nextracted for every 15-minute epoch. The model replicated 69% of physician's\ndecisions for the dosage of vasopressors and proposed higher or lower dosage of\nvasopressors than received in 10% and 21% of the treatments, respectively. In\nterms of IV fluids, the model's recommendations were within 0.05 ml/kg/15 min\nof the actual dose in 41% of the cases, with higher or lower doses recommended\nfor 27% and 32% of the treatments, respectively. The model resulted in a higher\nestimated policy value compared to the physicians' actual treatments, as well\nas random and zero-drug policies. AKI prevalence was the lowest in patients\nreceiving medication dosages that aligned with model's decisions. Our findings\nsuggest that implementation of the model's policy has the potential to reduce\npostoperative AKI and improve other outcomes driven by intraoperative\nhypotension."}
{"id": "2505.22298", "pdf": "https://arxiv.org/pdf/2505.22298", "abs": "https://arxiv.org/abs/2505.22298", "authors": ["Yifan Lu", "Jing Li", "Yigeng Zhou", "Yihui Zhang", "Wenya Wang", "Xiucheng Li", "Meishan Zhang", "Fangming Liu", "Jun Yu", "Min Zhang"], "title": "Adaptive Detoxification: Safeguarding General Capabilities of LLMs through Toxicity-Aware Knowledge Editing", "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Large language models (LLMs) exhibit impressive language capabilities but\nremain vulnerable to malicious prompts and jailbreaking attacks. Existing\nknowledge editing methods for LLM detoxification face two major challenges.\nFirst, they often rely on entity-specific localization, making them ineffective\nagainst adversarial inputs without explicit entities. Second, these methods\nsuffer from over-editing, where detoxified models reject legitimate queries,\ncompromising overall performance. In this paper, we propose ToxEdit, a\ntoxicity-aware knowledge editing approach that dynamically detects toxic\nactivation patterns during forward propagation. It then routes computations\nthrough adaptive inter-layer pathways to mitigate toxicity effectively. This\ndesign ensures precise toxicity mitigation while preserving LLMs' general\ncapabilities. To more accurately assess over-editing, we also enhance the\nSafeEdit benchmark by incorporating instruction-following evaluation tasks.\nExperimental results on multiple LLMs demonstrate that our ToxEdit outperforms\nprevious state-of-the-art methods in both detoxification performance and\nsafeguarding general capabilities of LLMs."}
{"id": "2505.21996", "pdf": "https://arxiv.org/pdf/2505.21996", "abs": "https://arxiv.org/abs/2505.21996", "authors": ["Taiye Chen", "Xun Hu", "Zihan Ding", "Chi Jin"], "title": "Learning World Models for Interactive Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Foundational world models must be both interactive and preserve\nspatiotemporal coherence for effective future planning with action choices.\nHowever, present models for long video generation have limited inherent world\nmodeling capabilities due to two main challenges: compounding errors and\ninsufficient memory mechanisms. We enhance image-to-video models with\ninteractive capabilities through additional action conditioning and\nautoregressive framework, and reveal that compounding error is inherently\nirreducible in autoregressive video generation, while insufficient memory\nmechanism leads to incoherence of world models. We propose video retrieval\naugmented generation (VRAG) with explicit global state conditioning, which\nsignificantly reduces long-term compounding errors and increases spatiotemporal\nconsistency of world models. In contrast, naive autoregressive generation with\nextended context windows and retrieval-augmented generation prove less\neffective for video generation, primarily due to the limited in-context\nlearning capabilities of current video models. Our work illuminates the\nfundamental challenges in video world models and establishes a comprehensive\nbenchmark for improving video generation models with internal world modeling\ncapabilities."}
{"id": "2505.21600", "pdf": "https://arxiv.org/pdf/2505.21600", "abs": "https://arxiv.org/abs/2505.21600", "authors": ["Tianyu Fu", "Yi Ge", "Yichen You", "Enshu Liu", "Zhihang Yuan", "Guohao Dai", "Shengen Yan", "Huazhong Yang", "Yu Wang"], "title": "R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large Model Token Routing", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PF", "I.2.7"], "comment": null, "summary": "Large Language Models (LLMs) achieve impressive reasoning capabilities at the\ncost of substantial inference overhead, posing substantial deployment\nchallenges. Although distilled Small Language Models (SLMs) significantly\nenhance efficiency, their performance suffers as they fail to follow LLMs'\nreasoning paths. Luckily, we reveal that only a small fraction of tokens\ngenuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens\nare either identical or exhibit neutral differences, such as minor variations\nin abbreviations or expressions. Leveraging this insight, we introduce **Roads\nto Rome (R2R)**, a neural token routing method that selectively utilizes LLMs\nonly for these critical, path-divergent tokens, while leaving the majority of\ntoken generation to the SLM. We also develop an automatic data generation\npipeline that identifies divergent tokens and generates token-level routing\nlabels to train the lightweight router. We apply R2R to combine R1-1.5B and\nR1-32B models from the DeepSeek family, and evaluate on challenging math,\ncoding, and QA benchmarks. With an average activated parameter size of 5.6B,\nR2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the\nR1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with\ncomparable performance, advancing the Pareto frontier of test-time scaling\nefficiency. Our code is available at https://github.com/thu-nics/R2R."}
{"id": "2505.22318", "pdf": "https://arxiv.org/pdf/2505.22318", "abs": "https://arxiv.org/abs/2505.22318", "authors": ["Ishwar B Balappanawar", "Vamshi Krishna Bonagiri", "Anish R Joishy", "Manas Gaur", "Krishnaprasad Thirunarayan", "Ponnurangam Kumaraguru"], "title": "If Pigs Could Fly... Can LLMs Logically Reason Through Counterfactuals?", "categories": ["cs.CL", "cs.LG"], "comment": "16 pages, 5 figures", "summary": "Large Language Models (LLMs) demonstrate impressive reasoning capabilities in\nfamiliar contexts, but struggle when the context conflicts with their\nparametric knowledge. To investigate this phenomenon, we introduce\nCounterLogic, a dataset containing 1,800 examples across 9 logical schemas,\nexplicitly designed to evaluate logical reasoning through counterfactual\n(hypothetical knowledge-conflicting) scenarios. Our systematic evaluation of 11\nLLMs across 6 different datasets reveals a consistent performance degradation,\nwith accuracies dropping by 27% on average when reasoning through\ncounterfactual information. We propose Self-Segregate, a prompting method\nenabling metacognitive awareness (explicitly identifying knowledge conflicts)\nbefore reasoning. Our method dramatically narrows the average performance gaps\nfrom 27% to just 11%, while significantly increasing the overall accuracy\n(+7.5%). We discuss the implications of these findings and draw parallels to\nhuman cognitive processes, particularly on how humans disambiguate conflicting\ninformation during reasoning tasks. Our findings offer practical insights for\nunderstanding and enhancing LLMs reasoning capabilities in real-world\napplications, especially where models must logically reason independently of\ntheir factual knowledge."}
{"id": "2505.22002", "pdf": "https://arxiv.org/pdf/2505.22002", "abs": "https://arxiv.org/abs/2505.22002", "authors": ["Zijing Hu", "Fengda Zhang", "Kun Kuang"], "title": "D-Fusion: Direct Preference Optimization for Aligning Diffusion Models with Visually Consistent Samples", "categories": ["cs.CV"], "comment": "Accepted to ICML 2025", "summary": "The practical applications of diffusion models have been limited by the\nmisalignment between generated images and corresponding text prompts. Recent\nstudies have introduced direct preference optimization (DPO) to enhance the\nalignment of these models. However, the effectiveness of DPO is constrained by\nthe issue of visual inconsistency, where the significant visual disparity\nbetween well-aligned and poorly-aligned images prevents diffusion models from\nidentifying which factors contribute positively to alignment during\nfine-tuning. To address this issue, this paper introduces D-Fusion, a method to\nconstruct DPO-trainable visually consistent samples. On one hand, by performing\nmask-guided self-attention fusion, the resulting images are not only\nwell-aligned, but also visually consistent with given poorly-aligned images. On\nthe other hand, D-Fusion can retain the denoising trajectories of the resulting\nimages, which are essential for DPO training. Extensive experiments demonstrate\nthe effectiveness of D-Fusion in improving prompt-image alignment when applied\nto different reinforcement learning algorithms."}
{"id": "2505.21603", "pdf": "https://arxiv.org/pdf/2505.21603", "abs": "https://arxiv.org/abs/2505.21603", "authors": ["Andre Massahiro Shimaoka", "Renato Cordeiro Ferreira", "Alfredo Goldman"], "title": "Leveraging XP and CRISP-DM for Agile Data Science Projects", "categories": ["cs.SE", "cs.AI", "cs.LG"], "comment": null, "summary": "This study explores the integration of eXtreme Programming (XP) and the\nCross-Industry Standard Process for Data Mining (CRISP-DM) in agile Data\nScience projects. We conducted a case study at the e-commerce company Elo7 to\nanswer the research question: How can the agility of the XP method be\nintegrated with CRISP-DM in Data Science projects? Data was collected through\ninterviews and questionnaires with a Data Science team consisting of data\nscientists, ML engineers, and data product managers. The results show that 86%\nof the team frequently or always applies CRISP-DM, while 71% adopt XP practices\nin their projects. Furthermore, the study demonstrates that it is possible to\ncombine CRISP-DM with XP in Data Science projects, providing a structured and\ncollaborative approach. Finally, the study generated improvement\nrecommendations for the company."}
{"id": "2505.22323", "pdf": "https://arxiv.org/pdf/2505.22323", "abs": "https://arxiv.org/abs/2505.22323", "authors": ["Hongcan Guo", "Haolang Lu", "Guoshun Nan", "Bolun Chu", "Jialin Zhuang", "Yuan Yang", "Wenhao Che", "Sicong Leng", "Qimei Cui", "Xudong Jiang"], "title": "Advancing Expert Specialization for Better MoE", "categories": ["cs.CL", "cs.SE", "68T07", "I.2.7"], "comment": "33pages, 6figures", "summary": "Mixture-of-Experts (MoE) models enable efficient scaling of large language\nmodels (LLMs) by activating only a subset of experts per input. However, we\nobserve that the commonly used auxiliary load balancing loss often leads to\nexpert overlap and overly uniform routing, which hinders expert specialization\nand degrades overall performance during post-training. To address this, we\npropose a simple yet effective solution that introduces two complementary\nobjectives: (1) an orthogonality loss to encourage experts to process distinct\ntypes of tokens, and (2) a variance loss to encourage more discriminative\nrouting decisions. Gradient-level analysis demonstrates that these objectives\nare compatible with the existing auxiliary loss and contribute to optimizing\nthe training process. Experimental results over various model architectures and\nacross multiple benchmarks show that our method significantly enhances expert\nspecialization. Notably, our method improves classic MoE baselines with\nauxiliary loss by up to 23.79%, while also maintaining load balancing in\ndownstream tasks, without any architectural modifications or additional\ncomponents. We will release our code to contribute to the community."}
{"id": "2505.22007", "pdf": "https://arxiv.org/pdf/2505.22007", "abs": "https://arxiv.org/abs/2505.22007", "authors": ["Wataru Ikeda", "Masashi Hatano", "Ryosei Hara", "Mariko Isogawa"], "title": "Event-based Egocentric Human Pose Estimation in Dynamic Environment", "categories": ["cs.CV"], "comment": "Accepted at ICIP 2025, Project Page:\n  https://wataru823.github.io/D-EventEgo/", "summary": "Estimating human pose using a front-facing egocentric camera is essential for\napplications such as sports motion analysis, VR/AR, and AI for wearable\ndevices. However, many existing methods rely on RGB cameras and do not account\nfor low-light environments or motion blur. Event-based cameras have the\npotential to address these challenges. In this work, we introduce a novel task\nof human pose estimation using a front-facing event-based camera mounted on the\nhead and propose D-EventEgo, the first framework for this task. The proposed\nmethod first estimates the head poses, and then these are used as conditions to\ngenerate body poses. However, when estimating head poses, the presence of\ndynamic objects mixed with background events may reduce head pose estimation\naccuracy. Therefore, we introduce the Motion Segmentation Module to remove\ndynamic objects and extract background information. Extensive experiments on\nour synthetic event-based dataset derived from EgoBody, demonstrate that our\napproach outperforms our baseline in four out of five evaluation metrics in\ndynamic environments."}
{"id": "2505.21604", "pdf": "https://arxiv.org/pdf/2505.21604", "abs": "https://arxiv.org/abs/2505.21604", "authors": ["Kristina Radivojevic", "Caleb Reinking", "Shaun Whitfield", "Paul Brenner"], "title": "Public Discourse Sandbox: Facilitating Human and AI Digital Communication Research", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": null, "summary": "Social media serves as a primary communication and information dissemination\nplatform for major global events, entertainment, and niche or topically focused\ncommunity discussions. Therefore, it represents a valuable resource for\nresearchers who aim to understand numerous questions. However, obtaining data\ncan be difficult, expensive, and often unreliable due to the presence of bots,\nfake accounts, and manipulated content. Additionally, there are ethical\nconcerns if researchers decide to conduct an online experiment without\nexplicitly notifying social media users about their intent. There is a need for\nmore controlled and scalable mechanisms to evaluate the impacts of digital\ndiscussion interventions on audiences. We introduce the Public Discourse\nSandbox (PDS), which serves as a digital discourse research platform for\nhuman-AI as well as AI-AI discourse research, testing, and training. PDS\nprovides a safe and secure space for research experiments that are not viable\non public, commercial social media platforms. Its main purpose is to enable the\nunderstanding of AI behaviors and the impacts of customized AI participants via\ntechniques such as prompt engineering, retrieval-augmented generation (RAG),\nand fine-tuning. We provide a hosted live version of the sandbox to support\nresearchers as well as the open-sourced code on GitHub for community\ncollaboration and contribution."}
{"id": "2505.22327", "pdf": "https://arxiv.org/pdf/2505.22327", "abs": "https://arxiv.org/abs/2505.22327", "authors": ["Antonia Karamolegkou", "Angana Borah", "Eunjung Cho", "Sagnik Ray Choudhury", "Martina Galletti", "Rajarshi Ghosh", "Pranav Gupta", "Oana Ignat", "Priyanka Kargupta", "Neema Kotonya", "Hemank Lamba", "Sun-Joo Lee", "Arushi Mangla", "Ishani Mondal", "Deniz Nazarova", "Poli Nemkova", "Dina Pisarevskaya", "Naquee Rizwan", "Nazanin Sabri", "Dominik Stammbach", "Anna Steinberg", "David Tomás", "Steven R Wilson", "Bowen Yi", "Jessica H Zhu", "Arkaitz Zubiaga", "Anders Søgaard", "Alexander Fraser", "Zhijing Jin", "Rada Mihalcea", "Joel R. Tetreault", "Daryna Dementieva"], "title": "NLP for Social Good: A Survey of Challenges, Opportunities, and Responsible Deployment", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have unlocked\nunprecedented possibilities across a range of applications. However, as a\ncommunity, we believe that the field of Natural Language Processing (NLP) has a\ngrowing need to approach deployment with greater intentionality and\nresponsibility. In alignment with the broader vision of AI for Social Good\n(Toma\\v{s}ev et al., 2020), this paper examines the role of NLP in addressing\npressing societal challenges. Through a cross-disciplinary analysis of social\ngoals and emerging risks, we highlight promising research directions and\noutline challenges that must be addressed to ensure responsible and equitable\nprogress in NLP4SG research."}
{"id": "2505.22011", "pdf": "https://arxiv.org/pdf/2505.22011", "abs": "https://arxiv.org/abs/2505.22011", "authors": ["Menghui Zhang", "Jing Zhang", "Lin Chen", "Li Zhuo"], "title": "Prototype Embedding Optimization for Human-Object Interaction Detection in Livestreaming", "categories": ["cs.CV"], "comment": null, "summary": "Livestreaming often involves interactions between streamers and objects,\nwhich is critical for understanding and regulating web content. While\nhuman-object interaction (HOI) detection has made some progress in\ngeneral-purpose video downstream tasks, when applied to recognize the\ninteraction behaviors between a streamer and different objects in\nlivestreaming, it tends to focuses too much on the objects and neglects their\ninteractions with the streamer, which leads to object bias. To solve this\nissue, we propose a prototype embedding optimization for human-object\ninteraction detection (PeO-HOI). First, the livestreaming is preprocessed using\nobject detection and tracking techniques to extract features of the\nhuman-object (HO) pairs. Then, prototype embedding optimization is adopted to\nmitigate the effect of object bias on HOI. Finally, after modelling the\nspatio-temporal context between HO pairs, the HOI detection results are\nobtained by the prediction head. The experimental results show that the\ndetection accuracy of the proposed PeO-HOI method has detection accuracies of\n37.19%@full, 51.42%@non-rare, 26.20%@rare on the publicly available dataset\nVidHOI, 45.13%@full, 62.78%@non-rare and 30.37%@rare on the self-built dataset\nBJUT-HOI, which effectively improves the HOI detection performance in\nlivestreaming."}
{"id": "2505.21605", "pdf": "https://arxiv.org/pdf/2505.21605", "abs": "https://arxiv.org/abs/2505.21605", "authors": ["Fengqing Jiang", "Fengbo Ma", "Zhangchen Xu", "Yuetai Li", "Bhaskar Ramasubramanian", "Luyao Niu", "Bo Li", "Xianyan Chen", "Zhen Xiang", "Radha Poovendran"], "title": "SOSBENCH: Benchmarking Safety Alignment on Scientific Knowledge", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Large language models (LLMs) exhibit advancing capabilities in complex tasks,\nsuch as reasoning and graduate-level question answering, yet their resilience\nagainst misuse, particularly involving scientifically sophisticated risks,\nremains underexplored. Existing safety benchmarks typically focus either on\ninstructions requiring minimal knowledge comprehension (e.g., ``tell me how to\nbuild a bomb\") or utilize prompts that are relatively low-risk (e.g.,\nmultiple-choice or classification tasks about hazardous content). Consequently,\nthey fail to adequately assess model safety when handling knowledge-intensive,\nhazardous scenarios.\n  To address this critical gap, we introduce SOSBench, a regulation-grounded,\nhazard-focused benchmark encompassing six high-risk scientific domains:\nchemistry, biology, medicine, pharmacology, physics, and psychology. The\nbenchmark comprises 3,000 prompts derived from real-world regulations and laws,\nsystematically expanded via an LLM-assisted evolutionary pipeline that\nintroduces diverse, realistic misuse scenarios (e.g., detailed explosive\nsynthesis instructions involving advanced chemical formulas). We evaluate\nfrontier models within a unified evaluation framework using our SOSBench.\nDespite their alignment claims, advanced models consistently disclose\npolicy-violating content across all domains, demonstrating alarmingly high\nrates of harmful responses (e.g., 79.1% for Deepseek-R1 and 47.3% for GPT-4.1).\nThese results highlight significant safety alignment deficiencies and\nunderscore urgent concerns regarding the responsible deployment of powerful\nLLMs."}
{"id": "2505.22334", "pdf": "https://arxiv.org/pdf/2505.22334", "abs": "https://arxiv.org/abs/2505.22334", "authors": ["Lai Wei", "Yuting Li", "Kaipeng Zheng", "Chen Wang", "Yue Wang", "Linghe Kong", "Lichao Sun", "Weiran Huang"], "title": "Advancing Multimodal Reasoning via Reinforcement Learning with Cold Start", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have demonstrated\nimpressive chain-of-thought reasoning capabilities, with reinforcement learning\n(RL) playing a crucial role in this progress. While \"aha moment\"\npatterns--where models exhibit self-correction through reflection--are often\nattributed to emergent properties from RL, we first demonstrate that these\npatterns exist in multimodal LLMs (MLLMs) prior to RL training but may not\nnecessarily correlate with improved reasoning performance. Building on these\ninsights, we present a comprehensive study on enhancing multimodal reasoning\nthrough a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start\nwith structured chain-of-thought reasoning patterns, followed by (2)\nreinforcement learning via GRPO to further refine these capabilities. Our\nextensive experiments show that this combined approach consistently outperforms\nboth SFT-only and RL-only methods across challenging multimodal reasoning\nbenchmarks. The resulting models achieve state-of-the-art performance among\nopen-source MLLMs at both 3B and 7B scales, with our 7B model showing\nsubstantial improvements over base models (e.g., 66.3 %$\\rightarrow$73.4 % on\nMathVista, 62.9 %$\\rightarrow$70.4 % on We-Math) and our 3B model achieving\nperformance competitive with several 7B models. Overall, this work provides\npractical guidance for building advanced multimodal reasoning models. Our code\nis available at https://github.com/waltonfuture/RL-with-Cold-Start."}
{"id": "2505.22016", "pdf": "https://arxiv.org/pdf/2505.22016", "abs": "https://arxiv.org/abs/2505.22016", "authors": ["Yifei Xia", "Shuchen Weng", "Siqi Yang", "Jingqi Liu", "Chengxuan Zhu", "Minggui Teng", "Zijian Jia", "Han Jiang", "Boxin Shi"], "title": "PanoWan: Lifting Diffusion Video Generation Models to 360° with Latitude/Longitude-aware Mechanisms", "categories": ["cs.CV"], "comment": null, "summary": "Panoramic video generation enables immersive 360{\\deg} content creation,\nvaluable in applications that demand scene-consistent world exploration.\nHowever, existing panoramic video generation models struggle to leverage\npre-trained generative priors from conventional text-to-video models for\nhigh-quality and diverse panoramic videos generation, due to limited dataset\nscale and the gap in spatial feature representations. In this paper, we\nintroduce PanoWan to effectively lift pre-trained text-to-video models to the\npanoramic domain, equipped with minimal modules. PanoWan employs latitude-aware\nsampling to avoid latitudinal distortion, while its rotated semantic denoising\nand padded pixel-wise decoding ensure seamless transitions at longitude\nboundaries. To provide sufficient panoramic videos for learning these lifted\nrepresentations, we contribute PanoVid, a high-quality panoramic video dataset\nwith captions and diverse scenarios. Consequently, PanoWan achieves\nstate-of-the-art performance in panoramic video generation and demonstrates\nrobustness for zero-shot downstream tasks."}
{"id": "2505.21608", "pdf": "https://arxiv.org/pdf/2505.21608", "abs": "https://arxiv.org/abs/2505.21608", "authors": ["Miao Peng", "Nuo Chen", "Jianheng Tang", "Jia Li"], "title": "How does Misinformation Affect Large Language Model Behaviors and Preferences?", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Large Language Models (LLMs) have shown remarkable capabilities in\nknowledge-intensive tasks, while they remain vulnerable when encountering\nmisinformation. Existing studies have explored the role of LLMs in combating\nmisinformation, but there is still a lack of fine-grained analysis on the\nspecific aspects and extent to which LLMs are influenced by misinformation. To\nbridge this gap, we present MisBench, the current largest and most\ncomprehensive benchmark for evaluating LLMs' behavior and knowledge preference\ntoward misinformation. MisBench consists of 10,346,712 pieces of\nmisinformation, which uniquely considers both knowledge-based conflicts and\nstylistic variations in misinformation. Empirical results reveal that while\nLLMs demonstrate comparable abilities in discerning misinformation, they still\nremain susceptible to knowledge conflicts and stylistic variations. Based on\nthese findings, we further propose a novel approach called Reconstruct to\nDiscriminate (RtD) to strengthen LLMs' ability to detect misinformation. Our\nstudy provides valuable insights into LLMs' interactions with misinformation,\nand we believe MisBench can serve as an effective benchmark for evaluating\nLLM-based detectors and enhancing their reliability in real-world applications.\nCodes and data are available at https://github.com/GKNL/MisBench."}
{"id": "2505.22338", "pdf": "https://arxiv.org/pdf/2505.22338", "abs": "https://arxiv.org/abs/2505.22338", "authors": ["Hanyang Wang", "Lu Wang", "Chaoyun Zhang", "Tianjun Mao", "Si Qin", "Qingwei Lin", "Saravan Rajmohan", "Dongmei Zhang"], "title": "Text2Grad: Reinforcement Learning from Natural Language Feedback", "categories": ["cs.CL", "cs.AI"], "comment": "The code for our method is available at\n  https://github.com/microsoft/Text2Grad", "summary": "Traditional RLHF optimizes language models with coarse, scalar rewards that\nmask the fine-grained reasons behind success or failure, leading to slow and\nopaque learning. Recent work augments RL with textual critiques through\nprompting or reflection, improving interpretability but leaving model\nparameters untouched. We introduce Text2Grad, a reinforcement-learning paradigm\nthat turns free-form textual feedback into span-level gradients. Given human\n(or programmatic) critiques, Text2Grad aligns each feedback phrase with the\nrelevant token spans, converts these alignments into differentiable reward\nsignals, and performs gradient updates that directly refine the offending\nportions of the model's policy. This yields precise, feedback-conditioned\nadjustments instead of global nudges. Text2Grad is realized through three\ncomponents: (1) a high-quality feedback-annotation pipeline that pairs\ncritiques with token spans; (2) a fine-grained reward model that predicts\nspan-level reward on answer while generating explanatory critiques; and (3) a\nspan-level policy optimizer that back-propagates natural-language gradients.\nAcross summarization, code generation, and question answering, Text2Grad\nconsistently surpasses scalar-reward RL and prompt-only baselines, providing\nboth higher task metrics and richer interpretability. Our results demonstrate\nthat natural-language feedback, when converted to gradients, is a powerful\nsignal for fine-grained policy optimization. The code for our method is\navailable at https://github.com/microsoft/Text2Grad"}
{"id": "2505.22021", "pdf": "https://arxiv.org/pdf/2505.22021", "abs": "https://arxiv.org/abs/2505.22021", "authors": ["Zhihong Tang", "Yang Li"], "title": "GL-PGENet: A Parameterized Generation Framework for Robust Document Image Enhancement", "categories": ["cs.CV", "cs.AI"], "comment": "12 pages, 7 figures", "summary": "Document Image Enhancement (DIE) serves as a critical component in Document\nAI systems, where its performance substantially determines the effectiveness of\ndownstream tasks. To address the limitations of existing methods confined to\nsingle-degradation restoration or grayscale image processing, we present Global\nwith Local Parametric Generation Enhancement Network (GL-PGENet), a novel\narchitecture designed for multi-degraded color document images, ensuring both\nefficiency and robustness in real-world scenarios. Our solution incorporates\nthree key innovations: First, a hierarchical enhancement framework that\nintegrates global appearance correction with local refinement, enabling\ncoarse-to-fine quality improvement. Second, a Dual-Branch Local-Refine Network\nwith parametric generation mechanisms that replaces conventional direct\nprediction, producing enhanced outputs through learned intermediate parametric\nrepresentations rather than pixel-wise mapping. This approach enhances local\nconsistency while improving model generalization. Finally, a modified NestUNet\narchitecture incorporating dense block to effectively fuse low-level pixel\nfeatures and high-level semantic features, specifically adapted for document\nimage characteristics. In addition, to enhance generalization performance, we\nadopt a two-stage training strategy: large-scale pretraining on a synthetic\ndataset of 500,000+ samples followed by task-specific fine-tuning. Extensive\nexperiments demonstrate the superiority of GL-PGENet, achieving\nstate-of-the-art SSIM scores of 0.7721 on DocUNet and 0.9480 on RealDAE. The\nmodel also exhibits remarkable cross-domain adaptability and maintains\ncomputational efficiency for high-resolution images without performance\ndegradation, confirming its practical utility in real-world scenarios."}
{"id": "2505.21609", "pdf": "https://arxiv.org/pdf/2505.21609", "abs": "https://arxiv.org/abs/2505.21609", "authors": ["Mathew J. Walter", "Aaron Barrett", "Kimberly Tam"], "title": "Preventing Adversarial AI Attacks Against Autonomous Situational Awareness: A Maritime Case Study", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Adversarial artificial intelligence (AI) attacks pose a significant threat to\nautonomous transportation, such as maritime vessels, that rely on AI\ncomponents. Malicious actors can exploit these systems to deceive and\nmanipulate AI-driven operations. This paper addresses three critical research\nchallenges associated with adversarial AI: the limited scope of traditional\ndefences, inadequate security metrics, and the need to build resilience beyond\nmodel-level defences. To address these challenges, we propose building defences\nutilising multiple inputs and data fusion to create defensive components and an\nAI security metric as a novel approach toward developing more secure AI\nsystems. We name this approach the Data Fusion Cyber Resilience (DFCR) method,\nand we evaluate it through real-world demonstrations and comprehensive\nquantitative analyses, comparing a system built with the DFCR method against\nsingle-input models and models utilising existing state-of-the-art defences.\nThe findings show that the DFCR approach significantly enhances resilience\nagainst adversarial machine learning attacks in maritime autonomous system\noperations, achieving up to a 35\\% reduction in loss for successful\nmulti-pronged perturbation attacks, up to a 100\\% reduction in loss for\nsuccessful adversarial patch attacks and up to 100\\% reduction in loss for\nsuccessful spoofing attacks when using these more resilient systems. We\ndemonstrate how DFCR and DFCR confidence scores can reduce adversarial AI\ncontact confidence and improve decision-making by the system, even when typical\nadversarial defences have been compromised. Ultimately, this work contributes\nto the development of more secure and resilient AI-driven systems against\nadversarial attacks."}
{"id": "2505.22354", "pdf": "https://arxiv.org/pdf/2505.22354", "abs": "https://arxiv.org/abs/2505.22354", "authors": ["Judith Sieker", "Clara Lachenmaier", "Sina Zarrieß"], "title": "LLMs Struggle to Reject False Presuppositions when Misinformation Stakes are High", "categories": ["cs.CL"], "comment": "8 pages (including References). Accepted at CogSci 2025", "summary": "This paper examines how LLMs handle false presuppositions and whether certain\nlinguistic factors influence their responses to falsely presupposed content.\nPresuppositions subtly introduce information as given, making them highly\neffective at embedding disputable or false information. This raises concerns\nabout whether LLMs, like humans, may fail to detect and correct misleading\nassumptions introduced as false presuppositions, even when the stakes of\nmisinformation are high. Using a systematic approach based on linguistic\npresupposition analysis, we investigate the conditions under which LLMs are\nmore or less sensitive to adopt or reject false presuppositions. Focusing on\npolitical contexts, we examine how factors like linguistic construction,\npolitical party, and scenario probability impact the recognition of false\npresuppositions. We conduct experiments with a newly created dataset and\nexamine three LLMs: OpenAI's GPT-4-o, Meta's LLama-3-8B, and MistralAI's\nMistral-7B-v03. Our results show that the models struggle to recognize false\npresuppositions, with performance varying by condition. This study highlights\nthat linguistic presupposition analysis is a valuable tool for uncovering the\nreinforcement of political misinformation in LLM responses."}
{"id": "2505.22025", "pdf": "https://arxiv.org/pdf/2505.22025", "abs": "https://arxiv.org/abs/2505.22025", "authors": ["Manchao Bao", "Shengjiang Fang", "Tao Yue", "Xuemei Hu"], "title": "Learnable Burst-Encodable Time-of-Flight Imaging for High-Fidelity Long-Distance Depth Sensing", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Long-distance depth imaging holds great promise for applications such as\nautonomous driving and robotics. Direct time-of-flight (dToF) imaging offers\nhigh-precision, long-distance depth sensing, yet demands ultra-short pulse\nlight sources and high-resolution time-to-digital converters. In contrast,\nindirect time-of-flight (iToF) imaging often suffers from phase wrapping and\nlow signal-to-noise ratio (SNR) as the sensing distance increases. In this\npaper, we introduce a novel ToF imaging paradigm, termed Burst-Encodable\nTime-of-Flight (BE-ToF), which facilitates high-fidelity, long-distance depth\nimaging. Specifically, the BE-ToF system emits light pulses in burst mode and\nestimates the phase delay of the reflected signal over the entire burst period,\nthereby effectively avoiding the phase wrapping inherent to conventional iToF\nsystems. Moreover, to address the low SNR caused by light attenuation over\nincreasing distances, we propose an end-to-end learnable framework that jointly\noptimizes the coding functions and the depth reconstruction network. A\nspecialized double well function and first-order difference term are\nincorporated into the framework to ensure the hardware implementability of the\ncoding functions. The proposed approach is rigorously validated through\ncomprehensive simulations and real-world prototype experiments, demonstrating\nits effectiveness and practical applicability."}
{"id": "2505.21620", "pdf": "https://arxiv.org/pdf/2505.21620", "abs": "https://arxiv.org/abs/2505.21620", "authors": ["Zhengyuan Jiang", "Moyang Guo", "Kecen Li", "Yuepeng Hu", "Yupu Wang", "Zhicong Huang", "Cheng Hong", "Neil Zhenqiang Gong"], "title": "VideoMarkBench: Benchmarking Robustness of Video Watermarking", "categories": ["cs.CR", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "The rapid development of video generative models has led to a surge in highly\nrealistic synthetic videos, raising ethical concerns related to disinformation\nand copyright infringement. Recently, video watermarking has been proposed as a\nmitigation strategy by embedding invisible marks into AI-generated videos to\nenable subsequent detection. However, the robustness of existing video\nwatermarking methods against both common and adversarial perturbations remains\nunderexplored. In this work, we introduce VideoMarkBench, the first systematic\nbenchmark designed to evaluate the robustness of video watermarks under\nwatermark removal and watermark forgery attacks. Our study encompasses a\nunified dataset generated by three state-of-the-art video generative models,\nacross three video styles, incorporating four watermarking methods and seven\naggregation strategies used during detection. We comprehensively evaluate 12\ntypes of perturbations under white-box, black-box, and no-box threat models.\nOur findings reveal significant vulnerabilities in current watermarking\napproaches and highlight the urgent need for more robust solutions. Our code is\navailable at https://github.com/zhengyuan-jiang/VideoMarkBench."}
{"id": "2505.22375", "pdf": "https://arxiv.org/pdf/2505.22375", "abs": "https://arxiv.org/abs/2505.22375", "authors": ["Hanting Chen", "Yasheng Wang", "Kai Han", "Dong Li", "Lin Li", "Zhenni Bi", "Jinpeng Li", "Haoyu Wang", "Fei Mi", "Mingjian Zhu", "Bin Wang", "Kaikai Song", "Yifei Fu", "Xu He", "Yu Luo", "Chong Zhu", "Quan He", "Xueyu Wu", "Wei He", "Hailin Hu", "Yehui Tang", "Dacheng Tao", "Xinghao Chen", "Yunhe Wang", "Other Contributors"], "title": "Pangu Embedded: An Efficient Dual-system LLM Reasoner with Metacognition", "categories": ["cs.CL"], "comment": null, "summary": "This work presents Pangu Embedded, an efficient Large Language Model (LLM)\nreasoner developed on Ascend Neural Processing Units (NPUs), featuring flexible\nfast and slow thinking capabilities. Pangu Embedded addresses the significant\ncomputational costs and inference latency challenges prevalent in existing\nreasoning-optimized LLMs. We propose a two-stage training framework for its\nconstruction. In Stage 1, the model is finetuned via an iterative distillation\nprocess, incorporating inter-iteration model merging to effectively aggregate\ncomplementary knowledge. This is followed by reinforcement learning on Ascend\nclusters, optimized by a latency-tolerant scheduler that combines stale\nsynchronous parallelism with prioritized data queues. The RL process is guided\nby a Multi-source Adaptive Reward System (MARS), which generates dynamic,\ntask-specific reward signals using deterministic metrics and lightweight LLM\nevaluators for mathematics, coding, and general problem-solving tasks. Stage 2\nintroduces a dual-system framework, endowing Pangu Embedded with a \"fast\" mode\nfor routine queries and a deeper \"slow\" mode for complex inference. This\nframework offers both manual mode switching for user control and an automatic,\ncomplexity-aware mode selection mechanism that dynamically allocates\ncomputational resources to balance latency and reasoning depth. Experimental\nresults on benchmarks including AIME 2024, GPQA, and LiveCodeBench demonstrate\nthat Pangu Embedded with 7B parameters, outperforms similar-size models like\nQwen3-8B and GLM4-9B. It delivers rapid responses and state-of-the-art\nreasoning quality within a single, unified model architecture, highlighting a\npromising direction for developing powerful yet practically deployable LLM\nreasoners."}
{"id": "2505.22031", "pdf": "https://arxiv.org/pdf/2505.22031", "abs": "https://arxiv.org/abs/2505.22031", "authors": ["Hasan Yucedag", "Adam Jatowt"], "title": "Guess the Age of Photos: An Interactive Web Platform for Historical Image Age Estimation", "categories": ["cs.CV"], "comment": "4 Pages,4 figures and 1 system architect", "summary": "This paper introduces Guess the Age of Photos, a web platform engaging users\nin estimating the years of historical photographs through two gamified modes:\nGuess the Year (predicting a single image's year) and Timeline Challenge\n(comparing two images to identify the older). Built with Python, Flask,\nBootstrap, and PostgreSQL, it uses a 10,150-image subset of the Date Estimation\nin the Wild dataset (1930-1999). Features like dynamic scoring and leaderboards\nboost engagement. Evaluated with 113 users and 15,473 gameplays, the platform\nearned a 4.25/5 satisfaction rating. Users excelled in relative comparisons\n(65.9% accuracy) over absolute year guesses (25.6% accuracy), with older\ndecades easier to identify. The platform serves as an educational tool,\nfostering historical awareness and analytical skills via interactive\nexploration of visual heritage. Furthermore, the platform provides a valuable\nresource for studying human perception of temporal cues in images and could be\nused to generate annotated data for training and evaluating computer vision\nmodels."}
{"id": "2505.21627", "pdf": "https://arxiv.org/pdf/2505.21627", "abs": "https://arxiv.org/abs/2505.21627", "authors": ["Ander Artola Velasco", "Stratis Tsirtsis", "Nastaran Okati", "Manuel Gomez-Rodriguez"], "title": "Is Your LLM Overcharging You? Tokenization, Transparency, and Incentives", "categories": ["cs.GT", "cs.AI", "cs.CY", "cs.LG"], "comment": null, "summary": "State-of-the-art large language models require specialized hardware and\nsubstantial energy to operate. As a consequence, cloud-based services that\nprovide access to large language models have become very popular. In these\nservices, the price users pay for an output provided by a model depends on the\nnumber of tokens the model uses to generate it -- they pay a fixed price per\ntoken. In this work, we show that this pricing mechanism creates a financial\nincentive for providers to strategize and misreport the (number of) tokens a\nmodel used to generate an output, and users cannot prove, or even know, whether\na provider is overcharging them. However, we also show that, if an unfaithful\nprovider is obliged to be transparent about the generative process used by the\nmodel, misreporting optimally without raising suspicion is hard. Nevertheless,\nas a proof-of-concept, we introduce an efficient heuristic algorithm that\nallows providers to significantly overcharge users without raising suspicion,\nhighlighting the vulnerability of users under the current pay-per-token pricing\nmechanism. Further, to completely eliminate the financial incentive to\nstrategize, we introduce a simple incentive-compatible token pricing mechanism.\nUnder this mechanism, the price users pay for an output provided by a model\ndepends on the number of characters of the output -- they pay a fixed price per\ncharacter. Along the way, to illustrate and complement our theoretical results,\nwe conduct experiments with several large language models from the\n$\\texttt{Llama}$, $\\texttt{Gemma}$ and $\\texttt{Ministral}$ families, and input\nprompts from the LMSYS Chatbot Arena platform."}
{"id": "2505.22430", "pdf": "https://arxiv.org/pdf/2505.22430", "abs": "https://arxiv.org/abs/2505.22430", "authors": ["Kun Li", "Yunxiang Li", "Tianhua Zhang", "Hongyin Luo", "Xixin Wu", "James Glass", "Helen Meng"], "title": "RAG-Zeval: Towards Robust and Interpretable Evaluation on RAG Responses through End-to-End Rule-Guided Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Robust evaluation is critical for deploying trustworthy retrieval-augmented\ngeneration (RAG) systems. However, current LLM-based evaluation frameworks\npredominantly rely on directly prompting resource-intensive models with complex\nmulti-stage prompts, underutilizing models' reasoning capabilities and\nintroducing significant computational cost. In this paper, we present RAG-Zeval\n(RAG-Zero Evaluator), a novel end-to-end framework that formulates faithfulness\nand correctness evaluation as a rule-guided reasoning task. Our approach trains\nevaluators with reinforcement learning, facilitating compact models to generate\ncomprehensive and sound assessments with detailed explanation in one-pass. We\nintroduce a ranking-based outcome reward mechanism, using preference judgments\nrather than absolute scores, to address the challenge of obtaining precise\npointwise reward signals. To this end, we synthesize the ranking references by\ngenerating quality-controlled responses with zero human annotation. Experiments\ndemonstrate RAG-Zeval's superior performance, achieving the strongest\ncorrelation with human judgments and outperforming baselines that rely on LLMs\nwith 10-100 times more parameters. Our approach also exhibits superior\ninterpretability in response evaluation."}
{"id": "2505.22038", "pdf": "https://arxiv.org/pdf/2505.22038", "abs": "https://arxiv.org/abs/2505.22038", "authors": ["Kaiyuan Li", "Xiaoyue Chen", "Chen Gao", "Yong Li", "Xinlei Chen"], "title": "Balanced Token Pruning: Accelerating Vision Language Models Beyond Local Optimization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) have shown impressive performance across\nmulti-modal tasks by encoding images into thousands of tokens. However, the\nlarge number of image tokens results in significant computational overhead, and\nthe use of dynamic high-resolution inputs further increases this burden.\nPrevious approaches have attempted to reduce the number of image tokens through\ntoken pruning, typically by selecting tokens based on attention scores or image\ntoken diversity. Through empirical studies, we observe that existing methods\noften overlook the joint impact of pruning on both the current layer's output\n(local) and the outputs of subsequent layers (global), leading to suboptimal\npruning decisions. To address this challenge, we propose Balanced Token Pruning\n(BTP), a plug-and-play method for pruning vision tokens. Specifically, our\nmethod utilizes a small calibration set to divide the pruning process into\nmultiple stages. In the early stages, our method emphasizes the impact of\npruning on subsequent layers, whereas in the deeper stages, the focus shifts\ntoward preserving the consistency of local outputs. Extensive experiments\nacross various LVLMs demonstrate the broad effectiveness of our approach on\nmultiple benchmarks. Our method achieves a 78% compression rate while\npreserving 96.7% of the original models' performance on average."}
{"id": "2505.21636", "pdf": "https://arxiv.org/pdf/2505.21636", "abs": "https://arxiv.org/abs/2505.21636", "authors": ["Alexander Nemecek", "Yuzhou Jiang", "Erman Ayday"], "title": "The Feasibility of Topic-Based Watermarking on Academic Peer Reviews", "categories": ["cs.CR", "cs.AI"], "comment": "8 pages main, 9 pages appendix", "summary": "Large language models (LLMs) are increasingly integrated into academic\nworkflows, with many conferences and journals permitting their use for tasks\nsuch as language refinement and literature summarization. However, their use in\npeer review remains prohibited due to concerns around confidentiality breaches,\nhallucinated content, and inconsistent evaluations. As LLM-generated text\nbecomes more indistinguishable from human writing, there is a growing need for\nreliable attribution mechanisms to preserve the integrity of the review\nprocess. In this work, we evaluate topic-based watermarking (TBW), a\nlightweight, semantic-aware technique designed to embed detectable signals into\nLLM-generated text. We conduct a comprehensive assessment across multiple LLM\nconfigurations, including base, few-shot, and fine-tuned variants, using\nauthentic peer review data from academic conferences. Our results show that TBW\nmaintains review quality relative to non-watermarked outputs, while\ndemonstrating strong robustness to paraphrasing-based evasion. These findings\nhighlight the viability of TBW as a minimally intrusive and practical solution\nfor enforcing LLM usage in peer review."}
{"id": "2505.22453", "pdf": "https://arxiv.org/pdf/2505.22453", "abs": "https://arxiv.org/abs/2505.22453", "authors": ["Lai Wei", "Yuting Li", "Chen Wang", "Yue Wang", "Linghe Kong", "Weiran Huang", "Lichao Sun"], "title": "Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Improving Multi-modal Large Language Models (MLLMs) in the post-training\nstage typically relies on supervised fine-tuning (SFT) or reinforcement\nlearning (RL). However, these supervised methods require expensive and manually\nannotated multi-modal data--an ultimately unsustainable resource. While recent\nefforts have explored unsupervised post-training, their methods are complex and\ndifficult to iterate. In this work, we are the first to investigate the use of\nGRPO, a stable and scalable online RL algorithm, for enabling continual\nself-improvement without any external supervision. We propose MM-UPT, a simple\nyet effective framework for unsupervised post-training of MLLMs. MM-UPT builds\nupon GRPO, replacing traditional reward signals with a self-rewarding mechanism\nbased on majority voting over multiple sampled responses. Our experiments\ndemonstrate that MM-UPT significantly improves the reasoning ability of\nQwen2.5-VL-7B (e.g., 66.3 %$\\rightarrow$72.9 % on MathVista, 62.9\n%$\\rightarrow$68.7 % on We-Math), using standard dataset without ground truth\nlabels. MM-UPT also outperforms prior unsupervised baselines and even\napproaches the results of supervised GRPO. Furthermore, we show that\nincorporating synthetic questions, generated solely by MLLM itself, can boost\nperformance as well, highlighting a promising approach for scalable\nself-improvement. Overall, MM-UPT offers a new paradigm for continual,\nautonomous enhancement of MLLMs in the absence of external supervision. Our\ncode is available at https://github.com/waltonfuture/MM-UPT."}
{"id": "2505.22039", "pdf": "https://arxiv.org/pdf/2505.22039", "abs": "https://arxiv.org/abs/2505.22039", "authors": ["Shifang Zhao", "Yiheng Lin", "Lu Han", "Yao Zhao", "Yunchao Wei"], "title": "OmniAD: Detect and Understand Industrial Anomaly via Multimodal Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "While anomaly detection has made significant progress, generating detailed\nanalyses that incorporate industrial knowledge remains a challenge. To address\nthis gap, we introduce OmniAD, a novel framework that unifies anomaly detection\nand understanding for fine-grained analysis. OmniAD is a multimodal reasoner\nthat combines visual and textual reasoning processes. The visual reasoning\nprovides detailed inspection by leveraging Text-as-Mask Encoding to perform\nanomaly detection through text generation without manually selected thresholds.\nFollowing this, Visual Guided Textual Reasoning conducts comprehensive analysis\nby integrating visual perception. To enhance few-shot generalization, we employ\nan integrated training strategy that combines supervised fine-tuning (SFT) with\nreinforcement learning (GRPO), incorporating three sophisticated reward\nfunctions. Experimental results demonstrate that OmniAD achieves a performance\nof 79.1 on the MMAD benchmark, surpassing models such as Qwen2.5-VL-7B and\nGPT-4o. It also shows strong results across multiple anomaly detection\nbenchmarks. These results highlight the importance of enhancing visual\nperception for effective reasoning in anomaly understanding. All codes and\nmodels will be publicly available."}
{"id": "2505.21640", "pdf": "https://arxiv.org/pdf/2505.21640", "abs": "https://arxiv.org/abs/2505.21640", "authors": ["Oren Mangoubi", "Neil He", "Nisheeth K. Vishnoi"], "title": "Efficient Diffusion Models for Symmetric Manifolds", "categories": ["cs.LG", "cs.AI", "cs.DS", "math.PR", "stat.ML"], "comment": "The conference version of this paper appears in ICML 2025", "summary": "We introduce a framework for designing efficient diffusion models for\n$d$-dimensional symmetric-space Riemannian manifolds, including the torus,\nsphere, special orthogonal group and unitary group. Existing manifold diffusion\nmodels often depend on heat kernels, which lack closed-form expressions and\nrequire either $d$ gradient evaluations or exponential-in-$d$ arithmetic\noperations per training step. We introduce a new diffusion model for symmetric\nmanifolds with a spatially-varying covariance, allowing us to leverage a\nprojection of Euclidean Brownian motion to bypass heat kernel computations. Our\ntraining algorithm minimizes a novel efficient objective derived via Ito's\nLemma, allowing each step to run in $O(1)$ gradient evaluations and\nnearly-linear-in-$d$ ($O(d^{1.19})$) arithmetic operations, reducing the gap\nbetween diffusions on symmetric manifolds and Euclidean space. Manifold\nsymmetries ensure the diffusion satisfies an \"average-case\" Lipschitz\ncondition, enabling accurate and efficient sample generation. Empirically, our\nmodel outperforms prior methods in training speed and improves sample quality\non synthetic datasets on the torus, special orthogonal group, and unitary\ngroup."}
{"id": "2505.22501", "pdf": "https://arxiv.org/pdf/2505.22501", "abs": "https://arxiv.org/abs/2505.22501", "authors": ["Dingchu Zhang", "Yida Zhao", "Jialong Wu", "Baixuan Li", "Wenbiao Yin", "Liwen Zhang", "Yong Jiang", "Yufeng Li", "Kewei Tu", "Pengjun Xie", "Fei Huang"], "title": "EvolveSearch: An Iterative Self-Evolving Search Agent", "categories": ["cs.CL"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) has transformed the\nlandscape of agentic information seeking capabilities through the integration\nof tools such as search engines and web browsers. However, current mainstream\napproaches for enabling LLM web search proficiency face significant challenges:\nsupervised fine-tuning struggles with data production in open-search domains,\nwhile RL converges quickly, limiting their data utilization efficiency. To\naddress these issues, we propose EvolveSearch, a novel iterative self-evolution\nframework that combines SFT and RL to enhance agentic web search capabilities\nwithout any external human-annotated reasoning data. Extensive experiments on\nseven multi-hop question-answering (MHQA) benchmarks demonstrate that\nEvolveSearch consistently improves performance across iterations, ultimately\nachieving an average improvement of 4.7\\% over the current state-of-the-art\nacross seven benchmarks, opening the door to self-evolution agentic\ncapabilities in open web search domains."}
{"id": "2505.22046", "pdf": "https://arxiv.org/pdf/2505.22046", "abs": "https://arxiv.org/abs/2505.22046", "authors": ["Ashkan Taghipour", "Morteza Ghahremani", "Mohammed Bennamoun", "Farid Boussaid", "Aref Miri Rekavandi", "Zinuo Li", "Qiuhong Ke", "Hamid Laga"], "title": "LatentMove: Towards Complex Human Movement Video Generation", "categories": ["cs.CV"], "comment": "12 pages", "summary": "Image-to-video (I2V) generation seeks to produce realistic motion sequences\nfrom a single reference image. Although recent methods exhibit strong temporal\nconsistency, they often struggle when dealing with complex, non-repetitive\nhuman movements, leading to unnatural deformations. To tackle this issue, we\npresent LatentMove, a DiT-based framework specifically tailored for highly\ndynamic human animation. Our architecture incorporates a conditional control\nbranch and learnable face/body tokens to preserve consistency as well as\nfine-grained details across frames. We introduce Complex-Human-Videos (CHV), a\ndataset featuring diverse, challenging human motions designed to benchmark the\nrobustness of I2V systems. We also introduce two metrics to assess the flow and\nsilhouette consistency of generated videos with their ground truth.\nExperimental results indicate that LatentMove substantially improves human\nanimation quality--particularly when handling rapid, intricate\nmovements--thereby pushing the boundaries of I2V generation. The code, the CHV\ndataset, and the evaluation metrics will be available at https://github.com/\n--."}
{"id": "2505.21652", "pdf": "https://arxiv.org/pdf/2505.21652", "abs": "https://arxiv.org/abs/2505.21652", "authors": ["Yifan Yin", "Zhengtao Han", "Shivam Aarya", "Jianxin Wang", "Shuhang Xu", "Jiawei Peng", "Angtian Wang", "Alan Yuille", "Tianmin Shu"], "title": "PartInstruct: Part-level Instruction Following for Fine-grained Robot Manipulation", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Fine-grained robot manipulation, such as lifting and rotating a bottle to\ndisplay the label on the cap, requires robust reasoning about object parts and\ntheir relationships with intended tasks. Despite recent advances in training\ngeneral-purpose robot manipulation policies guided by language instructions,\nthere is a notable lack of large-scale datasets for fine-grained manipulation\ntasks with part-level instructions and diverse 3D object instances annotated\nwith part-level labels. In this work, we introduce PartInstruct, the first\nlarge-scale benchmark for training and evaluating fine-grained robot\nmanipulation models using part-level instructions. PartInstruct comprises 513\nobject instances across 14 categories, each annotated with part-level\ninformation, and 1302 fine-grained manipulation tasks organized into 16 task\nclasses. Our training set consists of over 10,000 expert demonstrations\nsynthesized in a 3D simulator, where each demonstration is paired with a\nhigh-level task instruction, a chain of base part-based skill instructions, and\nground-truth 3D information about the object and its parts. Additionally, we\ndesigned a comprehensive test suite to evaluate the generalizability of learned\npolicies across new states, objects, and tasks. We evaluated several\nstate-of-the-art robot manipulation approaches, including end-to-end\nvision-language policy learning and bi-level planning models for robot\nmanipulation on our benchmark. The experimental results reveal that current\nmodels struggle to robustly ground part concepts and predict actions in 3D\nspace, and face challenges when manipulating object parts in long-horizon\ntasks."}
{"id": "2505.22517", "pdf": "https://arxiv.org/pdf/2505.22517", "abs": "https://arxiv.org/abs/2505.22517", "authors": ["Yimeng Gu", "Zhao Tong", "Ignacio Castro", "Shu Wu", "Gareth Tyson"], "title": "Multi-MLLM Knowledge Distillation for Out-of-Context News Detection", "categories": ["cs.CL", "cs.MM"], "comment": null, "summary": "Multimodal out-of-context news is a type of misinformation in which the image\nis used outside of its original context. Many existing works have leveraged\nmultimodal large language models (MLLMs) for detecting out-of-context news.\nHowever, observing the limited zero-shot performance of smaller MLLMs, they\ngenerally require label-rich fine-tuning and/or expensive API calls to GPT\nmodels to improve the performance, which is impractical in low-resource\nscenarios. In contrast, we aim to improve the performance of small MLLMs in a\nmore label-efficient and cost-effective manner. To this end, we first prompt\nmultiple teacher MLLMs to generate both label predictions and corresponding\nrationales, which collectively serve as the teachers' knowledge. We then\nintroduce a two-stage knowledge distillation framework to transfer this\nknowledge to a student MLLM. In Stage 1, we apply LoRA fine-tuning to the\nstudent model using all training data. In Stage 2, we further fine-tune the\nstudent model using both LoRA fine-tuning and DPO on the data points where\nteachers' predictions conflict. This two-stage strategy reduces annotation\ncosts and helps the student model uncover subtle patterns in more challenging\ncases. Experimental results demonstrate that our approach achieves\nstate-of-the-art performance using less than 10% labeled data."}
{"id": "2505.22065", "pdf": "https://arxiv.org/pdf/2505.22065", "abs": "https://arxiv.org/abs/2505.22065", "authors": ["Mikko Impiö", "Philipp M. Rehsen", "Tiina Laamanen", "Arne J. Beermann", "Florian Leese", "Jenni Raitoharju"], "title": "AquaMonitor: A multimodal multi-view image sequence dataset for real-life aquatic invertebrate biodiversity monitoring", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents the AquaMonitor dataset, the first large computer vision\ndataset of aquatic invertebrates collected during routine environmental\nmonitoring. While several large species identification datasets exist, they are\nrarely collected using standardized collection protocols, and none focus on\naquatic invertebrates, which are particularly laborious to collect. For\nAquaMonitor, we imaged all specimens from two years of monitoring whenever\nimaging was possible given practical limitations. The dataset enables the\nevaluation of automated identification methods for real-life monitoring\npurposes using a realistically challenging and unbiased setup. The dataset has\n2.7M images from 43,189 specimens, DNA sequences for 1358 specimens, and dry\nmass and size measurements for 1494 specimens, making it also one of the\nlargest biological multi-view and multimodal datasets to date. We define three\nbenchmark tasks and provide strong baselines for these: 1) Monitoring\nbenchmark, reflecting real-life deployment challenges such as open-set\nrecognition, distribution shift, and extreme class imbalance, 2) Classification\nbenchmark, which follows a standard fine-grained visual categorization setup,\nand 3) Few-shot benchmark, which targets classes with only few training\nexamples from very fine-grained categories. Advancements on the Monitoring\nbenchmark can directly translate to improvement of aquatic biodiversity\nmonitoring, which is an important component of regular legislative water\nquality assessment in many countries."}
{"id": "2505.21657", "pdf": "https://arxiv.org/pdf/2505.21657", "abs": "https://arxiv.org/abs/2505.21657", "authors": ["Zeinab Dehghani", "Koorosh Aslansefat", "Adil Khan", "Mohammed Naveed Akram"], "title": "Explainability of Large Language Models using SMILE: Statistical Model-agnostic Interpretability with Local Explanations", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "arXiv admin note: text overlap with arXiv:2412.16277", "summary": "Large language models like GPT, LLAMA, and Claude have become incredibly\npowerful at generating text, but they are still black boxes, so it is hard to\nunderstand how they decide what to say. That lack of transparency can be\nproblematic, especially in fields where trust and accountability matter. To\nhelp with this, we introduce SMILE, a new method that explains how these models\nrespond to different parts of a prompt. SMILE is model-agnostic and works by\nslightly changing the input, measuring how the output changes, and then\nhighlighting which words had the most impact. Create simple visual heat maps\nshowing which parts of a prompt matter the most. We tested SMILE on several\nleading LLMs and used metrics such as accuracy, consistency, stability, and\nfidelity to show that it gives clear and reliable explanations. By making these\nmodels easier to understand, SMILE brings us one step closer to making AI more\ntransparent and trustworthy."}
{"id": "2505.22548", "pdf": "https://arxiv.org/pdf/2505.22548", "abs": "https://arxiv.org/abs/2505.22548", "authors": ["Changhao Song", "Yazhou Zhang", "Peng Zhang"], "title": "Emotion-o1: Adaptive Long Reasoning for Emotion Understanding in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Emotion understanding includes basic tasks (e.g., sentiment/emotion\nclassification) and advanced tasks (e.g., sarcasm/humor detection). Current\nmethods rely on fixed-length CoT reasoning, failing to adapt to the varying\ncomplexity of emotions. We propose a task-adaptive reasoning framework that\nemploys DeepSeek-R1 to generate variable-length reasoning chains for different\nemotion tasks. By combining fine-tuning with reinforcement learning, we design\na composite reward function that balances four objectives: prediction accuracy,\nadaptive reasoning depth control, structural diversity in reasoning paths, and\nsuppression of repetitive logic. This approach achieves dynamic\ncontext-sensitive inference while enabling LLMs to autonomously develop deep\nreasoning capabilities. Experimental results demonstrate consistent\nimprovements in both Acc and F1 scores across four tasks: emotion, sentiment,\nhumor, and sarcasm. Notably, peak enhancements reached 3.56% F1 (2.76% Acc) for\nbasic tasks and 37.95% F1 (23.14% Acc) for advanced tasks. Our work bridges\nrigid CoT reasoning and emotional complexity through adaptive-depth analysis."}
{"id": "2505.22067", "pdf": "https://arxiv.org/pdf/2505.22067", "abs": "https://arxiv.org/abs/2505.22067", "authors": ["Xinyu Xia", "Xingjun Ma", "Yunfeng Hu", "Ting Qu", "Hong Chen", "Xun Gong"], "title": "From Failures to Fixes: LLM-Driven Scenario Repair for Self-Evolving Autonomous Driving", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "Ensuring robust and generalizable autonomous driving requires not only broad\nscenario coverage but also efficient repair of failure cases, particularly\nthose related to challenging and safety-critical scenarios. However, existing\nscenario generation and selection methods often lack adaptivity and semantic\nrelevance, limiting their impact on performance improvement. In this paper, we\npropose \\textbf{SERA}, an LLM-powered framework that enables autonomous driving\nsystems to self-evolve by repairing failure cases through targeted scenario\nrecommendation. By analyzing performance logs, SERA identifies failure patterns\nand dynamically retrieves semantically aligned scenarios from a structured\nbank. An LLM-based reflection mechanism further refines these recommendations\nto maximize relevance and diversity. The selected scenarios are used for\nfew-shot fine-tuning, enabling targeted adaptation with minimal data.\nExperiments on the benchmark show that SERA consistently improves key metrics\nacross multiple autonomous driving baselines, demonstrating its effectiveness\nand generalizability under safety-critical conditions."}
{"id": "2505.21664", "pdf": "https://arxiv.org/pdf/2505.21664", "abs": "https://arxiv.org/abs/2505.21664", "authors": ["Joe O'Brien", "Jeremy Dolan", "Jay Kim", "Jonah Dykhuizen", "Jeba Sania", "Sebastian Becker", "Jam Kraprayoon", "Cara Labrador"], "title": "Expert Survey: AI Reliability & Security Research Priorities", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Our survey of 53 specialists across 105 AI reliability and security research\nareas identifies the most promising research prospects to guide strategic AI\nR&D investment. As companies are seeking to develop AI systems with broadly\nhuman-level capabilities, research on reliability and security is urgently\nneeded to ensure AI's benefits can be safely and broadly realized and prevent\nsevere harms. This study is the first to quantify expert priorities across a\ncomprehensive taxonomy of AI safety and security research directions and to\nproduce a data-driven ranking of their potential impact. These rankings may\nsupport evidence-based decisions about how to effectively deploy resources\ntoward AI reliability and security research."}
{"id": "2505.22552", "pdf": "https://arxiv.org/pdf/2505.22552", "abs": "https://arxiv.org/abs/2505.22552", "authors": ["Hoang Pham", "Thanh-Do Nguyen", "Khac-Hoai Nam Bui"], "title": "ClaimPKG: Enhancing Claim Verification via Pseudo-Subgraph Generation with Lightweight Specialized LLM", "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": "Accepted by ACL 2025 findings", "summary": "Integrating knowledge graphs (KGs) to enhance the reasoning capabilities of\nlarge language models (LLMs) is an emerging research challenge in claim\nverification. While KGs provide structured, semantically rich representations\nwell-suited for reasoning, most existing verification methods rely on\nunstructured text corpora, limiting their ability to effectively leverage KGs.\nAdditionally, despite possessing strong reasoning abilities, modern LLMs\nstruggle with multi-step modular pipelines and reasoning over KGs without\nadaptation. To address these challenges, we propose ClaimPKG, an end-to-end\nframework that seamlessly integrates LLM reasoning with structured knowledge\nfrom KGs. Specifically, the main idea of ClaimPKG is to employ a lightweight,\nspecialized LLM to represent the input claim as pseudo-subgraphs, guiding a\ndedicated subgraph retrieval module to identify relevant KG subgraphs. These\nretrieved subgraphs are then processed by a general-purpose LLM to produce the\nfinal verdict and justification. Extensive experiments on the FactKG dataset\ndemonstrate that ClaimPKG achieves state-of-the-art performance, outperforming\nstrong baselines in this research field by 9%-12% accuracy points across\nmultiple categories. Furthermore, ClaimPKG exhibits zero-shot generalizability\nto unstructured datasets such as HoVer and FEVEROUS, effectively combining\nstructured knowledge from KGs with LLM reasoning across various LLM backbones."}
{"id": "2505.22079", "pdf": "https://arxiv.org/pdf/2505.22079", "abs": "https://arxiv.org/abs/2505.22079", "authors": ["Hanbin Ko", "Chang-Min Park"], "title": "Bringing CLIP to the Clinic: Dynamic Soft Labels and Negation-Aware Learning for Medical Analysis", "categories": ["cs.CV"], "comment": "16 pages (8 main, 2 references, 6 appendix), 13 figures. Accepted to\n  CVPR 2025. This author-accepted manuscript includes an expanded ethics/data\n  user agreement section. The final version will appear in the Proceedings of\n  CVPR 2025", "summary": "The development of large-scale image-text pair datasets has significantly\nadvanced self-supervised learning in Vision-Language Processing (VLP). However,\ndirectly applying general-domain architectures such as CLIP to medical data\npresents challenges, particularly in handling negations and addressing the\ninherent data imbalance of medical datasets. To address these issues, we\npropose a novel approach that integrates clinically-enhanced dynamic soft\nlabels and medical graphical alignment, thereby improving clinical\ncomprehension and the applicability of contrastive loss in medical contexts.\nFurthermore, we introduce negation-based hard negatives to deepen the model's\nunderstanding of the complexities of clinical language. Our approach is easily\nintegrated into the medical CLIP training pipeline and achieves\nstate-of-the-art performance across multiple tasks, including zero-shot,\nfine-tuned classification, and report retrieval. To comprehensively evaluate\nour model's capacity for understanding clinical language, we introduce\nCXR-Align, a benchmark uniquely designed to evaluate the understanding of\nnegation and clinical information within chest X-ray (CXR) datasets.\nExperimental results demonstrate that our proposed methods are straightforward\nto implement and generalize effectively across contrastive learning frameworks,\nenhancing medical VLP capabilities and advancing clinical language\nunderstanding in medical imaging."}
{"id": "2505.21666", "pdf": "https://arxiv.org/pdf/2505.21666", "abs": "https://arxiv.org/abs/2505.21666", "authors": ["Owen Oertell", "Shikun Sun", "Yiding Chen", "Jin Peng Zhou", "Zhiyong Wang", "Wen Sun"], "title": "Efficient Controllable Diffusion via Optimal Classifier Guidance", "categories": ["cs.LG", "cs.AI"], "comment": "28 pages, 9 figures, 3 tables", "summary": "The controllable generation of diffusion models aims to steer the model to\ngenerate samples that optimize some given objective functions. It is desirable\nfor a variety of applications including image generation, molecule generation,\nand DNA/sequence generation. Reinforcement Learning (RL) based fine-tuning of\nthe base model is a popular approach but it can overfit the reward function\nwhile requiring significant resources. We frame controllable generation as a\nproblem of finding a distribution that optimizes a KL-regularized objective\nfunction. We present SLCD -- Supervised Learning based Controllable Diffusion,\nwhich iteratively generates online data and trains a small classifier to guide\nthe generation of the diffusion model. Similar to the standard\nclassifier-guided diffusion, SLCD's key computation primitive is classification\nand does not involve any complex concepts from RL or control. Via a reduction\nto no-regret online learning analysis, we show that under KL divergence, the\noutput from SLCD provably converges to the optimal solution of the\nKL-regularized objective. Further, we empirically demonstrate that SLCD can\ngenerate high quality samples with nearly the same inference time as the base\nmodel in both image generation with continuous diffusion and biological\nsequence generation with discrete diffusion. Our code is available at\nhttps://github.com/Owen-Oertell/slcd"}
{"id": "2505.22563", "pdf": "https://arxiv.org/pdf/2505.22563", "abs": "https://arxiv.org/abs/2505.22563", "authors": ["Yu Lei", "Xingyang Ge", "Yi Zhang", "Yiming Yang", "Bolei Ma"], "title": "Do Large Language Models Think Like the Brain? Sentence-Level Evidence from fMRI and Hierarchical Embeddings", "categories": ["cs.CL", "q-bio.NC"], "comment": null, "summary": "Understanding whether large language models (LLMs) and the human brain\nconverge on similar computational principles remains a fundamental and\nimportant question in cognitive neuroscience and AI. Do the brain-like patterns\nobserved in LLMs emerge simply from scaling, or do they reflect deeper\nalignment with the architecture of human language processing? This study\nfocuses on the sentence-level neural mechanisms of language models,\nsystematically investigating how hierarchical representations in LLMs align\nwith the dynamic neural responses during human sentence comprehension. By\ncomparing hierarchical embeddings from 14 publicly available LLMs with fMRI\ndata collected from participants, who were exposed to a naturalistic narrative\nstory, we constructed sentence-level neural prediction models to precisely\nidentify the model layers most significantly correlated with brain region\nactivations. Results show that improvements in model performance drive the\nevolution of representational architectures toward brain-like hierarchies,\nparticularly achieving stronger functional and anatomical correspondence at\nhigher semantic abstraction levels."}
{"id": "2505.22084", "pdf": "https://arxiv.org/pdf/2505.22084", "abs": "https://arxiv.org/abs/2505.22084", "authors": ["Julie Tores", "Elisa Ancarani", "Lucile Sassatelli", "Hui-Yin Wu", "Clement Bergman", "Lea Andolfi", "Victor Ecrement", "Remy Sun", "Frederic Precioso", "Thierry Devars", "Magali Guaresi", "Virginie Julliard", "Sarah Lecossais"], "title": "MObyGaze: a film dataset of multimodal objectification densely annotated by experts", "categories": ["cs.CV"], "comment": null, "summary": "Characterizing and quantifying gender representation disparities in\naudiovisual storytelling contents is necessary to grasp how stereotypes may\nperpetuate on screen. In this article, we consider the high-level construct of\nobjectification and introduce a new AI task to the ML community: characterize\nand quantify complex multimodal (visual, speech, audio) temporal patterns\nproducing objectification in films. Building on film studies and psychology, we\ndefine the construct of objectification in a structured thesaurus involving 5\nsub-constructs manifesting through 11 concepts spanning 3 modalities. We\nintroduce the Multimodal Objectifying Gaze (MObyGaze) dataset, made of 20\nmovies annotated densely by experts for objectification levels and concepts\nover freely delimited segments: it amounts to 6072 segments over 43 hours of\nvideo with fine-grained localization and categorization. We formulate different\nlearning tasks, propose and investigate best ways to learn from the diversity\nof labels among a low number of annotators, and benchmark recent vision, text\nand audio models, showing the feasibility of the task. We make our code and our\ndataset available to the community and described in the Croissant format:\nhttps://anonymous.4open.science/r/MObyGaze-F600/."}
{"id": "2505.21670", "pdf": "https://arxiv.org/pdf/2505.21670", "abs": "https://arxiv.org/abs/2505.21670", "authors": ["Rahul Raman", "Khushi Sharma", "Sai Qian Zhang"], "title": "Rethinking the Outlier Distribution in Large Language Models: An In-depth Study", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Investigating outliers in large language models (LLMs) is crucial due to\ntheir significant impact on various aspects of LLM performance, including\nquantization and compression. Outliers often cause considerable quantization\nerrors, leading to degraded model performance. Identifying and addressing these\noutliers can enhance the accuracy and efficiency of the quantization process,\nenabling smoother deployment on edge devices or specialized hardware. Recent\nstudies have identified two common types of outliers in LLMs: massive\nactivations and channel-wise outliers. While numerous quantization algorithms\nhave been proposed to mitigate their effects and maintain satisfactory\naccuracy, few have thoroughly explored the root causes of these outliers in\ndepth. In this paper, we conduct a comprehensive investigation into the\nformation mechanisms of these outliers and propose potential strategies to\nmitigate their occurrence. Ultimately, we introduce some efficient approaches\nto eliminate most massive activations and channel-wise outliers with minimal\nimpact on accuracy."}
{"id": "2505.22571", "pdf": "https://arxiv.org/pdf/2505.22571", "abs": "https://arxiv.org/abs/2505.22571", "authors": ["Hoang Pham", "Khac-Hoai Nam Bui"], "title": "Agent-UniRAG: A Trainable Open-Source LLM Agent Framework for Unified Retrieval-Augmented Generation Systems", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR"], "comment": null, "summary": "This paper presents a novel approach for unified retrieval-augmented\ngeneration (RAG) systems using the recent emerging large language model (LLM)\nagent concept. Specifically, Agent LLM, which utilizes LLM as fundamental\ncontrollers, has become a promising approach to enable the interpretability of\nRAG tasks, especially for complex reasoning question-answering systems (e.g.,\nmulti-hop queries). Nonetheless, previous works mainly focus on solving RAG\nsystems with either single-hop or multi-hop approaches separately, which limits\nthe application of those approaches to real-world applications. In this study,\nwe propose a trainable agent framework called Agent-UniRAG for unified\nretrieval-augmented LLM systems, which enhances the effectiveness and\ninterpretability of RAG systems. The main idea is to design an LLM agent\nframework to solve RAG tasks step-by-step based on the complexity of the\ninputs, simultaneously including single-hop and multi-hop queries in an\nend-to-end manner. Furthermore, we introduce SynAgent-RAG, a synthetic dataset\nto enable the proposed agent framework for small open-source LLMs (e.g.,\nLlama-3-8B). The results show comparable performances with closed-source and\nlarger open-source LLMs across various RAG benchmarks. Our source code and\ndataset are publicly available for further exploitation."}
{"id": "2505.22089", "pdf": "https://arxiv.org/pdf/2505.22089", "abs": "https://arxiv.org/abs/2505.22089", "authors": ["San Jiang", "Kan You", "Wanshou Jiang", "Qingquan Li"], "title": "Fast Feature Matching of UAV Images via Matrix Band Reduction-based GPU Data Schedule", "categories": ["cs.CV"], "comment": null, "summary": "Feature matching dominats the time costs in structure from motion (SfM). The\nprimary contribution of this study is a GPU data schedule algorithm for\nefficient feature matching of Unmanned aerial vehicle (UAV) images. The core\nidea is to divide the whole dataset into blocks based on the matrix band\nreduction (MBR) and achieve efficient feature matching via GPU-accelerated\ncascade hashing. First, match pairs are selected by using an image retrieval\ntechnique, which converts images into global descriptors and searches\nhigh-dimension nearest neighbors with graph indexing. Second, compact image\nblocks are iteratively generated from a MBR-based data schedule strategy, which\nexploits image connections to avoid redundant data IO (input/output) burden and\nincreases the usage of GPU computing power. Third, guided by the generated\nimage blocks, feature matching is executed sequentially within the framework of\nGPU-accelerated cascade hashing, and initial candidate matches are refined by\ncombining a local geometric constraint and RANSAC-based global verification.\nFor further performance improvement, these two seps are designed to execute\nparallelly in GPU and CPU. Finally, the performance of the proposed solution is\nevaluated by using large-scale UAV datasets. The results demonstrate that it\nincreases the efficiency of feature matching with speedup ratios ranging from\n77.0 to 100.0 compared with KD-Tree based matching methods, and achieves\ncomparable accuracy in relative and absolute bundle adjustment (BA). The\nproposed algorithm is an efficient solution for feature matching of UAV images."}
{"id": "2505.21677", "pdf": "https://arxiv.org/pdf/2505.21677", "abs": "https://arxiv.org/abs/2505.21677", "authors": ["Hung Ahn Vu", "Galen Reeves", "Emily Wenger"], "title": "What happens when generative AI models train recursively on each others' generated outputs?", "categories": ["cs.LG", "cs.AI", "cs.CY"], "comment": "9 pages", "summary": "The internet is full of AI-generated content while also serving as a common\nsource of training data for generative AI (genAI) models. This duality raises\nthe possibility that future genAI models may be trained on other models'\ngenerated outputs. Prior work has studied consequences of models training on\ntheir own generated outputs, but limited work has considered what happens if\nmodels ingest content produced by other models. Given society's increasing\ndependence on genAI tools, understanding downstream effects of such\ndata-mediated model interactions is critical. To this end, we provide empirical\nevidence for how data-mediated interactions might unfold in practice, develop a\ntheoretical model for this interactive training process, and show\nexperimentally possible long-term results of such interactions. We find that\ndata-mediated interactions can benefit models by exposing them to novel\nconcepts perhaps missed in original training data, but also can homogenize\ntheir performance on shared tasks."}
{"id": "2505.22572", "pdf": "https://arxiv.org/pdf/2505.22572", "abs": "https://arxiv.org/abs/2505.22572", "authors": ["Waldemar Chang", "Alhassan Yasin"], "title": "Fusion Steering: Prompt-Specific Activation Control", "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 4 figures, 2 tables", "summary": "We present Fusion Steering, an activation steering methodology that improves\nfactual accuracy in large language models (LLMs) for question-answering (QA)\ntasks. This approach introduces flexible steering configurations, including\nfull-layer steering and segmented steering. Unlike traditional methods\nconstrained to single-layer or fixed-layer operations, Fusion Steering employs\ndynamic injection of prompt-specific activation deltas across all transformer\nlayers. These activation deltas are derived from reference completions that\ncombine the ground-truth answer with a model-generated explanation to\nfacilitate semantically enriched, example-specific steering. The injection\nweights are optimized per prompt using Optuna, targeting a joint objective that\nbalances token overlap (factual alignment) and perplexity (fluency proxy).\nEvaluation employs a composite score integrating token overlap and LLM-graded\nquality, encompassing factual accuracy, coherence, and relevance. Empirical\nresults on 260 SimpleQA prompts (selected from 500 where the baseline failed)\nshowcase the efficacy of segmented steering. Using Gemma-2-2B-IT with 8-bit\nquantization, segmented steering achieves an accuracy of 25.4% (outputs scoring\n$\\geq 0.6$), outperforming the baseline at 3.5% and full-layer steering at\n16.2%. Under the stricter SimpleQA rubric, segmented steering boosts fully\ncorrect responses from 0.0% to 13.1%. These findings highlight the strengths of\nsegmented, dynamic intervention strategies and the promise of per-prompt,\nfull-network activation control. Fusion Steering is also amenable to sparse\nrepresentations, such as Neuronpedia or sparse crosscoders, suggesting a\npromising direction for interpretable and scalable activation-level control in\nLLMs."}
{"id": "2505.22098", "pdf": "https://arxiv.org/pdf/2505.22098", "abs": "https://arxiv.org/abs/2505.22098", "authors": ["Junhuan Liu", "San Jiang", "Wei Ge", "Wei Huang", "Bingxuan Guo", "Qingquan Li"], "title": "UAVPairs: A Challenging Benchmark for Match Pair Retrieval of Large-scale UAV Images", "categories": ["cs.CV"], "comment": null, "summary": "The primary contribution of this paper is a challenging benchmark dataset,\nUAVPairs, and a training pipeline designed for match pair retrieval of\nlarge-scale UAV images. First, the UAVPairs dataset, comprising 21,622\nhigh-resolution images across 30 diverse scenes, is constructed; the 3D points\nand tracks generated by SfM-based 3D reconstruction are employed to define the\ngeometric similarity of image pairs, ensuring genuinely matchable image pairs\nare used for training. Second, to solve the problem of expensive mining cost\nfor global hard negative mining, a batched nontrivial sample mining strategy is\nproposed, leveraging the geometric similarity and multi-scene structure of the\nUAVPairs to generate training samples as to accelerate training. Third,\nrecognizing the limitation of pair-based losses, the ranked list loss is\ndesigned to improve the discrimination of image retrieval models, which\noptimizes the global similarity structure constructed from the positive set and\nnegative set. Finally, the effectiveness of the UAVPairs dataset and training\npipeline is validated through comprehensive experiments on three distinct\nlarge-scale UAV datasets. The experiment results demonstrate that models\ntrained with the UAVPairs dataset and the ranked list loss achieve\nsignificantly improved retrieval accuracy compared to models trained on\nexisting datasets or with conventional losses. Furthermore, these improvements\ntranslate to enhanced view graph connectivity and higher quality of\nreconstructed 3D models. The models trained by the proposed approach perform\nmore robustly compared with hand-crafted global features, particularly in\nchallenging repetitively textured scenes and weakly textured scenes. For match\npair retrieval of large-scale UAV images, the trained image retrieval models\noffer an effective solution. The dataset would be made publicly available at\nhttps://github.com/json87/UAVPairs."}
{"id": "2505.21680", "pdf": "https://arxiv.org/pdf/2505.21680", "abs": "https://arxiv.org/abs/2505.21680", "authors": ["Andrew J. Loza", "Jun Yup Kim", "Shangzheng Song", "Yihang Liu", "Joseph J. Y. Sung", "R Andrew Taylor", "Dennis L. Shung"], "title": "multivariateGPT: a decoder-only transformer for multivariate categorical and numeric data", "categories": ["cs.LG", "cs.AI", "68T07", "I.2.6; I.5.1"], "comment": "15 pates, 5 figures", "summary": "Real-world processes often generate data that are a mix of categorical and\nnumeric values that are recorded at irregular and informative intervals.\nDiscrete token-based approaches are limited in numeric representation capacity\nwhile methods like neural ordinary differential equations are not well suited\nfor categorical data or informative sampling and require augmentation to handle\ncertain classes of trajectories. Here, we present multivariateGPT, a single\narchitecture for modeling sequences of mixed categorical (including tokenized\ntext) and numeric data. This is accomplished with an autoregressive sequence\ndecomposition, embedding scheme, and loss function that extend the next token\nprediction task to likelihood estimation of the joint distribution of next\ntoken class and value. We demonstrate how this approach can efficiently learn\nto generalize patterns in simple physical systems and model complex time series\nincluding electrocardiograms and multivariate electronic health record data.\nThis work extends the utility of transformer based models to additional classes\nof data."}
{"id": "2505.22582", "pdf": "https://arxiv.org/pdf/2505.22582", "abs": "https://arxiv.org/abs/2505.22582", "authors": ["Xue Zhang", "Yunlong Liang", "Fandong Meng", "Songming Zhang", "Yufeng Chen", "Jinan Xu", "Jie Zhou"], "title": "Less, but Better: Efficient Multilingual Expansion for LLMs via Layer-wise Mixture-of-Experts", "categories": ["cs.CL"], "comment": "ACL 2025 (Main), 16 pages, 5 figures, 11 tables", "summary": "Continually expanding new languages for existing large language models (LLMs)\nis a promising yet challenging approach to building powerful multilingual LLMs.\nThe biggest challenge is to make the model continuously learn new languages\nwhile preserving the proficient ability of old languages. To achieve this,\nrecent work utilizes the Mixture-of-Experts (MoE) architecture to expand new\nlanguages by adding new experts and avoid catastrophic forgetting of old\nlanguages by routing corresponding tokens to the original model backbone (old\nexperts). Although intuitive, this kind of method is parameter-costly when\nexpanding new languages and still inevitably impacts the performance of old\nlanguages. To address these limitations, we analyze the language\ncharacteristics of different layers in LLMs and propose a layer-wise expert\nallocation algorithm (LayerMoE) to determine the appropriate number of new\nexperts for each layer. Specifically, we find different layers in LLMs exhibit\ndifferent representation similarities between languages and then utilize the\nsimilarity as the indicator to allocate experts for each layer, i.e., the\nhigher similarity, the fewer experts. Additionally, to further mitigate the\nforgetting of old languages, we add a classifier in front of the router network\non the layers with higher similarity to guide the routing of old language\ntokens. Experimental results show that our method outperforms the previous\nstate-of-the-art baseline with 60% fewer experts in the single-expansion\nsetting and with 33.3% fewer experts in the lifelong-expansion setting,\ndemonstrating the effectiveness of our method."}
{"id": "2505.22099", "pdf": "https://arxiv.org/pdf/2505.22099", "abs": "https://arxiv.org/abs/2505.22099", "authors": ["Wenwen Qiang", "Ziyin Gu", "Lingyu Si", "Jiangmeng Li", "Changwen Zheng", "Fuchun Sun", "Hui Xiong"], "title": "On the Transferability and Discriminability of Repersentation Learning in Unsupervised Domain Adaptation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "In this paper, we addressed the limitation of relying solely on distribution\nalignment and source-domain empirical risk minimization in Unsupervised Domain\nAdaptation (UDA). Our information-theoretic analysis showed that this standard\nadversarial-based framework neglects the discriminability of target-domain\nfeatures, leading to suboptimal performance. To bridge this\ntheoretical-practical gap, we defined \"good representation learning\" as\nguaranteeing both transferability and discriminability, and proved that an\nadditional loss term targeting target-domain discriminability is necessary.\nBuilding on these insights, we proposed a novel adversarial-based UDA framework\nthat explicitly integrates a domain alignment objective with a\ndiscriminability-enhancing constraint. Instantiated as Domain-Invariant\nRepresentation Learning with Global and Local Consistency (RLGLC), our method\nleverages Asymmetrically-Relaxed Wasserstein of Wasserstein Distance (AR-WWD)\nto address class imbalance and semantic dimension weighting, and employs a\nlocal consistency mechanism to preserve fine-grained target-domain\ndiscriminative information. Extensive experiments across multiple benchmark\ndatasets demonstrate that RLGLC consistently surpasses state-of-the-art\nmethods, confirming the value of our theoretical perspective and underscoring\nthe necessity of enforcing both transferability and discriminability in\nadversarial-based UDA."}
{"id": "2505.21689", "pdf": "https://arxiv.org/pdf/2505.21689", "abs": "https://arxiv.org/abs/2505.21689", "authors": ["Avijit Gayen", "Somyajit Chakraborty", "Mainak Sen", "Soham Paul", "Angshuman Jana"], "title": "LLMPR: A Novel LLM-Driven Transfer Learning based Petition Ranking Model", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "28 pages, 5 figures, journal paper, submitted to AI and Law", "summary": "The persistent accumulation of unresolved legal cases, especially within the\nIndian judiciary, significantly hampers the timely delivery of justice. Manual\nmethods of prioritizing petitions are often prone to inefficiencies and\nsubjective biases further exacerbating delays. To address this issue, we\npropose LLMPR (Large Language Model-based Petition Ranking), an automated\nframework that utilizes transfer learning and machine learning to assign\npriority rankings to legal petitions based on their contextual urgency.\nLeveraging the ILDC dataset comprising 7,593 annotated petitions, we process\nunstructured legal text and extract features through various embedding\ntechniques, including DistilBERT, LegalBERT, and MiniLM. These textual\nembeddings are combined with quantitative indicators such as gap days, rank\nscores, and word counts to train multiple machine learning models, including\nRandom Forest, Decision Tree, XGBoost, LightGBM, and CatBoost. Our experiments\ndemonstrate that Random Forest and Decision Tree models yield superior\nperformance, with accuracy exceeding 99% and a Spearman rank correlation of\n0.99. Notably, models using only numerical features achieve nearly optimal\nranking results (R2 = 0.988, \\r{ho} = 0.998), while LLM-based embeddings offer\nonly marginal gains. These findings suggest that automated petition ranking can\neffectively streamline judicial workflows, reduce case backlog, and improve\nfairness in legal prioritization."}
{"id": "2505.22586", "pdf": "https://arxiv.org/pdf/2505.22586", "abs": "https://arxiv.org/abs/2505.22586", "authors": ["Yoav Gur-Arieh", "Clara Suslik", "Yihuai Hong", "Fazl Barez", "Mor Geva"], "title": "Precise In-Parameter Concept Erasure in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) often acquire knowledge during pretraining that\nis undesirable in downstream deployments, e.g., sensitive information or\ncopyrighted content. Existing approaches for removing such knowledge rely on\nfine-tuning, training low-rank adapters or fact-level editing, but these are\neither too coarse, too shallow, or ineffective. In this work, we propose PISCES\n(Precise In-parameter Suppression for Concept EraSure), a novel framework for\nprecisely erasing entire concepts from model parameters by directly editing\ndirections that encode them in parameter space. PISCES uses a disentangler\nmodel to decompose MLP vectors into interpretable features, identifies those\nassociated with a target concept using automated interpretability techniques,\nand removes them from model parameters. Experiments on Gemma 2 and Llama 3.1\nover various concepts show that PISCES achieves modest gains in efficacy over\nleading erasure methods, reducing accuracy on the target concept to as low as\n7.7%, while dramatically improving erasure specificity (by up to 31%) and\nrobustness (by up to 38%). Overall, these results demonstrate that\nfeature-based in-parameter editing enables a more precise and reliable approach\nfor removing conceptual knowledge in language models."}
{"id": "2505.22105", "pdf": "https://arxiv.org/pdf/2505.22105", "abs": "https://arxiv.org/abs/2505.22105", "authors": ["Hang Chen", "Maoyuan Ye", "Peng Yang", "Haibin He", "Juhua Liu", "Bo Du"], "title": "Adapting Segment Anything Model for Power Transmission Corridor Hazard Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Power transmission corridor hazard segmentation (PTCHS) aims to separate\ntransmission equipment and surrounding hazards from complex background,\nconveying great significance to maintaining electric power transmission safety.\nRecently, the Segment Anything Model (SAM) has emerged as a foundational vision\nmodel and pushed the boundaries of segmentation tasks. However, SAM struggles\nto deal with the target objects in complex transmission corridor scenario,\nespecially those with fine structure. In this paper, we propose ELE-SAM,\nadapting SAM for the PTCHS task. Technically, we develop a Context-Aware Prompt\nAdapter to achieve better prompt tokens via incorporating global-local features\nand focusing more on key regions. Subsequently, to tackle the hazard objects\nwith fine structure in complex background, we design a High-Fidelity Mask\nDecoder by leveraging multi-granularity mask features and then scaling them to\na higher resolution. Moreover, to train ELE-SAM and advance this field, we\nconstruct the ELE-40K benchmark, the first large-scale and real-world dataset\nfor PTCHS including 44,094 image-mask pairs. Experimental results for ELE-40K\ndemonstrate the superior performance that ELE-SAM outperforms the baseline\nmodel with the average 16.8% mIoU and 20.6% mBIoU performance improvement.\nMoreover, compared with the state-of-the-art method on HQSeg-44K, the average\n2.9% mIoU and 3.8% mBIoU absolute improvements further validate the\neffectiveness of our method on high-quality generic object segmentation. The\nsource code and dataset are available at https://github.com/Hhaizee/ELE-SAM."}
{"id": "2505.21699", "pdf": "https://arxiv.org/pdf/2505.21699", "abs": "https://arxiv.org/abs/2505.21699", "authors": ["Zhengbo Zhou", "Dooman Arefan", "Margarita Zuley", "Jules Sumkin", "Shandong Wu"], "title": "STA-Risk: A Deep Dive of Spatio-Temporal Asymmetries for Breast Cancer Risk Prediction", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Predicting the risk of developing breast cancer is an important clinical tool\nto guide early intervention and tailoring personalized screening strategies.\nEarly risk models have limited performance and recently machine learning-based\nanalysis of mammogram images showed encouraging risk prediction effects. These\nmodels however are limited to the use of a single exam or tend to overlook\nnuanced breast tissue evolvement in spatial and temporal details of\nlongitudinal imaging exams that are indicative of breast cancer risk. In this\npaper, we propose STA-Risk (Spatial and Temporal Asymmetry-based Risk\nPrediction), a novel Transformer-based model that captures fine-grained\nmammographic imaging evolution simultaneously from bilateral and longitudinal\nasymmetries for breast cancer risk prediction. STA-Risk is innovative by the\nside encoding and temporal encoding to learn spatial-temporal asymmetries,\nregulated by a customized asymmetry loss. We performed extensive experiments\nwith two independent mammogram datasets and achieved superior performance than\nfour representative SOTA models for 1- to 5-year future risk prediction. Source\ncodes will be released upon publishing of the paper."}
{"id": "2505.22591", "pdf": "https://arxiv.org/pdf/2505.22591", "abs": "https://arxiv.org/abs/2505.22591", "authors": ["Erxin Yu", "Jing Li", "Ming Liao", "Qi Zhu", "Boyang Xue", "Minghui Xu", "Baojun Wang", "Lanqing Hong", "Fei Mi", "Lifeng Shang"], "title": "Self-Error-Instruct: Generalizing from Errors for LLMs Mathematical Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "16 pages, 9 figures", "summary": "Although large language models demonstrate strong performance across various\ndomains, they still struggle with numerous bad cases in mathematical reasoning.\nPrevious approaches to learning from errors synthesize training data by solely\nextrapolating from isolated bad cases, thereby failing to generalize the\nextensive patterns inherent within these cases. This paper presents\nSelf-Error-Instruct (SEI), a framework that addresses these model weaknesses\nand synthesizes more generalized targeted training data. Specifically, we\nexplore a target model on two mathematical datasets, GSM8K and MATH, to\npinpoint bad cases. Then, we generate error keyphrases for these cases based on\nthe instructor model's (GPT-4o) analysis and identify error types by clustering\nthese keyphrases. Next, we sample a few bad cases during each generation for\neach identified error type and input them into the instructor model, which\nsynthesizes additional training data using a self-instruct approach. This new\ndata is refined through a one-shot learning process to ensure that only the\nmost effective examples are kept. Finally, we use these curated data to\nfine-tune the target model, iteratively repeating the process to enhance\nperformance. We apply our framework to various models and observe improvements\nin their reasoning abilities across both in-domain and out-of-domain\nmathematics datasets. These results demonstrate the effectiveness of self-error\ninstruction in improving LLMs' mathematical reasoning through error\ngeneralization."}
{"id": "2505.22111", "pdf": "https://arxiv.org/pdf/2505.22111", "abs": "https://arxiv.org/abs/2505.22111", "authors": ["Woonho Ko", "Jin Bok Park", "Il Yong Chun"], "title": "Autoregression-free video prediction using diffusion model for mitigating error propagation", "categories": ["cs.CV"], "comment": "6 pages, 4 figures, 2 tables", "summary": "Existing long-term video prediction methods often rely on an autoregressive\nvideo prediction mechanism. However, this approach suffers from error\npropagation, particularly in distant future frames. To address this limitation,\nthis paper proposes the first AutoRegression-Free (ARFree) video prediction\nframework using diffusion models. Different from an autoregressive video\nprediction mechanism, ARFree directly predicts any future frame tuples from the\ncontext frame tuple. The proposed ARFree consists of two key components: 1) a\nmotion prediction module that predicts a future motion using motion feature\nextracted from the context frame tuple; 2) a training method that improves\nmotion continuity and contextual consistency between adjacent future frame\ntuples. Our experiments with two benchmark datasets show that the proposed\nARFree video prediction framework outperforms several state-of-the-art video\nprediction methods."}
{"id": "2505.21703", "pdf": "https://arxiv.org/pdf/2505.21703", "abs": "https://arxiv.org/abs/2505.21703", "authors": ["Julia Boone", "Tolunay Seyfi", "Fatemeh Afghah"], "title": "A Joint Reconstruction-Triplet Loss Autoencoder Approach Towards Unseen Attack Detection in IoV Networks", "categories": ["cs.CR", "cs.AI", "cs.NI"], "comment": "Accepted for publication in the IEEE Internet of Things Journal\n  (IoT-J)", "summary": "Internet of Vehicles (IoV) systems, while offering significant advancements\nin transportation efficiency and safety, introduce substantial security\nvulnerabilities due to their highly interconnected nature. These dynamic\nsystems produce massive amounts of data between vehicles, infrastructure, and\ncloud services and present a highly distributed framework with a wide attack\nsurface. In considering network-centered attacks on IoV systems, attacks such\nas Denial-of-Service (DoS) can prohibit the communication of essential physical\ntraffic safety information between system elements, illustrating that the\nsecurity concerns for these systems go beyond the traditional confidentiality,\nintegrity, and availability concerns of enterprise systems. Given the\ncomplexity and volume of data generated by IoV systems, traditional security\nmechanisms are often inadequate for accurately detecting sophisticated and\nevolving cyberattacks. Here, we present an unsupervised autoencoder method\ntrained entirely on benign network data for the purpose of unseen attack\ndetection in IoV networks. We leverage a weighted combination of reconstruction\nand triplet margin loss to guide the autoencoder training and develop a diverse\nrepresentation of the benign training set. We conduct extensive experiments on\nrecent network intrusion datasets from two different application domains,\nindustrial IoT and home IoT, that represent the modern IoV task. We show that\nour method performs robustly for all unseen attack types, with roughly 99%\naccuracy on benign data and between 97% and 100% performance on anomaly data.\nWe extend these results to show that our model is adaptable through the use of\ntransfer learning, achieving similarly high results while leveraging domain\nfeatures from one domain to another."}
{"id": "2505.22618", "pdf": "https://arxiv.org/pdf/2505.22618", "abs": "https://arxiv.org/abs/2505.22618", "authors": ["Chengyue Wu", "Hao Zhang", "Shuchen Xue", "Zhijian Liu", "Shizhe Diao", "Ligeng Zhu", "Ping Luo", "Song Han", "Enze Xie"], "title": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding", "categories": ["cs.CL"], "comment": null, "summary": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs."}
{"id": "2505.22126", "pdf": "https://arxiv.org/pdf/2505.22126", "abs": "https://arxiv.org/abs/2505.22126", "authors": ["Yifan Chang", "Yukang Feng", "Jianwen Sun", "Jiaxin Ai", "Chuanhao Li", "S. Kevin Zhou", "Kaipeng Zhang"], "title": "SridBench: Benchmark of Scientific Research Illustration Drawing of Image Generation Model", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent years have seen rapid advances in AI-driven image generation. Early\ndiffusion models emphasized perceptual quality, while newer multimodal models\nlike GPT-4o-image integrate high-level reasoning, improving semantic\nunderstanding and structural composition. Scientific illustration generation\nexemplifies this evolution: unlike general image synthesis, it demands accurate\ninterpretation of technical content and transformation of abstract ideas into\nclear, standardized visuals. This task is significantly more\nknowledge-intensive and laborious, often requiring hours of manual work and\nspecialized tools. Automating it in a controllable, intelligent manner would\nprovide substantial practical value. Yet, no benchmark currently exists to\nevaluate AI on this front. To fill this gap, we introduce SridBench, the first\nbenchmark for scientific figure generation. It comprises 1,120 instances\ncurated from leading scientific papers across 13 natural and computer science\ndisciplines, collected via human experts and MLLMs. Each sample is evaluated\nalong six dimensions, including semantic fidelity and structural accuracy.\nExperimental results reveal that even top-tier models like GPT-4o-image lag\nbehind human performance, with common issues in text/visual clarity and\nscientific correctness. These findings highlight the need for more advanced\nreasoning-driven visual generation capabilities."}
{"id": "2505.21715", "pdf": "https://arxiv.org/pdf/2505.21715", "abs": "https://arxiv.org/abs/2505.21715", "authors": ["Md. Zahid Hossain", "Mustofa Ahmed", "Most. Sharmin Sultana Samu", "Md. Rakibul Islam"], "title": "Privacy-Preserving Chest X-ray Report Generation via Multimodal Federated Learning with ViT and GPT-2", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Preprint, manuscript under-review", "summary": "The automated generation of radiology reports from chest X-ray images holds\nsignificant promise in enhancing diagnostic workflows while preserving patient\nprivacy. Traditional centralized approaches often require sensitive data\ntransfer, posing privacy concerns. To address this, the study proposes a\nMultimodal Federated Learning framework for chest X-ray report generation using\nthe IU-Xray dataset. The system utilizes a Vision Transformer (ViT) as the\nencoder and GPT-2 as the report generator, enabling decentralized training\nwithout sharing raw data. Three Federated Learning (FL) aggregation strategies:\nFedAvg, Krum Aggregation and a novel Loss-aware Federated Averaging (L-FedAvg)\nwere evaluated. Among these, Krum Aggregation demonstrated superior performance\nacross lexical and semantic evaluation metrics such as ROUGE, BLEU, BERTScore\nand RaTEScore. The results show that FL can match or surpass centralized models\nin generating clinically relevant and semantically rich radiology reports. This\nlightweight and privacy-preserving framework paves the way for collaborative\nmedical AI development without compromising data confidentiality."}
{"id": "2505.22627", "pdf": "https://arxiv.org/pdf/2505.22627", "abs": "https://arxiv.org/abs/2505.22627", "authors": ["Yijun Shen", "Delong Chen", "Fan Liu", "Xingyu Wang", "Chuanyi Zhang", "Liang Yao", "Yuhui Zheng"], "title": "Chain-of-Talkers (CoTalk): Fast Human Annotation of Dense Image Captions", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "While densely annotated image captions significantly facilitate the learning\nof robust vision-language alignment, methodologies for systematically\noptimizing human annotation efforts remain underexplored. We introduce\nChain-of-Talkers (CoTalk), an AI-in-the-loop methodology designed to maximize\nthe number of annotated samples and improve their comprehensiveness under fixed\nbudget constraints (e.g., total human annotation time). The framework is built\nupon two key insights. First, sequential annotation reduces redundant workload\ncompared to conventional parallel annotation, as subsequent annotators only\nneed to annotate the ``residual'' -- the missing visual information that\nprevious annotations have not covered. Second, humans process textual input\nfaster by reading while outputting annotations with much higher throughput via\ntalking; thus a multimodal interface enables optimized efficiency. We evaluate\nour framework from two aspects: intrinsic evaluations that assess the\ncomprehensiveness of semantic units, obtained by parsing detailed captions into\nobject-attribute trees and analyzing their effective connections; extrinsic\nevaluation measures the practical usage of the annotated captions in\nfacilitating vision-language alignment. Experiments with eight participants\nshow our Chain-of-Talkers (CoTalk) improves annotation speed (0.42 vs. 0.30\nunits/sec) and retrieval performance (41.13\\% vs. 40.52\\%) over the parallel\nmethod."}
{"id": "2505.22128", "pdf": "https://arxiv.org/pdf/2505.22128", "abs": "https://arxiv.org/abs/2505.22128", "authors": ["Alejandro D. Mousist"], "title": "Real-Time Blind Defocus Deblurring for Earth Observation: The IMAGIN-e Mission Approach", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This work addresses mechanical defocus in Earth observation images from the\nIMAGIN-e mission aboard the ISS, proposing a blind deblurring approach adapted\nto space-based edge computing constraints. Leveraging Sentinel-2 data, our\nmethod estimates the defocus kernel and trains a restoration model within a GAN\nframework, effectively operating without reference images.\n  On Sentinel-2 images with synthetic degradation, SSIM improved by 72.47% and\nPSNR by 25.00%, confirming the model's ability to recover lost details when the\noriginal clean image is known. On IMAGIN-e, where no reference images exist,\nperceptual quality metrics indicate a substantial enhancement, with NIQE\nimproving by 60.66% and BRISQUE by 48.38%, validating real-world onboard\nrestoration. The approach is currently deployed aboard the IMAGIN-e mission,\ndemonstrating its practical application in an operational space environment.\n  By efficiently handling high-resolution images under edge computing\nconstraints, the method enables applications such as water body segmentation\nand contour detection while maintaining processing viability despite resource\nlimitations."}
{"id": "2505.21717", "pdf": "https://arxiv.org/pdf/2505.21717", "abs": "https://arxiv.org/abs/2505.21717", "authors": ["Mónika Farsang", "Ramin Hasani", "Radu Grosu"], "title": "Scaling Up Liquid-Resistance Liquid-Capacitance Networks for Efficient Sequence Modeling", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": null, "summary": "We present LrcSSM, a \\textit{nonlinear} recurrent model that processes long\nsequences as fast as today's linear state-space layers. By forcing the\nstate-transition matrix to be diagonal and learned at every step, the full\nsequence can be solved in parallel with a single prefix-scan, giving\n$\\mathcal{O}(TD)$ time and memory and only $\\mathcal{O}(\\log T)$ sequential\ndepth, for input-sequence length $T$ and a state dimension $D$. Moreover,\nLrcSSM offers a formal gradient-stability guarantee that other input-varying\nsystems such as Liquid-S4 and Mamba do not provide. Lastly, for network depth\n$L$, as the forward and backward passes cost $\\Theta(T\\,D\\,L)$ FLOPs, with its\nlow sequential depth and parameter count $\\Theta(D\\,L)$, the model follows the\ncompute-optimal scaling law regime ($\\beta \\approx 0.42$) recently observed for\nMamba, outperforming quadratic-attention Transformers at equal compute while\navoiding the memory overhead of FFT-based long convolutions. We show that on a\nseries of long-range forecasting tasks, LrcSSM outperforms LRU, S5 and Mamba."}
{"id": "2505.22630", "pdf": "https://arxiv.org/pdf/2505.22630", "abs": "https://arxiv.org/abs/2505.22630", "authors": ["Ziling Cheng", "Meng Cao", "Marc-Antoine Rondeau", "Jackie Chi Kit Cheung"], "title": "Stochastic Chameleons: Irrelevant Context Hallucinations Reveal Class-Based (Mis)Generalization in LLMs", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Main Conference)", "summary": "The widespread success of large language models (LLMs) on NLP benchmarks has\nbeen accompanied by concerns that LLMs function primarily as stochastic parrots\nthat reproduce texts similar to what they saw during pre-training, often\nerroneously. But what is the nature of their errors, and do these errors\nexhibit any regularities? In this work, we examine irrelevant context\nhallucinations, in which models integrate misleading contextual cues into their\npredictions. Through behavioral analysis, we show that these errors result from\na structured yet flawed mechanism that we term class-based (mis)generalization,\nin which models combine abstract class cues with features extracted from the\nquery or context to derive answers. Furthermore, mechanistic interpretability\nexperiments on Llama-3, Mistral, and Pythia across 39 factual recall relation\ntypes reveal that this behavior is reflected in the model's internal\ncomputations: (i) abstract class representations are constructed in lower\nlayers before being refined into specific answers in higher layers, (ii)\nfeature selection is governed by two competing circuits -- one prioritizing\ndirect query-based reasoning, the other incorporating contextual cues -- whose\nrelative influences determine the final output. Our findings provide a more\nnuanced perspective on the stochastic parrot argument: through form-based\ntraining, LLMs can exhibit generalization leveraging abstractions, albeit in\nunreliable ways based on contextual cues -- what we term stochastic chameleons."}
{"id": "2505.22129", "pdf": "https://arxiv.org/pdf/2505.22129", "abs": "https://arxiv.org/abs/2505.22129", "authors": ["Jinhong Ni", "Chang-Bin Zhang", "Qiang Zhang", "Jing Zhang"], "title": "What Makes for Text to 360-degree Panorama Generation with Stable Diffusion?", "categories": ["cs.CV"], "comment": null, "summary": "Recent prosperity of text-to-image diffusion models, e.g. Stable Diffusion,\nhas stimulated research to adapt them to 360-degree panorama generation. Prior\nwork has demonstrated the feasibility of using conventional low-rank adaptation\ntechniques on pre-trained diffusion models to generate panoramic images.\nHowever, the substantial domain gap between perspective and panoramic images\nraises questions about the underlying mechanisms enabling this empirical\nsuccess. We hypothesize and examine that the trainable counterparts exhibit\ndistinct behaviors when fine-tuned on panoramic data, and such an adaptation\nconceals some intrinsic mechanism to leverage the prior knowledge within the\npre-trained diffusion models. Our analysis reveals the following: 1) the query\nand key matrices in the attention modules are responsible for common\ninformation that can be shared between the panoramic and perspective domains,\nthus are less relevant to panorama generation; and 2) the value and output\nweight matrices specialize in adapting pre-trained knowledge to the panoramic\ndomain, playing a more critical role during fine-tuning for panorama\ngeneration. We empirically verify these insights by introducing a simple\nframework called UniPano, with the objective of establishing an elegant\nbaseline for future research. UniPano not only outperforms existing methods but\nalso significantly reduces memory usage and training time compared to prior\ndual-branch approaches, making it scalable for end-to-end panorama generation\nwith higher resolution. The code will be released."}
{"id": "2505.21720", "pdf": "https://arxiv.org/pdf/2505.21720", "abs": "https://arxiv.org/abs/2505.21720", "authors": ["Vanessa Utz"], "title": "Responsible Data Stewardship: Generative AI and the Digital Waste Problem", "categories": ["cs.CY", "cs.AI"], "comment": "8 pages, submitted to AAAI/ACM Conference on AI, Ethics and Society", "summary": "As generative AI systems become widely adopted, they enable unprecedented\ncreation levels of synthetic data across text, images, audio, and video\nmodalities. While research has addressed the energy consumption of model\ntraining and inference, a critical sustainability challenge remains\nunderstudied: digital waste. This term refers to stored data that consumes\nresources without serving a specific (and/or immediate) purpose. This paper\npresents this terminology in the AI context and introduces digital waste as an\nethical imperative within (generative) AI development, positioning\nenvironmental sustainability as core for responsible innovation. Drawing from\nestablished digital resource management approaches, we examine how other\ndisciplines manage digital waste and identify transferable approaches for the\nAI community. We propose specific recommendations encompassing re-search\ndirections, technical interventions, and cultural shifts to mitigate the\nenvironmental consequences of in-definite data storage. By expanding AI ethics\nbeyond immediate concerns like bias and privacy to include inter-generational\nenvironmental justice, this work contributes to a more comprehensive ethical\nframework that considers the complete lifecycle impact of generative AI\nsystems."}
{"id": "2505.22633", "pdf": "https://arxiv.org/pdf/2505.22633", "abs": "https://arxiv.org/abs/2505.22633", "authors": ["Yida Xue", "Zhen Bi", "Jinnan Yang", "Jungang Lou", "Huajun Chen", "Ningyu Zhang"], "title": "Spatial Knowledge Graph-Guided Multimodal Synthesis", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "comment": "Ongoing work", "summary": "Recent advances in multimodal large language models (MLLMs) have\nsignificantly enhanced their capabilities; however, their spatial perception\nabilities remain a notable limitation. To address this challenge, multimodal\ndata synthesis offers a promising solution. Yet, ensuring that synthesized data\nadhere to spatial common sense is a non-trivial task. In this work, we\nintroduce SKG2Data, a novel multimodal synthesis approach guided by spatial\nknowledge graphs, grounded in the concept of knowledge-to-data generation.\nSKG2Data automatically constructs a Spatial Knowledge Graph (SKG) to emulate\nhuman-like perception of spatial directions and distances, which is\nsubsequently utilized to guide multimodal data synthesis. Extensive experiments\ndemonstrate that data synthesized from diverse types of spatial knowledge,\nincluding direction and distance, not only enhance the spatial perception and\nreasoning abilities of MLLMs but also exhibit strong generalization\ncapabilities. We hope that the idea of knowledge-based data synthesis can\nadvance the development of spatial intelligence."}
{"id": "2505.22141", "pdf": "https://arxiv.org/pdf/2505.22141", "abs": "https://arxiv.org/abs/2505.22141", "authors": ["Guanwen Feng", "Zhiyuan Ma", "Yunan Li", "Junwei Jing", "Jiahao Yang", "Qiguang Miao"], "title": "FaceEditTalker: Interactive Talking Head Generation with Facial Attribute Editing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in audio-driven talking head generation have achieved\nimpressive results in lip synchronization and emotional expression. However,\nthey largely overlook the crucial task of facial attribute editing. This\ncapability is crucial for achieving deep personalization and expanding the\nrange of practical applications, including user-tailored digital avatars,\nengaging online education content, and brand-specific digital customer service.\nIn these key domains, the flexible adjustment of visual attributes-such as\nhairstyle, accessories, and subtle facial features is essential for aligning\nwith user preferences, reflecting diverse brand identities, and adapting to\nvarying contextual demands. In this paper, we present FaceEditTalker, a unified\nframework that enables controllable facial attribute manipulation while\ngenerating high-quality, audio-synchronized talking head videos. Our method\nconsists of two key components: an image feature space editing module, which\nextracts semantic and detail features and allows flexible control over\nattributes like expression, hairstyle, and accessories; and an audio-driven\nvideo generation module, which fuses these edited features with audio-guided\nfacial landmarks to drive a diffusion-based generator. This design ensures\ntemporal coherence, visual fidelity, and identity preservation across frames.\nExtensive experiments on public datasets demonstrate that our method\noutperforms state-of-the-art approaches in lip-sync accuracy, video quality,\nand attribute controllability. Project page:\nhttps://peterfanfan.github.io/FaceEditTalker/"}
{"id": "2505.21722", "pdf": "https://arxiv.org/pdf/2505.21722", "abs": "https://arxiv.org/abs/2505.21722", "authors": ["Ioannis Bantzis", "James B. Simon", "Arthur Jacot"], "title": "Saddle-To-Saddle Dynamics in Deep ReLU Networks: Low-Rank Bias in the First Saddle Escape", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "When a deep ReLU network is initialized with small weights, GD is at first\ndominated by the saddle at the origin in parameter space. We study the\nso-called escape directions, which play a similar role as the eigenvectors of\nthe Hessian for strict saddles. We show that the optimal escape direction\nfeatures a low-rank bias in its deeper layers: the first singular value of the\n$\\ell$-th layer weight matrix is at least $\\ell^{\\frac{1}{4}}$ larger than any\nother singular value. We also prove a number of related results about these\nescape directions. We argue that this result is a first step in proving\nSaddle-to-Saddle dynamics in deep ReLU networks, where GD visits a sequence of\nsaddles with increasing bottleneck rank."}
{"id": "2505.22635", "pdf": "https://arxiv.org/pdf/2505.22635", "abs": "https://arxiv.org/abs/2505.22635", "authors": ["Fangcong Yin", "Zeyu Leo Liu", "Liu Leqi", "Xi Ye", "Greg Durrett"], "title": "Learning Composable Chains-of-Thought", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "A common approach for teaching large language models (LLMs) to reason is to\ntrain on chain-of-thought (CoT) traces of in-distribution reasoning problems,\nbut such annotated data is costly to obtain for every problem of interest. We\nwant reasoning models to generalize beyond their training distribution, and\nideally to generalize compositionally: combine atomic reasoning skills to solve\nharder, unseen reasoning tasks. We take a step towards compositional\ngeneralization of reasoning skills when addressing a target compositional task\nthat has no labeled CoT data. We find that simply training models on CoT data\nof atomic tasks leads to limited generalization, but minimally modifying CoT\nformats of constituent atomic tasks to be composable can lead to improvements.\nWe can train \"atomic CoT\" models on the atomic tasks with Composable CoT data\nand combine them with multitask learning or model merging for better zero-shot\nperformance on the target compositional task. Such a combined model can be\nfurther bootstrapped on a small amount of compositional data using rejection\nsampling fine-tuning (RFT). Results on string operations and natural language\nskill compositions show that training LLMs on Composable CoT outperforms\nmultitask learning and continued fine-tuning baselines within a given training\ndata budget."}
{"id": "2505.22143", "pdf": "https://arxiv.org/pdf/2505.22143", "abs": "https://arxiv.org/abs/2505.22143", "authors": ["Fengyun Wang", "Sicheng Yu", "Jiawei Wu", "Jinhui Tang", "Hanwang Zhang", "Qianru Sun"], "title": "3D Question Answering via only 2D Vision-Language Models", "categories": ["cs.CV"], "comment": "ICML2025", "summary": "Large vision-language models (LVLMs) have significantly advanced numerous\nfields. In this work, we explore how to harness their potential to address 3D\nscene understanding tasks, using 3D question answering (3D-QA) as a\nrepresentative example. Due to the limited training data in 3D, we do not train\nLVLMs but infer in a zero-shot manner. Specifically, we sample 2D views from a\n3D point cloud and feed them into 2D models to answer a given question. When\nthe 2D model is chosen, e.g., LLAVA-OV, the quality of sampled views matters\nthe most. We propose cdViews, a novel approach to automatically selecting\ncritical and diverse Views for 3D-QA. cdViews consists of two key components:\nviewSelector prioritizing critical views based on their potential to provide\nanswer-specific information, and viewNMS enhancing diversity by removing\nredundant views based on spatial overlap. We evaluate cdViews on the\nwidely-used ScanQA and SQA benchmarks, demonstrating that it achieves\nstate-of-the-art performance in 3D-QA while relying solely on 2D models without\nfine-tuning. These findings support our belief that 2D LVLMs are currently the\nmost effective alternative (of the resource-intensive 3D LVLMs) for addressing\n3D tasks."}
{"id": "2505.21724", "pdf": "https://arxiv.org/pdf/2505.21724", "abs": "https://arxiv.org/abs/2505.21724", "authors": ["Cheng Luo", "Jianghui Wang", "Bing Li", "Siyang Song", "Bernard Ghanem"], "title": "OmniResponse: Online Multimodal Conversational Response Generation in Dyadic Interactions", "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": "23 pages, 9 figures", "summary": "In this paper, we introduce Online Multimodal Conversational Response\nGeneration (OMCRG), a novel task that aims to online generate synchronized\nverbal and non-verbal listener feedback, conditioned on the speaker's\nmultimodal input. OMCRG reflects natural dyadic interactions and poses new\nchallenges in achieving synchronization between the generated audio and facial\nresponses of the listener. To address these challenges, we innovatively\nintroduce text as an intermediate modality to bridge the audio and facial\nresponses. We hence propose OmniResponse, a Multimodal Large Language Model\n(MLLM) that autoregressively generates high-quality multi-modal listener\nresponses. OmniResponse leverages a pretrained LLM enhanced with two novel\ncomponents: Chrono-Text, which temporally anchors generated text tokens, and\nTempoVoice, a controllable online TTS module that produces speech synchronized\nwith facial reactions. To support further OMCRG research, we present\nResponseNet, a new dataset comprising 696 high-quality dyadic interactions\nfeaturing synchronized split-screen videos, multichannel audio, transcripts,\nand facial behavior annotations. Comprehensive evaluations conducted on\nResponseNet demonstrate that OmniResponse significantly outperforms baseline\nmodels in terms of semantic speech content, audio-visual synchronization, and\ngeneration quality."}
{"id": "2505.22645", "pdf": "https://arxiv.org/pdf/2505.22645", "abs": "https://arxiv.org/abs/2505.22645", "authors": ["Hanjia Lyu", "Jiebo Luo", "Jian Kang", "Allison Koenecke"], "title": "Characterizing Bias: Benchmarking Large Language Models in Simplified versus Traditional Chinese", "categories": ["cs.CL", "cs.CY"], "comment": "To appear in the 2025 ACM Conference on Fairness, Accountability, and\n  Transparency (FAccT '25)", "summary": "While the capabilities of Large Language Models (LLMs) have been studied in\nboth Simplified and Traditional Chinese, it is yet unclear whether LLMs exhibit\ndifferential performance when prompted in these two variants of written\nChinese. This understanding is critical, as disparities in the quality of LLM\nresponses can perpetuate representational harms by ignoring the different\ncultural contexts underlying Simplified versus Traditional Chinese, and can\nexacerbate downstream harms in LLM-facilitated decision-making in domains such\nas education or hiring. To investigate potential LLM performance disparities,\nwe design two benchmark tasks that reflect real-world scenarios: regional term\nchoice (prompting the LLM to name a described item which is referred to\ndifferently in Mainland China and Taiwan), and regional name choice (prompting\nthe LLM to choose who to hire from a list of names in both Simplified and\nTraditional Chinese). For both tasks, we audit the performance of 11 leading\ncommercial LLM services and open-sourced models -- spanning those primarily\ntrained on English, Simplified Chinese, or Traditional Chinese. Our analyses\nindicate that biases in LLM responses are dependent on both the task and\nprompting language: while most LLMs disproportionately favored Simplified\nChinese responses in the regional term choice task, they surprisingly favored\nTraditional Chinese names in the regional name choice task. We find that these\ndisparities may arise from differences in training data representation, written\ncharacter preferences, and tokenization of Simplified and Traditional Chinese.\nThese findings highlight the need for further analysis of LLM biases; as such,\nwe provide an open-sourced benchmark dataset to foster reproducible evaluations\nof future LLM behavior across Chinese language variants\n(https://github.com/brucelyu17/SC-TC-Bench)."}
{"id": "2505.22146", "pdf": "https://arxiv.org/pdf/2505.22146", "abs": "https://arxiv.org/abs/2505.22146", "authors": ["Guangfu Hao", "Haojie Wen", "Liangxuna Guo", "Yang Chen", "Yanchao Bi", "Shan Yu"], "title": "Flexible Tool Selection through Low-dimensional Attribute Alignment of Vision and Language", "categories": ["cs.CV", "cs.AI", "cs.CL", "q-bio.NC"], "comment": null, "summary": "Flexible tool selection reflects a complex cognitive ability that\ndistinguishes humans from other species, yet computational models that capture\nthis ability remain underdeveloped. We developed a framework using\nlow-dimensional attribute representations to bridge visual tool perception and\nlinguistic task understanding. We constructed a comprehensive dataset (ToolNet)\ncontaining 115 common tools labeled with 13 carefully designed attributes\nspanning physical, functional, and psychological properties, paired with\nnatural language scenarios describing tool usage. Visual encoders (ResNet or\nViT) extract attributes from tool images while fine-tuned language models\n(GPT-2, LLaMA, DeepSeek) derive required attributes from task descriptions. Our\napproach achieves 74% accuracy in tool selection tasks-significantly\noutperforming direct tool matching (20%) and smaller multimodal models\n(21%-58%), while approaching performance of much larger models like GPT-4o\n(73%) with substantially fewer parameters. Ablation studies revealed that\nmanipulation-related attributes (graspability, hand-relatedness, elongation)\nconsistently prove most critical across modalities. This work provides a\nparameter-efficient, interpretable solution that mimics human-like tool\ncognition, advancing both cognitive science understanding and practical\napplications in tool selection tasks."}
{"id": "2505.21731", "pdf": "https://arxiv.org/pdf/2505.21731", "abs": "https://arxiv.org/abs/2505.21731", "authors": ["Quentin Delfosse", "Jannis Blüml", "Fabian Tatai", "Théo Vincent", "Bjarne Gregori", "Elisabeth Dillies", "Jan Peters", "Constantin Rothkopf", "Kristian Kersting"], "title": "Deep Reinforcement Learning Agents are not even close to Human Intelligence", "categories": ["cs.LG", "cs.AI"], "comment": "49 pages in total, 5 main figures, 14 figures total", "summary": "Deep reinforcement learning (RL) agents achieve impressive results in a wide\nvariety of tasks, but they lack zero-shot adaptation capabilities. While most\nrobustness evaluations focus on tasks complexifications, for which human also\nstruggle to maintain performances, no evaluation has been performed on tasks\nsimplifications. To tackle this issue, we introduce HackAtari, a set of task\nvariations of the Arcade Learning Environments. We use it to demonstrate that,\ncontrary to humans, RL agents systematically exhibit huge performance drops on\nsimpler versions of their training tasks, uncovering agents' consistent\nreliance on shortcuts. Our analysis across multiple algorithms and\narchitectures highlights the persistent gap between RL agents and human\nbehavioral intelligence, underscoring the need for new benchmarks and\nmethodologies that enforce systematic generalization testing beyond static\nevaluation protocols. Training and testing in the same environment is not\nenough to obtain agents equipped with human-like intelligence."}
{"id": "2505.22648", "pdf": "https://arxiv.org/pdf/2505.22648", "abs": "https://arxiv.org/abs/2505.22648", "authors": ["Jialong Wu", "Baixuan Li", "Runnan Fang", "Wenbiao Yin", "Liwen Zhang", "Zhengwei Tao", "Dingchu Zhang", "Zekun Xi", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Jingren Zhou"], "title": "WebDancer: Towards Autonomous Information Seeking Agency", "categories": ["cs.CL"], "comment": null, "summary": "Addressing intricate real-world problems necessitates in-depth information\nseeking and multi-step reasoning. Recent progress in agentic systems,\nexemplified by Deep Research, underscores the potential for autonomous\nmulti-step research. In this work, we present a cohesive paradigm for building\nend-to-end agentic information seeking agents from a data-centric and\ntraining-stage perspective. Our approach consists of four key stages: (1)\nbrowsing data construction, (2) trajectories sampling, (3) supervised\nfine-tuning for effective cold start, and (4) reinforcement learning for\nenhanced generalisation. We instantiate this framework in a web agent based on\nthe ReAct, WebDancer. Empirical evaluations on the challenging information\nseeking benchmarks, GAIA and WebWalkerQA, demonstrate the strong performance of\nWebDancer, achieving considerable results and highlighting the efficacy of our\ntraining paradigm. Further analysis of agent training provides valuable\ninsights and actionable, systematic pathways for developing more capable\nagentic models. The codes and demo will be released in\nhttps://github.com/Alibaba-NLP/WebAgent."}
{"id": "2505.22150", "pdf": "https://arxiv.org/pdf/2505.22150", "abs": "https://arxiv.org/abs/2505.22150", "authors": ["Runze Xia", "Shuo Feng", "Renzhi Wang", "Congchi Yin", "Xuyun Wen", "Piji Li"], "title": "Improving Brain-to-Image Reconstruction via Fine-Grained Text Bridging", "categories": ["cs.CV", "cs.CL"], "comment": "CogSci2025", "summary": "Brain-to-Image reconstruction aims to recover visual stimuli perceived by\nhumans from brain activity. However, the reconstructed visual stimuli often\nmissing details and semantic inconsistencies, which may be attributed to\ninsufficient semantic information. To address this issue, we propose an\napproach named Fine-grained Brain-to-Image reconstruction (FgB2I), which\nemploys fine-grained text as bridge to improve image reconstruction. FgB2I\ncomprises three key stages: detail enhancement, decoding fine-grained text\ndescriptions, and text-bridged brain-to-image reconstruction. In the\ndetail-enhancement stage, we leverage large vision-language models to generate\nfine-grained captions for visual stimuli and experimentally validate its\nimportance. We propose three reward metrics (object accuracy, text-image\nsemantic similarity, and image-image semantic similarity) to guide the language\nmodel in decoding fine-grained text descriptions from fMRI signals. The\nfine-grained text descriptions can be integrated into existing reconstruction\nmethods to achieve fine-grained Brain-to-Image reconstruction."}
{"id": "2505.21740", "pdf": "https://arxiv.org/pdf/2505.21740", "abs": "https://arxiv.org/abs/2505.21740", "authors": ["Marvin Limpijankit", "Yanda Chen", "Melanie Subbiah", "Nicholas Deas", "Kathleen McKeown"], "title": "Counterfactual Simulatability of LLM Explanations for Generation Tasks", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "LLMs can be unpredictable, as even slight alterations to the prompt can cause\nthe output to change in unexpected ways. Thus, the ability of models to\naccurately explain their behavior is critical, especially in high-stakes\nsettings. One approach for evaluating explanations is counterfactual\nsimulatability, how well an explanation allows users to infer the model's\noutput on related counterfactuals. Counterfactual simulatability has been\npreviously studied for yes/no question answering tasks. We provide a general\nframework for extending this method to generation tasks, using news\nsummarization and medical suggestion as example use cases. We find that while\nLLM explanations do enable users to better predict LLM outputs on\ncounterfactuals in the summarization setting, there is significant room for\nimprovement for medical suggestion. Furthermore, our results suggest that the\nevaluation for counterfactual simulatability may be more appropriate for\nskill-based tasks as opposed to knowledge-based tasks."}
{"id": "2505.22653", "pdf": "https://arxiv.org/pdf/2505.22653", "abs": "https://arxiv.org/abs/2505.22653", "authors": ["Ang Lv", "Ruobing Xie", "Xingwu Sun", "Zhanhui Kang", "Rui Yan"], "title": "The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in Learning to Reason", "categories": ["cs.CL"], "comment": "Preprint", "summary": "Recent studies on post-training large language models (LLMs) for reasoning\nthrough reinforcement learning (RL) typically focus on tasks that can be\naccurately verified and rewarded, such as solving math problems. In contrast,\nour research investigates the impact of reward noise, a more practical\nconsideration for real-world scenarios involving the post-training of LLMs\nusing reward models. We found that LLMs demonstrate strong robustness to\nsubstantial reward noise. For example, manually flipping 40% of the reward\nfunction's outputs in math tasks still allows a Qwen-2.5-7B model to achieve\nrapid convergence, improving its performance on math tasks from 5% to 72%,\ncompared to the 75% accuracy achieved by a model trained with noiseless\nrewards. Surprisingly, by only rewarding the appearance of key reasoning\nphrases (namely reasoning pattern reward, RPR), such as ``first, I need\nto''-without verifying the correctness of answers, the model achieved peak\ndownstream performance (over 70% accuracy for Qwen-2.5-7B) comparable to models\ntrained with strict correctness verification and accurate rewards. Recognizing\nthe importance of the reasoning process over the final results, we combined RPR\nwith noisy reward models. RPR helped calibrate the noisy reward models,\nmitigating potential false negatives and enhancing the LLM's performance on\nopen-ended tasks. These findings suggest the importance of improving models'\nfoundational abilities during the pre-training phase while providing insights\nfor advancing post-training techniques. Our code and scripts are available at\nhttps://github.com/trestad/Noisy-Rewards-in-Learning-to-Reason."}
{"id": "2505.22154", "pdf": "https://arxiv.org/pdf/2505.22154", "abs": "https://arxiv.org/abs/2505.22154", "authors": ["Chao Tian", "Chao Yang", "Guoqing Zhu", "Qiang Wang", "Zhenyu He"], "title": "Learning A Robust RGB-Thermal Detector for Extreme Modality Imbalance", "categories": ["cs.CV"], "comment": null, "summary": "RGB-Thermal (RGB-T) object detection utilizes thermal infrared (TIR) images\nto complement RGB data, improving robustness in challenging conditions.\nTraditional RGB-T detectors assume balanced training data, where both\nmodalities contribute equally. However, in real-world scenarios, modality\ndegradation-due to environmental factors or technical issues-can lead to\nextreme modality imbalance, causing out-of-distribution (OOD) issues during\ntesting and disrupting model convergence during training. This paper addresses\nthese challenges by proposing a novel base-and-auxiliary detector architecture.\nWe introduce a modality interaction module to adaptively weigh modalities based\non their quality and handle imbalanced samples effectively. Additionally, we\nleverage modality pseudo-degradation to simulate real-world imbalances in\ntraining data. The base detector, trained on high-quality pairs, provides a\nconsistency constraint for the auxiliary detector, which receives degraded\nsamples. This framework enhances model robustness, ensuring reliable\nperformance even under severe modality degradation. Experimental results\ndemonstrate the effectiveness of our method in handling extreme modality\nimbalances~(decreasing the Missing Rate by 55%) and improving performance\nacross various baseline detectors."}
{"id": "2505.21743", "pdf": "https://arxiv.org/pdf/2505.21743", "abs": "https://arxiv.org/abs/2505.21743", "authors": ["Zihao Li", "Xinyuan Cao", "Xiangbo Gao", "Kexin Tian", "Keshu Wu", "Mohammad Anis", "Hao Zhang", "Keke Long", "Jiwan Jiang", "Xiaopeng Li", "Yunlong Zhang", "Tianbao Yang", "Dominique Lord", "Zhengzhong Tu", "Yang Zhou"], "title": "Simulating the Unseen: Crash Prediction Must Learn from What Did Not Happen", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Traffic safety science has long been hindered by a fundamental data paradox:\nthe crashes we most wish to prevent are precisely those events we rarely\nobserve. Existing crash-frequency models and surrogate safety metrics rely\nheavily on sparse, noisy, and under-reported records, while even sophisticated,\nhigh-fidelity simulations undersample the long-tailed situations that trigger\ncatastrophic outcomes such as fatalities. We argue that the path to achieving\nVision Zero, i.e., the complete elimination of traffic fatalities and severe\ninjuries, requires a paradigm shift from traditional crash-only learning to a\nnew form of counterfactual safety learning: reasoning not only about what\nhappened, but also about the vast set of plausible yet perilous scenarios that\ncould have happened under slightly different circumstances. To operationalize\nthis shift, our proposed agenda bridges macro to micro. Guided by crash-rate\npriors, generative scene engines, diverse driver models, and causal learning,\nnear-miss events are synthesized and explained. A crash-focused digital twin\ntestbed links micro scenes to macro patterns, while a multi-objective validator\nensures that simulations maintain statistical realism. This pipeline transforms\nsparse crash data into rich signals for crash prediction, enabling the\nstress-testing of vehicles, roads, and policies before deployment. By learning\nfrom crashes that almost happened, we can shift traffic safety from reactive\nforensics to proactive prevention, advancing Vision Zero."}
{"id": "2505.22661", "pdf": "https://arxiv.org/pdf/2505.22661", "abs": "https://arxiv.org/abs/2505.22661", "authors": ["Qingchen Yu", "Zifan Zheng", "Ding Chen", "Simin Niu", "Bo Tang", "Feiyu Xiong", "Zhiyu Li"], "title": "GuessArena: Guess Who I Am? A Self-Adaptive Framework for Evaluating LLMs in Domain-Specific Knowledge and Reasoning", "categories": ["cs.CL"], "comment": "Accepted by ACL 2025", "summary": "The evaluation of large language models (LLMs) has traditionally relied on\nstatic benchmarks, a paradigm that poses two major limitations: (1) predefined\ntest sets lack adaptability to diverse application domains, and (2)\nstandardized evaluation protocols often fail to capture fine-grained\nassessments of domain-specific knowledge and contextual reasoning abilities. To\novercome these challenges, we propose GuessArena, an adaptive evaluation\nframework grounded in adversarial game-based interactions. Inspired by the\ninteractive structure of the Guess Who I Am? game, our framework seamlessly\nintegrates dynamic domain knowledge modeling with progressive reasoning\nassessment to improve evaluation fidelity. Empirical studies across five\nvertical domains-finance, healthcare, manufacturing, information technology,\nand education-demonstrate that GuessArena effectively distinguishes LLMs in\nterms of domain knowledge coverage and reasoning chain completeness. Compared\nto conventional benchmarks, our method provides substantial advantages in\ninterpretability, scalability, and scenario adaptability."}
{"id": "2505.22167", "pdf": "https://arxiv.org/pdf/2505.22167", "abs": "https://arxiv.org/abs/2505.22167", "authors": ["Weilun Feng", "Chuanguang Yang", "Haotong Qin", "Xiangqi Li", "Yu Wang", "Zhulin An", "Libo Huang", "Boyu Diao", "Zixiang Zhao", "Yongjun Xu", "Michele Magno"], "title": "Q-VDiT: Towards Accurate Quantization and Distillation of Video-Generation Diffusion Transformers", "categories": ["cs.CV"], "comment": "Accepted to ICML2025", "summary": "Diffusion transformers (DiT) have demonstrated exceptional performance in\nvideo generation. However, their large number of parameters and high\ncomputational complexity limit their deployment on edge devices. Quantization\ncan reduce storage requirements and accelerate inference by lowering the\nbit-width of model parameters. Yet, existing quantization methods for image\ngeneration models do not generalize well to video generation tasks. We identify\ntwo primary challenges: the loss of information during quantization and the\nmisalignment between optimization objectives and the unique requirements of\nvideo generation. To address these challenges, we present Q-VDiT, a\nquantization framework specifically designed for video DiT models. From the\nquantization perspective, we propose the Token-aware Quantization Estimator\n(TQE), which compensates for quantization errors in both the token and feature\ndimensions. From the optimization perspective, we introduce Temporal\nMaintenance Distillation (TMD), which preserves the spatiotemporal correlations\nbetween frames and enables the optimization of each frame with respect to the\noverall video context. Our W3A6 Q-VDiT achieves a scene consistency of 23.40,\nsetting a new benchmark and outperforming current state-of-the-art quantization\nmethods by 1.9$\\times$. Code will be available at\nhttps://github.com/cantbebetter2/Q-VDiT."}
{"id": "2505.21746", "pdf": "https://arxiv.org/pdf/2505.21746", "abs": "https://arxiv.org/abs/2505.21746", "authors": ["Arif Masrur", "Peder A. Olsen", "Paul R. Adler", "Carlan Jackson", "Matthew W. Myers", "Nathan Sedghi", "Ray R. Weil"], "title": "Learning to See More: UAS-Guided Super-Resolution of Satellite Imagery for Precision Agriculture", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Unmanned Aircraft Systems (UAS) and satellites are key data sources for\nprecision agriculture, yet each presents trade-offs. Satellite data offer broad\nspatial, temporal, and spectral coverage but lack the resolution needed for\nmany precision farming applications, while UAS provide high spatial detail but\nare limited by coverage and cost, especially for hyperspectral data. This study\npresents a novel framework that fuses satellite and UAS imagery using\nsuper-resolution methods. By integrating data across spatial, spectral, and\ntemporal domains, we leverage the strengths of both platforms cost-effectively.\nWe use estimation of cover crop biomass and nitrogen (N) as a case study to\nevaluate our approach. By spectrally extending UAS RGB data to the vegetation\nred edge and near-infrared regions, we generate high-resolution Sentinel-2\nimagery and improve biomass and N estimation accuracy by 18% and 31%,\nrespectively. Our results show that UAS data need only be collected from a\nsubset of fields and time points. Farmers can then 1) enhance the spectral\ndetail of UAS RGB imagery; 2) increase the spatial resolution by using\nsatellite data; and 3) extend these enhancements spatially and across the\ngrowing season at the frequency of the satellite flights. Our SRCNN-based\nspectral extension model shows considerable promise for model transferability\nover other cropping systems in the Upper and Lower Chesapeake Bay regions.\nAdditionally, it remains effective even when cloud-free satellite data are\nunavailable, relying solely on the UAS RGB input. The spatial extension model\nproduces better biomass and N predictions than models built on raw UAS RGB\nimages. Once trained with targeted UAS RGB data, the spatial extension model\nallows farmers to stop repeated UAS flights. While we introduce\nsuper-resolution advances, the core contribution is a lightweight and scalable\nsystem for affordable on-farm use."}
{"id": "2505.22662", "pdf": "https://arxiv.org/pdf/2505.22662", "abs": "https://arxiv.org/abs/2505.22662", "authors": ["Feng Luo", "Yu-Neng Chuang", "Guanchu Wang", "Hoang Anh Duy Le", "Shaochen Zhong", "Hongyi Liu", "Jiayi Yuan", "Yang Sui", "Vladimir Braverman", "Vipin Chaudhary", "Xia Hu"], "title": "AutoL2S: Auto Long-Short Reasoning for Efficient Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The reasoning-capable large language models (LLMs) demonstrate strong\nperformance on complex reasoning tasks but often suffer from overthinking,\ngenerating unnecessarily long chain-of-thought (CoT) reasoning paths for easy\nreasoning questions, thereby increasing inference cost and latency. Recent\napproaches attempt to address this challenge by manually deciding when to apply\nlong or short reasoning. However, they lack the flexibility to adapt CoT length\ndynamically based on question complexity. In this paper, we propose Auto\nLong-Short Reasoning (AutoL2S), a dynamic and model-agnostic framework that\nenables LLMs to dynamically compress their generated reasoning path based on\nthe complexity of the reasoning question. AutoL2S enables a learned paradigm,\nin which LLMs themselves can decide when longer reasoning is necessary and when\nshorter reasoning suffices, by training on data annotated with our proposed\nmethod, which includes both long and short CoT paths and a special <EASY>\ntoken. We then use <EASY> token to indicate when the model can skip generating\nlengthy CoT reasoning. This proposed annotation strategy can enhance the LLMs'\nability to generate shorter CoT reasoning paths with improved quality after\ntraining. Extensive evaluation results show that AutoL2S reduces the length of\nreasoning generation by up to 57% without compromising performance,\ndemonstrating the effectiveness of AutoL2S for scalable and efficient LLM\nreasoning."}
{"id": "2505.22195", "pdf": "https://arxiv.org/pdf/2505.22195", "abs": "https://arxiv.org/abs/2505.22195", "authors": ["Guoan Xu", "Wenfeng Huang", "Wenjing Jia", "Jiamao Li", "Guangwei Gao", "Guo-Jun Qi"], "title": "S2AFormer: Strip Self-Attention for Efficient Vision Transformer", "categories": ["cs.CV"], "comment": "12 pages, 6 figures, 8 tables", "summary": "Vision Transformer (ViT) has made significant advancements in computer\nvision, thanks to its token mixer's sophisticated ability to capture global\ndependencies between all tokens. However, the quadratic growth in computational\ndemands as the number of tokens increases limits its practical efficiency.\nAlthough recent methods have combined the strengths of convolutions and\nself-attention to achieve better trade-offs, the expensive pairwise token\naffinity and complex matrix operations inherent in self-attention remain a\nbottleneck. To address this challenge, we propose S2AFormer, an efficient\nVision Transformer architecture featuring novel Strip Self-Attention (SSA). We\ndesign simple yet effective Hybrid Perception Blocks (HPBs) to effectively\nintegrate the local perception capabilities of CNNs with the global context\nmodeling of Transformer's attention mechanisms. A key innovation of SSA lies in\nits reducing the spatial dimensions of $K$ and $V$ while compressing the\nchannel dimensions of $Q$ and $K$. This design significantly reduces\ncomputational overhead while preserving accuracy, striking an optimal balance\nbetween efficiency and effectiveness. We evaluate the robustness and efficiency\nof S2AFormer through extensive experiments on multiple vision benchmarks,\nincluding ImageNet-1k for image classification, ADE20k for semantic\nsegmentation, and COCO for object detection and instance segmentation. Results\ndemonstrate that S2AFormer achieves significant accuracy gains with superior\nefficiency in both GPU and non-GPU environments, making it a strong candidate\nfor efficient vision Transformers."}
{"id": "2505.21755", "pdf": "https://arxiv.org/pdf/2505.21755", "abs": "https://arxiv.org/abs/2505.21755", "authors": ["Chengyue Huang", "Brisa Maneechotesuwan", "Shivang Chopra", "Zsolt Kira"], "title": "FRAMES-VQA: Benchmarking Fine-Tuning Robustness across Multi-Modal Shifts in Visual Question Answering", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted to CVPR 2025", "summary": "Visual question answering (VQA) systems face significant challenges when\nadapting to real-world data shifts, especially in multi-modal contexts. While\nrobust fine-tuning strategies are essential for maintaining performance across\nin-distribution (ID) and out-of-distribution (OOD) scenarios, current\nevaluation settings are primarily unimodal or particular to some types of OOD,\noffering limited insight into the complexities of multi-modal contexts. In this\nwork, we propose a new benchmark FRAMES-VQA (Fine-Tuning Robustness across\nMulti-Modal Shifts in VQA) for evaluating robust fine-tuning for VQA tasks. We\nutilize ten existing VQA benchmarks, including VQAv2, IV-VQA, VQA-CP, OK-VQA\nand others, and categorize them into ID, near and far OOD datasets covering\nuni-modal, multi-modal and adversarial distribution shifts. We first conduct a\ncomprehensive comparison of existing robust fine-tuning methods. We then\nquantify the distribution shifts by calculating the Mahalanobis distance using\nuni-modal and multi-modal embeddings extracted from various models. Further, we\nperform an extensive analysis to explore the interactions between uni- and\nmulti-modal shifts as well as modality importance for ID and OOD samples. These\nanalyses offer valuable guidance on developing more robust fine-tuning methods\nto handle multi-modal distribution shifts. The code is available at\nhttps://github.com/chengyuehuang511/FRAMES-VQA ."}
{"id": "2505.20162", "pdf": "https://arxiv.org/pdf/2505.20162", "abs": "https://arxiv.org/abs/2505.20162", "authors": ["Alexander Panfilov", "Paul Kassianik", "Maksym Andriushchenko", "Jonas Geiping"], "title": "Capability-Based Scaling Laws for LLM Red-Teaming", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "As large language models grow in capability and agency, identifying\nvulnerabilities through red-teaming becomes vital for safe deployment. However,\ntraditional prompt-engineering approaches may prove ineffective once\nred-teaming turns into a weak-to-strong problem, where target models surpass\nred-teamers in capabilities. To study this shift, we frame red-teaming through\nthe lens of the capability gap between attacker and target. We evaluate more\nthan 500 attacker-target pairs using LLM-based jailbreak attacks that mimic\nhuman red-teamers across diverse families, sizes, and capability levels. Three\nstrong trends emerge: (i) more capable models are better attackers, (ii) attack\nsuccess drops sharply once the target's capability exceeds the attacker's, and\n(iii) attack success rates correlate with high performance on social science\nsplits of the MMLU-Pro benchmark. From these trends, we derive a jailbreaking\nscaling law that predicts attack success for a fixed target based on\nattacker-target capability gap. These findings suggest that fixed-capability\nattackers (e.g., humans) may become ineffective against future models,\nincreasingly capable open-source models amplify risks for existing systems, and\nmodel providers must accurately measure and control models' persuasive and\nmanipulative abilities to limit their effectiveness as attackers."}
{"id": "2505.22200", "pdf": "https://arxiv.org/pdf/2505.22200", "abs": "https://arxiv.org/abs/2505.22200", "authors": ["Darshana Saravanan", "Makarand Tapaswi", "Vineet Gandhi"], "title": "Investigating Mechanisms for In-Context Vision Language Binding", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to MIV at CVPRW 2025 (Oral)", "summary": "To understand a prompt, Vision-Language models (VLMs) must perceive the\nimage, comprehend the text, and build associations within and across both\nmodalities. For instance, given an 'image of a red toy car', the model should\nassociate this image to phrases like 'car', 'red toy', 'red object', etc. Feng\nand Steinhardt propose the Binding ID mechanism in LLMs, suggesting that the\nentity and its corresponding attribute tokens share a Binding ID in the model\nactivations. We investigate this for image-text binding in VLMs using a\nsynthetic dataset and task that requires models to associate 3D objects in an\nimage with their descriptions in the text. Our experiments demonstrate that\nVLMs assign a distinct Binding ID to an object's image tokens and its textual\nreferences, enabling in-context association."}
{"id": "2505.21771", "pdf": "https://arxiv.org/pdf/2505.21771", "abs": "https://arxiv.org/abs/2505.21771", "authors": ["Prasham Yatinkumar Titiya", "Jainil Trivedi", "Chitta Baral", "Vivek Gupta"], "title": "MMTBENCH: A Unified Benchmark for Complex Multimodal Table Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multimodal tables those that integrate semi structured data with visual\nelements such as charts and maps are ubiquitous across real world domains, yet\nthey pose a formidable challenge to current vision language models (VLMs).\nWhile Large Language models (LLMs) and VLMs have demonstrated strong\ncapabilities in text and image understanding, their performance on complex,\nreal world multimodal table reasoning remains unexplored. To bridge this gap,\nwe introduce MMTBENCH (Multimodal Table Benchmark), a benchmark consisting of\n500 real world multimodal tables drawn from diverse real world sources, with a\ntotal of 4021 question answer pairs. MMTBENCH questions cover four question\ntypes (Explicit, Implicit, Answer Mention, and Visual Based), five reasoning\ntypes (Mathematical, Extrema Identification, Fact Verification, Vision Based,\nand Others), and eight table types (Single/Multiple Entity, Maps and Charts\nwith Entities, Single/Multiple Charts, Maps, and Visualizations). Extensive\nevaluation of state of the art models on all types reveals substantial\nperformance gaps, particularly on questions requiring visual-based reasoning\nand multi-step inference. These findings show the urgent need for improved\narchitectures that more tightly integrate vision and language processing. By\nproviding a challenging, high-quality resource that mirrors the complexity of\nreal-world tasks, MMTBENCH underscores its value as a resource for future\nresearch on multimodal tables."}
{"id": "2505.21510", "pdf": "https://arxiv.org/pdf/2505.21510", "abs": "https://arxiv.org/abs/2505.21510", "authors": ["Chundra Cathcart"], "title": "Complexity counts: global and local perspectives on Indo-Aryan numeral systems", "categories": ["physics.soc-ph", "cs.CL"], "comment": null, "summary": "The numeral systems of Indo-Aryan languages such as Hindi, Gujarati, and\nBengali are highly unusual in that unlike most numeral systems (e.g., those of\nEnglish, Chinese, etc.), forms referring to 1--99 are highly non-transparent\nand are cannot be constructed using straightforward rules. As an example,\nHindi/Urdu *iky\\=anve* `91' is not decomposable into the composite elements\n*ek* `one' and *nave* `ninety' in the way that its English counterpart is. This\npaper situates Indo-Aryan languages within the typology of cross-linguistic\nnumeral systems, and explores the linguistic and non-linguistic factors that\nmay be responsible for the persistence of complex systems in these languages.\nUsing cross-linguistic data from multiple databases, we develop and employ a\nnumber of cross-linguistically applicable metrics to quantifies the complexity\nof languages' numeral systems, and demonstrate that Indo-Aryan languages have\ndecisively more complex numeral systems than the world's languages as a whole,\nthough individual Indo-Aryan languages differ from each other in terms of the\ncomplexity of the patterns they display. We investigate the factors (e.g.,\nreligion, geographic isolation, etc.) that underlie complexity in numeral\nsystems, with a focus on South Asia, in an attempt to develop an account of why\ncomplex numeral systems developed and persisted in certain Indo-Aryan languages\nbut not elsewhere. Finally, we demonstrate that Indo-Aryan numeral systems\nadhere to certain general pressures toward efficient communication found\ncross-linguistically, despite their high complexity. We call for this somewhat\noverlooked dimension of complexity to be taken seriously when discussing\ngeneral variation in cross-linguistic numeral systems."}
{"id": "2505.22209", "pdf": "https://arxiv.org/pdf/2505.22209", "abs": "https://arxiv.org/abs/2505.22209", "authors": ["Naomi Kombol", "Ivan Martinović", "Siniša Šegvić"], "title": "A Survey on Training-free Open-Vocabulary Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Semantic segmentation is one of the most fundamental tasks in image\nunderstanding with a long history of research, and subsequently a myriad of\ndifferent approaches. Traditional methods strive to train models up from\nscratch, requiring vast amounts of computational resources and training data.\nIn the advent of moving to open-vocabulary semantic segmentation, which asks\nmodels to classify beyond learned categories, large quantities of finely\nannotated data would be prohibitively expensive. Researchers have instead\nturned to training-free methods where they leverage existing models made for\ntasks where data is more easily acquired. Specifically, this survey will cover\nthe history, nuance, idea development and the state-of-the-art in training-free\nopen-vocabulary semantic segmentation that leverages existing multi-modal\nclassification models. We will first give a preliminary on the task definition\nfollowed by an overview of popular model archetypes and then spotlight over 30\napproaches split into broader research branches: purely CLIP-based, those\nleveraging auxiliary visual foundation models and ones relying on generative\nmethods. Subsequently, we will discuss the limitations and potential problems\nof current research, as well as provide some underexplored ideas for future\nstudy. We believe this survey will serve as a good onboarding read to new\nresearchers and spark increased interest in the area."}
{"id": "2505.21775", "pdf": "https://arxiv.org/pdf/2505.21775", "abs": "https://arxiv.org/abs/2505.21775", "authors": ["Michael Klamkin", "Arnaud Deza", "Sikai Cheng", "Haoruo Zhao", "Pascal Van Hentenryck"], "title": "DualSchool: How Reliable are LLMs for Optimization Education?", "categories": ["cs.LG", "cs.AI", "math.OC"], "comment": null, "summary": "Consider the following task taught in introductory optimization courses which\naddresses challenges articulated by the community at the intersection of\n(generative) AI and OR: generate the dual of a linear program. LLMs, being\ntrained at web-scale, have the conversion process and many instances of Primal\nto Dual Conversion (P2DC) at their disposal. Students may thus reasonably\nexpect that LLMs would perform well on the P2DC task. To assess this\nexpectation, this paper introduces DualSchool, a comprehensive framework for\ngenerating and verifying P2DC instances. The verification procedure of\nDualSchool uses the Canonical Graph Edit Distance, going well beyond existing\nevaluation methods for optimization models, which exhibit many false positives\nand negatives when applied to P2DC. Experiments performed by DualSchool reveal\ninteresting findings. Although LLMs can recite the conversion procedure\naccurately, state-of-the-art open LLMs fail to consistently produce correct\nduals. This finding holds even for the smallest two-variable instances and for\nderivative tasks, such as correctness, verification, and error classification.\nThe paper also discusses the implications for educators, students, and the\ndevelopment of large reasoning systems."}
{"id": "2505.21527", "pdf": "https://arxiv.org/pdf/2505.21527", "abs": "https://arxiv.org/abs/2505.21527", "authors": ["Jianheng Zhuo", "Yifan Yang", "Yiwen Shao", "Yong Xu", "Dong Yu", "Kai Yu", "Xie Chen"], "title": "VietASR: Achieving Industry-level Vietnamese ASR with 50-hour labeled data and Large-Scale Speech Pretraining", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": null, "summary": "Automatic speech recognition (ASR) has made remarkable progress but heavily\nrelies on large-scale labeled data, which is scarce for low-resource languages\nlike Vietnamese. While existing systems such as Whisper, USM, and MMS achieve\npromising performance, their efficacy remains inadequate in terms of training\ncosts, latency, and accessibility. To address these issues, we propose VietASR,\na novel ASR training pipeline that leverages vast amounts of unlabeled data and\na small set of labeled data. Through multi-iteration ASR-biased self-supervised\nlearning on a large-scale unlabeled dataset, VietASR offers a cost-effective\nand practical solution for enhancing ASR performance. Experiments demonstrate\nthat pre-training on 70,000-hour unlabeled data and fine-tuning on merely\n50-hour labeled data yield a lightweight but powerful ASR model. It outperforms\nWhisper Large-v3 and commercial ASR systems on real-world data. Our code and\nmodels will be open-sourced to facilitate research in low-resource ASR."}
{"id": "2505.22222", "pdf": "https://arxiv.org/pdf/2505.22222", "abs": "https://arxiv.org/abs/2505.22222", "authors": ["Yunsoo Kim", "Jinge Wu", "Su-Hwan Kim", "Pardeep Vasudev", "Jiashu Shen", "Honghan Wu"], "title": "Look & Mark: Leveraging Radiologist Eye Fixations and Bounding boxes in Multimodal Large Language Models for Chest X-ray Report Generation", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Recent advancements in multimodal Large Language Models (LLMs) have\nsignificantly enhanced the automation of medical image analysis, particularly\nin generating radiology reports from chest X-rays (CXR). However, these models\nstill suffer from hallucinations and clinically significant errors, limiting\ntheir reliability in real-world applications. In this study, we propose Look &\nMark (L&M), a novel grounding fixation strategy that integrates radiologist eye\nfixations (Look) and bounding box annotations (Mark) into the LLM prompting\nframework. Unlike conventional fine-tuning, L&M leverages in-context learning\nto achieve substantial performance gains without retraining. When evaluated\nacross multiple domain-specific and general-purpose models, L&M demonstrates\nsignificant gains, including a 1.2% improvement in overall metrics (A.AVG) for\nCXR-LLaVA compared to baseline prompting and a remarkable 9.2% boost for\nLLaVA-Med. General-purpose models also benefit from L&M combined with\nin-context learning, with LLaVA-OV achieving an 87.3% clinical average\nperformance (C.AVG)-the highest among all models, even surpassing those\nexplicitly trained for CXR report generation. Expert evaluations further\nconfirm that L&M reduces clinically significant errors (by 0.43 average errors\nper report), such as false predictions and omissions, enhancing both accuracy\nand reliability. These findings highlight L&M's potential as a scalable and\nefficient solution for AI-assisted radiology, paving the way for improved\ndiagnostic workflows in low-resource clinical settings."}
{"id": "2505.21786", "pdf": "https://arxiv.org/pdf/2505.21786", "abs": "https://arxiv.org/abs/2505.21786", "authors": ["Dasha Metropolitansky", "Jonathan Larson"], "title": "VeriTrail: Closed-Domain Hallucination Detection with Traceability", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Even when instructed to adhere to source material, Language Models often\ngenerate unsubstantiated content - a phenomenon known as \"closed-domain\nhallucination.\" This risk is amplified in processes with multiple generative\nsteps (MGS), compared to processes with a single generative step (SGS).\nHowever, due to the greater complexity of MGS processes, we argue that\ndetecting hallucinations in their final outputs is necessary but not\nsufficient: it is equally important to trace where hallucinated content was\nlikely introduced and how faithful content may have been derived from the\nsource through intermediate outputs. To address this need, we present\nVeriTrail, the first closed-domain hallucination detection method designed to\nprovide traceability for both MGS and SGS processes. We also introduce the\nfirst datasets to include all intermediate outputs as well as human annotations\nof final outputs' faithfulness for their respective MGS processes. We\ndemonstrate that VeriTrail outperforms baseline methods on both datasets."}
{"id": "2505.21531", "pdf": "https://arxiv.org/pdf/2505.21531", "abs": "https://arxiv.org/abs/2505.21531", "authors": ["Kunhang Li", "Jason Naradowsky", "Yansong Feng", "Yusuke Miyao"], "title": "How Much Do Large Language Models Know about Human Motion? A Case Study in 3D Avatar Control", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "comment": null, "summary": "We explore Large Language Models (LLMs)' human motion knowledge through 3D\navatar control. Given a motion instruction, we prompt LLMs to first generate a\nhigh-level movement plan with consecutive steps (High-level Planning), then\nspecify body part positions in each step (Low-level Planning), which we\nlinearly interpolate into avatar animations as a clear verification lens for\nhuman evaluators. Through carefully designed 20 representative motion\ninstructions with full coverage of basic movement primitives and balanced body\npart usage, we conduct comprehensive evaluations including human assessment of\nboth generated animations and high-level movement plans, as well as automatic\ncomparison with oracle positions in low-level planning. We find that LLMs are\nstrong at interpreting the high-level body movements but struggle with precise\nbody part positioning. While breaking down motion queries into atomic\ncomponents improves planning performance, LLMs have difficulty with multi-step\nmovements involving high-degree-of-freedom body parts. Furthermore, LLMs\nprovide reasonable approximation for general spatial descriptions, but fail to\nhandle precise spatial specifications in text, and the precise spatial-temporal\nparameters needed for avatar control. Notably, LLMs show promise in\nconceptualizing creative motions and distinguishing culturally-specific motion\npatterns."}
{"id": "2505.22226", "pdf": "https://arxiv.org/pdf/2505.22226", "abs": "https://arxiv.org/abs/2505.22226", "authors": ["Xuyang Zhang", "Xi Zhang", "Liang Chen", "Hao Shi", "Qingshan Guo"], "title": "Hadaptive-Net: Efficient Vision Models via Adaptive Cross-Hadamard Synergy", "categories": ["cs.CV"], "comment": "10 pages, 5 figures", "summary": "Recent studies have revealed the immense potential of Hadamard product in\nenhancing network representational capacity and dimensional compression.\nHowever, despite its theoretical promise, this technique has not been\nsystematically explored or effectively applied in practice, leaving its full\ncapabilities underdeveloped. In this work, we first analyze and identify the\nadvantages of Hadamard product over standard convolutional operations in\ncross-channel interaction and channel expansion. Building upon these insights,\nwe propose a computationally efficient module: Adaptive Cross-Hadamard (ACH),\nwhich leverages adaptive cross-channel Hadamard products for high-dimensional\nchannel expansion. Furthermore, we introduce Hadaptive-Net (Hadamard Adaptive\nNetwork), a lightweight network backbone for visual tasks, which is\ndemonstrated through experiments that it achieves an unprecedented balance\nbetween inference speed and accuracy through our proposed module."}
{"id": "2505.21792", "pdf": "https://arxiv.org/pdf/2505.21792", "abs": "https://arxiv.org/abs/2505.21792", "authors": ["Yuanzhe Peng", "Jieming Bian", "Lei Wang", "Yin Huang", "Jie Xu"], "title": "Multimodal Federated Learning: A Survey through the Lens of Different FL Paradigms", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Multimodal Federated Learning (MFL) lies at the intersection of two pivotal\nresearch areas: leveraging complementary information from multiple modalities\nto improve downstream inference performance and enabling distributed training\nto enhance efficiency and preserve privacy. Despite the growing interest in\nMFL, there is currently no comprehensive taxonomy that organizes MFL through\nthe lens of different Federated Learning (FL) paradigms. This perspective is\nimportant because multimodal data introduces distinct challenges across various\nFL settings. These challenges, including modality heterogeneity, privacy\nheterogeneity, and communication inefficiency, are fundamentally different from\nthose encountered in traditional unimodal or non-FL scenarios. In this paper,\nwe systematically examine MFL within the context of three major FL paradigms:\nhorizontal FL (HFL), vertical FL (VFL), and hybrid FL. For each paradigm, we\npresent the problem formulation, review representative training algorithms, and\nhighlight the most prominent challenge introduced by multimodal data in\ndistributed settings. We also discuss open challenges and provide insights for\nfuture research. By establishing this taxonomy, we aim to uncover the novel\nchallenges posed by multimodal data from the perspective of different FL\nparadigms and to offer a new lens through which to understand and advance the\ndevelopment of MFL."}
{"id": "2505.21544", "pdf": "https://arxiv.org/pdf/2505.21544", "abs": "https://arxiv.org/abs/2505.21544", "authors": ["Semanto Mondal"], "title": "Vision Meets Language: A RAG-Augmented YOLOv8 Framework for Coffee Disease Diagnosis and Farmer Assistance", "categories": ["cs.CV", "cs.CL"], "comment": "There are 14 pages, 8 figures", "summary": "As a social being, we have an intimate bond with the environment. A plethora\nof things in human life, such as lifestyle, health, and food are dependent on\nthe environment and agriculture. It comes under our responsibility to support\nthe environment as well as agriculture. However, traditional farming practices\noften result in inefficient resource use and environmental challenges. To\naddress these issues, precision agriculture has emerged as a promising approach\nthat leverages advanced technologies to optimise agricultural processes. In\nthis work, a hybrid approach is proposed that combines the three different\npotential fields of model AI: object detection, large language model (LLM), and\nRetrieval-Augmented Generation (RAG). In this novel framework, we have tried to\ncombine the vision and language models to work together to identify potential\ndiseases in the tree leaf. This study introduces a novel AI-based precision\nagriculture system that uses Retrieval Augmented Generation (RAG) to provide\ncontext-aware diagnoses and natural language processing (NLP) and YOLOv8 for\ncrop disease detection. The system aims to tackle major issues with large\nlanguage models (LLMs), especially hallucinations and allows for adaptive\ntreatment plans and real-time disease detection. The system provides an\neasy-to-use interface to the farmers, which they can use to detect the\ndifferent diseases related to coffee leaves by just submitting the image of the\naffected leaf the model will detect the diseases as well as suggest potential\nremediation methodologies which aim to lower the use of pesticides, preserving\nlivelihoods, and encouraging environmentally friendly methods. With an emphasis\non scalability, dependability, and user-friendliness, the project intends to\nimprove RAG-integrated object detection systems for wider agricultural\napplications in the future."}
{"id": "2505.22228", "pdf": "https://arxiv.org/pdf/2505.22228", "abs": "https://arxiv.org/abs/2505.22228", "authors": ["Haibin He", "Jing Zhang", "Maoyuan Ye", "Juhua Liu", "Bo Du", "Dacheng Tao"], "title": "GoMatching++: Parameter- and Data-Efficient Arbitrary-Shaped Video Text Spotting and Benchmarking", "categories": ["cs.CV"], "comment": "arXiv admin note: text overlap with arXiv:2401.07080", "summary": "Video text spotting (VTS) extends image text spotting (ITS) by adding text\ntracking, significantly increasing task complexity. Despite progress in VTS,\nexisting methods still fall short of the performance seen in ITS. This paper\nidentifies a key limitation in current video text spotters: limited recognition\ncapability, even after extensive end-to-end training. To address this, we\npropose GoMatching++, a parameter- and data-efficient method that transforms an\noff-the-shelf image text spotter into a video specialist. The core idea lies in\nfreezing the image text spotter and introducing a lightweight, trainable\ntracker, which can be optimized efficiently with minimal training data. Our\napproach includes two key components: (1) a rescoring mechanism to bridge the\ndomain gap between image and video data, and (2) the LST-Matcher, which\nenhances the frozen image text spotter's ability to handle video text. We\nexplore various architectures for LST-Matcher to ensure efficiency in both\nparameters and training data. As a result, GoMatching++ sets new performance\nrecords on challenging benchmarks such as ICDAR15-video, DSText, and BOVText,\nwhile significantly reducing training costs. To address the lack of curved text\ndatasets in VTS, we introduce ArTVideo, a new benchmark featuring over 30%\ncurved text with detailed annotations. We also provide a comprehensive\nstatistical analysis and experimental results for ArTVideo. We believe that\nGoMatching++ and the ArTVideo benchmark will drive future advancements in video\ntext spotting. The source code, models and dataset are publicly available at\nhttps://github.com/Hxyz-123/GoMatching."}
{"id": "2505.21811", "pdf": "https://arxiv.org/pdf/2505.21811", "abs": "https://arxiv.org/abs/2505.21811", "authors": ["Clark Mingxuan Ju", "Leonardo Neves", "Bhuvesh Kumar", "Liam Collins", "Tong Zhao", "Yuwei Qiu", "Qing Dou", "Sohail Nizam", "Sen Yang", "Neil Shah"], "title": "Revisiting Self-attention for Cross-domain Sequential Recommendation", "categories": ["cs.IR", "cs.AI"], "comment": "Accepted to KDD'25", "summary": "Sequential recommendation is a popular paradigm in modern recommender\nsystems. In particular, one challenging problem in this space is cross-domain\nsequential recommendation (CDSR), which aims to predict future behaviors given\nuser interactions across multiple domains. Existing CDSR frameworks are mostly\nbuilt on the self-attention transformer and seek to improve by explicitly\ninjecting additional domain-specific components (e.g. domain-aware module\nblocks). While these additional components help, we argue they overlook the\ncore self-attention module already present in the transformer, a naturally\npowerful tool to learn correlations among behaviors. In this work, we aim to\nimprove the CDSR performance for simple models from a novel perspective of\nenhancing the self-attention. Specifically, we introduce a Pareto-optimal\nself-attention and formulate the cross-domain learning as a multi-objective\nproblem, where we optimize the recommendation task while dynamically minimizing\nthe cross-domain attention scores. Our approach automates knowledge transfer in\nCDSR (dubbed as AutoCDSR) -- it not only mitigates negative transfer but also\nencourages complementary knowledge exchange among auxiliary domains. Based on\nthe idea, we further introduce AutoCDSR+, a more performant variant with slight\nadditional cost. Our proposal is easy to implement and works as a plug-and-play\nmodule that can be incorporated into existing transformer-based recommenders.\nBesides flexibility, it is practical to deploy because it brings little extra\ncomputational overheads without heavy hyper-parameter tuning. AutoCDSR on\naverage improves Recall@10 for SASRec and Bert4Rec by 9.8% and 16.0% and\nNDCG@10 by 12.0% and 16.7%, respectively. Code is available at\nhttps://github.com/snap-research/AutoCDSR."}
{"id": "2505.21548", "pdf": "https://arxiv.org/pdf/2505.21548", "abs": "https://arxiv.org/abs/2505.21548", "authors": ["Dhruv Agarwal", "Anya Shukla", "Sunayana Sitaram", "Aditya Vashistha"], "title": "Fluent but Culturally Distant: Can Regional Training Teach Cultural Understanding?", "categories": ["physics.soc-ph", "cs.AI", "cs.CL", "cs.CY"], "comment": "Under review", "summary": "Large language models (LLMs) are used around the world but exhibit Western\ncultural tendencies. To address this cultural misalignment, many countries have\nbegun developing \"regional\" LLMs tailored to local communities. Yet it remains\nunclear whether these models merely speak the language of their users or also\nreflect their cultural values and practices. Using India as a case study, we\nevaluate five Indic and five global LLMs along two key dimensions: values (via\nthe Inglehart-Welzel map and GlobalOpinionQA) and practices (via CulturalBench\nand NormAd). Across all four tasks, we find that Indic models do not align more\nclosely with Indian cultural norms than global models. In fact, an average\nAmerican person is a better proxy for Indian cultural values than any Indic\nmodel. Even prompting strategies fail to meaningfully improve alignment.\nAblations show that regional fine-tuning does not enhance cultural competence\nand may in fact hurt it by impeding recall of existing knowledge. We trace this\nfailure to the scarcity of high-quality, untranslated, and culturally grounded\npretraining and fine-tuning data. Our study positions cultural evaluation as a\nfirst-class requirement alongside multilingual benchmarks and offers a reusable\nmethodology for developers. We call for deeper investments in culturally\nrepresentative data to build and evaluate truly sovereign LLMs."}
{"id": "2505.22230", "pdf": "https://arxiv.org/pdf/2505.22230", "abs": "https://arxiv.org/abs/2505.22230", "authors": ["Zhisong Wang", "Yiwen Ye", "Ziyang Chen", "Yong Xia"], "title": "Enjoying Information Dividend: Gaze Track-based Medical Weakly Supervised Segmentation", "categories": ["cs.CV"], "comment": "10 pages, 4 figures, MICCAI 2025 (Early Accept)", "summary": "Weakly supervised semantic segmentation (WSSS) in medical imaging struggles\nwith effectively using sparse annotations. One promising direction for WSSS\nleverages gaze annotations, captured via eye trackers that record regions of\ninterest during diagnostic procedures. However, existing gaze-based methods,\nsuch as GazeMedSeg, do not fully exploit the rich information embedded in gaze\ndata. In this paper, we propose GradTrack, a framework that utilizes\nphysicians' gaze track, including fixation points, durations, and temporal\norder, to enhance WSSS performance. GradTrack comprises two key components:\nGaze Track Map Generation and Track Attention, which collaboratively enable\nprogressive feature refinement through multi-level gaze supervision during the\ndecoding process. Experiments on the Kvasir-SEG and NCI-ISBI datasets\ndemonstrate that GradTrack consistently outperforms existing gaze-based\nmethods, achieving Dice score improvements of 3.21\\% and 2.61\\%, respectively.\nMoreover, GradTrack significantly narrows the performance gap with fully\nsupervised models such as nnUNet."}
{"id": "2505.21815", "pdf": "https://arxiv.org/pdf/2505.21815", "abs": "https://arxiv.org/abs/2505.21815", "authors": ["Yunyi Zhang", "Ruozhen Yang", "Siqi Jiao", "SeongKu Kang", "Jiawei Han"], "title": "Scientific Paper Retrieval with LLM-Guided Semantic-Based Ranking", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Scientific paper retrieval is essential for supporting literature discovery\nand research. While dense retrieval methods demonstrate effectiveness in\ngeneral-purpose tasks, they often fail to capture fine-grained scientific\nconcepts that are essential for accurate understanding of scientific queries.\nRecent studies also use large language models (LLMs) for query understanding;\nhowever, these methods often lack grounding in corpus-specific knowledge and\nmay generate unreliable or unfaithful content. To overcome these limitations,\nwe propose SemRank, an effective and efficient paper retrieval framework that\ncombines LLM-guided query understanding with a concept-based semantic index.\nEach paper is indexed using multi-granular scientific concepts, including\ngeneral research topics and detailed key phrases. At query time, an LLM\nidentifies core concepts derived from the corpus to explicitly capture the\nquery's information need. These identified concepts enable precise semantic\nmatching, significantly enhancing retrieval accuracy. Experiments show that\nSemRank consistently improves the performance of various base retrievers,\nsurpasses strong existing LLM-based baselines, and remains highly efficient."}
{"id": "2505.21549", "pdf": "https://arxiv.org/pdf/2505.21549", "abs": "https://arxiv.org/abs/2505.21549", "authors": ["Daniel Csizmadia", "Andrei Codreanu", "Victor Sim", "Vighnesh Prabeau", "Michael Lu", "Kevin Zhu", "Sean O'Brien", "Vasu Sharma"], "title": "Distill CLIP (DCLIP): Enhancing Image-Text Retrieval via Cross-Modal Transformer Distillation", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "We present Distill CLIP (DCLIP), a fine-tuned variant of the CLIP model that\nenhances multimodal image-text retrieval while preserving the original model's\nstrong zero-shot classification capabilities. CLIP models are typically\nconstrained by fixed image resolutions and limited context, which can hinder\ntheir effectiveness in retrieval tasks that require fine-grained cross-modal\nunderstanding. DCLIP addresses these challenges through a meta teacher-student\ndistillation framework, where a cross-modal transformer teacher is fine-tuned\nto produce enriched embeddings via bidirectional cross-attention between\nYOLO-extracted image regions and corresponding textual spans. These\nsemantically and spatially aligned global representations guide the training of\na lightweight student model using a hybrid loss that combines contrastive\nlearning and cosine similarity objectives. Despite being trained on only\n~67,500 samples curated from MSCOCO, Flickr30k, and Conceptual Captions-just a\nfraction of CLIP's original dataset-DCLIP significantly improves image-text\nretrieval metrics (Recall@K, MAP), while retaining approximately 94% of CLIP's\nzero-shot classification performance. These results demonstrate that DCLIP\neffectively mitigates the trade-off between task specialization and\ngeneralization, offering a resource-efficient, domain-adaptive, and\ndetail-sensitive solution for advanced vision-language tasks. Code available at\nhttps://anonymous.4open.science/r/DCLIP-B772/README.md."}
{"id": "2505.22246", "pdf": "https://arxiv.org/pdf/2505.22246", "abs": "https://arxiv.org/abs/2505.22246", "authors": ["Nedko Savov", "Naser Kazemi", "Deheng Zhang", "Danda Pani Paudel", "Xi Wang", "Luc Van Gool"], "title": "StateSpaceDiffuser: Bringing Long Context to Diffusion World Models", "categories": ["cs.CV"], "comment": null, "summary": "World models have recently become promising tools for predicting realistic\nvisuals based on actions in complex environments. However, their reliance on a\nshort sequence of observations causes them to quickly lose track of context. As\na result, visual consistency breaks down after just a few steps, and generated\nscenes no longer reflect information seen earlier. This limitation of the\nstate-of-the-art diffusion-based world models comes from their lack of a\nlasting environment state. To address this problem, we introduce\nStateSpaceDiffuser, where a diffusion model is enabled to perform on\nlong-context tasks by integrating a sequence representation from a state-space\nmodel (Mamba), representing the entire interaction history. This design\nrestores long-term memory without sacrificing the high-fidelity synthesis of\ndiffusion models. To rigorously measure temporal consistency, we develop an\nevaluation protocol that probes a model's ability to reinstantiate seen content\nin extended rollouts. Comprehensive experiments show that StateSpaceDiffuser\nsignificantly outperforms a strong diffusion-only baseline, maintaining a\ncoherent visual context for an order of magnitude more steps. It delivers\nconsistent views in both a 2D maze navigation and a complex 3D environment.\nThese results establish that bringing state-space representations into\ndiffusion models is highly effective in demonstrating both visual details and\nlong-term memory."}
{"id": "2505.21825", "pdf": "https://arxiv.org/pdf/2505.21825", "abs": "https://arxiv.org/abs/2505.21825", "authors": ["Parsa Mirtaheri", "Ezra Edelman", "Samy Jelassi", "Eran Malach", "Enric Boix-Adsera"], "title": "Let Me Think! A Long Chain-of-Thought Can Be Worth Exponentially Many Short Ones", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Inference-time computation has emerged as a promising scaling axis for\nimproving large language model reasoning. However, despite yielding impressive\nperformance, the optimal allocation of inference-time computation remains\npoorly understood. A central question is whether to prioritize sequential\nscaling (e.g., longer chains of thought) or parallel scaling (e.g., majority\nvoting across multiple short chains of thought). In this work, we seek to\nilluminate the landscape of test-time scaling by demonstrating the existence of\nreasoning settings where sequential scaling offers an exponential advantage\nover parallel scaling. These settings are based on graph connectivity problems\nin challenging distributions of graphs. We validate our theoretical findings\nwith comprehensive experiments across a range of language models, including\nmodels trained from scratch for graph connectivity with different chain of\nthought strategies as well as large reasoning models."}
{"id": "2505.21569", "pdf": "https://arxiv.org/pdf/2505.21569", "abs": "https://arxiv.org/abs/2505.21569", "authors": ["Zhucong Li", "Bowei Zhang", "Jin Xiao", "Zhijian Zhou", "Fenglei Cao", "Jiaqing Liang", "Yuan Qi"], "title": "ChemHAS: Hierarchical Agent Stacking for Enhancing Chemistry Tools", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "9 pages", "summary": "Large Language Model (LLM)-based agents have demonstrated the ability to\nimprove performance in chemistry-related tasks by selecting appropriate tools.\nHowever, their effectiveness remains limited by the inherent prediction errors\nof chemistry tools. In this paper, we take a step further by exploring how\nLLMbased agents can, in turn, be leveraged to reduce prediction errors of the\ntools. To this end, we propose ChemHAS (Chemical Hierarchical Agent Stacking),\na simple yet effective method that enhances chemistry tools through optimizing\nagent-stacking structures from limited data. ChemHAS achieves state-of-the-art\nperformance across four fundamental chemistry tasks, demonstrating that our\nmethod can effectively compensate for prediction errors of the tools.\nFurthermore, we identify and characterize four distinct agent-stacking\nbehaviors, potentially improving interpretability and revealing new\npossibilities for AI agent applications in scientific research. Our code and\ndataset are publicly available at https:\n//anonymous.4open.science/r/ChemHAS-01E4/README.md."}
{"id": "2505.22250", "pdf": "https://arxiv.org/pdf/2505.22250", "abs": "https://arxiv.org/abs/2505.22250", "authors": ["Mingzhuang Wang", "Yvyang Li", "Xiyang Zhang", "Fei Tan", "Qi Shi", "Guotao Zhang", "Siqi Chen", "Yufei Liu", "Lei Lei", "Ming Zhou", "Qiang Lin", "Hongqiang Yang"], "title": "YH-MINER: Multimodal Intelligent System for Natural Ecological Reef Metric Extraction", "categories": ["cs.CV", "q-bio.QM"], "comment": null, "summary": "Coral reefs, crucial for sustaining marine biodiversity and ecological\nprocesses (e.g., nutrient cycling, habitat provision), face escalating threats,\nunderscoring the need for efficient monitoring. Coral reef ecological\nmonitoring faces dual challenges of low efficiency in manual analysis and\ninsufficient segmentation accuracy in complex underwater scenarios. This study\ndevelops the YH-OSI system, establishing an intelligent framework centered on\nthe Multimodal Large Model (MLLM) for \"object detection-semantic\nsegmentation-prior input\". The system uses the object detection module\n(mAP@0.5=0.78) to generate spatial prior boxes for coral instances, driving the\nsegment module to complete pixel-level segmentation in low-light and densely\noccluded scenarios. The segmentation masks and finetuned classification\ninstructions are fed into the Qwen2-VL-based multimodal model as prior inputs,\nachieving a genus-level classification accuracy of 88% and simultaneously\nextracting core ecological metrics. Meanwhile, the system retains the\nscalability of the multimodal model through standardized interfaces, laying a\nfoundation for future integration into multimodal agent-based underwater robots\nand supporting the full-process automation of \"image acquisition-prior\ngeneration-real-time analysis.\""}
{"id": "2505.21827", "pdf": "https://arxiv.org/pdf/2505.21827", "abs": "https://arxiv.org/abs/2505.21827", "authors": ["Yongyi Zang", "Zheqi Dai", "Mark D. Plumbley", "Qiuqiang Kong"], "title": "Music Source Restoration", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "comment": "A modified version of this paper is in review", "summary": "We introduce Music Source Restoration (MSR), a novel task addressing the gap\nbetween idealized source separation and real-world music production. Current\nMusic Source Separation (MSS) approaches assume mixtures are simple sums of\nsources, ignoring signal degradations employed during music production like\nequalization, compression, and reverb. MSR models mixtures as degraded sums of\nindividually degraded sources, with the goal of recovering original, undegraded\nsignals. Due to the lack of data for MSR, we present RawStems, a dataset\nannotation of 578 songs with unprocessed source signals organized into 8\nprimary and 17 secondary instrument groups, totaling 354.13 hours. To the best\nof our knowledge, RawStems is the first dataset that contains unprocessed music\nstems with hierarchical categories. We consider spectral filtering, dynamic\nrange compression, harmonic distortion, reverb and lossy codec as possible\ndegradations, and establish U-Former as a baseline method, demonstrating the\nfeasibility of MSR on our dataset. We release the RawStems dataset annotations,\ndegradation simulation pipeline, training code and pre-trained models to be\npublicly available."}
{"id": "2505.21668", "pdf": "https://arxiv.org/pdf/2505.21668", "abs": "https://arxiv.org/abs/2505.21668", "authors": ["Yongchao Chen", "Yueying Liu", "Junwei Zhou", "Yilun Hao", "Jingquan Wang", "Yang Zhang", "Chuchu Fan"], "title": "R1-Code-Interpreter: Training LLMs to Reason with Code via Supervised and Reinforcement Learning", "categories": ["cs.AI", "cs.CL", "cs.SC"], "comment": "33 pages, 8 figures", "summary": "Despite advances in reasoning and planning of R1-like models, Large Language\nModels (LLMs) still struggle with tasks requiring precise computation, symbolic\nmanipulation, optimization, and algorithmic reasoning, in which textual\nreasoning lacks the rigor of code execution. A key challenge is enabling LLMs\nto decide when to use textual reasoning versus code generation. While OpenAI\ntrains models to invoke a Code Interpreter as needed, public research lacks\nguidance on aligning pre-trained LLMs to effectively leverage code and\ngeneralize across diverse tasks. We present R1-Code-Interpreter, an extension\nof a text-only LLM trained via multi-turn supervised fine-tuning (SFT) and\nreinforcement learning (RL) to autonomously generate multiple code queries\nduring step-by-step reasoning. We curate 144 reasoning and planning tasks (107\nfor training, 37 for testing), each with over 200 diverse questions. We\nfine-tune Qwen-2.5 models (3B/7B/14B) using various SFT and RL strategies,\ninvestigating different answer formats, reasoning vs. non-reasoning models,\ncold vs. warm starts, GRPO vs. PPO, and masked vs. unmasked code outputs.\nUnlike prior RL work on narrow domains, we find that Code Interpreter training\nis significantly harder due to high task diversity and expensive code\nexecution, highlighting the critical role of the SFT stage. Our final model,\nR1-CI-14B, improves average accuracy on the 37 test tasks from 44.0\\% to\n64.1\\%, outperforming GPT-4o (text-only: 58.6\\%) and approaching GPT-4o with\nCode Interpreter (70.9\\%), with the emergent self-checking behavior via code\ngeneration. Datasets, Codes, and Models are available at\nhttps://github.com/yongchao98/R1-Code-Interpreter and\nhttps://huggingface.co/yongchao98."}
{"id": "2505.22259", "pdf": "https://arxiv.org/pdf/2505.22259", "abs": "https://arxiv.org/abs/2505.22259", "authors": ["Kiyoon Jeong", "Jaehyuk Heo", "Junyeong Son", "Pilsung Kang"], "title": "Domain Adaptation of Attention Heads for Zero-shot Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Zero-shot anomaly detection (ZSAD) in images is an approach that can detect\nanomalies without access to normal samples, which can be beneficial in various\nrealistic scenarios where model training is not possible. However, existing\nZSAD research has shown limitations by either not considering domain adaptation\nof general-purpose backbone models to anomaly detection domains or by\nimplementing only partial adaptation to some model components. In this paper,\nwe propose HeadCLIP to overcome these limitations by effectively adapting both\ntext and image encoders to the domain. HeadCLIP generalizes the concepts of\nnormality and abnormality through learnable prompts in the text encoder, and\nintroduces learnable head weights to the image encoder to dynamically adjust\nthe features held by each attention head according to domain characteristics.\nAdditionally, we maximize the effect of domain adaptation by introducing a\njoint anomaly score that utilizes domain-adapted pixel-level information for\nimage-level anomaly detection. Experimental results using multiple real\ndatasets in both industrial and medical domains show that HeadCLIP outperforms\nexisting ZSAD techniques at both pixel and image levels. In the industrial\ndomain, improvements of up to 4.9%p in pixel-level mean anomaly detection score\n(mAD) and up to 3.0%p in image-level mAD were achieved, with similar\nimprovements (3.2%p, 3.1%p) in the medical domain."}
{"id": "2505.21835", "pdf": "https://arxiv.org/pdf/2505.21835", "abs": "https://arxiv.org/abs/2505.21835", "authors": ["Xiangyu Chen", "Jing Liu", "Ye Wang", "Matthew Brand", "Pu", "Wang", "Toshiaki Koike-Akino"], "title": "TuneComp: Joint Fine-tuning and Compression for Large Foundation Models", "categories": ["cs.LG", "cs.AI"], "comment": "Preliminary Work", "summary": "To reduce model size during post-training, compression methods, including\nknowledge distillation, low-rank approximation, and pruning, are often applied\nafter fine-tuning the model. However, sequential fine-tuning and compression\nsacrifices performance, while creating a larger than necessary model as an\nintermediate step. In this work, we aim to reduce this gap, by directly\nconstructing a smaller model while guided by the downstream task. We propose to\njointly fine-tune and compress the model by gradually distilling it to a pruned\nlow-rank structure. Experiments demonstrate that joint fine-tuning and\ncompression significantly outperforms other sequential compression methods."}
{"id": "2505.21749", "pdf": "https://arxiv.org/pdf/2505.21749", "abs": "https://arxiv.org/abs/2505.21749", "authors": ["M. Reza Ebrahimi", "Roland Memisevic"], "title": "Revisiting Bi-Linear State Transitions in Recurrent Neural Networks", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "The role of hidden units in recurrent neural networks is typically seen as\nmodeling memory, with research focusing on enhancing information retention\nthrough gating mechanisms. A less explored perspective views hidden units as\nactive participants in the computation performed by the network, rather than\npassive memory stores. In this work, we revisit bi-linear operations, which\ninvolve multiplicative interactions between hidden units and input embeddings.\nWe demonstrate theoretically and empirically that they constitute a natural\ninductive bias for representing the evolution of hidden states in state\ntracking tasks. These are the simplest type of task that require hidden units\nto actively contribute to the behavior of the network. We also show that\nbi-linear state updates form a natural hierarchy corresponding to state\ntracking tasks of increasing complexity, with popular linear recurrent networks\nsuch as Mamba residing at the lowest-complexity center of that hierarchy."}
{"id": "2505.22279", "pdf": "https://arxiv.org/pdf/2505.22279", "abs": "https://arxiv.org/abs/2505.22279", "authors": ["Wenjun Lu", "Haodong Chen", "Anqi Yi", "Yuk Ying Chung", "Zhiyong Wang", "Kun Hu"], "title": "Learning Fine-Grained Geometry for Sparse-View Splatting via Cascade Depth Loss", "categories": ["cs.CV"], "comment": null, "summary": "Novel view synthesis is a fundamental task in 3D computer vision that aims to\nreconstruct realistic images from a set of posed input views. However,\nreconstruction quality degrades significantly under sparse-view conditions due\nto limited geometric cues. Existing methods, such as Neural Radiance Fields\n(NeRF) and the more recent 3D Gaussian Splatting (3DGS), often suffer from\nblurred details and structural artifacts when trained with insufficient views.\nRecent works have identified the quality of rendered depth as a key factor in\nmitigating these artifacts, as it directly affects geometric accuracy and view\nconsistency. In this paper, we address these challenges by introducing\nHierarchical Depth-Guided Splatting (HDGS), a depth supervision framework that\nprogressively refines geometry from coarse to fine levels. Central to HDGS is a\nnovel Cascade Pearson Correlation Loss (CPCL), which aligns rendered and\nestimated monocular depths across multiple spatial scales. By enforcing\nmulti-scale depth consistency, our method substantially improves structural\nfidelity in sparse-view scenarios. Extensive experiments on the LLFF and DTU\nbenchmarks demonstrate that HDGS achieves state-of-the-art performance under\nsparse-view settings while maintaining efficient and high-quality rendering"}
{"id": "2505.21838", "pdf": "https://arxiv.org/pdf/2505.21838", "abs": "https://arxiv.org/abs/2505.21838", "authors": ["Maobin Lu", "Martin Guay", "Telema Harry", "Shimin Wang", "Jordan Cooper"], "title": "Nonadaptive Output Regulation of Second-Order Nonlinear Uncertain Systems", "categories": ["eess.SY", "cs.AI", "cs.SY", "math.OC", "nlin.CD"], "comment": "8 pages, 3 figures", "summary": "This paper investigates the robust output regulation problem of second-order\nnonlinear uncertain systems with an unknown exosystem. Instead of the adaptive\ncontrol approach, this paper resorts to a robust control methodology to solve\nthe problem and thus avoid the bursting phenomenon. In particular, this paper\nconstructs generic internal models for the steady-state state and input\nvariables of the system. By introducing a coordinate transformation, this paper\nconverts the robust output regulation problem into a nonadaptive stabilization\nproblem of an augmented system composed of the second-order nonlinear uncertain\nsystem and the generic internal models. Then, we design the stabilization\ncontrol law and construct a strict Lyapunov function that guarantees the\nrobustness with respect to unmodeled disturbances. The analysis shows that the\noutput zeroing manifold of the augmented system can be made attractive by the\nproposed nonadaptive control law, which solves the robust output regulation\nproblem. Finally, we demonstrate the effectiveness of the proposed nonadaptive\ninternal model approach by its application to the control of the Duffing\nsystem."}
{"id": "2505.21753", "pdf": "https://arxiv.org/pdf/2505.21753", "abs": "https://arxiv.org/abs/2505.21753", "authors": ["Roberto Ulloa", "Eve M. Zucker", "Daniel Bultmann", "David J. Simon", "Mykola Makhortykh"], "title": "From prosthetic memory to prosthetic denial: Auditing whether large language models are prone to mass atrocity denialism", "categories": ["cs.CY", "cs.CL"], "comment": null, "summary": "The proliferation of large language models (LLMs) can influence how\nhistorical narratives are disseminated and perceived. This study explores the\nimplications of LLMs' responses on the representation of mass atrocity memory,\nexamining whether generative AI systems contribute to prosthetic memory, i.e.,\nmediated experiences of historical events, or to what we term \"prosthetic\ndenial,\" the AI-mediated erasure or distortion of atrocity memories. We argue\nthat LLMs function as interfaces that can elicit prosthetic memories and,\ntherefore, act as experiential sites for memory transmission, but also\nintroduce risks of denialism, particularly when their outputs align with\ncontested or revisionist narratives. To empirically assess these risks, we\nconducted a comparative audit of five LLMs (Claude, GPT, Llama, Mixtral, and\nGemini) across four historical case studies: the Holodomor, the Holocaust, the\nCambodian Genocide, and the genocide against the Tutsis in Rwanda. Each model\nwas prompted with questions addressing common denialist claims in English and\nan alternative language relevant to each case (Ukrainian, German, Khmer, and\nFrench). Our findings reveal that while LLMs generally produce accurate\nresponses for widely documented events like the Holocaust, significant\ninconsistencies and susceptibility to denialist framings are observed for more\nunderrepresented cases like the Cambodian Genocide. The disparities highlight\nthe influence of training data availability and the probabilistic nature of LLM\nresponses on memory integrity. We conclude that while LLMs extend the concept\nof prosthetic memory, their unmoderated use risks reinforcing historical\ndenialism, raising ethical concerns for (digital) memory preservation, and\npotentially challenging the advantageous role of technology associated with the\noriginal values of prosthetic memory."}
{"id": "2505.22284", "pdf": "https://arxiv.org/pdf/2505.22284", "abs": "https://arxiv.org/abs/2505.22284", "authors": ["Junyu Fan", "Chuanlin Liao", "Yi Lin"], "title": "From Controlled Scenarios to Real-World: Cross-Domain Degradation Pattern Matching for All-in-One Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "As a fundamental imaging task, All-in-One Image Restoration (AiOIR) aims to\nachieve image restoration caused by multiple degradation patterns via a single\nmodel with unified parameters. Although existing AiOIR approaches obtain\npromising performance in closed and controlled scenarios, they still suffered\nfrom considerable performance reduction in real-world scenarios since the gap\nof data distributions between the training samples (source domain) and\nreal-world test samples (target domain) can lead inferior degradation awareness\nability. To address this issue, a Unified Domain-Adaptive Image Restoration\n(UDAIR) framework is proposed to effectively achieve AiOIR by leveraging the\nlearned knowledge from source domain to target domain. To improve the\ndegradation identification, a codebook is designed to learn a group of discrete\nembeddings to denote the degradation patterns, and the cross-sample contrastive\nlearning mechanism is further proposed to capture shared features from\ndifferent samples of certain degradation. To bridge the data gap, a domain\nadaptation strategy is proposed to build the feature projection between the\nsource and target domains by dynamically aligning their codebook embeddings,\nand a correlation alignment-based test-time adaptation mechanism is designed to\nfine-tune the alignment discrepancies by tightening the degradation embeddings\nto the corresponding cluster center in the source domain. Experimental results\non 10 open-source datasets demonstrate that UDAIR achieves new state-of-the-art\nperformance for the AiOIR task. Most importantly, the feature cluster validate\nthe degradation identification under unknown conditions, and qualitative\ncomparisons showcase robust generalization to real-world scenarios."}
{"id": "2505.21841", "pdf": "https://arxiv.org/pdf/2505.21841", "abs": "https://arxiv.org/abs/2505.21841", "authors": ["Jiahui Zhu", "Kihyun Yu", "Dabeen Lee", "Xin Liu", "Honghao Wei"], "title": "An Optimistic Algorithm for online CMDPS with Anytime Adversarial Constraints", "categories": ["cs.LG", "cs.AI"], "comment": "Proceedings of the 41 st International Conference on Machine Learning", "summary": "Online safe reinforcement learning (RL) plays a key role in dynamic\nenvironments, with applications in autonomous driving, robotics, and\ncybersecurity. The objective is to learn optimal policies that maximize rewards\nwhile satisfying safety constraints modeled by constrained Markov decision\nprocesses (CMDPs). Existing methods achieve sublinear regret under stochastic\nconstraints but often fail in adversarial settings, where constraints are\nunknown, time-varying, and potentially adversarially designed. In this paper,\nwe propose the Optimistic Mirror Descent Primal-Dual (OMDPD) algorithm, the\nfirst to address online CMDPs with anytime adversarial constraints. OMDPD\nachieves optimal regret O(sqrt(K)) and strong constraint violation O(sqrt(K))\nwithout relying on Slater's condition or the existence of a strictly known safe\npolicy. We further show that access to accurate estimates of rewards and\ntransitions can further improve these bounds. Our results offer practical\nguarantees for safe decision-making in adversarial environments."}
{"id": "2505.21755", "pdf": "https://arxiv.org/pdf/2505.21755", "abs": "https://arxiv.org/abs/2505.21755", "authors": ["Chengyue Huang", "Brisa Maneechotesuwan", "Shivang Chopra", "Zsolt Kira"], "title": "FRAMES-VQA: Benchmarking Fine-Tuning Robustness across Multi-Modal Shifts in Visual Question Answering", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted to CVPR 2025", "summary": "Visual question answering (VQA) systems face significant challenges when\nadapting to real-world data shifts, especially in multi-modal contexts. While\nrobust fine-tuning strategies are essential for maintaining performance across\nin-distribution (ID) and out-of-distribution (OOD) scenarios, current\nevaluation settings are primarily unimodal or particular to some types of OOD,\noffering limited insight into the complexities of multi-modal contexts. In this\nwork, we propose a new benchmark FRAMES-VQA (Fine-Tuning Robustness across\nMulti-Modal Shifts in VQA) for evaluating robust fine-tuning for VQA tasks. We\nutilize ten existing VQA benchmarks, including VQAv2, IV-VQA, VQA-CP, OK-VQA\nand others, and categorize them into ID, near and far OOD datasets covering\nuni-modal, multi-modal and adversarial distribution shifts. We first conduct a\ncomprehensive comparison of existing robust fine-tuning methods. We then\nquantify the distribution shifts by calculating the Mahalanobis distance using\nuni-modal and multi-modal embeddings extracted from various models. Further, we\nperform an extensive analysis to explore the interactions between uni- and\nmulti-modal shifts as well as modality importance for ID and OOD samples. These\nanalyses offer valuable guidance on developing more robust fine-tuning methods\nto handle multi-modal distribution shifts. The code is available at\nhttps://github.com/chengyuehuang511/FRAMES-VQA ."}
{"id": "2505.22291", "pdf": "https://arxiv.org/pdf/2505.22291", "abs": "https://arxiv.org/abs/2505.22291", "authors": ["Saptarshi Neil Sinha", "P. Julius Kuehn", "Johannes Koppe", "Arjan Kuijper", "Michael Weinmann"], "title": "Neural Restoration of Greening Defects in Historical Autochrome Photographs Based on Purely Synthetic Data", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The preservation of early visual arts, particularly color photographs, is\nchallenged by deterioration caused by aging and improper storage, leading to\nissues like blurring, scratches, color bleeding, and fading defects. In this\npaper, we present the first approach for the automatic removal of greening\ncolor defects in digitized autochrome photographs. Our main contributions\ninclude a method based on synthetic dataset generation and the use of\ngenerative AI with a carefully designed loss function for the restoration of\nvisual arts. To address the lack of suitable training datasets for analyzing\ngreening defects in damaged autochromes, we introduce a novel approach for\naccurately simulating such defects in synthetic data. We also propose a\nmodified weighted loss function for the ChaIR method to account for color\nimbalances between defected and non-defected areas. While existing methods\nstruggle with accurately reproducing original colors and may require\nsignificant manual effort, our method allows for efficient restoration with\nreduced time requirements."}
{"id": "2505.21847", "pdf": "https://arxiv.org/pdf/2505.21847", "abs": "https://arxiv.org/abs/2505.21847", "authors": ["Xuwei Xu", "Yang Li", "Yudong Chen", "Jiajun Liu", "Sen Wang"], "title": "RePaViT: Scalable Vision Transformer Acceleration via Structural Reparameterization on Feedforward Network Layers", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to ICML2025", "summary": "We reveal that feedforward network (FFN) layers, rather than attention\nlayers, are the primary contributors to Vision Transformer (ViT) inference\nlatency, with their impact signifying as model size increases. This finding\nhighlights a critical opportunity for optimizing the efficiency of large-scale\nViTs by focusing on FFN layers. In this work, we propose a novel channel idle\nmechanism that facilitates post-training structural reparameterization for\nefficient FFN layers during testing. Specifically, a set of feature channels\nremains idle and bypasses the nonlinear activation function in each FFN layer,\nthereby forming a linear pathway that enables structural reparameterization\nduring inference. This mechanism results in a family of ReParameterizable\nVision Transformers (RePaViTs), which achieve remarkable latency reductions\nwith acceptable sacrifices (sometimes gains) in accuracy across various ViTs.\nThe benefits of our method scale consistently with model sizes, demonstrating\ngreater speed improvements and progressively narrowing accuracy gaps or even\nhigher accuracies on larger models. In particular, RePa-ViT-Large and\nRePa-ViT-Huge enjoy 66.8% and 68.7% speed-ups with +1.7% and +1.1% higher top-1\naccuracies under the same training strategy, respectively. RePaViT is the first\nto employ structural reparameterization on FFN layers to expedite ViTs to our\nbest knowledge, and we believe that it represents an auspicious direction for\nefficient ViTs. Source code is available at\nhttps://github.com/Ackesnal/RePaViT."}
{"id": "2505.21784", "pdf": "https://arxiv.org/pdf/2505.21784", "abs": "https://arxiv.org/abs/2505.21784", "authors": ["Tharindu Kumarage", "Ninareh Mehrabi", "Anil Ramakrishna", "Xinyan Zhao", "Richard Zemel", "Kai-Wei Chang", "Aram Galstyan", "Rahul Gupta", "Charith Peris"], "title": "Towards Safety Reasoning in LLMs: AI-agentic Deliberation for Policy-embedded CoT Data Creation", "categories": ["cs.AI", "cs.CL"], "comment": "Accepted to ACL 2025 (Findings)", "summary": "Safety reasoning is a recent paradigm where LLMs reason over safety policies\nbefore generating responses, thereby mitigating limitations in existing safety\nmeasures such as over-refusal and jailbreak vulnerabilities. However,\nimplementing this paradigm is challenging due to the resource-intensive process\nof creating high-quality policy-embedded chain-of-thought (CoT) datasets while\nensuring reasoning remains accurate and free from hallucinations or policy\nconflicts. To tackle this, we propose AIDSAFE: Agentic Iterative Deliberation\nfor Safety Reasoning, a novel data generation recipe that leverages multi-agent\ndeliberation to iteratively expand reasoning on safety policies. A data refiner\nstage in AIDSAFE ensures high-quality outputs by eliminating repetitive,\nredundant, and deceptive thoughts. AIDSAFE-generated CoTs provide a strong\nfoundation for supervised fine-tuning (SFT)-based safety training.\nAdditionally, to address the need of preference data in alignment stages, such\nas DPO training, we introduce a supplemental recipe that uses belief\naugmentation to create distinct selected and rejected CoT samples. Our\nevaluations demonstrate that AIDSAFE-generated CoTs achieve superior policy\nadherence and reasoning quality. Consequently, we show that fine-tuning\nopen-source LLMs on these CoTs can significantly improve safety generalization\nand jailbreak robustness while maintaining acceptable utility and over-refusal\naccuracy. AIDSAFE-generated CoT datasets can be found here:\nhttps://huggingface.co/datasets/AmazonScience/AIDSAFE"}
{"id": "2505.22304", "pdf": "https://arxiv.org/pdf/2505.22304", "abs": "https://arxiv.org/abs/2505.22304", "authors": ["Jiali Chen", "Xusen Hei", "HongFei Liu", "Yuancheng Wei", "Zikun Deng", "Jiayuan Xie", "Yi Cai", "Li Qing"], "title": "CADReview: Automatically Reviewing CAD Programs with Error Detection and Correction", "categories": ["cs.CV"], "comment": "ACL 2025 main conference", "summary": "Computer-aided design (CAD) is crucial in prototyping 3D objects through\ngeometric instructions (i.e., CAD programs). In practical design workflows,\ndesigners often engage in time-consuming reviews and refinements of these\nprototypes by comparing them with reference images. To bridge this gap, we\nintroduce the CAD review task to automatically detect and correct potential\nerrors, ensuring consistency between the constructed 3D objects and reference\nimages. However, recent advanced multimodal large language models (MLLMs)\nstruggle to recognize multiple geometric components and perform spatial\ngeometric operations within the CAD program, leading to inaccurate reviews. In\nthis paper, we propose the CAD program repairer (ReCAD) framework to\neffectively detect program errors and provide helpful feedback on error\ncorrection. Additionally, we create a dataset, CADReview, consisting of over\n20K program-image pairs, with diverse errors for the CAD review task. Extensive\nexperiments demonstrate that our ReCAD significantly outperforms existing\nMLLMs, which shows great potential in design applications."}
{"id": "2505.21849", "pdf": "https://arxiv.org/pdf/2505.21849", "abs": "https://arxiv.org/abs/2505.21849", "authors": ["Bo Tang", "Junyi Zhu", "Chenyang Xi", "Yunhang Ge", "Jiahao Wu", "Yuchen Feng", "Yijun Niu", "Wenqiang Wei", "Yu Yu", "Chunyu Li", "Zehao Lin", "Hao Wu", "Ning Liao", "Yebin Yang", "Jiajia Wang", "Zhiyu Li", "Feiyu Xiong", "Jingrun Chen"], "title": "Xinyu AI Search: Enhanced Relevance and Comprehensive Results with Rich Answer Presentations", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Traditional search engines struggle to synthesize fragmented information for\ncomplex queries, while generative AI search engines face challenges in\nrelevance, comprehensiveness, and presentation. To address these limitations,\nwe introduce Xinyu AI Search, a novel system that incorporates a\nquery-decomposition graph to dynamically break down complex queries into\nsub-queries, enabling stepwise retrieval and generation. Our retrieval pipeline\nenhances diversity through multi-source aggregation and query expansion, while\nfiltering and re-ranking strategies optimize passage relevance. Additionally,\nXinyu AI Search introduces a novel approach for fine-grained, precise built-in\ncitation and innovates in result presentation by integrating timeline\nvisualization and textual-visual choreography. Evaluated on recent real-world\nqueries, Xinyu AI Search outperforms eight existing technologies in human\nassessments, excelling in relevance, comprehensiveness, and insightfulness.\nAblation studies validate the necessity of its key sub-modules. Our work\npresents the first comprehensive framework for generative AI search engines,\nbridging retrieval, generation, and user-centric presentation."}
{"id": "2505.21785", "pdf": "https://arxiv.org/pdf/2505.21785", "abs": "https://arxiv.org/abs/2505.21785", "authors": ["Yana Veitsman", "Mayank Jobanputra", "Yash Sarrof", "Aleksandra Bakalova", "Vera Demberg", "Ellie Pavlick", "Michael Hahn"], "title": "Born a Transformer -- Always a Transformer?", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Transformers have theoretical limitations in modeling certain\nsequence-to-sequence tasks, yet it remains largely unclear if these limitations\nplay a role in large-scale pretrained LLMs, or whether LLMs might effectively\novercome these constraints in practice due to the scale of both the models\nthemselves and their pretraining data. We explore how these architectural\nconstraints manifest after pretraining, by studying a family of\n$\\textit{retrieval}$ and $\\textit{copying}$ tasks inspired by Liu et al.\n[2024]. We use the recently proposed C-RASP framework for studying length\ngeneralization [Huang et al., 2025b] to provide guarantees for each of our\nsettings. Empirically, we observe an $\\textit{induction-versus-anti-induction}$\nasymmetry, where pretrained models are better at retrieving tokens to the right\n(induction) rather than the left (anti-induction) of a query token. This\nasymmetry disappears upon targeted fine-tuning if length-generalization is\nguaranteed by theory. Mechanistic analysis reveals that this asymmetry is\nconnected to the differences in the strength of induction versus anti-induction\ncircuits within pretrained Transformers. We validate our findings through\npractical experiments on real-world tasks demonstrating reliability risks. Our\nresults highlight that pretraining selectively enhances certain Transformer\ncapabilities, but does not overcome fundamental length-generalization limits."}
{"id": "2505.22305", "pdf": "https://arxiv.org/pdf/2505.22305", "abs": "https://arxiv.org/abs/2505.22305", "authors": ["Md Touhidul Islam", "Imran Kabir", "Md Alimoor Reza", "Syed Masum Billah"], "title": "IKIWISI: An Interactive Visual Pattern Generator for Evaluating the Reliability of Vision-Language Models Without Ground Truth", "categories": ["cs.CV"], "comment": "Accepted at DIS'25 (Funchal, Portugal)", "summary": "We present IKIWISI (\"I Know It When I See It\"), an interactive visual pattern\ngenerator for assessing vision-language models in video object recognition when\nground truth is unavailable. IKIWISI transforms model outputs into a binary\nheatmap where green cells indicate object presence and red cells indicate\nobject absence. This visualization leverages humans' innate pattern recognition\nabilities to evaluate model reliability. IKIWISI introduces \"spy objects\":\nadversarial instances users know are absent, to discern models hallucinating on\nnonexistent items. The tool functions as a cognitive audit mechanism, surfacing\nmismatches between human and machine perception by visualizing where models\ndiverge from human understanding.\n  Our study with 15 participants found that users considered IKIWISI easy to\nuse, made assessments that correlated with objective metrics when available,\nand reached informed conclusions by examining only a small fraction of heatmap\ncells. This approach not only complements traditional evaluation methods\nthrough visual assessment of model behavior with custom object sets, but also\nreveals opportunities for improving alignment between human perception and\nmachine understanding in vision-language systems."}
{"id": "2505.21850", "pdf": "https://arxiv.org/pdf/2505.21850", "abs": "https://arxiv.org/abs/2505.21850", "authors": ["Yanbei Jiang", "Yihao Ding", "Chao Lei", "Jiayang Ao", "Jey Han Lau", "Krista A. Ehinger"], "title": "Beyond Perception: Evaluating Abstract Visual Reasoning through Multi-Stage Task", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at ACL Findings", "summary": "Current Multimodal Large Language Models (MLLMs) excel in general visual\nreasoning but remain underexplored in Abstract Visual Reasoning (AVR), which\ndemands higher-order reasoning to identify abstract rules beyond simple\nperception. Existing AVR benchmarks focus on single-step reasoning, emphasizing\nthe end result but neglecting the multi-stage nature of reasoning process. Past\nstudies found MLLMs struggle with these benchmarks, but it doesn't explain how\nthey fail. To address this gap, we introduce MultiStAR, a Multi-Stage AVR\nbenchmark, based on RAVEN, designed to assess reasoning across varying levels\nof complexity. Additionally, existing metrics like accuracy only focus on the\nfinal outcomes while do not account for the correctness of intermediate steps.\nTherefore, we propose a novel metric, MSEval, which considers the correctness\nof intermediate steps in addition to the final outcomes. We conduct\ncomprehensive experiments on MultiStAR using 17 representative close-source and\nopen-source MLLMs. The results reveal that while existing MLLMs perform\nadequately on basic perception tasks, they continue to face challenges in more\ncomplex rule detection stages."}
{"id": "2505.21800", "pdf": "https://arxiv.org/pdf/2505.21800", "abs": "https://arxiv.org/abs/2505.21800", "authors": ["Stanley Yu", "Vaidehi Bulusu", "Oscar Yasunaga", "Clayton Lau", "Cole Blondin", "Sean O'Brien", "Kevin Zhu", "Vasu Sharma"], "title": "From Directions to Cones: Exploring Multidimensional Representations of Propositional Facts in LLMs", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) exhibit strong conversational abilities but\noften generate falsehoods. Prior work suggests that the truthfulness of simple\npropositions can be represented as a single linear direction in a model's\ninternal activations, but this may not fully capture its underlying geometry.\nIn this work, we extend the concept cone framework, recently introduced for\nmodeling refusal, to the domain of truth. We identify multi-dimensional cones\nthat causally mediate truth-related behavior across multiple LLM families. Our\nresults are supported by three lines of evidence: (i) causal interventions\nreliably flip model responses to factual statements, (ii) learned cones\ngeneralize across model architectures, and (iii) cone-based interventions\npreserve unrelated model behavior. These findings reveal the richer,\nmultidirectional structure governing simple true/false propositions in LLMs and\nhighlight concept cones as a promising tool for probing abstract behaviors."}
{"id": "2505.22337", "pdf": "https://arxiv.org/pdf/2505.22337", "abs": "https://arxiv.org/abs/2505.22337", "authors": ["Samara Ghrer", "Christophe Godin", "Stefanie Wuhrer"], "title": "Learning to Infer Parameterized Representations of Plants from 3D Scans", "categories": ["cs.CV"], "comment": null, "summary": "Reconstructing faithfully the 3D architecture of plants from unstructured\nobservations is a challenging task. Plants frequently contain numerous organs,\norganized in branching systems in more or less complex spatial networks,\nleading to specific computational issues due to self-occlusion or spatial\nproximity between organs. Existing works either consider inverse modeling where\nthe aim is to recover the procedural rules that allow to simulate virtual\nplants, or focus on specific tasks such as segmentation or skeletonization. We\npropose a unified approach that, given a 3D scan of a plant, allows to infer a\nparameterized representation of the plant. This representation describes the\nplant's branching structure, contains parametric information for each plant\norgan, and can therefore be used directly in a variety of tasks. In this\ndata-driven approach, we train a recursive neural network with virtual plants\ngenerated using an L-systems-based procedural model. After training, the\nnetwork allows to infer a parametric tree-like representation based on an input\n3D point cloud. Our method is applicable to any plant that can be represented\nas binary axial tree. We evaluate our approach on Chenopodium Album plants,\nusing experiments on synthetic plants to show that our unified framework allows\nfor different tasks including reconstruction, segmentation and skeletonization,\nwhile achieving results on-par with state-of-the-art for each task."}
{"id": "2505.21851", "pdf": "https://arxiv.org/pdf/2505.21851", "abs": "https://arxiv.org/abs/2505.21851", "authors": ["Sunshine Jiang", "Xiaolin Fang", "Nicholas Roy", "Tomás Lozano-Pérez", "Leslie Pack Kaelbling", "Siddharth Ancha"], "title": "Streaming Flow Policy: Simplifying diffusion$/$flow-matching policies by treating action trajectories as flow trajectories", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "ICRA 2025 Beyond Pick and Place Workshop", "summary": "Recent advances in diffusion$/$flow-matching policies have enabled imitation\nlearning of complex, multi-modal action trajectories. However, they are\ncomputationally expensive because they sample a trajectory of trajectories: a\ndiffusion$/$flow trajectory of action trajectories. They discard intermediate\naction trajectories, and must wait for the sampling process to complete before\nany actions can be executed on the robot. We simplify diffusion$/$flow policies\nby treating action trajectories as flow trajectories. Instead of starting from\npure noise, our algorithm samples from a narrow Gaussian around the last\naction. Then, it incrementally integrates a velocity field learned via flow\nmatching to produce a sequence of actions that constitute a single trajectory.\nThis enables actions to be streamed to the robot on-the-fly during the flow\nsampling process, and is well-suited for receding horizon policy execution.\nDespite streaming, our method retains the ability to model multi-modal\nbehavior. We train flows that stabilize around demonstration trajectories to\nreduce distribution shift and improve imitation learning performance. Streaming\nflow policy outperforms prior methods while enabling faster policy execution\nand tighter sensorimotor loops for learning-based robot control. Project\nwebsite: https://streaming-flow-policy.github.io/"}
{"id": "2505.21815", "pdf": "https://arxiv.org/pdf/2505.21815", "abs": "https://arxiv.org/abs/2505.21815", "authors": ["Yunyi Zhang", "Ruozhen Yang", "Siqi Jiao", "SeongKu Kang", "Jiawei Han"], "title": "Scientific Paper Retrieval with LLM-Guided Semantic-Based Ranking", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Scientific paper retrieval is essential for supporting literature discovery\nand research. While dense retrieval methods demonstrate effectiveness in\ngeneral-purpose tasks, they often fail to capture fine-grained scientific\nconcepts that are essential for accurate understanding of scientific queries.\nRecent studies also use large language models (LLMs) for query understanding;\nhowever, these methods often lack grounding in corpus-specific knowledge and\nmay generate unreliable or unfaithful content. To overcome these limitations,\nwe propose SemRank, an effective and efficient paper retrieval framework that\ncombines LLM-guided query understanding with a concept-based semantic index.\nEach paper is indexed using multi-granular scientific concepts, including\ngeneral research topics and detailed key phrases. At query time, an LLM\nidentifies core concepts derived from the corpus to explicitly capture the\nquery's information need. These identified concepts enable precise semantic\nmatching, significantly enhancing retrieval accuracy. Experiments show that\nSemRank consistently improves the performance of various base retrievers,\nsurpasses strong existing LLM-based baselines, and remains highly efficient."}
{"id": "2505.22342", "pdf": "https://arxiv.org/pdf/2505.22342", "abs": "https://arxiv.org/abs/2505.22342", "authors": ["Shriram M S", "Xinyue Hao", "Shihao Hou", "Yang Lu", "Laura Sevilla-Lara", "Anurag Arnab", "Shreyank N Gowda"], "title": "Progressive Data Dropout: An Embarrassingly Simple Approach to Faster Training", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The success of the machine learning field has reliably depended on training\non large datasets. While effective, this trend comes at an extraordinary cost.\nThis is due to two deeply intertwined factors: the size of models and the size\nof datasets. While promising research efforts focus on reducing the size of\nmodels, the other half of the equation remains fairly mysterious. Indeed, it is\nsurprising that the standard approach to training remains to iterate over and\nover, uniformly sampling the training dataset. In this paper we explore a\nseries of alternative training paradigms that leverage insights from\nhard-data-mining and dropout, simple enough to implement and use that can\nbecome the new training standard. The proposed Progressive Data Dropout reduces\nthe number of effective epochs to as little as 12.4% of the baseline. This\nsavings actually do not come at any cost for accuracy. Surprisingly, the\nproposed method improves accuracy by up to 4.82%. Our approach requires no\nchanges to model architecture or optimizer, and can be applied across standard\ntraining pipelines, thus posing an excellent opportunity for wide adoption.\nCode can be found here: https://github.com/bazyagami/LearningWithRevision"}
{"id": "2505.21852", "pdf": "https://arxiv.org/pdf/2505.21852", "abs": "https://arxiv.org/abs/2505.21852", "authors": ["Akifumi Wachi", "Kohei Miyaguchi", "Takumi Tanabe", "Rei Sato", "Youhei Akimoto"], "title": "A Provable Approach for End-to-End Safe Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.IT", "cs.RO", "math.IT"], "comment": "27 pages", "summary": "A longstanding goal in safe reinforcement learning (RL) is a method to ensure\nthe safety of a policy throughout the entire process, from learning to\noperation. However, existing safe RL paradigms inherently struggle to achieve\nthis objective. We propose a method, called Provably Lifetime Safe RL (PLS),\nthat integrates offline safe RL with safe policy deployment to address this\nchallenge. Our proposed method learns a policy offline using return-conditioned\nsupervised learning and then deploys the resulting policy while cautiously\noptimizing a limited set of parameters, known as target returns, using Gaussian\nprocesses (GPs). Theoretically, we justify the use of GPs by analyzing the\nmathematical relationship between target and actual returns. We then prove that\nPLS finds near-optimal target returns while guaranteeing safety with high\nprobability. Empirically, we demonstrate that PLS outperforms baselines both in\nsafety and reward performance, thereby achieving the longstanding goal to\nobtain high rewards while ensuring the safety of a policy throughout the\nlifetime from learning to operation."}
{"id": "2505.21825", "pdf": "https://arxiv.org/pdf/2505.21825", "abs": "https://arxiv.org/abs/2505.21825", "authors": ["Parsa Mirtaheri", "Ezra Edelman", "Samy Jelassi", "Eran Malach", "Enric Boix-Adsera"], "title": "Let Me Think! A Long Chain-of-Thought Can Be Worth Exponentially Many Short Ones", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Inference-time computation has emerged as a promising scaling axis for\nimproving large language model reasoning. However, despite yielding impressive\nperformance, the optimal allocation of inference-time computation remains\npoorly understood. A central question is whether to prioritize sequential\nscaling (e.g., longer chains of thought) or parallel scaling (e.g., majority\nvoting across multiple short chains of thought). In this work, we seek to\nilluminate the landscape of test-time scaling by demonstrating the existence of\nreasoning settings where sequential scaling offers an exponential advantage\nover parallel scaling. These settings are based on graph connectivity problems\nin challenging distributions of graphs. We validate our theoretical findings\nwith comprehensive experiments across a range of language models, including\nmodels trained from scratch for graph connectivity with different chain of\nthought strategies as well as large reasoning models."}
{"id": "2505.22344", "pdf": "https://arxiv.org/pdf/2505.22344", "abs": "https://arxiv.org/abs/2505.22344", "authors": ["Nikhil Behari", "Aaron Young", "Akshat Dave", "Ramesh Raskar"], "title": "Task-Driven Implicit Representations for Automated Design of LiDAR Systems", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Imaging system design is a complex, time-consuming, and largely manual\nprocess; LiDAR design, ubiquitous in mobile devices, autonomous vehicles, and\naerial imaging platforms, adds further complexity through unique spatial and\ntemporal sampling requirements. In this work, we propose a framework for\nautomated, task-driven LiDAR system design under arbitrary constraints. To\nachieve this, we represent LiDAR configurations in a continuous six-dimensional\ndesign space and learn task-specific implicit densities in this space via\nflow-based generative modeling. We then synthesize new LiDAR systems by\nmodeling sensors as parametric distributions in 6D space and fitting these\ndistributions to our learned implicit density using expectation-maximization,\nenabling efficient, constraint-aware LiDAR system design. We validate our\nmethod on diverse tasks in 3D vision, enabling automated LiDAR system design\nacross real-world-inspired applications in face scanning, robotic tracking, and\nobject detection."}
{"id": "2505.21854", "pdf": "https://arxiv.org/pdf/2505.21854", "abs": "https://arxiv.org/abs/2505.21854", "authors": ["Jun Chen", "Xinke Li", "Mingyue Xu", "Tianrui Li", "Chongshou Li"], "title": "Rethinking Gradient-based Adversarial Attacks on Point Cloud Classification", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Gradient-based adversarial attacks have become a dominant approach for\nevaluating the robustness of point cloud classification models. However,\nexisting methods often rely on uniform update rules that fail to consider the\nheterogeneous nature of point clouds, resulting in excessive and perceptible\nperturbations. In this paper, we rethink the design of gradient-based attacks\nby analyzing the limitations of conventional gradient update mechanisms and\npropose two new strategies to improve both attack effectiveness and\nimperceptibility. First, we introduce WAAttack, a novel framework that\nincorporates weighted gradients and an adaptive step-size strategy to account\nfor the non-uniform contribution of points during optimization. This approach\nenables more targeted and subtle perturbations by dynamically adjusting updates\naccording to the local structure and sensitivity of each point. Second, we\npropose SubAttack, a complementary strategy that decomposes the point cloud\ninto subsets and focuses perturbation efforts on structurally critical regions.\nTogether, these methods represent a principled rethinking of gradient-based\nadversarial attacks for 3D point cloud classification. Extensive experiments\ndemonstrate that our approach outperforms state-of-the-art baselines in\ngenerating highly imperceptible adversarial examples. Code will be released\nupon paper acceptance."}
{"id": "2505.21863", "pdf": "https://arxiv.org/pdf/2505.21863", "abs": "https://arxiv.org/abs/2505.21863", "authors": ["Shikhhar Siingh", "Abhinav Rawat", "Vivek Gupta", "Chitta Baral"], "title": "GETReason: Enhancing Image Context Extraction through Hierarchical Multi-Agent Reasoning", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Publicly significant images from events hold valuable contextual information,\ncrucial for journalism and education. However, existing methods often struggle\nto extract this relevance accurately. To address this, we introduce GETReason\n(Geospatial Event Temporal Reasoning), a framework that moves beyond\nsurface-level image descriptions to infer deeper contextual meaning. We propose\nthat extracting global event, temporal, and geospatial information enhances\nunderstanding of an image's significance. Additionally, we introduce GREAT\n(Geospatial Reasoning and Event Accuracy with Temporal Alignment), a new metric\nfor evaluating reasoning-based image understanding. Our layered multi-agent\napproach, assessed using a reasoning-weighted metric, demonstrates that\nmeaningful insights can be inferred, effectively linking images to their\nbroader event context."}
{"id": "2505.22353", "pdf": "https://arxiv.org/pdf/2505.22353", "abs": "https://arxiv.org/abs/2505.22353", "authors": ["Noora Al-Emadi", "Ingmar Weber", "Yin Yang", "Ferda Ofli"], "title": "VME: A Satellite Imagery Dataset and Benchmark for Detecting Vehicles in the Middle East and Beyond", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Detecting vehicles in satellite images is crucial for traffic management,\nurban planning, and disaster response. However, current models struggle with\nreal-world diversity, particularly across different regions. This challenge is\namplified by geographic bias in existing datasets, which often focus on\nspecific areas and overlook regions like the Middle East. To address this gap,\nwe present the Vehicles in the Middle East (VME) dataset, designed explicitly\nfor vehicle detection in high-resolution satellite images from Middle Eastern\ncountries. Sourced from Maxar, the VME dataset spans 54 cities across 12\ncountries, comprising over 4,000 image tiles and more than 100,000 vehicles,\nannotated using both manual and semi-automated methods. Additionally, we\nintroduce the largest benchmark dataset for Car Detection in Satellite Imagery\n(CDSI), combining images from multiple sources to enhance global car detection.\nOur experiments demonstrate that models trained on existing datasets perform\npoorly on Middle Eastern images, while the VME dataset significantly improves\ndetection accuracy in this region. Moreover, state-of-the-art models trained on\nCDSI achieve substantial improvements in global car detection."}
{"id": "2505.21855", "pdf": "https://arxiv.org/pdf/2505.21855", "abs": "https://arxiv.org/abs/2505.21855", "authors": ["Jiseung Yoo", "Curran Mahowald", "Meiyu Li", "Wei Ai"], "title": "Extracting Research Instruments from Educational Literature Using LLMs", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are transforming information extraction from\nacademic literature, offering new possibilities for knowledge management. This\nstudy presents an LLM-based system designed to extract detailed information\nabout research instruments used in the education field, including their names,\ntypes, target respondents, measured constructs, and outcomes. Using multi-step\nprompting and a domain-specific data schema, it generates structured outputs\noptimized for educational research. Our evaluation shows that this system\nsignificantly outperforms other approaches, particularly in identifying\ninstrument names and detailed information. This demonstrates the potential of\nLLM-powered information extraction in educational contexts, offering a\nsystematic way to organize research instrument information. The ability to\naggregate such information at scale enhances accessibility for researchers and\neducation leaders, facilitating informed decision-making in educational\nresearch and policy."}
{"id": "2505.21880", "pdf": "https://arxiv.org/pdf/2505.21880", "abs": "https://arxiv.org/abs/2505.21880", "authors": ["Yu-Lun Song", "Chung-En Tsern", "Che-Cheng Wu", "Yu-Ming Chang", "Syuan-Bo Huang", "Wei-Chu Chen", "Michael Chia-Liang Lin", "Yu-Ta Lin"], "title": "Incorporating LLMs for Large-Scale Urban Complex Mobility Simulation", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.CY"], "comment": "8 pages, 8 figures. This paper is reviewed and accepted by the CUPUM\n  (Computational Urban Planning and Urban Management) Conference held by\n  University College London (UCL) in 2025", "summary": "This study presents an innovative approach to urban mobility simulation by\nintegrating a Large Language Model (LLM) with Agent-Based Modeling (ABM).\nUnlike traditional rule-based ABM, the proposed framework leverages LLM to\nenhance agent diversity and realism by generating synthetic population\nprofiles, allocating routine and occasional locations, and simulating\npersonalized routes. Using real-world data, the simulation models individual\nbehaviors and large-scale mobility patterns in Taipei City. Key insights, such\nas route heat maps and mode-specific indicators, provide urban planners with\nactionable information for policy-making. Future work focuses on establishing\nrobust validation frameworks to ensure accuracy and reliability in urban\nplanning applications."}
{"id": "2505.22360", "pdf": "https://arxiv.org/pdf/2505.22360", "abs": "https://arxiv.org/abs/2505.22360", "authors": ["Kewen Chen", "Xiaobin Hu", "Wenqi Ren"], "title": "Identity-Preserving Text-to-Image Generation via Dual-Level Feature Decoupling and Expert-Guided Fusion", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in large-scale text-to-image generation models have led to a\nsurge in subject-driven text-to-image generation, which aims to produce\ncustomized images that align with textual descriptions while preserving the\nidentity of specific subjects. Despite significant progress, current methods\nstruggle to disentangle identity-relevant information from identity-irrelevant\ndetails in the input images, resulting in overfitting or failure to maintain\nsubject identity. In this work, we propose a novel framework that improves the\nseparation of identity-related and identity-unrelated features and introduces\nan innovative feature fusion mechanism to improve the quality and text\nalignment of generated images. Our framework consists of two key components: an\nImplicit-Explicit foreground-background Decoupling Module (IEDM) and a Feature\nFusion Module (FFM) based on a Mixture of Experts (MoE). IEDM combines\nlearnable adapters for implicit decoupling at the feature level with inpainting\ntechniques for explicit foreground-background separation at the image level.\nFFM dynamically integrates identity-irrelevant features with identity-related\nfeatures, enabling refined feature representations even in cases of incomplete\ndecoupling. In addition, we introduce three complementary loss functions to\nguide the decoupling process. Extensive experiments demonstrate the\neffectiveness of our proposed method in enhancing image generation quality,\nimproving flexibility in scene adaptation, and increasing the diversity of\ngenerated outputs across various textual descriptions."}
{"id": "2505.21866", "pdf": "https://arxiv.org/pdf/2505.21866", "abs": "https://arxiv.org/abs/2505.21866", "authors": ["Guozhen Zhu", "Yuqian Hu", "Weihang Gao", "Wei-Hsiang Wang", "Beibei Wang", "K. J. Ray Liu"], "title": "CSI-Bench: A Large-Scale In-the-Wild Dataset for Multi-task WiFi Sensing", "categories": ["eess.SP", "cs.AI", "cs.DB"], "comment": "21 pages, 4 figures", "summary": "WiFi sensing has emerged as a compelling contactless modality for human\nactivity monitoring by capturing fine-grained variations in Channel State\nInformation (CSI). Its ability to operate continuously and non-intrusively\nwhile preserving user privacy makes it particularly suitable for health\nmonitoring. However, existing WiFi sensing systems struggle to generalize in\nreal-world settings, largely due to datasets collected in controlled\nenvironments with homogeneous hardware and fragmented, session-based recordings\nthat fail to reflect continuous daily activity.\n  We present CSI-Bench, a large-scale, in-the-wild benchmark dataset collected\nusing commercial WiFi edge devices across 26 diverse indoor environments with\n35 real users. Spanning over 461 hours of effective data, CSI-Bench captures\nrealistic signal variability under natural conditions. It includes\ntask-specific datasets for fall detection, breathing monitoring, localization,\nand motion source recognition, as well as a co-labeled multitask dataset with\njoint annotations for user identity, activity, and proximity. To support the\ndevelopment of robust and generalizable models, CSI-Bench provides standardized\nevaluation splits and baseline results for both single-task and multi-task\nlearning. CSI-Bench offers a foundation for scalable, privacy-preserving WiFi\nsensing systems in health and broader human-centric applications."}
{"id": "2505.21907", "pdf": "https://arxiv.org/pdf/2505.21907", "abs": "https://arxiv.org/abs/2505.21907", "authors": ["Saleh Afzoon", "Zahra Jahanandish", "Phuong Thao Huynh", "Amin Beheshti", "Usman Naseem"], "title": "Modeling and Optimizing User Preferences in AI Copilots: A Comprehensive Survey and Taxonomy", "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "AI copilots, context-aware, AI-powered systems designed to assist users in\ntasks such as software development and content creation, are becoming integral\nto modern workflows. As these systems grow in capability and adoption,\npersonalization has emerged as a cornerstone for ensuring usability, trust, and\nproductivity. Central to this personalization is preference optimization: the\nability of AI copilots to detect, interpret, and align with individual user\npreferences. While personalization techniques are well-established in domains\nlike recommender systems and dialogue agents, their adaptation to interactive,\nreal-time systems like AI copilots remains fragmented and underexplored. This\nsurvey addresses this gap by synthesizing research on how user preferences are\ncaptured, modeled, and refined within the design of AI copilots. We introduce a\nunified definition of AI copilots and propose a phase-based taxonomy of\npreference optimization strategies, structured around pre-interaction,\nmid-interaction, and post-interaction stages. We analyze techniques for\nacquiring preference signals, modeling user intent, and integrating feedback\nloops, highlighting both established approaches and recent innovations. By\nbridging insights from AI personalization, human-AI collaboration, and large\nlanguage model adaptation, this survey provides a structured foundation for\ndesigning adaptive, preference-aware AI copilots. It offers a holistic view of\nthe available preference resources, how they can be leveraged, and which\ntechnical approaches are most suited to each stage of system design."}
{"id": "2505.22387", "pdf": "https://arxiv.org/pdf/2505.22387", "abs": "https://arxiv.org/abs/2505.22387", "authors": ["Jaehyun Choi", "Gyojin Han", "Dong-Jae Lee", "Sunghyun Baek", "Junmo Kim"], "title": "DAM: Domain-Aware Module for Multi-Domain Dataset Condensation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Dataset Condensation (DC) has emerged as a promising solution to mitigate the\ncomputational and storage burdens associated with training deep learning\nmodels. However, existing DC methods largely overlook the multi-domain nature\nof modern datasets, which are increasingly composed of heterogeneous images\nspanning multiple domains. In this paper, we extend DC and introduce\nMulti-Domain Dataset Condensation (MDDC), which aims to condense data that\ngeneralizes across both single-domain and multi-domain settings. To this end,\nwe propose the Domain-Aware Module (DAM), a training-time module that embeds\ndomain-related features into each synthetic image via learnable spatial masks.\nAs explicit domain labels are mostly unavailable in real-world datasets, we\nemploy frequency-based pseudo-domain labeling, which leverages low-frequency\namplitude statistics. DAM is only active during the condensation process, thus\npreserving the same images per class (IPC) with prior methods. Experiments show\nthat DAM consistently improves in-domain, out-of-domain, and cross-architecture\nperformance over baseline dataset condensation methods."}
{"id": "2505.21870", "pdf": "https://arxiv.org/pdf/2505.21870", "abs": "https://arxiv.org/abs/2505.21870", "authors": ["Shuyang Cao", "Karthik Radhakrishnan", "David Rosenberg", "Steven Lu", "Pengxiang Cheng", "Lu Wang", "Shiyue Zhang"], "title": "Evaluating the Retrieval Robustness of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "19 pages", "summary": "Retrieval-augmented generation (RAG) generally enhances large language\nmodels' (LLMs) ability to solve knowledge-intensive tasks. But RAG may also\nlead to performance degradation due to imperfect retrieval and the model's\nlimited ability to leverage retrieved content. In this work, we evaluate the\nrobustness of LLMs in practical RAG setups (henceforth retrieval robustness).\nWe focus on three research questions: (1) whether RAG is always better than\nnon-RAG; (2) whether more retrieved documents always lead to better\nperformance; (3) and whether document orders impact results. To facilitate this\nstudy, we establish a benchmark of 1500 open-domain questions, each with\nretrieved documents from Wikipedia. We introduce three robustness metrics, each\ncorresponds to one research question. Our comprehensive experiments, involving\n11 LLMs and 3 prompting strategies, reveal that all of these LLMs exhibit\nsurprisingly high retrieval robustness; nonetheless, different degrees of\nimperfect robustness hinders them from fully utilizing the benefits of RAG."}
{"id": "2505.21930", "pdf": "https://arxiv.org/pdf/2505.21930", "abs": "https://arxiv.org/abs/2505.21930", "authors": ["Dongyue Li", "Ziniu Zhang", "Lu Wang", "Hongyang R. Zhang"], "title": "Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets", "categories": ["cs.LG", "cs.CL"], "comment": "17 pages. To appear in ACL'25", "summary": "This paper develops an ensemble method for fine-tuning a language model to\nmultiple datasets. Existing methods, such as quantized LoRA (QLoRA), are\nefficient when adapting to a single dataset. When training on multiple datasets\nof different tasks, a common setup in practice, it remains unclear how to\ndesign an efficient adaptation for fine-tuning language models. We propose to\nuse an ensemble of multiple smaller adapters instead of a single adapter per\ntask. We design an efficient algorithm that partitions $n$ datasets into $m$\ngroups, where $m$ is typically much smaller than $n$ in practice, and train one\nadapter for each group before taking a weighted combination to form the\nensemble. The algorithm leverages a first-order approximation property of\nlow-rank adaptation to quickly obtain the fine-tuning performances of dataset\ncombinations since methods like LoRA stay close to the base model. Hence, we\nuse the gradients of the base model to estimate its behavior during\nfine-tuning. Empirically, this approximation holds with less than $1\\%$ error\non models with up to $34$ billion parameters, leading to an estimation of true\nfine-tuning performances under $5\\%$ error while speeding up computation\ncompared to base fine-tuning by $105$ times. When applied to fine-tune Llama\nand GPT models on ten text classification tasks, our approach provides up to\n$10\\%$ higher average test accuracy over QLoRA, with only $9\\%$ more FLOPs. On\na Llama model with $34$ billion parameters, an ensemble of QLoRA increases test\naccuracy by $3\\%$ compared to QLoRA, with only $8\\%$ more FLOPs."}
{"id": "2505.22394", "pdf": "https://arxiv.org/pdf/2505.22394", "abs": "https://arxiv.org/abs/2505.22394", "authors": ["Fan Fei", "Jiajun Tang", "Fei-Peng Tian", "Boxin Shi", "Ping Tan"], "title": "PacTure: Efficient PBR Texture Generation on Packed Views with Visual Autoregressive Models", "categories": ["cs.CV"], "comment": "20 pages, 7 figures", "summary": "We present PacTure, a novel framework for generating physically-based\nrendering (PBR) material textures from an untextured 3D mesh, a text\ndescription, and an optional image prompt. Early 2D generation-based texturing\napproaches generate textures sequentially from different views, resulting in\nlong inference times and globally inconsistent textures. More recent approaches\nadopt multi-view generation with cross-view attention to enhance global\nconsistency, which, however, limits the resolution for each view. In response\nto these weaknesses, we first introduce view packing, a novel technique that\nsignificantly increases the effective resolution for each view during\nmulti-view generation without imposing additional inference cost, by\nformulating the arrangement of multi-view maps as a 2D rectangle bin packing\nproblem. In contrast to UV mapping, it preserves the spatial proximity\nessential for image generation and maintains full compatibility with current 2D\ngenerative models. To further reduce the inference cost, we enable fine-grained\ncontrol and multi-domain generation within the next-scale prediction\nautoregressive framework to create an efficient multi-view multi-domain\ngenerative backbone. Extensive experiments show that PacTure outperforms\nstate-of-the-art methods in both quality of generated PBR textures and\nefficiency in training and inference."}
{"id": "2505.21873", "pdf": "https://arxiv.org/pdf/2505.21873", "abs": "https://arxiv.org/abs/2505.21873", "authors": ["Jie Gao", "Jun Li", "Jing Hu", "Shanzhuo Zhang", "Kunrui Zhu", "Yueyang Huang", "Xiaonan Zhang", "Xiaomin Fang"], "title": "HelixDesign-Binder: A Scalable Production-Grade Platform for Binder Design Built on HelixFold3", "categories": ["q-bio.BM", "cs.AI", "q-bio.QM"], "comment": null, "summary": "Protein binder design is central to therapeutics, diagnostics, and synthetic\nbiology, yet practical deployment remains challenging due to fragmented\nworkflows, high computational costs, and complex tool integration. We present\nHelixDesign-Binder, a production-grade, high-throughput platform built on\nHelixFold3 that automates the full binder design pipeline, from backbone\ngeneration and sequence design to structural evaluation and multi-dimensional\nscoring. By unifying these stages into a scalable and user-friendly system,\nHelixDesign-Binder enables efficient exploration of binder candidates with\nfavorable structural, energetic, and physicochemical properties. The platform\nleverages Baidu Cloud's high-performance infrastructure to support large-scale\ndesign and incorporates advanced scoring metrics, including ipTM, predicted\nbinding free energy, and interface hydrophobicity. Benchmarking across six\nprotein targets demonstrates that HelixDesign-Binder reliably produces diverse\nand high-quality binders, some of which match or exceed validated designs in\npredicted binding affinity. HelixDesign-Binder is accessible via an interactive\nweb interface in PaddleHelix platform, supporting both academic research and\nindustrial applications in antibody and protein binder development."}
{"id": "2505.21956", "pdf": "https://arxiv.org/pdf/2505.21956", "abs": "https://arxiv.org/abs/2505.21956", "authors": ["Mengdan Zhu", "Senhao Cheng", "Guangji Bai", "Yifei Zhang", "Liang Zhao"], "title": "Cross-modal RAG: Sub-dimensional Retrieval-Augmented Text-to-Image Generation", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Text-to-image generation increasingly demands access to domain-specific,\nfine-grained, and rapidly evolving knowledge that pretrained models cannot\nfully capture. Existing Retrieval-Augmented Generation (RAG) methods attempt to\naddress this by retrieving globally relevant images, but they fail when no\nsingle image contains all desired elements from a complex user query. We\npropose Cross-modal RAG, a novel framework that decomposes both queries and\nimages into sub-dimensional components, enabling subquery-aware retrieval and\ngeneration. Our method introduces a hybrid retrieval strategy - combining a\nsub-dimensional sparse retriever with a dense retriever - to identify a\nPareto-optimal set of images, each contributing complementary aspects of the\nquery. During generation, a multimodal large language model is guided to\nselectively condition on relevant visual features aligned to specific\nsubqueries, ensuring subquery-aware image synthesis. Extensive experiments on\nMS-COCO, Flickr30K, WikiArt, CUB, and ImageNet-LT demonstrate that Cross-modal\nRAG significantly outperforms existing baselines in both retrieval and\ngeneration quality, while maintaining high efficiency."}
{"id": "2505.22396", "pdf": "https://arxiv.org/pdf/2505.22396", "abs": "https://arxiv.org/abs/2505.22396", "authors": ["Xudong Li", "Mengdan Zhang", "Peixian Chen", "Xiawu Zheng", "Yan Zhang", "Jingyuan Zheng", "Yunhang Shen", "Ke Li", "Chaoyou Fu", "Xing Sun", "Rongrong Ji"], "title": "Zooming from Context to Cue: Hierarchical Preference Optimization for Multi-Image MLLMs", "categories": ["cs.CV"], "comment": null, "summary": "Multi-modal Large Language Models (MLLMs) excel at single-image tasks but\nstruggle with multi-image understanding due to cross-modal misalignment,\nleading to hallucinations (context omission, conflation, and\nmisinterpretation). Existing methods using Direct Preference Optimization (DPO)\nconstrain optimization to a solitary image reference within the input sequence,\nneglecting holistic context modeling. We propose Context-to-Cue Direct\nPreference Optimization (CcDPO), a multi-level preference optimization\nframework that enhances per-image perception in multi-image settings by zooming\ninto visual clues -- from sequential context to local details. It features: (i)\nContext-Level Optimization : Re-evaluates cognitive biases underlying MLLMs'\nmulti-image context comprehension and integrates a spectrum of low-cost global\nsequence preferences for bias mitigation. (ii) Needle-Level Optimization :\nDirects attention to fine-grained visual details through region-targeted visual\nprompts and multimodal preference supervision. To support scalable\noptimization, we also construct MultiScope-42k, an automatically generated\ndataset with high-quality multi-level preference pairs. Experiments show that\nCcDPO significantly reduces hallucinations and yields consistent performance\ngains across general single- and multi-image tasks."}
{"id": "2505.21876", "pdf": "https://arxiv.org/pdf/2505.21876", "abs": "https://arxiv.org/abs/2505.21876", "authors": ["Zun Wang", "Jaemin Cho", "Jialu Li", "Han Lin", "Jaehong Yoon", "Yue Zhang", "Mohit Bansal"], "title": "EPiC: Efficient Video Camera Control Learning with Precise Anchor-Video Guidance", "categories": ["cs.CV", "cs.AI"], "comment": "Project website: https://zunwang1.github.io/Epic", "summary": "Recent approaches on 3D camera control in video diffusion models (VDMs) often\ncreate anchor videos to guide diffusion models as a structured prior by\nrendering from estimated point clouds following annotated camera trajectories.\nHowever, errors inherent in point cloud estimation often lead to inaccurate\nanchor videos. Moreover, the requirement for extensive camera trajectory\nannotations further increases resource demands. To address these limitations,\nwe introduce EPiC, an efficient and precise camera control learning framework\nthat automatically constructs high-quality anchor videos without expensive\ncamera trajectory annotations. Concretely, we create highly precise anchor\nvideos for training by masking source videos based on first-frame visibility.\nThis approach ensures high alignment, eliminates the need for camera trajectory\nannotations, and thus can be readily applied to any in-the-wild video to\ngenerate image-to-video (I2V) training pairs. Furthermore, we introduce\nAnchor-ControlNet, a lightweight conditioning module that integrates anchor\nvideo guidance in visible regions to pretrained VDMs, with less than 1% of\nbackbone model parameters. By combining the proposed anchor video data and\nControlNet module, EPiC achieves efficient training with substantially fewer\nparameters, training steps, and less data, without requiring modifications to\nthe diffusion model backbone typically needed to mitigate rendering\nmisalignments. Although being trained on masking-based anchor videos, our\nmethod generalizes robustly to anchor videos made with point clouds during\ninference, enabling precise 3D-informed camera control. EPiC achieves SOTA\nperformance on RealEstate10K and MiraData for I2V camera control task,\ndemonstrating precise and robust camera control ability both quantitatively and\nqualitatively. Notably, EPiC also exhibits strong zero-shot generalization to\nvideo-to-video scenarios."}
{"id": "2505.21959", "pdf": "https://arxiv.org/pdf/2505.21959", "abs": "https://arxiv.org/abs/2505.21959", "authors": ["Aakriti Agrawal", "Mucong Ding", "Zora Che", "Chenghao Deng", "Anirudh Satheesh", "Bang An", "Bayan Bruss", "John Langford", "Furong Huang"], "title": "EnsemW2S: Enhancing Weak-to-Strong Generalization with Large Language Model Ensembles", "categories": ["cs.LG", "cs.CL"], "comment": "Superalignment. arXiv admin note: substantial text overlap with\n  arXiv:2410.04571", "summary": "With Large Language Models (LLMs) rapidly approaching and potentially\nsurpassing human-level performance, it has become imperative to develop\napproaches capable of effectively supervising and enhancing these powerful\nmodels using smaller, human-level models exposed to only human-level data. We\naddress this critical weak-to-strong (W2S) generalization challenge by\nproposing a novel method aimed at improving weak experts, by training on the\nsame limited human-level data, enabling them to generalize to complex,\nsuper-human-level tasks. Our approach, called \\textbf{EnsemW2S}, employs a\ntoken-level ensemble strategy that iteratively combines multiple weak experts,\nsystematically addressing the shortcomings identified in preceding iterations.\nBy continuously refining these weak models, we significantly enhance their\ncollective ability to supervise stronger student models. We extensively\nevaluate the generalization performance of both the ensemble of weak experts\nand the subsequent strong student model across in-distribution (ID) and\nout-of-distribution (OOD) datasets. For OOD, we specifically introduce question\ndifficulty as an additional dimension for defining distributional shifts. Our\nempirical results demonstrate notable improvements, achieving 4\\%, and 3.2\\%\nimprovements on ID datasets and, upto 6\\% and 2.28\\% on OOD datasets for\nexperts and student models respectively, underscoring the effectiveness of our\nproposed method in advancing W2S generalization."}
{"id": "2505.22407", "pdf": "https://arxiv.org/pdf/2505.22407", "abs": "https://arxiv.org/abs/2505.22407", "authors": ["Jiadong Pan", "Zhiyuan Ma", "Kaiyan Zhang", "Ning Ding", "Bowen Zhou"], "title": "Self-Reflective Reinforcement Learning for Diffusion-based Image Reasoning Generation", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have recently demonstrated exceptional performance in image\ngeneration task. However, existing image generation methods still significantly\nsuffer from the dilemma of image reasoning, especially in logic-centered image\ngeneration tasks. Inspired by the success of Chain of Thought (CoT) and\nReinforcement Learning (RL) in LLMs, we propose SRRL, a self-reflective RL\nalgorithm for diffusion models to achieve reasoning generation of logical\nimages by performing reflection and iteration across generation trajectories.\nThe intermediate samples in the denoising process carry noise, making accurate\nreward evaluation difficult. To address this challenge, SRRL treats the entire\ndenoising trajectory as a CoT step with multi-round reflective denoising\nprocess and introduces condition guided forward process, which allows for\nreflective iteration between CoT steps. Through SRRL-based iterative diffusion\ntraining, we introduce image reasoning through CoT into generation tasks\nadhering to physical laws and unconventional physical phenomena for the first\ntime. Notably, experimental results of case study exhibit that the superior\nperformance of our SRRL algorithm even compared with GPT-4o. The project page\nis https://jadenpan0.github.io/srrl.github.io/."}
{"id": "2505.21879", "pdf": "https://arxiv.org/pdf/2505.21879", "abs": "https://arxiv.org/abs/2505.21879", "authors": ["Weiting Liu", "Jiaxu Cui", "Jiao Hu", "En Wang", "Bo Yang"], "title": "Symbolic Foundation Regressor on Complex Networks", "categories": ["cs.SC", "cs.AI", "cs.LG"], "comment": "60 pages", "summary": "In science, we are interested not only in forecasting but also in\nunderstanding how predictions are made, specifically what the interpretable\nunderlying model looks like. Data-driven machine learning technology can\nsignificantly streamline the complex and time-consuming traditional manual\nprocess of discovering scientific laws, helping us gain insights into\nfundamental issues in modern science. In this work, we introduce a pre-trained\nsymbolic foundation regressor that can effectively compress complex data with\nnumerous interacting variables while producing interpretable physical\nrepresentations. Our model has been rigorously tested on non-network symbolic\nregression, symbolic regression on complex networks, and the inference of\nnetwork dynamics across various domains, including physics, biochemistry,\necology, and epidemiology. The results indicate a remarkable improvement in\nequation inference efficiency, being three times more effective than baseline\napproaches while maintaining accurate predictions. Furthermore, we apply our\nmodel to uncover more intuitive laws of interaction transmission from global\nepidemic outbreak data, achieving optimal data fitting. This model extends the\napplication boundary of pre-trained symbolic regression models to complex\nnetworks, and we believe it provides a foundational solution for revealing the\nhidden mechanisms behind changes in complex phenomena, enhancing\ninterpretability, and inspiring further scientific discoveries."}
{"id": "2505.21964", "pdf": "https://arxiv.org/pdf/2505.21964", "abs": "https://arxiv.org/abs/2505.21964", "authors": ["Ziyun Zhang", "Xinyi Liu", "Xiaoyi Zhang", "Jun Wang", "Gang Chen", "Yan Lu"], "title": "UI-Evol: Automatic Knowledge Evolving for Computer Use Agents", "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "External knowledge has played a crucial role in the recent development of\ncomputer use agents. We identify a critical knowledge-execution gap: retrieved\nknowledge often fails to translate into effective real-world task execution.\nOur analysis shows even 90\\% correct knowledge yields only 41\\% execution\nsuccess rate. To bridge this gap, we propose UI-Evol, a plug-and-play module\nfor autonomous GUI knowledge evolution. UI-Evol consists of two stages: a\nRetrace Stage that extracts faithful objective action sequences from actual\nagent-environment interactions, and a Critique Stage that refines existing\nknowledge by comparing these sequences against external references. We conduct\ncomprehensive experiments on the OSWorld benchmark with the state-of-the-art\nAgent S2. Our results demonstrate that UI-Evol not only significantly boosts\ntask performance but also addresses a previously overlooked issue of high\nbehavioral standard deviation in computer use agents, leading to superior\nperformance on computer use tasks and substantially improved agent reliability."}
{"id": "2505.22408", "pdf": "https://arxiv.org/pdf/2505.22408", "abs": "https://arxiv.org/abs/2505.22408", "authors": ["Victor Enescu", "Hichem Sahbi"], "title": "Frugal Incremental Generative Modeling using Variational Autoencoders", "categories": ["cs.CV"], "comment": null, "summary": "Continual or incremental learning holds tremendous potential in deep learning\nwith different challenges including catastrophic forgetting. The advent of\npowerful foundation and generative models has propelled this paradigm even\nfurther, making it one of the most viable solution to train these models.\nHowever, one of the persisting issues lies in the increasing volume of data\nparticularly with replay-based methods. This growth introduces challenges with\nscalability since continuously expanding data becomes increasingly demanding as\nthe number of tasks grows. In this paper, we attenuate this issue by devising a\nnovel replay-free incremental learning model based on Variational Autoencoders\n(VAEs). The main contribution of this work includes (i) a novel incremental\ngenerative modelling, built upon a well designed multi-modal latent space, and\nalso (ii) an orthogonality criterion that mitigates catastrophic forgetting of\nthe learned VAEs. The proposed method considers two variants of these VAEs:\nstatic and dynamic with no (or at most a controlled) growth in the number of\nparameters. Extensive experiments show that our method is (at least) an order\nof magnitude more ``memory-frugal'' compared to the closely related works while\nachieving SOTA accuracy scores."}
{"id": "2505.21880", "pdf": "https://arxiv.org/pdf/2505.21880", "abs": "https://arxiv.org/abs/2505.21880", "authors": ["Yu-Lun Song", "Chung-En Tsern", "Che-Cheng Wu", "Yu-Ming Chang", "Syuan-Bo Huang", "Wei-Chu Chen", "Michael Chia-Liang Lin", "Yu-Ta Lin"], "title": "Incorporating LLMs for Large-Scale Urban Complex Mobility Simulation", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.CY"], "comment": "8 pages, 8 figures. This paper is reviewed and accepted by the CUPUM\n  (Computational Urban Planning and Urban Management) Conference held by\n  University College London (UCL) in 2025", "summary": "This study presents an innovative approach to urban mobility simulation by\nintegrating a Large Language Model (LLM) with Agent-Based Modeling (ABM).\nUnlike traditional rule-based ABM, the proposed framework leverages LLM to\nenhance agent diversity and realism by generating synthetic population\nprofiles, allocating routine and occasional locations, and simulating\npersonalized routes. Using real-world data, the simulation models individual\nbehaviors and large-scale mobility patterns in Taipei City. Key insights, such\nas route heat maps and mode-specific indicators, provide urban planners with\nactionable information for policy-making. Future work focuses on establishing\nrobust validation frameworks to ensure accuracy and reliability in urban\nplanning applications."}
{"id": "2505.21966", "pdf": "https://arxiv.org/pdf/2505.21966", "abs": "https://arxiv.org/abs/2505.21966", "authors": ["Aditya Gunturu", "Ben Pearman", "Keiichi Ihara", "Morteza Faraji", "Bryan Wang", "Rubaiat Habib Kazi", "Ryo Suzuki"], "title": "MapStory: LLM-Powered Text-Driven Map Animation Prototyping with Human-in-the-Loop Editing", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.MM", "H.5.2, H.5.1"], "comment": "16 pages and 15 figures", "summary": "We introduce MapStory, an LLM-powered animation authoring tool that generates\neditable map animation sequences directly from natural language text. Given a\nuser-written script, MapStory leverages an agentic architecture to\nautomatically produce a scene breakdown, which decomposes the script into key\nanimation building blocks such as camera movements, visual highlights, and\nanimated elements. Our system includes a researcher component that accurately\nqueries geospatial information by leveraging an LLM with web search, enabling\nthe automatic extraction of relevant regions, paths, and coordinates while\nallowing users to edit and query for changes or additional information to\nrefine the results. Additionally, users can fine-tune parameters of these\nblocks through an interactive timeline editor. We detail the system's design\nand architecture, informed by formative interviews with professional animators\nand an analysis of 200 existing map animation videos. Our evaluation, which\nincludes expert interviews (N=5) and a usability study (N=12), demonstrates\nthat MapStory enables users to create map animations with ease, facilitates\nfaster iteration, encourages creative exploration, and lowers barriers to\ncreating map-centric stories."}
{"id": "2505.22421", "pdf": "https://arxiv.org/pdf/2505.22421", "abs": "https://arxiv.org/abs/2505.22421", "authors": ["Anthony Chen", "Wenzhao Zheng", "Yida Wang", "Xueyang Zhang", "Kun Zhan", "Peng Jia", "Kurt Keutzer", "Shangbang Zhang"], "title": "GeoDrive: 3D Geometry-Informed Driving World Model with Precise Action Control", "categories": ["cs.CV", "cs.RO"], "comment": "code will be released at https://github.com/antonioo-c/GeoDrive", "summary": "Recent advancements in world models have revolutionized dynamic environment\nsimulation, allowing systems to foresee future states and assess potential\nactions. In autonomous driving, these capabilities help vehicles anticipate the\nbehavior of other road users, perform risk-aware planning, accelerate training\nin simulation, and adapt to novel scenarios, thereby enhancing safety and\nreliability. Current approaches exhibit deficiencies in maintaining robust 3D\ngeometric consistency or accumulating artifacts during occlusion handling, both\ncritical for reliable safety assessment in autonomous navigation tasks. To\naddress this, we introduce GeoDrive, which explicitly integrates robust 3D\ngeometry conditions into driving world models to enhance spatial understanding\nand action controllability. Specifically, we first extract a 3D representation\nfrom the input frame and then obtain its 2D rendering based on the\nuser-specified ego-car trajectory. To enable dynamic modeling, we propose a\ndynamic editing module during training to enhance the renderings by editing the\npositions of the vehicles. Extensive experiments demonstrate that our method\nsignificantly outperforms existing models in both action accuracy and 3D\nspatial awareness, leading to more realistic, adaptable, and reliable scene\nmodeling for safer autonomous driving. Additionally, our model can generalize\nto novel trajectories and offers interactive scene editing capabilities, such\nas object editing and object trajectory control."}
{"id": "2505.21893", "pdf": "https://arxiv.org/pdf/2505.21893", "abs": "https://arxiv.org/abs/2505.21893", "authors": ["Xiaomeng Yang", "Zhiyu Tan", "Junyan Wang", "Zhijian Zhou", "Hao Li"], "title": "SDPO: Importance-Sampled Direct Preference Optimization for Stable Diffusion Training", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Preference learning has become a central technique for aligning generative\nmodels with human expectations. Recently, it has been extended to diffusion\nmodels through methods like Direct Preference Optimization (DPO). However,\nexisting approaches such as Diffusion-DPO suffer from two key challenges:\ntimestep-dependent instability, caused by a mismatch between the reverse and\nforward diffusion processes and by high gradient variance in early noisy\ntimesteps, and off-policy bias arising from the mismatch between optimization\nand data collection policies. We begin by analyzing the reverse diffusion\ntrajectory and observe that instability primarily occurs at early timesteps\nwith low importance weights. To address these issues, we first propose\nDPO-C\\&M, a practical strategy that improves stability by clipping and masking\nuninformative timesteps while partially mitigating off-policy bias. Building on\nthis, we introduce SDPO (Importance-Sampled Direct Preference Optimization), a\nprincipled framework that incorporates importance sampling into the objective\nto fully correct for off-policy bias and emphasize informative updates during\nthe diffusion process. Experiments on CogVideoX-2B, CogVideoX-5B, and\nWan2.1-1.3B demonstrate that both methods outperform standard Diffusion-DPO,\nwith SDPO achieving superior VBench scores, human preference alignment, and\ntraining robustness. These results highlight the importance of timestep-aware,\ndistribution-corrected optimization in diffusion-based preference learning."}
{"id": "2505.21981", "pdf": "https://arxiv.org/pdf/2505.21981", "abs": "https://arxiv.org/abs/2505.21981", "authors": ["Weiyu Liu", "Neil Nie", "Ruohan Zhang", "Jiayuan Mao", "Jiajun Wu"], "title": "Learning Compositional Behaviors from Demonstration and Language", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "comment": "Presented at CoRL 2024 and as an Oral Presentation at the 2024 CoRL\n  LEAP Workshop. The first two authors contributed equally. The last two\n  authors jointly advised the project. For videos and additional results,\n  visit: https://blade-bot.github.io/", "summary": "We introduce Behavior from Language and Demonstration (BLADE), a framework\nfor long-horizon robotic manipulation by integrating imitation learning and\nmodel-based planning. BLADE leverages language-annotated demonstrations,\nextracts abstract action knowledge from large language models (LLMs), and\nconstructs a library of structured, high-level action representations. These\nrepresentations include preconditions and effects grounded in visual perception\nfor each high-level action, along with corresponding controllers implemented as\nneural network-based policies. BLADE can recover such structured\nrepresentations automatically, without manually labeled states or symbolic\ndefinitions. BLADE shows significant capabilities in generalizing to novel\nsituations, including novel initial states, external state perturbations, and\nnovel goals. We validate the effectiveness of our approach both in simulation\nand on real robots with a diverse set of objects with articulated parts,\npartial observability, and geometric constraints."}
{"id": "2505.22427", "pdf": "https://arxiv.org/pdf/2505.22427", "abs": "https://arxiv.org/abs/2505.22427", "authors": ["Van-Tin Luu", "Yon-Lin Cai", "Vu-Hoang Tran", "Wei-Chen Chiu", "Yi-Ting Chen", "Ching-Chun Huang"], "title": "RC-AutoCalib: An End-to-End Radar-Camera Automatic Calibration Network", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "This paper presents a groundbreaking approach - the first online automatic\ngeometric calibration method for radar and camera systems. Given the\nsignificant data sparsity and measurement uncertainty in radar height data,\nachieving automatic calibration during system operation has long been a\nchallenge. To address the sparsity issue, we propose a Dual-Perspective\nrepresentation that gathers features from both frontal and bird's-eye views.\nThe frontal view contains rich but sensitive height information, whereas the\nbird's-eye view provides robust features against height uncertainty. We thereby\npropose a novel Selective Fusion Mechanism to identify and fuse reliable\nfeatures from both perspectives, reducing the effect of height uncertainty.\nMoreover, for each view, we incorporate a Multi-Modal Cross-Attention Mechanism\nto explicitly find location correspondences through cross-modal matching.\nDuring the training phase, we also design a Noise-Resistant Matcher to provide\nbetter supervision and enhance the robustness of the matching mechanism against\nsparsity and height uncertainty. Our experimental results, tested on the\nnuScenes dataset, demonstrate that our method significantly outperforms\nprevious radar-camera auto-calibration methods, as well as existing\nstate-of-the-art LiDAR-camera calibration techniques, establishing a new\nbenchmark for future research. The code is available at\nhttps://github.com/nycu-acm/RC-AutoCalib."}
{"id": "2505.21895", "pdf": "https://arxiv.org/pdf/2505.21895", "abs": "https://arxiv.org/abs/2505.21895", "authors": ["Cameron Gordon", "Yiping Ji", "Hemanth Saratchandran", "Paul Albert", "Simon Lucey"], "title": "Compressing Sine-Activated Low-Rank Adapters through Post-Training Quantization", "categories": ["cs.LG", "cs.AI"], "comment": "23 pages, 9 figures", "summary": "Low-Rank Adaptation (LoRA) has become a standard approach for\nparameter-efficient fine-tuning, offering substantial reductions in trainable\nparameters by modeling updates as the product of two low-rank matrices. While\neffective, the low-rank constraint inherently limits representational capacity,\noften resulting in reduced performance compared to full-rank fine-tuning.\nRecent work by Ji et al. (2025) has addressed this limitation by applying a\nfixed-frequency sinusoidal transformation to low-rank adapters, increasing\ntheir stable rank without introducing additional parameters. This raises a\ncrucial question: can the same sine-activated technique be successfully applied\nwithin the context of Post-Training Quantization to retain benefits even after\nmodel compression? In this paper, we investigate this question by extending the\nsinusoidal transformation framework to quantized LoRA adapters. We develop a\ntheoretical analysis showing that the stable rank of a quantized adapter is\ntightly linked to that of its full-precision counterpart, motivating the use of\nsuch rank-enhancing functions even under quantization. Our results demonstrate\nthat the expressivity gains from a sinusoidal non-linearity persist after\nquantization, yielding highly compressed adapters with negligible loss in\nperformance. We validate our approach across a range of fine-tuning tasks for\nlanguage, vision and text-to-image generation achieving significant memory\nsavings while maintaining competitive accuracy."}
{"id": "2505.22088", "pdf": "https://arxiv.org/pdf/2505.22088", "abs": "https://arxiv.org/abs/2505.22088", "authors": ["Sam O'Connor Russell", "Naomi Harte"], "title": "Visual Cues Support Robust Turn-taking Prediction in Noise", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "5 pages", "summary": "Accurate predictive turn-taking models (PTTMs) are essential for naturalistic\nhuman-robot interaction. However, little is known about their performance in\nnoise. This study therefore explores PTTM performance in types of noise likely\nto be encountered once deployed. Our analyses reveal PTTMs are highly sensitive\nto noise. Hold/shift accuracy drops from 84% in clean speech to just 52% in 10\ndB music noise. Training with noisy data enables a multimodal PTTM, which\nincludes visual features to better exploit visual cues, with 72% accuracy in 10\ndB music noise. The multimodal PTTM outperforms the audio-only PTTM across all\nnoise types and SNRs, highlighting its ability to exploit visual cues; however,\nthis does not always generalise to new types of noise. Analysis also reveals\nthat successful training relies on accurate transcription, limiting the use of\nASR-derived transcriptions to clean conditions. We make code publicly available\nfor future research."}
{"id": "2505.22429", "pdf": "https://arxiv.org/pdf/2505.22429", "abs": "https://arxiv.org/abs/2505.22429", "authors": ["Rong Li", "Shijie Li", "Lingdong Kong", "Xulei Yang", "Junwei Liang"], "title": "Zero-Shot 3D Visual Grounding from Vision-Language Models", "categories": ["cs.CV", "cs.RO"], "comment": "3D-LLM/VLA @ CVPR 2025; Project Page at https://seeground.github.io/", "summary": "3D Visual Grounding (3DVG) seeks to locate target objects in 3D scenes using\nnatural language descriptions, enabling downstream applications such as\naugmented reality and robotics. Existing approaches typically rely on labeled\n3D data and predefined categories, limiting scalability to open-world settings.\nWe present SeeGround, a zero-shot 3DVG framework that leverages 2D\nVision-Language Models (VLMs) to bypass the need for 3D-specific training. To\nbridge the modality gap, we introduce a hybrid input format that pairs\nquery-aligned rendered views with spatially enriched textual descriptions. Our\nframework incorporates two core components: a Perspective Adaptation Module\nthat dynamically selects optimal viewpoints based on the query, and a Fusion\nAlignment Module that integrates visual and spatial signals to enhance\nlocalization precision. Extensive evaluations on ScanRefer and Nr3D confirm\nthat SeeGround achieves substantial improvements over existing zero-shot\nbaselines -- outperforming them by 7.7% and 7.1%, respectively -- and even\nrivals fully supervised alternatives, demonstrating strong generalization under\nchallenging conditions."}
{"id": "2505.21898", "pdf": "https://arxiv.org/pdf/2505.21898", "abs": "https://arxiv.org/abs/2505.21898", "authors": ["Rennai Qiu", "Chen Qian", "Ran Li", "Yufan Dang", "Weize Chen", "Cheng Yang", "Yingli Zhang", "Ye Tian", "Xuantang Xiong", "Lei Han", "Zhiyuan Liu", "Maosong Sun"], "title": "Co-Saving: Resource Aware Multi-Agent Collaboration for Software Development", "categories": ["cs.CL", "cs.AI", "cs.MA", "cs.SE"], "comment": "Work in Progress", "summary": "Recent advancements in Large Language Models (LLMs) and autonomous agents\nhave demonstrated remarkable capabilities across various domains. However,\nstandalone agents frequently encounter limitations when handling complex tasks\nthat demand extensive interactions and substantial computational resources.\nAlthough Multi-Agent Systems (MAS) alleviate some of these limitations through\ncollaborative mechanisms like task decomposition, iterative communication, and\nrole specialization, they typically remain resource-unaware, incurring\nsignificant inefficiencies due to high token consumption and excessive\nexecution time. To address these limitations, we propose a resource-aware\nmulti-agent system -- Co-Saving (meaning that multiple agents collaboratively\nengage in resource-saving activities), which leverages experiential knowledge\nto enhance operational efficiency and solution quality. Our key innovation is\nthe introduction of \"shortcuts\" -- instructional transitions learned from\nhistorically successful trajectories -- which allows to bypass redundant\nreasoning agents and expedite the collective problem-solving process.\nExperiments for software development tasks demonstrate significant advantages\nover existing methods. Specifically, compared to the state-of-the-art MAS\nChatDev, our method achieves an average reduction of 50.85% in token usage, and\nimproves the overall code quality by 10.06%."}
{"id": "2505.22146", "pdf": "https://arxiv.org/pdf/2505.22146", "abs": "https://arxiv.org/abs/2505.22146", "authors": ["Guangfu Hao", "Haojie Wen", "Liangxuna Guo", "Yang Chen", "Yanchao Bi", "Shan Yu"], "title": "Flexible Tool Selection through Low-dimensional Attribute Alignment of Vision and Language", "categories": ["cs.CV", "cs.AI", "cs.CL", "q-bio.NC"], "comment": null, "summary": "Flexible tool selection reflects a complex cognitive ability that\ndistinguishes humans from other species, yet computational models that capture\nthis ability remain underdeveloped. We developed a framework using\nlow-dimensional attribute representations to bridge visual tool perception and\nlinguistic task understanding. We constructed a comprehensive dataset (ToolNet)\ncontaining 115 common tools labeled with 13 carefully designed attributes\nspanning physical, functional, and psychological properties, paired with\nnatural language scenarios describing tool usage. Visual encoders (ResNet or\nViT) extract attributes from tool images while fine-tuned language models\n(GPT-2, LLaMA, DeepSeek) derive required attributes from task descriptions. Our\napproach achieves 74% accuracy in tool selection tasks-significantly\noutperforming direct tool matching (20%) and smaller multimodal models\n(21%-58%), while approaching performance of much larger models like GPT-4o\n(73%) with substantially fewer parameters. Ablation studies revealed that\nmanipulation-related attributes (graspability, hand-relatedness, elongation)\nconsistently prove most critical across modalities. This work provides a\nparameter-efficient, interpretable solution that mimics human-like tool\ncognition, advancing both cognitive science understanding and practical\napplications in tool selection tasks."}
{"id": "2505.22434", "pdf": "https://arxiv.org/pdf/2505.22434", "abs": "https://arxiv.org/abs/2505.22434", "authors": ["Zobia Batool", "Huseyin Ozkan", "Erchan Aptoula"], "title": "Distance Transform Guided Mixup for Alzheimer's Detection", "categories": ["cs.CV"], "comment": null, "summary": "Alzheimer's detection efforts aim to develop accurate models for early\ndisease diagnosis. Significant advances have been achieved with convolutional\nneural networks and vision transformer based approaches. However, medical\ndatasets suffer heavily from class imbalance, variations in imaging protocols,\nand limited dataset diversity, which hinder model generalization. To overcome\nthese challenges, this study focuses on single-domain generalization by\nextending the well-known mixup method. The key idea is to compute the distance\ntransform of MRI scans, separate them spatially into multiple layers and then\ncombine layers stemming from distinct samples to produce augmented images. The\nproposed approach generates diverse data while preserving the brain's\nstructure. Experimental results show generalization performance improvement\nacross both ADNI and AIBL datasets."}
{"id": "2505.21904", "pdf": "https://arxiv.org/pdf/2505.21904", "abs": "https://arxiv.org/abs/2505.21904", "authors": ["Pardis Taghavi", "Tian Liu", "Renjie Li", "Reza Langari", "Zhengzhong Tu"], "title": "CAST: Contrastive Adaptation and Distillation for Semi-Supervised Instance Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Instance segmentation demands costly per-pixel annotations and large models.\nWe introduce CAST, a semi-supervised knowledge distillation (SSKD) framework\nthat compresses pretrained vision foundation models (VFM) into compact experts\nusing limited labeled and abundant unlabeled data. CAST unfolds in three\nstages: (1) domain adaptation of the VFM teacher(s) via self-training with\ncontrastive pixel calibration, (2) distillation into a compact student via a\nunified multi-objective loss that couples standard supervision and\npseudo-labels with our instance-aware pixel-wise contrastive term, and (3)\nfine-tuning on labeled data to remove residual pseudo-label bias. Central to\nCAST is an \\emph{instance-aware pixel-wise contrastive loss} that fuses mask\nand class scores to mine informative negatives and enforce clear inter-instance\nmargins. By maintaining this contrastive signal across both adaptation and\ndistillation, we align teacher and student embeddings and fully leverage\nunlabeled images. On Cityscapes and ADE20K, our ~11X smaller student surpasses\nits adapted VFM teacher(s) by +3.4 AP (33.9 vs. 30.5) and +1.5 AP (16.7 vs.\n15.2) and outperforms state-of-the-art semi-supervised approaches."}
{"id": "2505.22150", "pdf": "https://arxiv.org/pdf/2505.22150", "abs": "https://arxiv.org/abs/2505.22150", "authors": ["Runze Xia", "Shuo Feng", "Renzhi Wang", "Congchi Yin", "Xuyun Wen", "Piji Li"], "title": "Improving Brain-to-Image Reconstruction via Fine-Grained Text Bridging", "categories": ["cs.CV", "cs.CL"], "comment": "CogSci2025", "summary": "Brain-to-Image reconstruction aims to recover visual stimuli perceived by\nhumans from brain activity. However, the reconstructed visual stimuli often\nmissing details and semantic inconsistencies, which may be attributed to\ninsufficient semantic information. To address this issue, we propose an\napproach named Fine-grained Brain-to-Image reconstruction (FgB2I), which\nemploys fine-grained text as bridge to improve image reconstruction. FgB2I\ncomprises three key stages: detail enhancement, decoding fine-grained text\ndescriptions, and text-bridged brain-to-image reconstruction. In the\ndetail-enhancement stage, we leverage large vision-language models to generate\nfine-grained captions for visual stimuli and experimentally validate its\nimportance. We propose three reward metrics (object accuracy, text-image\nsemantic similarity, and image-image semantic similarity) to guide the language\nmodel in decoding fine-grained text descriptions from fMRI signals. The\nfine-grained text descriptions can be integrated into existing reconstruction\nmethods to achieve fine-grained Brain-to-Image reconstruction."}
{"id": "2505.22441", "pdf": "https://arxiv.org/pdf/2505.22441", "abs": "https://arxiv.org/abs/2505.22441", "authors": ["Chaitanya Amballa", "Sattwik Basu", "Yu-Lin Wei", "Zhijian Yang", "Mehmet Ergezer", "Romit Roy Choudhury"], "title": "Can NeRFs See without Cameras?", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Neural Radiance Fields (NeRFs) have been remarkably successful at\nsynthesizing novel views of 3D scenes by optimizing a volumetric scene\nfunction. This scene function models how optical rays bring color information\nfrom a 3D object to the camera pixels. Radio frequency (RF) or audio signals\ncan also be viewed as a vehicle for delivering information about the\nenvironment to a sensor. However, unlike camera pixels, an RF/audio sensor\nreceives a mixture of signals that contain many environmental reflections (also\ncalled \"multipath\"). Is it still possible to infer the environment using such\nmultipath signals? We show that with redesign, NeRFs can be taught to learn\nfrom multipath signals, and thereby \"see\" the environment. As a grounding\napplication, we aim to infer the indoor floorplan of a home from sparse WiFi\nmeasurements made at multiple locations inside the home. Although a difficult\ninverse problem, our implicitly learnt floorplans look promising, and enables\nforward applications, such as indoor signal prediction and basic ray tracing."}
{"id": "2505.21906", "pdf": "https://arxiv.org/pdf/2505.21906", "abs": "https://arxiv.org/abs/2505.21906", "authors": ["Zhongyi Zhou", "Yichen Zhu", "Junjie Wen", "Chaomin Shen", "Yi Xu"], "title": "Vision-Language-Action Model with Open-World Embodied Reasoning from Pretrained Knowledge", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Project page: https://chatvla-2.github.io/", "summary": "Vision-language-action (VLA) models have emerged as the next generation of\nmodels in robotics. However, despite leveraging powerful pre-trained\nVision-Language Models (VLMs), existing end-to-end VLA systems often lose key\ncapabilities during fine-tuning as the model adapts to specific robotic tasks.\nWe argue that a generalizable VLA model should retain and expand upon the VLM's\ncore competencies: 1) Open-world embodied reasoning - the VLA should inherit\nthe knowledge from VLM, i.e., recognize anything that the VLM can recognize,\ncapable of solving math problems, possessing visual-spatial intelligence, 2)\nReasoning following - effectively translating the open-world reasoning into\nactionable steps for the robot. In this work, we introduce ChatVLA-2, a novel\nmixture-of-expert VLA model coupled with a specialized three-stage training\npipeline designed to preserve the VLM's original strengths while enabling\nactionable reasoning. To validate our approach, we design a math-matching task\nwherein a robot interprets math problems written on a whiteboard and picks\ncorresponding number cards from a table to solve equations. Remarkably, our\nmethod exhibits exceptional mathematical reasoning and OCR capabilities,\ndespite these abilities not being explicitly trained within the VLA.\nFurthermore, we demonstrate that the VLA possesses strong spatial reasoning\nskills, enabling it to interpret novel directional instructions involving\npreviously unseen objects. Overall, our method showcases reasoning and\ncomprehension abilities that significantly surpass state-of-the-art imitation\nlearning methods such as OpenVLA, DexVLA, and pi-zero. This work represents a\nsubstantial advancement toward developing truly generalizable robotic\nfoundation models endowed with robust reasoning capacities."}
{"id": "2505.22203", "pdf": "https://arxiv.org/pdf/2505.22203", "abs": "https://arxiv.org/abs/2505.22203", "authors": ["Yuzhen Huang", "Weihao Zeng", "Xingshan Zeng", "Qi Zhu", "Junxian He"], "title": "Pitfalls of Rule- and Model-based Verifiers -- A Case Study on Mathematical Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Trustworthy verifiers are essential for the success of reinforcement learning\nwith verifiable reward (RLVR), which is the core methodology behind various\nlarge reasoning models such as DeepSeek-R1. In complex domains like\nmathematical reasoning, rule-based verifiers have been widely adopted in\nprevious works to train strong reasoning models. However, the reliability of\nthese verifiers and their impact on the RL training process remain poorly\nunderstood. In this work, we take mathematical reasoning as a case study and\nconduct a comprehensive analysis of various verifiers in both static evaluation\nand RL training scenarios. First, we find that current open-source rule-based\nverifiers often fail to recognize equivalent answers presented in different\nformats across multiple commonly used mathematical datasets, resulting in\nnon-negligible false negative rates. This limitation adversely affects RL\ntraining performance and becomes more pronounced as the policy model gets\nstronger. Subsequently, we investigate model-based verifiers as a potential\nsolution to address these limitations. While the static evaluation shows that\nmodel-based verifiers achieve significantly higher verification accuracy,\nfurther analysis and RL training results imply that they are highly susceptible\nto hacking, where they misclassify certain patterns in responses as correct\n(i.e., false positives). This vulnerability is exploited during policy model\noptimization, leading to artificially inflated rewards. Our findings underscore\nthe unique risks inherent to both rule-based and model-based verifiers, aiming\nto offer valuable insights to develop more robust reward systems in\nreinforcement learning."}
{"id": "2505.22444", "pdf": "https://arxiv.org/pdf/2505.22444", "abs": "https://arxiv.org/abs/2505.22444", "authors": ["Liyao Tang", "Zhe Chen", "Dacheng Tao"], "title": "On Geometry-Enhanced Parameter-Efficient Fine-Tuning for 3D Scene Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "The emergence of large-scale pre-trained point cloud models has significantly\nadvanced 3D scene understanding, but adapting these models to specific\ndownstream tasks typically demands full fine-tuning, incurring high\ncomputational and storage costs. Parameter-efficient fine-tuning (PEFT)\ntechniques, successful in natural language processing and 2D vision tasks,\nwould underperform when naively applied to 3D point cloud models due to\nsignificant geometric and spatial distribution shifts. Existing PEFT methods\ncommonly treat points as orderless tokens, neglecting important local spatial\nstructures and global geometric contexts in 3D modeling. To bridge this gap, we\nintroduce the Geometric Encoding Mixer (GEM), a novel geometry-aware PEFT\nmodule specifically designed for 3D point cloud transformers. GEM explicitly\nintegrates fine-grained local positional encodings with a lightweight latent\nattention mechanism to capture comprehensive global context, thereby\neffectively addressing the spatial and geometric distribution mismatch.\nExtensive experiments demonstrate that GEM achieves performance comparable to\nor sometimes even exceeding full fine-tuning, while only updating 1.6% of the\nmodel's parameters, fewer than other PEFT methods. With significantly reduced\ntraining time and memory requirements, our approach thus sets a new benchmark\nfor efficient, scalable, and geometry-aware fine-tuning of large-scale 3D point\ncloud models. Code will be released."}
{"id": "2505.21908", "pdf": "https://arxiv.org/pdf/2505.21908", "abs": "https://arxiv.org/abs/2505.21908", "authors": ["Hanyin Wang", "Zhenbang Wu", "Gururaj Kolar", "Hariprasad Korsapati", "Brian Bartlett", "Bryan Hull", "Jimeng Sun"], "title": "Reinforcement Learning for Out-of-Distribution Reasoning in LLMs: An Empirical Study on Diagnosis-Related Group Coding", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Diagnosis-Related Group (DRG) codes are essential for hospital reimbursement\nand operations but require labor-intensive assignment. Large Language Models\n(LLMs) struggle with DRG coding due to the out-of-distribution (OOD) nature of\nthe task: pretraining corpora rarely contain private clinical or billing data.\nWe introduce DRG-Sapphire, which uses large-scale reinforcement learning (RL)\nfor automated DRG coding from clinical notes. Built on Qwen2.5-7B and trained\nwith Group Relative Policy Optimization (GRPO) using rule-based rewards,\nDRG-Sapphire introduces a series of RL enhancements to address domain-specific\nchallenges not seen in previous mathematical tasks. Our model achieves\nstate-of-the-art accuracy on the MIMIC-IV benchmark and generates\nphysician-validated reasoning for DRG assignments, significantly enhancing\nexplainability. Our study further sheds light on broader challenges of applying\nRL to knowledge-intensive, OOD tasks. We observe that RL performance scales\napproximately linearly with the logarithm of the number of supervised\nfine-tuning (SFT) examples, suggesting that RL effectiveness is fundamentally\nconstrained by the domain knowledge encoded in the base model. For OOD tasks\nlike DRG coding, strong RL performance requires sufficient knowledge infusion\nprior to RL. Consequently, scaling SFT may be more effective and\ncomputationally efficient than scaling RL alone for such tasks."}
{"id": "2505.22222", "pdf": "https://arxiv.org/pdf/2505.22222", "abs": "https://arxiv.org/abs/2505.22222", "authors": ["Yunsoo Kim", "Jinge Wu", "Su-Hwan Kim", "Pardeep Vasudev", "Jiashu Shen", "Honghan Wu"], "title": "Look & Mark: Leveraging Radiologist Eye Fixations and Bounding boxes in Multimodal Large Language Models for Chest X-ray Report Generation", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Recent advancements in multimodal Large Language Models (LLMs) have\nsignificantly enhanced the automation of medical image analysis, particularly\nin generating radiology reports from chest X-rays (CXR). However, these models\nstill suffer from hallucinations and clinically significant errors, limiting\ntheir reliability in real-world applications. In this study, we propose Look &\nMark (L&M), a novel grounding fixation strategy that integrates radiologist eye\nfixations (Look) and bounding box annotations (Mark) into the LLM prompting\nframework. Unlike conventional fine-tuning, L&M leverages in-context learning\nto achieve substantial performance gains without retraining. When evaluated\nacross multiple domain-specific and general-purpose models, L&M demonstrates\nsignificant gains, including a 1.2% improvement in overall metrics (A.AVG) for\nCXR-LLaVA compared to baseline prompting and a remarkable 9.2% boost for\nLLaVA-Med. General-purpose models also benefit from L&M combined with\nin-context learning, with LLaVA-OV achieving an 87.3% clinical average\nperformance (C.AVG)-the highest among all models, even surpassing those\nexplicitly trained for CXR report generation. Expert evaluations further\nconfirm that L&M reduces clinically significant errors (by 0.43 average errors\nper report), such as false predictions and omissions, enhancing both accuracy\nand reliability. These findings highlight L&M's potential as a scalable and\nefficient solution for AI-assisted radiology, paving the way for improved\ndiagnostic workflows in low-resource clinical settings."}
{"id": "2505.22445", "pdf": "https://arxiv.org/pdf/2505.22445", "abs": "https://arxiv.org/abs/2505.22445", "authors": ["Puhua Jiang", "Zhangquan Chen", "Mingze Sun", "Ruqi Huang"], "title": "NFR: Neural Feature-Guided Non-Rigid Shape Registration", "categories": ["cs.CV", "cs.AI", "I.4.m; I.2.6"], "comment": "20 pages, 9 figures. arXiv admin note: substantial text overlap with\n  arXiv:2311.04494", "summary": "In this paper, we propose a novel learning-based framework for 3D shape\nregistration, which overcomes the challenges of significant non-rigid\ndeformation and partiality undergoing among input shapes, and, remarkably,\nrequires no correspondence annotation during training. Our key insight is to\nincorporate neural features learned by deep learning-based shape matching\nnetworks into an iterative, geometric shape registration pipeline. The\nadvantage of our approach is two-fold -- On one hand, neural features provide\nmore accurate and semantically meaningful correspondence estimation than\nspatial features (e.g., coordinates), which is critical in the presence of\nlarge non-rigid deformations; On the other hand, the correspondences are\ndynamically updated according to the intermediate registrations and filtered by\nconsistency prior, which prominently robustify the overall pipeline. Empirical\nresults show that, with as few as dozens of training shapes of limited\nvariability, our pipeline achieves state-of-the-art results on several\nbenchmarks of non-rigid point cloud matching and partial shape matching across\nvarying settings, but also delivers high-quality correspondences between unseen\nchallenging shape pairs that undergo both significant extrinsic and intrinsic\ndeformations, in which case neither traditional registration methods nor\nintrinsic methods work."}
{"id": "2505.21918", "pdf": "https://arxiv.org/pdf/2505.21918", "abs": "https://arxiv.org/abs/2505.21918", "authors": ["Haruki Kai", "Tsuyoshi Okita"], "title": "Self-supervised Learning Method Using Transformer for Multi-dimensional Sensor Data Processing", "categories": ["cs.LG", "cs.AI"], "comment": "25 pages, 4 figures", "summary": "We developed a deep learning algorithm for human activity recognition using\nsensor signals as input. In this study, we built a pretrained language model\nbased on the Transformer architecture, which is widely used in natural language\nprocessing. By leveraging this pretrained model, we aimed to improve\nperformance on the downstream task of human activity recognition. While this\ntask can be addressed using a vanilla Transformer, we propose an enhanced\nn-dimensional numerical processing Transformer that incorporates three key\nfeatures: embedding n-dimensional numerical data through a linear layer,\nbinning-based pre-processing, and a linear transformation in the output layer.\nWe evaluated the effectiveness of our proposed model across five different\ndatasets. Compared to the vanilla Transformer, our model demonstrated 10%-15%\nimprovements in accuracy."}
{"id": "2505.22231", "pdf": "https://arxiv.org/pdf/2505.22231", "abs": "https://arxiv.org/abs/2505.22231", "authors": ["Stefan Bleeck"], "title": "Advancing Hearing Assessment: An ASR-Based Frequency-Specific Speech Test for Diagnosing Presbycusis", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "Traditional audiometry often fails to fully characterize the functional\nimpact of hearing loss on speech understanding, particularly supra-threshold\ndeficits and frequency-specific perception challenges in conditions like\npresbycusis. This paper presents the development and simulated evaluation of a\nnovel Automatic Speech Recognition (ASR)-based frequency-specific speech test\ndesigned to provide granular diagnostic insights. Our approach leverages ASR to\nsimulate the perceptual effects of moderate sloping hearing loss by processing\nspeech stimuli under controlled acoustic degradation and subsequently analyzing\nphoneme-level confusion patterns. Key findings indicate that simulated hearing\nloss introduces specific phoneme confusions, predominantly affecting\nhigh-frequency consonants (e.g., alveolar/palatal to labiodental substitutions)\nand leading to significant phoneme deletions, consistent with the acoustic cues\ndegraded in presbycusis. A test battery curated from these ASR-derived\nconfusions demonstrated diagnostic value, effectively differentiating between\nsimulated normal-hearing and hearing-impaired listeners in a comprehensive\nsimulation. This ASR-driven methodology offers a promising avenue for\ndeveloping objective, granular, and frequency-specific hearing assessment tools\nthat complement traditional audiometry. Future work will focus on validating\nthese findings with human participants and exploring the integration of\nadvanced AI models for enhanced diagnostic precision."}
{"id": "2505.22457", "pdf": "https://arxiv.org/pdf/2505.22457", "abs": "https://arxiv.org/abs/2505.22457", "authors": ["Haonan Wang", "Hongfu Liu", "Xiangyan Liu", "Chao Du", "Kenji Kawaguchi", "Ye Wang", "Tianyu Pang"], "title": "Fostering Video Reasoning via Next-Event Prediction", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Next-token prediction serves as the foundational learning task enabling\nreasoning in LLMs. But what should the learning task be when aiming to equip\nMLLMs with temporal reasoning capabilities over video inputs? Existing tasks\nsuch as video question answering often rely on annotations from humans or much\nstronger MLLMs, while video captioning tends to entangle temporal reasoning\nwith spatial information. To address this gap, we propose next-event prediction\n(NEP), a learning task that harnesses future video segments as a rich,\nself-supervised signal to foster temporal reasoning. We segment each video into\npast and future frames: the MLLM takes the past frames as input and predicts a\nsummary of events derived from the future frames, thereby encouraging the model\nto reason temporally in order to complete the task. To support this task, we\ncurate V1-33K, a dataset comprising 33,000 automatically extracted video\nsegments spanning diverse real-world scenarios. We further explore a range of\nvideo instruction-tuning strategies to study their effects on temporal\nreasoning. To evaluate progress, we introduce FutureBench to assess coherence\nin predicting unseen future events. Experiments validate that NEP offers a\nscalable and effective training paradigm for fostering temporal reasoning in\nMLLMs."}
{"id": "2505.21919", "pdf": "https://arxiv.org/pdf/2505.21919", "abs": "https://arxiv.org/abs/2505.21919", "authors": ["Yue Zhu", "Hao Yu", "Chen Wang", "Zhuoran Liu", "Eun Kyung Lee"], "title": "Towards Efficient Key-Value Cache Management for Prefix Prefilling in LLM Inference", "categories": ["cs.ET", "cs.AI", "cs.DC"], "comment": "This paper has been accepted at IEEE Cloud 2025 as WIP paper. The\n  final version will appear in IEEE Xplore", "summary": "The increasing adoption of large language models (LLMs) with extended context\nwindows necessitates efficient Key-Value Cache (KVC) management to optimize\ninference performance. Inference workloads like Retrieval-Augmented Generation\n(RAG) and agents exhibit high cache reusability, making efficient caching\ncritical to reducing redundancy and improving speed. We analyze real-world KVC\naccess patterns using publicly available traces and evaluate commercial\nkey-value stores like Redis and state-of-the-art RDMA-based systems (CHIME [1]\nand Sherman [2]) for KVC metadata management. Our work demonstrates the lack of\ntailored storage solution for KVC prefilling, underscores the need for an\nefficient distributed caching system with optimized metadata management for LLM\nworkloads, and provides insights into designing improved KVC management systems\nfor scalable, low-latency inference."}
{"id": "2505.22251", "pdf": "https://arxiv.org/pdf/2505.22251", "abs": "https://arxiv.org/abs/2505.22251", "authors": ["Yuan Tseng", "Titouan Parcollet", "Rogier van Dalen", "Shucong Zhang", "Sourav Bhattacharya"], "title": "Evaluation of LLMs in Speech is Often Flawed: Test Set Contamination in Large Language Models for Speech Recognition", "categories": ["eess.AS", "cs.CL"], "comment": null, "summary": "Recent work suggests that large language models (LLMs) can improve\nperformance of speech tasks compared to existing systems. To support their\nclaims, results on LibriSpeech and Common Voice are often quoted. However, this\nwork finds that a substantial amount of the LibriSpeech and Common Voice\nevaluation sets appear in public LLM pretraining corpora. This calls into\nquestion the reliability of findings drawn from these two datasets. To measure\nthe impact of contamination, LLMs trained with or without contamination are\ncompared, showing that a contaminated LLM is more likely to generate test\nsentences it has seen during training. Speech recognisers using contaminated\nLLMs shows only subtle differences in error rates, but assigns significantly\nhigher probabilities to transcriptions seen during training. Results show that\nLLM outputs can be biased by tiny amounts of data contamination, highlighting\nthe importance of evaluating LLM-based speech systems with held-out data."}
{"id": "2505.22458", "pdf": "https://arxiv.org/pdf/2505.22458", "abs": "https://arxiv.org/abs/2505.22458", "authors": ["Seun-An Choe", "Keon-Hee Park", "Jinwoo Choi", "Gyeong-Moon Park"], "title": "Universal Domain Adaptation for Semantic Segmentation", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Unsupervised domain adaptation for semantic segmentation (UDA-SS) aims to\ntransfer knowledge from labeled source data to unlabeled target data. However,\ntraditional UDA-SS methods assume that category settings between source and\ntarget domains are known, which is unrealistic in real-world scenarios. This\nleads to performance degradation if private classes exist. To address this\nlimitation, we propose Universal Domain Adaptation for Semantic Segmentation\n(UniDA-SS), achieving robust adaptation even without prior knowledge of\ncategory settings. We define the problem in the UniDA-SS scenario as low\nconfidence scores of common classes in the target domain, which leads to\nconfusion with private classes. To solve this problem, we propose UniMAP:\nUniDA-SS with Image Matching and Prototype-based Distinction, a novel framework\ncomposed of two key components. First, Domain-Specific Prototype-based\nDistinction (DSPD) divides each class into two domain-specific prototypes,\nenabling finer separation of domain-specific features and enhancing the\nidentification of common classes across domains. Second, Target-based Image\nMatching (TIM) selects a source image containing the most common-class pixels\nbased on the target pseudo-label and pairs it in a batch to promote effective\nlearning of common classes. We also introduce a new UniDA-SS benchmark and\ndemonstrate through various experiments that UniMAP significantly outperforms\nbaselines. The code is available at\n\\href{https://github.com/KU-VGI/UniMAP}{this https URL}."}
{"id": "2505.21923", "pdf": "https://arxiv.org/pdf/2505.21923", "abs": "https://arxiv.org/abs/2505.21923", "authors": ["Asal Mehradfar", "Xuzhe Zhao", "Yilun Huang", "Emir Ceyani", "Yankai Yang", "Shihao Han", "Hamidreza Aghasi", "Salman Avestimehr"], "title": "FALCON: An ML Framework for Fully Automated Layout-Constrained Analog Circuit Design", "categories": ["cs.LG", "cs.AI", "cs.AR", "cs.CE"], "comment": null, "summary": "Designing analog circuits from performance specifications is a complex,\nmulti-stage process encompassing topology selection, parameter inference, and\nlayout feasibility. We introduce FALCON, a unified machine learning framework\nthat enables fully automated, specification-driven analog circuit synthesis\nthrough topology selection and layout-constrained optimization. Given a target\nperformance, FALCON first selects an appropriate circuit topology using a\nperformance-driven classifier guided by human design heuristics. Next, it\nemploys a custom, edge-centric graph neural network trained to map circuit\ntopology and parameters to performance, enabling gradient-based parameter\ninference through the learned forward model. This inference is guided by a\ndifferentiable layout cost, derived from analytical equations capturing\nparasitic and frequency-dependent effects, and constrained by design rules. We\ntrain and evaluate FALCON on a large-scale custom dataset of 1M analog mm-wave\ncircuits, generated and simulated using Cadence Spectre across 20\nexpert-designed topologies. Through this evaluation, FALCON demonstrates >99\\%\naccuracy in topology inference, <10\\% relative error in performance prediction,\nand efficient layout-aware design that completes in under 1 second per\ninstance. Together, these results position FALCON as a practical and extensible\nfoundation model for end-to-end analog circuit design automation."}
{"id": "2505.22255", "pdf": "https://arxiv.org/pdf/2505.22255", "abs": "https://arxiv.org/abs/2505.22255", "authors": ["Vadim Kurochkin", "Yaroslav Aksenov", "Daniil Laptev", "Daniil Gavrilov", "Nikita Balagansky"], "title": "Train Sparse Autoencoders Efficiently by Utilizing Features Correlation", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Sparse Autoencoders (SAEs) have demonstrated significant promise in\ninterpreting the hidden states of language models by decomposing them into\ninterpretable latent directions. However, training SAEs at scale remains\nchallenging, especially when large dictionary sizes are used. While decoders\ncan leverage sparse-aware kernels for efficiency, encoders still require\ncomputationally intensive linear operations with large output dimensions. To\naddress this, we propose KronSAE, a novel architecture that factorizes the\nlatent representation via Kronecker product decomposition, drastically reducing\nmemory and computational overhead. Furthermore, we introduce mAND, a\ndifferentiable activation function approximating the binary AND operation,\nwhich improves interpretability and performance in our factorized framework."}
{"id": "2505.22461", "pdf": "https://arxiv.org/pdf/2505.22461", "abs": "https://arxiv.org/abs/2505.22461", "authors": ["Qiucheng Yu", "Yuan Xie", "Xin Tan"], "title": "SHTOcc: Effective 3D Occupancy Prediction with Sparse Head and Tail Voxels", "categories": ["cs.CV"], "comment": null, "summary": "3D occupancy prediction has attracted much attention in the field of\nautonomous driving due to its powerful geometric perception and object\nrecognition capabilities. However, existing methods have not explored the most\nessential distribution patterns of voxels, resulting in unsatisfactory results.\nThis paper first explores the inter-class distribution and geometric\ndistribution of voxels, thereby solving the long-tail problem caused by the\ninter-class distribution and the poor performance caused by the geometric\ndistribution. Specifically, this paper proposes SHTOcc (Sparse Head-Tail\nOccupancy), which uses sparse head-tail voxel construction to accurately\nidentify and balance key voxels in the head and tail classes, while using\ndecoupled learning to reduce the model's bias towards the dominant (head)\ncategory and enhance the focus on the tail class. Experiments show that\nsignificant improvements have been made on multiple baselines: SHTOcc reduces\nGPU memory usage by 42.2%, increases inference speed by 58.6%, and improves\naccuracy by about 7%, verifying its effectiveness and efficiency. The code is\navailable at https://github.com/ge95net/SHTOcc"}
{"id": "2505.21926", "pdf": "https://arxiv.org/pdf/2505.21926", "abs": "https://arxiv.org/abs/2505.21926", "authors": ["Yin Hua", "Zhiqiang Liu", "Mingyang Chen", "Zheng Fang", "Chi Man Wong", "Lingxiao Li", "Chi Man Vong", "Huajun Chen", "Wen Zhang"], "title": "Beyond Completion: A Foundation Model for General Knowledge Graph Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings", "summary": "In natural language processing (NLP) and computer vision (CV), the successful\napplication of foundation models across diverse tasks has demonstrated their\nremarkable potential. However, despite the rich structural and textual\ninformation embedded in knowledge graphs (KGs), existing research of foundation\nmodel for KG has primarily focused on their structural aspects, with most\nefforts restricted to in-KG tasks (e.g., knowledge graph completion, KGC). This\nlimitation has hindered progress in addressing more challenging out-of-KG\ntasks. In this paper, we introduce MERRY, a foundation model for general\nknowledge graph reasoning, and investigate its performance across two task\ncategories: in-KG reasoning tasks (e.g., KGC) and out-of-KG tasks (e.g., KG\nquestion answering, KGQA). We not only utilize the structural information, but\nalso the textual information in KGs. Specifically, we propose a\nmulti-perspective Conditional Message Passing (CMP) encoding architecture to\nbridge the gap between textual and structural modalities, enabling their\nseamless integration. Additionally, we introduce a dynamic residual fusion\nmodule to selectively retain relevant textual information and a flexible edge\nscoring mechanism to adapt to diverse downstream tasks. Comprehensive\nevaluations on 28 datasets demonstrate that MERRY outperforms existing\nbaselines in most scenarios, showcasing strong reasoning capabilities within\nKGs and excellent generalization to out-of-KG tasks such as KGQA."}
{"id": "2505.22271", "pdf": "https://arxiv.org/pdf/2505.22271", "abs": "https://arxiv.org/abs/2505.22271", "authors": ["Yongcan Yu", "Yanbo Wang", "Ran He", "Jian Liang"], "title": "Test-Time Immunization: A Universal Defense Framework Against Jailbreaks for (Multimodal) Large Language Models", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": "Under Review", "summary": "While (multimodal) large language models (LLMs) have attracted widespread\nattention due to their exceptional capabilities, they remain vulnerable to\njailbreak attacks. Various defense methods are proposed to defend against\njailbreak attacks, however, they are often tailored to specific types of\njailbreak attacks, limiting their effectiveness against diverse adversarial\nstrategies. For instance, rephrasing-based defenses are effective against text\nadversarial jailbreaks but fail to counteract image-based attacks. To overcome\nthese limitations, we propose a universal defense framework, termed Test-time\nIMmunization (TIM), which can adaptively defend against various jailbreak\nattacks in a self-evolving way. Specifically, TIM initially trains a gist token\nfor efficient detection, which it subsequently applies to detect jailbreak\nactivities during inference. When jailbreak attempts are identified, TIM\nimplements safety fine-tuning using the detected jailbreak instructions paired\nwith refusal answers. Furthermore, to mitigate potential performance\ndegradation in the detector caused by parameter updates during safety\nfine-tuning, we decouple the fine-tuning process from the detection module.\nExtensive experiments on both LLMs and multimodal LLMs demonstrate the efficacy\nof TIM."}
{"id": "2505.22465", "pdf": "https://arxiv.org/pdf/2505.22465", "abs": "https://arxiv.org/abs/2505.22465", "authors": ["Zobia Batool", "Huseyin Ozkan", "Erchan Aptoula"], "title": "Single Domain Generalization for Alzheimer's Detection from 3D MRIs with Pseudo-Morphological Augmentations and Contrastive Learning", "categories": ["cs.CV"], "comment": null, "summary": "Although Alzheimer's disease detection via MRIs has advanced significantly\nthanks to contemporary deep learning models, challenges such as class\nimbalance, protocol variations, and limited dataset diversity often hinder\ntheir generalization capacity. To address this issue, this article focuses on\nthe single domain generalization setting, where given the data of one domain, a\nmodel is designed and developed with maximal performance w.r.t. an unseen\ndomain of distinct distribution. Since brain morphology is known to play a\ncrucial role in Alzheimer's diagnosis, we propose the use of learnable\npseudo-morphological modules aimed at producing shape-aware, anatomically\nmeaningful class-specific augmentations in combination with a supervised\ncontrastive learning module to extract robust class-specific representations.\nExperiments conducted across three datasets show improved performance and\ngeneralization capacity, especially under class imbalance and imaging protocol\nvariations. The source code will be made available upon acceptance at\nhttps://github.com/zobia111/SDG-Alzheimer."}
{"id": "2505.21928", "pdf": "https://arxiv.org/pdf/2505.21928", "abs": "https://arxiv.org/abs/2505.21928", "authors": ["Lianghui Zhu", "Xitong Ling", "Minxi Ouyang", "Xiaoping Liu", "Mingxi Fu", "Tian Guan", "Fanglei Fu", "Xuanyu Wang", "Maomao Zeng", "Mingxi Zhu", "Yibo Jin", "Liming Liu", "Song Duan", "Qiming He", "Yizhi Wang", "Luxi Xie", "Houqiang Li", "Yonghong He", "Sufang Tian"], "title": "Subspecialty-Specific Foundation Model for Intelligent Gastrointestinal Pathology", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Gastrointestinal (GI) diseases represent a clinically significant burden,\nnecessitating precise diagnostic approaches to optimize patient outcomes.\nConventional histopathological diagnosis, heavily reliant on the subjective\ninterpretation of pathologists, suffers from limited reproducibility and\ndiagnostic variability. To overcome these limitations and address the lack of\npathology-specific foundation models for GI diseases, we develop Digepath, a\nspecialized foundation model for GI pathology. Our framework introduces a\ndual-phase iterative optimization strategy combining pretraining with\nfine-screening, specifically designed to address the detection of sparsely\ndistributed lesion areas in whole-slide images. Digepath is pretrained on more\nthan 353 million image patches from over 200,000 hematoxylin and eosin-stained\nslides of GI diseases. It attains state-of-the-art performance on 33 out of 34\ntasks related to GI pathology, including pathological diagnosis, molecular\nprediction, gene mutation prediction, and prognosis evaluation, particularly in\ndiagnostically ambiguous cases and resolution-agnostic tissue classification.We\nfurther translate the intelligent screening module for early GI cancer and\nachieve near-perfect 99.6% sensitivity across 9 independent medical\ninstitutions nationwide. The outstanding performance of Digepath highlights its\npotential to bridge critical gaps in histopathological practice. This work not\nonly advances AI-driven precision pathology for GI diseases but also\nestablishes a transferable paradigm for other pathology subspecialties."}
{"id": "2505.22290", "pdf": "https://arxiv.org/pdf/2505.22290", "abs": "https://arxiv.org/abs/2505.22290", "authors": ["Fanzeng Xia", "Yidong Luo", "Tinko Sebastian Bartels", "Yaqi Xu", "Tongxin Li"], "title": "Rethinking the Unsolvable: When In-Context Search Meets Test-Time Scaling", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Recent research has highlighted that Large Language Models (LLMs), even when\ntrained to generate extended long reasoning steps, still face significant\nchallenges on hard reasoning problems. However, much of the existing literature\nrelies on direct prompting with simple in-context learning examples for\nevaluation, which largely overlooks advanced techniques to elicit LLMs'\ndeliberate reasoning before drawing conclusions that LLMs hit a performance\nceiling. In this paper, we systematically explore the combined potential of\nin-context search and test-time scaling on super hard reasoning tasks. We find\nthat by employing advanced in-context search prompting to LLMs augmented with\ninternal scaling, one can achieve transformative performance breakthroughs on\ntasks previously deemed \"unsolvable\" (e.g., reported success rates below 5%).\nWe provide both empirical results and theoretical analysis of how this\ncombination can unleash LLM reasoning capabilities: i) Empirically, on\ncontrolled NP-hard tasks and complex real-world planning benchmarks, our\napproach achieves up to a 30x improvement in success rates compared to\npreviously reported results without any external mechanisms; ii) Theoretically,\nwe show that in-context search prompting, when combined with internal scaling,\nsignificantly extends the complexity class of solvable reasoning problems.\nThese findings challenge prevailing assumptions about the limitations of LLMs\non complex tasks, indicating that current evaluation paradigms systematically\nunderestimate their true potential. Our work calls for a critical reassessment\nof how LLM reasoning is benchmarked and a more robust evaluation strategy that\nfully captures the true capabilities of contemporary LLMs, which can lead to a\nbetter understanding of their operational reasoning boundaries in real-world\ndeployments."}
{"id": "2505.22490", "pdf": "https://arxiv.org/pdf/2505.22490", "abs": "https://arxiv.org/abs/2505.22490", "authors": ["Ke Zhang", "Tianyu Ding", "Jiachen Jiang", "Tianyi Chen", "Ilya Zharkov", "Vishal M. Patel", "Luming Liang"], "title": "ProCrop: Learning Aesthetic Image Cropping from Professional Compositions", "categories": ["cs.CV"], "comment": "16 pages, 15 figures", "summary": "Image cropping is crucial for enhancing the visual appeal and narrative\nimpact of photographs, yet existing rule-based and data-driven approaches often\nlack diversity or require annotated training data. We introduce ProCrop, a\nretrieval-based method that leverages professional photography to guide\ncropping decisions. By fusing features from professional photographs with those\nof the query image, ProCrop learns from professional compositions,\nsignificantly boosting performance. Additionally, we present a large-scale\ndataset of 242K weakly-annotated images, generated by out-painting professional\nimages and iteratively refining diverse crop proposals. This composition-aware\ndataset generation offers diverse high-quality crop proposals guided by\naesthetic principles and becomes the largest publicly available dataset for\nimage cropping. Extensive experiments show that ProCrop significantly\noutperforms existing methods in both supervised and weakly-supervised settings.\nNotably, when trained on the new dataset, our ProCrop surpasses previous\nweakly-supervised methods and even matches fully supervised approaches. Both\nthe code and dataset will be made publicly available to advance research in\nimage aesthetics and composition analysis."}
{"id": "2505.21938", "pdf": "https://arxiv.org/pdf/2505.21938", "abs": "https://arxiv.org/abs/2505.21938", "authors": ["Qirun Zeng", "Eric He", "Richard Hoffmann", "Xuchuang Wang", "Jinhang Zuo"], "title": "Practical Adversarial Attacks on Stochastic Bandits via Fake Data Injection", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Adversarial attacks on stochastic bandits have traditionally relied on some\nunrealistic assumptions, such as per-round reward manipulation and unbounded\nperturbations, limiting their relevance to real-world systems. We propose a\nmore practical threat model, Fake Data Injection, which reflects realistic\nadversarial constraints: the attacker can inject only a limited number of\nbounded fake feedback samples into the learner's history, simulating legitimate\ninteractions. We design efficient attack strategies under this model,\nexplicitly addressing both magnitude constraints (on reward values) and\ntemporal constraints (on when and how often data can be injected). Our\ntheoretical analysis shows that these attacks can mislead both Upper Confidence\nBound (UCB) and Thompson Sampling algorithms into selecting a target arm in\nnearly all rounds while incurring only sublinear attack cost. Experiments on\nsynthetic and real-world datasets validate the effectiveness of our strategies,\nrevealing significant vulnerabilities in widely used stochastic bandit\nalgorithms under practical adversarial scenarios."}
{"id": "2505.22312", "pdf": "https://arxiv.org/pdf/2505.22312", "abs": "https://arxiv.org/abs/2505.22312", "authors": ["Jujie He", "Jiacai Liu", "Chris Yuhao Liu", "Rui Yan", "Chaojie Wang", "Peng Cheng", "Xiaoyu Zhang", "Fuxiang Zhang", "Jiacheng Xu", "Wei Shen", "Siyuan Li", "Liang Zeng", "Tianwen Wei", "Cheng Cheng", "Bo An", "Yang Liu", "Yahui Zhou"], "title": "Skywork Open Reasoner 1 Technical Report", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The success of DeepSeek-R1 underscores the significant role of reinforcement\nlearning (RL) in enhancing the reasoning capabilities of large language models\n(LLMs). In this work, we present Skywork-OR1, an effective and scalable RL\nimplementation for long Chain-of-Thought (CoT) models. Building on the\nDeepSeek-R1-Distill model series, our RL approach achieves notable performance\ngains, increasing average accuracy across AIME24, AIME25, and LiveCodeBench\nfrom 57.8% to 72.8% (+15.0%) for the 32B model and from 43.6% to 57.5% (+13.9%)\nfor the 7B model. Our Skywork-OR1-32B model surpasses both DeepSeek-R1 and\nQwen3-32B on the AIME24 and AIME25 benchmarks, while achieving comparable\nresults on LiveCodeBench. The Skywork-OR1-7B and Skywork-OR1-Math-7B models\ndemonstrate competitive reasoning capabilities among models of similar size. We\nperform comprehensive ablation studies on the core components of our training\npipeline to validate their effectiveness. Additionally, we thoroughly\ninvestigate the phenomenon of entropy collapse, identify key factors affecting\nentropy dynamics, and demonstrate that mitigating premature entropy collapse is\ncritical for improved test performance. To support community research, we fully\nopen-source our model weights, training code, and training datasets."}
{"id": "2505.22499", "pdf": "https://arxiv.org/pdf/2505.22499", "abs": "https://arxiv.org/abs/2505.22499", "authors": ["Aixuan Li", "Mochu Xiang", "Jing Zhang", "Yuchao Dai"], "title": "The Meeseeks Mesh: Spatially Consistent 3D Adversarial Objects for BEV Detector", "categories": ["cs.CV"], "comment": null, "summary": "3D object detection is a critical component in autonomous driving systems. It\nallows real-time recognition and detection of vehicles, pedestrians and\nobstacles under varying environmental conditions. Among existing methods, 3D\nobject detection in the Bird's Eye View (BEV) has emerged as the mainstream\nframework. To guarantee a safe, robust and trustworthy 3D object detection, 3D\nadversarial attacks are investigated, where attacks are placed in 3D\nenvironments to evaluate the model performance, e.g., putting a film on a car,\nclothing a pedestrian. The vulnerability of 3D object detection models to 3D\nadversarial attacks serves as an important indicator to evaluate the robustness\nof the model against perturbations. To investigate this vulnerability, we\ngenerate non-invasive 3D adversarial objects tailored for real-world attack\nscenarios. Our method verifies the existence of universal adversarial objects\nthat are spatially consistent across time and camera views. Specifically, we\nemploy differentiable rendering techniques to accurately model the spatial\nrelationship between adversarial objects and the target vehicle. Furthermore,\nwe introduce an occlusion-aware module to enhance visual consistency and\nrealism under different viewpoints. To maintain attack effectiveness across\nmultiple frames, we design a BEV spatial feature-guided optimization strategy.\nExperimental results demonstrate that our approach can reliably suppress\nvehicle predictions from state-of-the-art 3D object detectors, serving as an\nimportant tool to test robustness of 3D object detection models before\ndeployment. Moreover, the generated adversarial objects exhibit strong\ngeneralization capabilities, retaining its effectiveness at various positions\nand distances in the scene."}
{"id": "2505.21954", "pdf": "https://arxiv.org/pdf/2505.21954", "abs": "https://arxiv.org/abs/2505.21954", "authors": ["Le Thien Phuc Nguyen", "Zhuoran Yu", "Khoa Quang Nhat Cao", "Yuwei Guo", "Tu Ho Manh Pham", "Tuan Tai Nguyen", "Toan Ngo Duc Vo", "Lucas Poon", "Soochahn Lee", "Yong Jae Lee"], "title": "UniTalk: Towards Universal Active Speaker Detection in Real World Scenarios", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We present UniTalk, a novel dataset specifically designed for the task of\nactive speaker detection, emphasizing challenging scenarios to enhance model\ngeneralization. Unlike previously established benchmarks such as AVA, which\npredominantly features old movies and thus exhibits significant domain gaps,\nUniTalk focuses explicitly on diverse and difficult real-world conditions.\nThese include underrepresented languages, noisy backgrounds, and crowded scenes\n- such as multiple visible speakers speaking concurrently or in overlapping\nturns. It contains over 44.5 hours of video with frame-level active speaker\nannotations across 48,693 speaking identities, and spans a broad range of video\ntypes that reflect real-world conditions. Through rigorous evaluation, we show\nthat state-of-the-art models, while achieving nearly perfect scores on AVA,\nfail to reach saturation on UniTalk, suggesting that the ASD task remains far\nfrom solved under realistic conditions. Nevertheless, models trained on UniTalk\ndemonstrate stronger generalization to modern \"in-the-wild\" datasets like\nTalkies and ASW, as well as to AVA. UniTalk thus establishes a new benchmark\nfor active speaker detection, providing researchers with a valuable resource\nfor developing and evaluating versatile and resilient models.\n  Dataset: https://huggingface.co/datasets/plnguyen2908/UniTalk-ASD\n  Code: https://github.com/plnguyen2908/UniTalk-ASD-code"}
{"id": "2505.22411", "pdf": "https://arxiv.org/pdf/2505.22411", "abs": "https://arxiv.org/abs/2505.22411", "authors": ["Yao Huang", "Huanran Chen", "Shouwei Ruan", "Yichi Zhang", "Xingxing Wei", "Yinpeng Dong"], "title": "Mitigating Overthinking in Large Reasoning Models via Manifold Steering", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "17 pages, 7 figures", "summary": "Recent advances in Large Reasoning Models (LRMs) have demonstrated remarkable\ncapabilities in solving complex tasks such as mathematics and coding. However,\nthese models frequently exhibit a phenomenon known as overthinking during\ninference, characterized by excessive validation loops and redundant\ndeliberation, leading to substantial computational overheads. In this paper, we\naim to mitigate overthinking by investigating the underlying mechanisms from\nthe perspective of mechanistic interpretability. We first showcase that the\ntendency of overthinking can be effectively captured by a single direction in\nthe model's activation space and the issue can be eased by intervening the\nactivations along this direction. However, this efficacy soon reaches a plateau\nand even deteriorates as the intervention strength increases. We therefore\nsystematically explore the activation space and find that the overthinking\nphenomenon is actually tied to a low-dimensional manifold, which indicates that\nthe limited effect stems from the noises introduced by the high-dimensional\nsteering direction. Based on this insight, we propose Manifold Steering, a\nnovel approach that elegantly projects the steering direction onto the\nlow-dimensional activation manifold given the theoretical approximation of the\ninterference noise. Extensive experiments on DeepSeek-R1 distilled models\nvalidate that our method reduces output tokens by up to 71% while maintaining\nand even improving the accuracy on several mathematical benchmarks. Our method\nalso exhibits robust cross-domain transferability, delivering consistent token\nreduction performance in code generation and knowledge-based QA tasks. Code is\navailable at: https://github.com/Aries-iai/Manifold_Steering."}
{"id": "2505.22522", "pdf": "https://arxiv.org/pdf/2505.22522", "abs": "https://arxiv.org/abs/2505.22522", "authors": ["Yuan Zhang", "Feng Chen", "Yaolei Qi", "Guanyu Yang", "Huazhu Fu"], "title": "PathFL: Multi-Alignment Federated Learning for Pathology Image Segmentation", "categories": ["cs.CV"], "comment": "17 pages, 5 figures; Accepted by MedIA", "summary": "Pathology image segmentation across multiple centers encounters significant\nchallenges due to diverse sources of heterogeneity including imaging\nmodalities, organs, and scanning equipment, whose variability brings\nrepresentation bias and impedes the development of generalizable segmentation\nmodels. In this paper, we propose PathFL, a novel multi-alignment Federated\nLearning framework for pathology image segmentation that addresses these\nchallenges through three-level alignment strategies of image, feature, and\nmodel aggregation. Firstly, at the image level, a collaborative style\nenhancement module aligns and diversifies local data by facilitating style\ninformation exchange across clients. Secondly, at the feature level, an\nadaptive feature alignment module ensures implicit alignment in the\nrepresentation space by infusing local features with global insights, promoting\nconsistency across heterogeneous client features learning. Finally, at the\nmodel aggregation level, a stratified similarity aggregation strategy\nhierarchically aligns and aggregates models on the server, using layer-specific\nsimilarity to account for client discrepancies and enhance global\ngeneralization. Comprehensive evaluations on four sets of heterogeneous\npathology image datasets, encompassing cross-source, cross-modality,\ncross-organ, and cross-scanner variations, validate the effectiveness of our\nPathFL in achieving better performance and robustness against data\nheterogeneity."}
{"id": "2505.21955", "pdf": "https://arxiv.org/pdf/2505.21955", "abs": "https://arxiv.org/abs/2505.21955", "authors": ["Insu Lee", "Wooje Park", "Jaeyun Jang", "Minyoung Noh", "Kyuhong Shim", "Byonghyo Shim"], "title": "Towards Comprehensive Scene Understanding: Integrating First and Third-Person Views for LVLMs", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large vision-language models (LVLMs) are increasingly deployed in interactive\napplications such as virtual and augmented reality, where first-person\n(egocentric) view captured by head-mounted cameras serves as key input. While\nthis view offers fine-grained cues about user attention and hand-object\ninteractions, their narrow field of view and lack of global context often lead\nto failures on spatially or contextually demanding queries. To address this, we\nintroduce a framework that augments egocentric inputs with third-person\n(exocentric) views, providing complementary information such as global scene\nlayout and object visibility to LVLMs. We present E3VQA, the first benchmark\nfor multi-view question answering with 4K high-quality question-answer pairs\ngrounded in synchronized ego-exo image pairs. Additionally, we propose M3CoT, a\ntraining-free prompting technique that constructs a unified scene\nrepresentation by integrating scene graphs from three complementary\nperspectives. M3CoT enables LVLMs to reason more effectively across views,\nyielding consistent performance gains (4.84% for GPT-4o and 5.94% for Gemini\n2.0 Flash) over a recent CoT baseline. Our extensive evaluation reveals key\nstrengths and limitations of LVLMs in multi-view reasoning and highlights the\nvalue of leveraging both egocentric and exocentric inputs."}
{"id": "2505.22425", "pdf": "https://arxiv.org/pdf/2505.22425", "abs": "https://arxiv.org/abs/2505.22425", "authors": ["Xueliang Zhao", "Wei Wu", "Lingpeng Kong"], "title": "Scaling Reasoning without Attention", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "preprint", "summary": "Large language models (LLMs) have made significant advances in complex\nreasoning tasks, yet they remain bottlenecked by two core challenges:\narchitectural inefficiency due to reliance on Transformers, and a lack of\nstructured fine-tuning for high-difficulty domains. We introduce \\ourmodel, an\nattention-free language model that addresses both issues through architectural\nand data-centric innovations. Built on the state space dual (SSD) layers of\nMamba-2, our model eliminates the need for self-attention and key-value\ncaching, enabling fixed-memory, constant-time inference. To train it for\ncomplex reasoning, we propose a two-phase curriculum fine-tuning strategy based\non the \\textsc{PromptCoT} synthesis paradigm, which generates pedagogically\nstructured problems via abstract concept selection and rationale-guided\ngeneration. On benchmark evaluations, \\ourmodel-7B outperforms strong\nTransformer and hybrid models of comparable scale, and even surpasses the much\nlarger Gemma3-27B by 2.6\\% on AIME 24, 0.6\\% on AIME 25, and 3.0\\% on\nLivecodebench. These results highlight the potential of state space models as\nefficient and scalable alternatives to attention-based architectures for\nhigh-capacity reasoning."}
{"id": "2505.22523", "pdf": "https://arxiv.org/pdf/2505.22523", "abs": "https://arxiv.org/abs/2505.22523", "authors": ["Junwen Chen", "Heyang Jiang", "Yanbin Wang", "Keming Wu", "Ji Li", "Chao Zhang", "Keiji Yanai", "Dong Chen", "Yuhui Yuan"], "title": "PrismLayers: Open Data for High-Quality Multi-Layer Transparent Image Generative Models", "categories": ["cs.CV"], "comment": "Homepage: https://prism-layers.github.io/", "summary": "Generating high-quality, multi-layer transparent images from text prompts can\nunlock a new level of creative control, allowing users to edit each layer as\neffortlessly as editing text outputs from LLMs. However, the development of\nmulti-layer generative models lags behind that of conventional text-to-image\nmodels due to the absence of a large, high-quality corpus of multi-layer\ntransparent data. In this paper, we address this fundamental challenge by: (i)\nreleasing the first open, ultra-high-fidelity PrismLayers (PrismLayersPro)\ndataset of 200K (20K) multilayer transparent images with accurate alpha mattes,\n(ii) introducing a trainingfree synthesis pipeline that generates such data on\ndemand using off-the-shelf diffusion models, and (iii) delivering a strong,\nopen-source multi-layer generation model, ART+, which matches the aesthetics of\nmodern text-to-image generation models. The key technical contributions\ninclude: LayerFLUX, which excels at generating high-quality single transparent\nlayers with accurate alpha mattes, and MultiLayerFLUX, which composes multiple\nLayerFLUX outputs into complete images, guided by human-annotated semantic\nlayout. To ensure higher quality, we apply a rigorous filtering stage to remove\nartifacts and semantic mismatches, followed by human selection. Fine-tuning the\nstate-of-the-art ART model on our synthetic PrismLayersPro yields ART+, which\noutperforms the original ART in 60% of head-to-head user study comparisons and\neven matches the visual quality of images generated by the FLUX.1-[dev] model.\nWe anticipate that our work will establish a solid dataset foundation for the\nmulti-layer transparent image generation task, enabling research and\napplications that require precise, editable, and visually compelling layered\nimagery."}
{"id": "2505.21956", "pdf": "https://arxiv.org/pdf/2505.21956", "abs": "https://arxiv.org/abs/2505.21956", "authors": ["Mengdan Zhu", "Senhao Cheng", "Guangji Bai", "Yifei Zhang", "Liang Zhao"], "title": "Cross-modal RAG: Sub-dimensional Retrieval-Augmented Text-to-Image Generation", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Text-to-image generation increasingly demands access to domain-specific,\nfine-grained, and rapidly evolving knowledge that pretrained models cannot\nfully capture. Existing Retrieval-Augmented Generation (RAG) methods attempt to\naddress this by retrieving globally relevant images, but they fail when no\nsingle image contains all desired elements from a complex user query. We\npropose Cross-modal RAG, a novel framework that decomposes both queries and\nimages into sub-dimensional components, enabling subquery-aware retrieval and\ngeneration. Our method introduces a hybrid retrieval strategy - combining a\nsub-dimensional sparse retriever with a dense retriever - to identify a\nPareto-optimal set of images, each contributing complementary aspects of the\nquery. During generation, a multimodal large language model is guided to\nselectively condition on relevant visual features aligned to specific\nsubqueries, ensuring subquery-aware image synthesis. Extensive experiments on\nMS-COCO, Flickr30K, WikiArt, CUB, and ImageNet-LT demonstrate that Cross-modal\nRAG significantly outperforms existing baselines in both retrieval and\ngeneration quality, while maintaining high efficiency."}
{"id": "2505.22457", "pdf": "https://arxiv.org/pdf/2505.22457", "abs": "https://arxiv.org/abs/2505.22457", "authors": ["Haonan Wang", "Hongfu Liu", "Xiangyan Liu", "Chao Du", "Kenji Kawaguchi", "Ye Wang", "Tianyu Pang"], "title": "Fostering Video Reasoning via Next-Event Prediction", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Next-token prediction serves as the foundational learning task enabling\nreasoning in LLMs. But what should the learning task be when aiming to equip\nMLLMs with temporal reasoning capabilities over video inputs? Existing tasks\nsuch as video question answering often rely on annotations from humans or much\nstronger MLLMs, while video captioning tends to entangle temporal reasoning\nwith spatial information. To address this gap, we propose next-event prediction\n(NEP), a learning task that harnesses future video segments as a rich,\nself-supervised signal to foster temporal reasoning. We segment each video into\npast and future frames: the MLLM takes the past frames as input and predicts a\nsummary of events derived from the future frames, thereby encouraging the model\nto reason temporally in order to complete the task. To support this task, we\ncurate V1-33K, a dataset comprising 33,000 automatically extracted video\nsegments spanning diverse real-world scenarios. We further explore a range of\nvideo instruction-tuning strategies to study their effects on temporal\nreasoning. To evaluate progress, we introduce FutureBench to assess coherence\nin predicting unseen future events. Experiments validate that NEP offers a\nscalable and effective training paradigm for fostering temporal reasoning in\nMLLMs."}
{"id": "2505.22525", "pdf": "https://arxiv.org/pdf/2505.22525", "abs": "https://arxiv.org/abs/2505.22525", "authors": ["Ethan Chern", "Zhulin Hu", "Steffi Chern", "Siqi Kou", "Jiadi Su", "Yan Ma", "Zhijie Deng", "Pengfei Liu"], "title": "Thinking with Generated Images", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "We present Thinking with Generated Images, a novel paradigm that\nfundamentally transforms how large multimodal models (LMMs) engage with visual\nreasoning by enabling them to natively think across text and vision modalities\nthrough spontaneous generation of intermediate visual thinking steps. Current\nvisual reasoning with LMMs is constrained to either processing fixed\nuser-provided images or reasoning solely through text-based chain-of-thought\n(CoT). Thinking with Generated Images unlocks a new dimension of cognitive\ncapability where models can actively construct intermediate visual thoughts,\ncritique their own visual hypotheses, and refine them as integral components of\ntheir reasoning process. We demonstrate the effectiveness of our approach\nthrough two complementary mechanisms: (1) vision generation with intermediate\nvisual subgoals, where models decompose complex visual tasks into manageable\ncomponents that are generated and integrated progressively, and (2) vision\ngeneration with self-critique, where models generate an initial visual\nhypothesis, analyze its shortcomings through textual reasoning, and produce\nrefined outputs based on their own critiques. Our experiments on vision\ngeneration benchmarks show substantial improvements over baseline approaches,\nwith our models achieving up to 50% (from 38% to 57%) relative improvement in\nhandling complex multi-object scenarios. From biochemists exploring novel\nprotein structures, and architects iterating on spatial designs, to forensic\nanalysts reconstructing crime scenes, and basketball players envisioning\nstrategic plays, our approach enables AI models to engage in the kind of visual\nimagination and iterative refinement that characterizes human creative,\nanalytical, and strategic thinking. We release our open-source suite at\nhttps://github.com/GAIR-NLP/thinking-with-generated-images."}
{"id": "2505.21963", "pdf": "https://arxiv.org/pdf/2505.21963", "abs": "https://arxiv.org/abs/2505.21963", "authors": ["Taro Yano", "Yoichi Ishibashi", "Masafumi Oyamada"], "title": "LaMDAgent: An Autonomous Framework for Post-Training Pipeline Optimization via LLM Agents", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated exceptional performance across\na wide range of tasks. To further tailor LLMs to specific domains or\napplications, post-training techniques such as Supervised Fine-Tuning (SFT),\nPreference Learning, and model merging are commonly employed. While each of\nthese methods has been extensively studied in isolation, the automated\nconstruction of complete post-training pipelines remains an underexplored area.\nExisting approaches typically rely on manual design or focus narrowly on\noptimizing individual components, such as data ordering or merging strategies.\nIn this work, we introduce LaMDAgent (short for Language Model Developing\nAgent), a novel framework that autonomously constructs and optimizes full\npost-training pipelines through the use of LLM-based agents. LaMDAgent\nsystematically explores diverse model generation techniques, datasets, and\nhyperparameter configurations, leveraging task-based feedback to discover\nhigh-performing pipelines with minimal human intervention. Our experiments show\nthat LaMDAgent improves tool-use accuracy by 9.0 points while preserving\ninstruction-following capabilities. Moreover, it uncovers effective\npost-training strategies that are often overlooked by conventional human-driven\nexploration. We further analyze the impact of data and model size scaling to\nreduce computational costs on the exploration, finding that model size scalings\nintroduces new challenges, whereas scaling data size enables cost-effective\npipeline discovery."}
{"id": "2505.22487", "pdf": "https://arxiv.org/pdf/2505.22487", "abs": "https://arxiv.org/abs/2505.22487", "authors": ["Yen Meng", "Sharon Goldwater", "Hao Tang"], "title": "Effective Context in Neural Speech Models", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Modern neural speech models benefit from having longer context, and many\napproaches have been proposed to increase the maximum context a model can use.\nHowever, few have attempted to measure how much context these models actually\nuse, i.e., the effective context. Here, we propose two approaches to measuring\nthe effective context, and use them to analyze different speech Transformers.\nFor supervised models, we find that the effective context correlates well with\nthe nature of the task, with fundamental frequency tracking, phone\nclassification, and word classification requiring increasing amounts of\neffective context. For self-supervised models, we find that effective context\nincreases mainly in the early layers, and remains relatively short -- similar\nto the supervised phone model. Given that these models do not use a long\ncontext during prediction, we show that HuBERT can be run in streaming mode\nwithout modification to the architecture and without further fine-tuning."}
{"id": "2505.22535", "pdf": "https://arxiv.org/pdf/2505.22535", "abs": "https://arxiv.org/abs/2505.22535", "authors": ["Mohamad Hakam Shams Eddin", "Yikui Zhang", "Stefan Kollet", "Juergen Gall"], "title": "RiverMamba: A State Space Model for Global River Discharge and Flood Forecasting", "categories": ["cs.CV", "cs.LG"], "comment": "Main paper 10 pages, Appendix 53 pages", "summary": "Recent deep learning approaches for river discharge forecasting have improved\nthe accuracy and efficiency in flood forecasting, enabling more reliable early\nwarning systems for risk management. Nevertheless, existing deep learning\napproaches in hydrology remain largely confined to local-scale applications and\ndo not leverage the inherent spatial connections of bodies of water. Thus,\nthere is a strong need for new deep learning methodologies that are capable of\nmodeling spatio-temporal relations to improve river discharge and flood\nforecasting for scientific and operational applications. To address this, we\npresent RiverMamba, a novel deep learning model that is pretrained with\nlong-term reanalysis data and that can forecast global river discharge and\nfloods on a $0.05^\\circ$ grid up to 7 days lead time, which is of high\nrelevance in early warning. To achieve this, RiverMamba leverages efficient\nMamba blocks that enable the model to capture global-scale channel network\nrouting and enhance its forecast capability for longer lead times. The forecast\nblocks integrate ECMWF HRES meteorological forecasts, while accounting for\ntheir inaccuracies through spatio-temporal modeling. Our analysis demonstrates\nthat RiverMamba delivers reliable predictions of river discharge, including\nextreme floods across return periods and lead times, surpassing both\noperational AI- and physics-based models."}
{"id": "2505.21966", "pdf": "https://arxiv.org/pdf/2505.21966", "abs": "https://arxiv.org/abs/2505.21966", "authors": ["Aditya Gunturu", "Ben Pearman", "Keiichi Ihara", "Morteza Faraji", "Bryan Wang", "Rubaiat Habib Kazi", "Ryo Suzuki"], "title": "MapStory: LLM-Powered Text-Driven Map Animation Prototyping with Human-in-the-Loop Editing", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.MM", "H.5.2, H.5.1"], "comment": "16 pages and 15 figures", "summary": "We introduce MapStory, an LLM-powered animation authoring tool that generates\neditable map animation sequences directly from natural language text. Given a\nuser-written script, MapStory leverages an agentic architecture to\nautomatically produce a scene breakdown, which decomposes the script into key\nanimation building blocks such as camera movements, visual highlights, and\nanimated elements. Our system includes a researcher component that accurately\nqueries geospatial information by leveraging an LLM with web search, enabling\nthe automatic extraction of relevant regions, paths, and coordinates while\nallowing users to edit and query for changes or additional information to\nrefine the results. Additionally, users can fine-tune parameters of these\nblocks through an interactive timeline editor. We detail the system's design\nand architecture, informed by formative interviews with professional animators\nand an analysis of 200 existing map animation videos. Our evaluation, which\nincludes expert interviews (N=5) and a usability study (N=12), demonstrates\nthat MapStory enables users to create map animations with ease, facilitates\nfaster iteration, encourages creative exploration, and lowers barriers to\ncreating map-centric stories."}
{"id": "2505.22525", "pdf": "https://arxiv.org/pdf/2505.22525", "abs": "https://arxiv.org/abs/2505.22525", "authors": ["Ethan Chern", "Zhulin Hu", "Steffi Chern", "Siqi Kou", "Jiadi Su", "Yan Ma", "Zhijie Deng", "Pengfei Liu"], "title": "Thinking with Generated Images", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "We present Thinking with Generated Images, a novel paradigm that\nfundamentally transforms how large multimodal models (LMMs) engage with visual\nreasoning by enabling them to natively think across text and vision modalities\nthrough spontaneous generation of intermediate visual thinking steps. Current\nvisual reasoning with LMMs is constrained to either processing fixed\nuser-provided images or reasoning solely through text-based chain-of-thought\n(CoT). Thinking with Generated Images unlocks a new dimension of cognitive\ncapability where models can actively construct intermediate visual thoughts,\ncritique their own visual hypotheses, and refine them as integral components of\ntheir reasoning process. We demonstrate the effectiveness of our approach\nthrough two complementary mechanisms: (1) vision generation with intermediate\nvisual subgoals, where models decompose complex visual tasks into manageable\ncomponents that are generated and integrated progressively, and (2) vision\ngeneration with self-critique, where models generate an initial visual\nhypothesis, analyze its shortcomings through textual reasoning, and produce\nrefined outputs based on their own critiques. Our experiments on vision\ngeneration benchmarks show substantial improvements over baseline approaches,\nwith our models achieving up to 50% (from 38% to 57%) relative improvement in\nhandling complex multi-object scenarios. From biochemists exploring novel\nprotein structures, and architects iterating on spatial designs, to forensic\nanalysts reconstructing crime scenes, and basketball players envisioning\nstrategic plays, our approach enables AI models to engage in the kind of visual\nimagination and iterative refinement that characterizes human creative,\nanalytical, and strategic thinking. We release our open-source suite at\nhttps://github.com/GAIR-NLP/thinking-with-generated-images."}
{"id": "2505.22543", "pdf": "https://arxiv.org/pdf/2505.22543", "abs": "https://arxiv.org/abs/2505.22543", "authors": ["Ziheng Jia", "Zicheng Zhang", "Zeyu Zhang", "Yingji Liang", "Xiaorong Zhu", "Chunyi Li", "Jinliang Han", "Haoning Wu", "Bin Wang", "Haoran Zhang", "Guanyu Zhu", "Qiyong Zhao", "Xiaohong Liu", "Guangtao Zhai", "Xiongkuo Min"], "title": "Scaling-up Perceptual Video Quality Assessment", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The data scaling law has been shown to significantly enhance the performance\nof large multi-modal models (LMMs) across various downstream tasks. However, in\nthe domain of perceptual video quality assessment (VQA), the potential of\nscaling law remains unprecedented due to the scarcity of labeled resources and\nthe insufficient scale of datasets. To address this, we propose\n\\textbf{OmniVQA}, an efficient framework designed to efficiently build\nhigh-quality, human-in-the-loop VQA multi-modal instruction databases (MIDBs).\nWe then scale up to create \\textbf{OmniVQA-Chat-400K}, the largest MIDB in the\nVQA field concurrently. Our focus is on the technical and aesthetic quality\ndimensions, with abundant in-context instruction data to provide fine-grained\nVQA knowledge. Additionally, we have built the \\textbf{OmniVQA-MOS-20K} dataset\nto enhance the model's quantitative quality rating capabilities. We then\nintroduce a \\textbf{complementary} training strategy that effectively leverages\nthe knowledge from datasets for quality understanding and quality rating tasks.\nFurthermore, we propose the \\textbf{OmniVQA-FG (fine-grain)-Benchmark} to\nevaluate the fine-grained performance of the models. Our results demonstrate\nthat our models achieve state-of-the-art performance in both quality\nunderstanding and rating tasks."}
{"id": "2505.21969", "pdf": "https://arxiv.org/pdf/2505.21969", "abs": "https://arxiv.org/abs/2505.21969", "authors": ["Tianjun Gu", "Linfeng Li", "Xuhong Wang", "Chenghua Gong", "Jingyu Gong", "Zhizhong Zhang", "Yuan Xie", "Lizhuang Ma", "Xin Tan"], "title": "DORAEMON: Decentralized Ontology-aware Reliable Agent with Enhanced Memory Oriented Navigation", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Adaptive navigation in unfamiliar environments is crucial for household\nservice robots but remains challenging due to the need for both low-level path\nplanning and high-level scene understanding. While recent vision-language model\n(VLM) based zero-shot approaches reduce dependence on prior maps and\nscene-specific training data, they face significant limitations: spatiotemporal\ndiscontinuity from discrete observations, unstructured memory representations,\nand insufficient task understanding leading to navigation failures. We propose\nDORAEMON (Decentralized Ontology-aware Reliable Agent with Enhanced Memory\nOriented Navigation), a novel cognitive-inspired framework consisting of\nVentral and Dorsal Streams that mimics human navigation capabilities. The\nDorsal Stream implements the Hierarchical Semantic-Spatial Fusion and Topology\nMap to handle spatiotemporal discontinuities, while the Ventral Stream combines\nRAG-VLM and Policy-VLM to improve decision-making. Our approach also develops\nNav-Ensurance to ensure navigation safety and efficiency. We evaluate DORAEMON\non the HM3D, MP3D, and GOAT datasets, where it achieves state-of-the-art\nperformance on both success rate (SR) and success weighted by path length (SPL)\nmetrics, significantly outperforming existing methods. We also introduce a new\nevaluation metric (AORI) to assess navigation intelligence better.\nComprehensive experiments demonstrate DORAEMON's effectiveness in zero-shot\nautonomous navigation without requiring prior map building or pre-training."}
{"id": "2505.22613", "pdf": "https://arxiv.org/pdf/2505.22613", "abs": "https://arxiv.org/abs/2505.22613", "authors": ["Yuchi Wang", "Yishuo Cai", "Shuhuai Ren", "Sihan Yang", "Linli Yao", "Yuanxin Liu", "Yuanxing Zhang", "Pengfei Wan", "Xu Sun"], "title": "RICO: Improving Accuracy and Completeness in Image Recaptioning via Visual Reconstruction", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "code: https://github.com/wangyuchi369/RICO", "summary": "Image recaptioning is widely used to generate training datasets with enhanced\nquality for various multimodal tasks. Existing recaptioning methods typically\nrely on powerful multimodal large language models (MLLMs) to enhance textual\ndescriptions, but often suffer from inaccuracies due to hallucinations and\nincompleteness caused by missing fine-grained details. To address these\nlimitations, we propose RICO, a novel framework that refines captions through\nvisual reconstruction. Specifically, we leverage a text-to-image model to\nreconstruct a caption into a reference image, and prompt an MLLM to identify\ndiscrepancies between the original and reconstructed images to refine the\ncaption. This process is performed iteratively, further progressively promoting\nthe generation of more faithful and comprehensive descriptions. To mitigate the\nadditional computational cost induced by the iterative process, we introduce\nRICO-Flash, which learns to generate captions like RICO using DPO. Extensive\nexperiments demonstrate that our approach significantly improves caption\naccuracy and completeness, outperforms most baselines by approximately 10% on\nboth CapsBench and CompreCap. Code released at\nhttps://github.com/wangyuchi369/RICO."}
{"id": "2505.22551", "pdf": "https://arxiv.org/pdf/2505.22551", "abs": "https://arxiv.org/abs/2505.22551", "authors": ["Long Hui", "Wai Lok Yeung"], "title": "Deep Learning-Based BMD Estimation from Radiographs with Conformal Uncertainty Quantification", "categories": ["cs.CV", "stat.AP"], "comment": null, "summary": "Limited DXA access hinders osteoporosis screening. This proof-of-concept\nstudy proposes using widely available knee X-rays for opportunistic Bone\nMineral Density (BMD) estimation via deep learning, emphasizing robust\nuncertainty quantification essential for clinical use. An EfficientNet model\nwas trained on the OAI dataset to predict BMD from bilateral knee radiographs.\nTwo Test-Time Augmentation (TTA) methods were compared: traditional averaging\nand a multi-sample approach. Crucially, Split Conformal Prediction was\nimplemented to provide statistically rigorous, patient-specific prediction\nintervals with guaranteed coverage. Results showed a Pearson correlation of\n0.68 (traditional TTA). While traditional TTA yielded better point predictions,\nthe multi-sample approach produced slightly tighter confidence intervals (90%,\n95%, 99%) while maintaining coverage. The framework appropriately expressed\nhigher uncertainty for challenging cases. Although anatomical mismatch between\nknee X-rays and standard DXA limits immediate clinical use, this method\nestablishes a foundation for trustworthy AI-assisted BMD screening using\nroutine radiographs, potentially improving early osteoporosis detection."}
{"id": "2505.21972", "pdf": "https://arxiv.org/pdf/2505.21972", "abs": "https://arxiv.org/abs/2505.21972", "authors": ["Patrick Vossler", "Fan Xia", "Yifan Mai", "Jean Feng"], "title": "Judging LLMs on a Simplex", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "28 pages, 7 figures", "summary": "Automated evaluation of free-form outputs from large language models (LLMs)\nis challenging because many distinct answers can be equally valid. A common\npractice is to use LLMs themselves as judges, but the theoretical properties of\nthis approach are not yet well understood. We show that a geometric framework\nthat represents both judges and candidates as points on a probability simplex\ncan provide helpful insight on what is or is not identifiable using LLM judges.\nOur theoretical analysis uncovers a \"phase transition\" in ranking\nidentifiability: for binary scoring systems, true rankings are identifiable\neven with weak judges under mild assumptions, while rankings become\nnon-identifiable for three or more scoring levels even with infinite data,\nabsent additional prior knowledge. This non-identifiability highlights how\nuncertainty in rankings stems from not only aleatoric uncertainty (i.e.,\ninherent stochasticity in the data) but also epistemic uncertainty regarding\nwhich assumptions hold, an aspect that has received limited attention until\nnow. To integrate both types of uncertainty, we use Bayesian inference to\nencode assumptions as priors and conduct sensitivity analysis of ranking\nestimates and credible intervals. Empirical evaluations across multiple\nbenchmarks demonstrate that Bayesian inference yields more accurate rankings\nand substantially improves coverage rates. These results underscore the\nimportance of taking a more holistic approach to uncertainty quantification\nwhen using LLMs as judges."}
{"id": "2505.22617", "pdf": "https://arxiv.org/pdf/2505.22617", "abs": "https://arxiv.org/abs/2505.22617", "authors": ["Ganqu Cui", "Yuchen Zhang", "Jiacheng Chen", "Lifan Yuan", "Zhi Wang", "Yuxin Zuo", "Haozhan Li", "Yuchen Fan", "Huayu Chen", "Weize Chen", "Zhiyuan Liu", "Hao Peng", "Lei Bai", "Wanli Ouyang", "Yu Cheng", "Bowen Zhou", "Ning Ding"], "title": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "This paper aims to overcome a major obstacle in scaling RL for reasoning with\nLLMs, namely the collapse of policy entropy. Such phenomenon is consistently\nobserved across vast RL runs without entropy intervention, where the policy\nentropy dropped sharply at the early training stage, this diminished\nexploratory ability is always accompanied with the saturation of policy\nperformance. In practice, we establish a transformation equation R=-a*e^H+b\nbetween entropy H and downstream performance R. This empirical law strongly\nindicates that, the policy performance is traded from policy entropy, thus\nbottlenecked by its exhaustion, and the ceiling is fully predictable H=0,\nR=-a+b. Our finding necessitates entropy management for continuous exploration\ntoward scaling compute for RL. To this end, we investigate entropy dynamics\nboth theoretically and empirically. Our derivation highlights that, the change\nin policy entropy is driven by the covariance between action probability and\nthe change in logits, which is proportional to its advantage when using Policy\nGradient-like algorithms. Empirical study shows that, the values of covariance\nterm and entropy differences matched exactly, supporting the theoretical\nconclusion. Moreover, the covariance term stays mostly positive throughout\ntraining, further explaining why policy entropy would decrease monotonically.\nThrough understanding the mechanism behind entropy dynamics, we motivate to\ncontrol entropy by restricting the update of high-covariance tokens.\nSpecifically, we propose two simple yet effective techniques, namely Clip-Cov\nand KL-Cov, which clip and apply KL penalty to tokens with high covariances\nrespectively. Experiments show that these methods encourage exploration, thus\nhelping policy escape entropy collapse and achieve better downstream\nperformance."}
{"id": "2505.22555", "pdf": "https://arxiv.org/pdf/2505.22555", "abs": "https://arxiv.org/abs/2505.22555", "authors": ["Yanyi Qu", "Haoyang Ma", "Wenhui Xiong"], "title": "MultiFormer: A Multi-Person Pose Estimation System Based on CSI and Attention Mechanism", "categories": ["cs.CV", "eess.SP"], "comment": null, "summary": "Human pose estimation based on Channel State Information (CSI) has emerged as\na promising approach for non-intrusive and precise human activity monitoring,\nyet faces challenges including accurate multi-person pose recognition and\neffective CSI feature learning. This paper presents MultiFormer, a wireless\nsensing system that accurately estimates human pose through CSI. The proposed\nsystem adopts a Transformer based time-frequency dual-token feature extractor\nwith multi-head self-attention. This feature extractor is able to model\ninter-subcarrier correlations and temporal dependencies of the CSI. The\nextracted CSI features and the pose probability heatmaps are then fused by\nMulti-Stage Feature Fusion Network (MSFN) to enforce the anatomical\nconstraints. Extensive experiments conducted on on the public MM-Fi dataset and\nour self-collected dataset show that the MultiFormer achieves higher accuracy\nover state-of-the-art approaches, especially for high-mobility keypoints\n(wrists, elbows) that are particularly difficult for previous methods to\naccurately estimate."}
{"id": "2505.21981", "pdf": "https://arxiv.org/pdf/2505.21981", "abs": "https://arxiv.org/abs/2505.21981", "authors": ["Weiyu Liu", "Neil Nie", "Ruohan Zhang", "Jiayuan Mao", "Jiajun Wu"], "title": "Learning Compositional Behaviors from Demonstration and Language", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "comment": "Presented at CoRL 2024 and as an Oral Presentation at the 2024 CoRL\n  LEAP Workshop. The first two authors contributed equally. The last two\n  authors jointly advised the project. For videos and additional results,\n  visit: https://blade-bot.github.io/", "summary": "We introduce Behavior from Language and Demonstration (BLADE), a framework\nfor long-horizon robotic manipulation by integrating imitation learning and\nmodel-based planning. BLADE leverages language-annotated demonstrations,\nextracts abstract action knowledge from large language models (LLMs), and\nconstructs a library of structured, high-level action representations. These\nrepresentations include preconditions and effects grounded in visual perception\nfor each high-level action, along with corresponding controllers implemented as\nneural network-based policies. BLADE can recover such structured\nrepresentations automatically, without manually labeled states or symbolic\ndefinitions. BLADE shows significant capabilities in generalizing to novel\nsituations, including novel initial states, external state perturbations, and\nnovel goals. We validate the effectiveness of our approach both in simulation\nand on real robots with a diverse set of objects with articulated parts,\npartial observability, and geometric constraints."}
{"id": "2505.22651", "pdf": "https://arxiv.org/pdf/2505.22651", "abs": "https://arxiv.org/abs/2505.22651", "authors": ["Yi Ding", "Ruqi Zhang"], "title": "Sherlock: Self-Correcting Reasoning in Vision-Language Models", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "27 pages", "summary": "Reasoning Vision-Language Models (VLMs) have shown promising performance on\ncomplex multimodal tasks. However, they still face significant challenges: they\nare highly sensitive to reasoning errors, require large volumes of annotated\ndata or accurate verifiers, and struggle to generalize beyond specific domains.\nTo address these limitations, we explore self-correction as a strategy to\nenhance reasoning VLMs. We first conduct an in-depth analysis of reasoning\nVLMs' self-correction abilities and identify key gaps. Based on our findings,\nwe introduce Sherlock, a self-correction and self-improvement training\nframework. Sherlock introduces a trajectory-level self-correction objective, a\npreference data construction method based on visual perturbation, and a dynamic\n$\\beta$ for preference tuning. Once the model acquires self-correction\ncapabilities using only 20k randomly sampled annotated data, it continues to\nself-improve without external supervision. Built on the Llama3.2-Vision-11B\nmodel, Sherlock achieves remarkable results across eight benchmarks, reaching\nan average accuracy of 64.1 with direct generation and 65.4 after\nself-correction. It outperforms LLaVA-CoT (63.2), Mulberry (63.9), and\nLlamaV-o1 (63.4) while using less than 20% of the annotated data."}
{"id": "2505.22564", "pdf": "https://arxiv.org/pdf/2505.22564", "abs": "https://arxiv.org/abs/2505.22564", "authors": ["Jaehyun Choi", "Jiwan Hur", "Gyojin Han", "Jaemyung Yu", "Junmo Kim"], "title": "PRISM: Video Dataset Condensation with Progressive Refinement and Insertion for Sparse Motion", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Video dataset condensation has emerged as a critical technique for addressing\nthe computational challenges associated with large-scale video data processing\nin deep learning applications. While significant progress has been made in\nimage dataset condensation, the video domain presents unique challenges due to\nthe complex interplay between spatial content and temporal dynamics. This paper\nintroduces PRISM, Progressive Refinement and Insertion for Sparse Motion, for\nvideo dataset condensation, a novel approach that fundamentally reconsiders how\nvideo data should be condensed. Unlike the previous method that separates\nstatic content from dynamic motion, our method preserves the essential\ninterdependence between these elements. Our approach progressively refines and\ninserts frames to fully accommodate the motion in an action while achieving\nbetter performance but less storage, considering the relation of gradients for\neach frame. Extensive experiments across standard video action recognition\nbenchmarks demonstrate that PRISM outperforms existing disentangled approaches\nwhile maintaining compact representations suitable for resource-constrained\nenvironments."}
{"id": "2505.21985", "pdf": "https://arxiv.org/pdf/2505.21985", "abs": "https://arxiv.org/abs/2505.21985", "authors": ["Naoto Yoshida", "Tadahiro Taniguchi"], "title": "Reward-Independent Messaging for Decentralized Multi-Agent Reinforcement Learning", "categories": ["cs.MA", "cs.AI", "cs.LG"], "comment": null, "summary": "In multi-agent reinforcement learning (MARL), effective communication\nimproves agent performance, particularly under partial observability. We\npropose MARL-CPC, a framework that enables communication among fully\ndecentralized, independent agents without parameter sharing. MARL-CPC\nincorporates a message learning model based on collective predictive coding\n(CPC) from emergent communication research. Unlike conventional methods that\ntreat messages as part of the action space and assume cooperation, MARL-CPC\nlinks messages to state inference, supporting communication in non-cooperative,\nreward-independent settings. We introduce two algorithms -Bandit-CPC and\nIPPO-CPC- and evaluate them in non-cooperative MARL tasks. Benchmarks show that\nboth outperform standard message-as-action approaches, establishing effective\ncommunication even when messages offer no direct benefit to the sender. These\nresults highlight MARL-CPC's potential for enabling coordination in complex,\ndecentralized environments."}
{"id": "2505.22655", "pdf": "https://arxiv.org/pdf/2505.22655", "abs": "https://arxiv.org/abs/2505.22655", "authors": ["Michael Kirchhof", "Gjergji Kasneci", "Enkelejda Kasneci"], "title": "Position: Uncertainty Quantification Needs Reassessment for Large-language Model Agents", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted at ICML 2025", "summary": "Large-language models (LLMs) and chatbot agents are known to provide wrong\noutputs at times, and it was recently found that this can never be fully\nprevented. Hence, uncertainty quantification plays a crucial role, aiming to\nquantify the level of ambiguity in either one overall number or two numbers for\naleatoric and epistemic uncertainty. This position paper argues that this\ntraditional dichotomy of uncertainties is too limited for the open and\ninteractive setup that LLM agents operate in when communicating with a user,\nand that we need to research avenues that enrich uncertainties in this novel\nscenario. We review the literature and find that popular definitions of\naleatoric and epistemic uncertainties directly contradict each other and lose\ntheir meaning in interactive LLM agent settings. Hence, we propose three novel\nresearch directions that focus on uncertainties in such human-computer\ninteractions: Underspecification uncertainties, for when users do not provide\nall information or define the exact task at the first go, interactive learning,\nto ask follow-up questions and reduce the uncertainty about the current\ncontext, and output uncertainties, to utilize the rich language and speech\nspace to express uncertainties as more than mere numbers. We expect that these\nnew ways of dealing with and communicating uncertainties will lead to LLM agent\ninteractions that are more transparent, trustworthy, and intuitive."}
{"id": "2505.22566", "pdf": "https://arxiv.org/pdf/2505.22566", "abs": "https://arxiv.org/abs/2505.22566", "authors": ["Yifan Xie", "Mingyang Li", "Shoujie Li", "Xingting Li", "Guangyu Chen", "Fei Ma", "Fei Richard Yu", "Wenbo Ding"], "title": "Universal Visuo-Tactile Video Understanding for Embodied Interaction", "categories": ["cs.CV", "cs.AI"], "comment": "13 pages, 5 figures", "summary": "Tactile perception is essential for embodied agents to understand physical\nattributes of objects that cannot be determined through visual inspection\nalone. While existing approaches have made progress in visual and language\nmodalities for physical understanding, they fail to effectively incorporate\ntactile information that provides crucial haptic feedback for real-world\ninteraction. In this paper, we present VTV-LLM, the first multi-modal large\nlanguage model for universal Visuo-Tactile Video (VTV) understanding that\nbridges the gap between tactile perception and natural language. To address the\nchallenges of cross-sensor and cross-modal integration, we contribute VTV150K,\na comprehensive dataset comprising 150,000 video frames from 100 diverse\nobjects captured across three different tactile sensors (GelSight Mini, DIGIT,\nand Tac3D), annotated with four fundamental tactile attributes (hardness,\nprotrusion, elasticity, and friction). We develop a novel three-stage training\nparadigm that includes VTV enhancement for robust visuo-tactile representation,\nVTV-text alignment for cross-modal correspondence, and text prompt finetuning\nfor natural language generation. Our framework enables sophisticated tactile\nreasoning capabilities including feature assessment, comparative analysis,\nscenario-based decision making and so on. Experimental evaluations demonstrate\nthat VTV-LLM achieves superior performance in tactile video understanding\ntasks, establishing a foundation for more intuitive human-machine interaction\nin tactile domains."}
{"id": "2505.21996", "pdf": "https://arxiv.org/pdf/2505.21996", "abs": "https://arxiv.org/abs/2505.21996", "authors": ["Taiye Chen", "Xun Hu", "Zihan Ding", "Chi Jin"], "title": "Learning World Models for Interactive Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Foundational world models must be both interactive and preserve\nspatiotemporal coherence for effective future planning with action choices.\nHowever, present models for long video generation have limited inherent world\nmodeling capabilities due to two main challenges: compounding errors and\ninsufficient memory mechanisms. We enhance image-to-video models with\ninteractive capabilities through additional action conditioning and\nautoregressive framework, and reveal that compounding error is inherently\nirreducible in autoregressive video generation, while insufficient memory\nmechanism leads to incoherence of world models. We propose video retrieval\naugmented generation (VRAG) with explicit global state conditioning, which\nsignificantly reduces long-term compounding errors and increases spatiotemporal\nconsistency of world models. In contrast, naive autoregressive generation with\nextended context windows and retrieval-augmented generation prove less\neffective for video generation, primarily due to the limited in-context\nlearning capabilities of current video models. Our work illuminates the\nfundamental challenges in video world models and establishes a comprehensive\nbenchmark for improving video generation models with internal world modeling\ncapabilities."}
{"id": "2505.22657", "pdf": "https://arxiv.org/pdf/2505.22657", "abs": "https://arxiv.org/abs/2505.22657", "authors": ["Wenbo Hu", "Yining Hong", "Yanjun Wang", "Leison Gao", "Zibu Wei", "Xingcheng Yao", "Nanyun Peng", "Yonatan Bitton", "Idan Szpektor", "Kai-Wei Chang"], "title": "3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "demos at: https://3dllm-mem.github.io", "summary": "Humans excel at performing complex tasks by leveraging long-term memory\nacross temporal and spatial experiences. In contrast, current Large Language\nModels (LLMs) struggle to effectively plan and act in dynamic, multi-room 3D\nenvironments. We posit that part of this limitation is due to the lack of\nproper 3D spatial-temporal memory modeling in LLMs. To address this, we first\nintroduce 3DMem-Bench, a comprehensive benchmark comprising over 26,000\ntrajectories and 2,892 embodied tasks, question-answering and captioning,\ndesigned to evaluate an agent's ability to reason over long-term memory in 3D\nenvironments. Second, we propose 3DLLM-Mem, a novel dynamic memory management\nand fusion model for embodied spatial-temporal reasoning and actions in LLMs.\nOur model uses working memory tokens, which represents current observations, as\nqueries to selectively attend to and fuse the most useful spatial and temporal\nfeatures from episodic memory, which stores past observations and interactions.\nOur approach allows the agent to focus on task-relevant information while\nmaintaining memory efficiency in complex, long-horizon environments.\nExperimental results demonstrate that 3DLLM-Mem achieves state-of-the-art\nperformance across various tasks, outperforming the strongest baselines by\n16.5% in success rate on 3DMem-Bench's most challenging in-the-wild embodied\ntasks."}
{"id": "2505.22569", "pdf": "https://arxiv.org/pdf/2505.22569", "abs": "https://arxiv.org/abs/2505.22569", "authors": ["Dmitrii Sorokin", "Maksim Nakhodnov", "Andrey Kuznetsov", "Aibek Alanov"], "title": "ImageReFL: Balancing Quality and Diversity in Human-Aligned Diffusion Models", "categories": ["cs.CV"], "comment": "The source code can be found at\n  https://github.com/ControlGenAI/ImageReFL", "summary": "Recent advances in diffusion models have led to impressive image generation\ncapabilities, but aligning these models with human preferences remains\nchallenging. Reward-based fine-tuning using models trained on human feedback\nimproves alignment but often harms diversity, producing less varied outputs. In\nthis work, we address this trade-off with two contributions. First, we\nintroduce \\textit{combined generation}, a novel sampling strategy that applies\na reward-tuned diffusion model only in the later stages of the generation\nprocess, while preserving the base model for earlier steps. This approach\nmitigates early-stage overfitting and helps retain global structure and\ndiversity. Second, we propose \\textit{ImageReFL}, a fine-tuning method that\nimproves image diversity with minimal loss in quality by training on real\nimages and incorporating multiple regularizers, including diffusion and ReFL\nlosses. Our approach outperforms conventional reward tuning methods on standard\nquality and diversity metrics. A user study further confirms that our method\nbetter balances human preference alignment and visual diversity. The source\ncode can be found at https://github.com/ControlGenAI/ImageReFL ."}
{"id": "2505.22003", "pdf": "https://arxiv.org/pdf/2505.22003", "abs": "https://arxiv.org/abs/2505.22003", "authors": ["Jatin Gupta", "Akhil Sharma", "Saransh Singhania", "Ali Imam Abidi"], "title": "Legal Assist AI: Leveraging Transformer-Based Model for Effective Legal Assistance", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 5 tables, 4 figures. This is a revised version of a preprint\n  previously available at this URL: https://doi.org/10.21203/rs.3.rs-5351879/v1", "summary": "Pursuit of accessible legal assistance in India faces a critical gap, as many\ncitizens struggle to leverage their legal rights due to limited awareness and\naccess to relevant legal information. This paper introduces Legal Assist AI, a\ntransformer-based model designed to bridge this gap by offering effective legal\nassistance through large language models (LLMs). The system retrieves relevant\nlegal information from a curated database and generates accurate responses,\nenabling effective assistance for diverse users, including legal professionals,\nscholars, and the general public. The model was fine-tuned on extensive\ndatasets from the Indian legal domain, including Indian Constitution, Bharatiya\nNyaya Sanhita (BNS), Bharatiya Nagarik Suraksha Sanhita (BNSS) and so forth,\nproviding a robust understanding of the complexities of Indian law. By\nincorporating domain-specific legal datasets, the proposed model demonstrated\nremarkable efficiency and specialization in legal Question-Answering. The model\nwas evaluated against state-of-the-art models such as GPT-3.5 Turbo and Mistral\n7B, achieving a 60.08% score on the AIBE, outperforming its competitors in\nlegal reasoning and accuracy. Unlike other models, Legal Assist AI avoided\ncommon issues such as hallucinations, making it highly reliable for practical\nlegal applications. It showcases the model's applicability in real-world legal\nscenarios, with future iterations aiming to enhance performance and expand its\ndataset to cover a broader range of multilingual and case-specific queries as\nwell."}
{"id": "2505.22581", "pdf": "https://arxiv.org/pdf/2505.22581", "abs": "https://arxiv.org/abs/2505.22581", "authors": ["Kartik Kuckreja", "Parul Gupta", "Injy Hamed", "Thamar Solorio", "Muhammad Haris Khan", "Abhinav Dhall"], "title": "Tell me Habibi, is it Real or Fake?", "categories": ["cs.CV", "cs.AI"], "comment": "9 pages, 2 figures, 12 tables", "summary": "Deepfake generation methods are evolving fast, making fake media harder to\ndetect and raising serious societal concerns. Most deepfake detection and\ndataset creation research focuses on monolingual content, often overlooking the\nchallenges of multilingual and code-switched speech, where multiple languages\nare mixed within the same discourse. Code-switching, especially between Arabic\nand English, is common in the Arab world and is widely used in digital\ncommunication. This linguistic mixing poses extra challenges for deepfake\ndetection, as it can confuse models trained mostly on monolingual data. To\naddress this, we introduce \\textbf{ArEnAV}, the first large-scale\nArabic-English audio-visual deepfake dataset featuring intra-utterance\ncode-switching, dialectal variation, and monolingual Arabic content. It\n\\textbf{contains 387k videos and over 765 hours of real and fake videos}. Our\ndataset is generated using a novel pipeline integrating four Text-To-Speech and\ntwo lip-sync models, enabling comprehensive analysis of multilingual multimodal\ndeepfake detection. We benchmark our dataset against existing monolingual and\nmultilingual datasets, state-of-the-art deepfake detection models, and a human\nevaluation, highlighting its potential to advance deepfake research. The\ndataset can be accessed\n\\href{https://huggingface.co/datasets/kartik060702/ArEnAV-Full}{here}."}
{"id": "2505.22019", "pdf": "https://arxiv.org/pdf/2505.22019", "abs": "https://arxiv.org/abs/2505.22019", "authors": ["Qiuchen Wang", "Ruixue Ding", "Yu Zeng", "Zehui Chen", "Lin Chen", "Shihang Wang", "Pengjun Xie", "Fei Huang", "Feng Zhao"], "title": "VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Effectively retrieving, reasoning and understanding visually rich information\nremains a challenge for RAG methods. Traditional text-based methods cannot\nhandle visual-related information. On the other hand, current vision-based RAG\napproaches are often limited by fixed pipelines and frequently struggle to\nreason effectively due to the insufficient activation of the fundamental\ncapabilities of models. As RL has been proven to be beneficial for model\nreasoning, we introduce VRAG-RL, a novel RL framework tailored for complex\nreasoning across visually rich information. With this framework, VLMs interact\nwith search engines, autonomously sampling single-turn or multi-turn reasoning\ntrajectories with the help of visual perception tokens and undergoing continual\noptimization based on these samples. Our approach highlights key limitations of\nRL in RAG domains: (i) Prior Multi-modal RAG approaches tend to merely\nincorporate images into the context, leading to insufficient reasoning token\nallocation and neglecting visual-specific perception; and (ii) When models\ninteract with search engines, their queries often fail to retrieve relevant\ninformation due to the inability to articulate requirements, thereby leading to\nsuboptimal performance. To address these challenges, we define an action space\ntailored for visually rich inputs, with actions including cropping and scaling,\nallowing the model to gather information from a coarse-to-fine perspective.\nFurthermore, to bridge the gap between users' original inquiries and the\nretriever, we employ a simple yet effective reward that integrates query\nrewriting and retrieval performance with a model-based reward. Our VRAG-RL\noptimizes VLMs for RAG tasks using specially designed RL strategies, aligning\nthe model with real-world applications. The code is available at\n\\hyperlink{https://github.com/Alibaba-NLP/VRAG}{https://github.com/Alibaba-NLP/VRAG}."}
{"id": "2505.22596", "pdf": "https://arxiv.org/pdf/2505.22596", "abs": "https://arxiv.org/abs/2505.22596", "authors": ["Jiaqi Huang", "Zunnan Xu", "Jun Zhou", "Ting Liu", "Yicheng Xiao", "Mingwen Ou", "Bowen Ji", "Xiu Li", "Kehong Yuan"], "title": "SAM-R1: Leveraging SAM for Reward Feedback in Multimodal Segmentation via Reinforcement Learning", "categories": ["cs.CV"], "comment": null, "summary": "Leveraging multimodal large models for image segmentation has become a\nprominent research direction. However, existing approaches typically rely\nheavily on manually annotated datasets that include explicit reasoning\nprocesses, which are costly and time-consuming to produce. Recent advances\nsuggest that reinforcement learning (RL) can endow large models with reasoning\ncapabilities without requiring such reasoning-annotated data. In this paper, we\npropose SAM-R1, a novel framework that enables multimodal large models to\nperform fine-grained reasoning in image understanding tasks. Our approach is\nthe first to incorporate fine-grained segmentation settings during the training\nof multimodal reasoning models. By integrating task-specific, fine-grained\nrewards with a tailored optimization objective, we further enhance the model's\nreasoning and segmentation alignment. We also leverage the Segment Anything\nModel (SAM) as a strong and flexible reward provider to guide the learning\nprocess. With only 3k training samples, SAM-R1 achieves strong performance\nacross multiple benchmarks, demonstrating the effectiveness of reinforcement\nlearning in equipping multimodal models with segmentation-oriented reasoning\ncapabilities."}
{"id": "2505.22021", "pdf": "https://arxiv.org/pdf/2505.22021", "abs": "https://arxiv.org/abs/2505.22021", "authors": ["Zhihong Tang", "Yang Li"], "title": "GL-PGENet: A Parameterized Generation Framework for Robust Document Image Enhancement", "categories": ["cs.CV", "cs.AI"], "comment": "12 pages, 7 figures", "summary": "Document Image Enhancement (DIE) serves as a critical component in Document\nAI systems, where its performance substantially determines the effectiveness of\ndownstream tasks. To address the limitations of existing methods confined to\nsingle-degradation restoration or grayscale image processing, we present Global\nwith Local Parametric Generation Enhancement Network (GL-PGENet), a novel\narchitecture designed for multi-degraded color document images, ensuring both\nefficiency and robustness in real-world scenarios. Our solution incorporates\nthree key innovations: First, a hierarchical enhancement framework that\nintegrates global appearance correction with local refinement, enabling\ncoarse-to-fine quality improvement. Second, a Dual-Branch Local-Refine Network\nwith parametric generation mechanisms that replaces conventional direct\nprediction, producing enhanced outputs through learned intermediate parametric\nrepresentations rather than pixel-wise mapping. This approach enhances local\nconsistency while improving model generalization. Finally, a modified NestUNet\narchitecture incorporating dense block to effectively fuse low-level pixel\nfeatures and high-level semantic features, specifically adapted for document\nimage characteristics. In addition, to enhance generalization performance, we\nadopt a two-stage training strategy: large-scale pretraining on a synthetic\ndataset of 500,000+ samples followed by task-specific fine-tuning. Extensive\nexperiments demonstrate the superiority of GL-PGENet, achieving\nstate-of-the-art SSIM scores of 0.7721 on DocUNet and 0.9480 on RealDAE. The\nmodel also exhibits remarkable cross-domain adaptability and maintains\ncomputational efficiency for high-resolution images without performance\ndegradation, confirming its practical utility in real-world scenarios."}
{"id": "2505.22604", "pdf": "https://arxiv.org/pdf/2505.22604", "abs": "https://arxiv.org/abs/2505.22604", "authors": ["Ruixuan Zhang", "He Wang", "Zhengyu Zhao", "Zhiqing Guo", "Xun Yang", "Yunfeng Diao", "Meng Wang"], "title": "Adversarially Robust AI-Generated Image Detection for Free: An Information Theoretic Perspective", "categories": ["cs.CV"], "comment": null, "summary": "Rapid advances in Artificial Intelligence Generated Images (AIGI) have\nfacilitated malicious use, such as forgery and misinformation. Therefore,\nnumerous methods have been proposed to detect fake images. Although such\ndetectors have been proven to be universally vulnerable to adversarial attacks,\ndefenses in this field are scarce. In this paper, we first identify that\nadversarial training (AT), widely regarded as the most effective defense,\nsuffers from performance collapse in AIGI detection. Through an\ninformation-theoretic lens, we further attribute the cause of collapse to\nfeature entanglement, which disrupts the preservation of feature-label mutual\ninformation. Instead, standard detectors show clear feature separation.\nMotivated by this difference, we propose Training-free Robust Detection via\nInformation-theoretic Measures (TRIM), the first training-free adversarial\ndefense for AIGI detection. TRIM builds on standard detectors and quantifies\nfeature shifts using prediction entropy and KL divergence. Extensive\nexperiments across multiple datasets and attacks validate the superiority of\nour TRIM, e.g., outperforming the state-of-the-art defense by 33.88% (28.91%)\non ProGAN (GenImage), while well maintaining original accuracy."}
{"id": "2505.22027", "pdf": "https://arxiv.org/pdf/2505.22027", "abs": "https://arxiv.org/abs/2505.22027", "authors": ["Miika Toikkanen", "June-Woo Kim"], "title": "Improving Respiratory Sound Classification with Architecture-Agnostic Knowledge Distillation from Ensembles", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Respiratory sound datasets are limited in size and quality, making high\nperformance difficult to achieve. Ensemble models help but inevitably increase\ncompute cost at inference time. Soft label training distills knowledge\nefficiently with extra cost only at training. In this study, we explore soft\nlabels for respiratory sound classification as an architecture-agnostic\napproach to distill an ensemble of teacher models into a student model. We\nexamine different variations of our approach and find that even a single\nteacher, identical to the student, considerably improves performance beyond its\nown capability, with optimal gains achieved using only a few teachers. We\nachieve the new state-of-the-art Score of 64.39 on ICHBI, surpassing the\nprevious best by 0.85 and improving average Scores across architectures by more\nthan 1.16. Our results highlight the effectiveness of knowledge distillation\nwith soft labels for respiratory sound classification, regardless of size or\narchitecture."}
{"id": "2505.22613", "pdf": "https://arxiv.org/pdf/2505.22613", "abs": "https://arxiv.org/abs/2505.22613", "authors": ["Yuchi Wang", "Yishuo Cai", "Shuhuai Ren", "Sihan Yang", "Linli Yao", "Yuanxin Liu", "Yuanxing Zhang", "Pengfei Wan", "Xu Sun"], "title": "RICO: Improving Accuracy and Completeness in Image Recaptioning via Visual Reconstruction", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "code: https://github.com/wangyuchi369/RICO", "summary": "Image recaptioning is widely used to generate training datasets with enhanced\nquality for various multimodal tasks. Existing recaptioning methods typically\nrely on powerful multimodal large language models (MLLMs) to enhance textual\ndescriptions, but often suffer from inaccuracies due to hallucinations and\nincompleteness caused by missing fine-grained details. To address these\nlimitations, we propose RICO, a novel framework that refines captions through\nvisual reconstruction. Specifically, we leverage a text-to-image model to\nreconstruct a caption into a reference image, and prompt an MLLM to identify\ndiscrepancies between the original and reconstructed images to refine the\ncaption. This process is performed iteratively, further progressively promoting\nthe generation of more faithful and comprehensive descriptions. To mitigate the\nadditional computational cost induced by the iterative process, we introduce\nRICO-Flash, which learns to generate captions like RICO using DPO. Extensive\nexperiments demonstrate that our approach significantly improves caption\naccuracy and completeness, outperforms most baselines by approximately 10% on\nboth CapsBench and CompreCap. Code released at\nhttps://github.com/wangyuchi369/RICO."}
{"id": "2505.22029", "pdf": "https://arxiv.org/pdf/2505.22029", "abs": "https://arxiv.org/abs/2505.22029", "authors": ["Jinming Zhang", "Xuanru Zhou", "Jiachen Lian", "Shuhe Li", "William Li", "Zoe Ezzes", "Rian Bogley", "Lisa Wauters", "Zachary Miller", "Jet Vonk", "Brittany Morin", "Maria Gorno-Tempini", "Gopala Anumanchipalli"], "title": "Analysis and Evaluation of Synthetic Data Generation in Speech Dysfluency Detection", "categories": ["eess.AS", "cs.AI", "cs.SD"], "comment": "Submitted to Interspeech 2025", "summary": "Speech dysfluency detection is crucial for clinical diagnosis and language\nassessment, but existing methods are limited by the scarcity of high-quality\nannotated data. Although recent advances in TTS model have enabled synthetic\ndysfluency generation, existing synthetic datasets suffer from unnatural\nprosody and limited contextual diversity. To address these limitations, we\npropose LLM-Dys -- the most comprehensive dysfluent speech corpus with\nLLM-enhanced dysfluency simulation. This dataset captures 11 dysfluency\ncategories spanning both word and phoneme levels. Building upon this resource,\nwe improve an end-to-end dysfluency detection framework. Experimental\nvalidation demonstrates state-of-the-art performance. All data, models, and\ncode are open-sourced at https://github.com/Berkeley-Speech-Group/LLM-Dys."}
{"id": "2505.22616", "pdf": "https://arxiv.org/pdf/2505.22616", "abs": "https://arxiv.org/abs/2505.22616", "authors": ["Yezhi Shen", "Qiuchen Zhai", "Fengqing Zhu"], "title": "PS4PRO: Pixel-to-pixel Supervision for Photorealistic Rendering and Optimization", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted to the CVPR 2025 Workshop on Autonomous Driving (WAD)", "summary": "Neural rendering methods have gained significant attention for their ability\nto reconstruct 3D scenes from 2D images. The core idea is to take multiple\nviews as input and optimize the reconstructed scene by minimizing the\nuncertainty in geometry and appearance across the views. However, the\nreconstruction quality is limited by the number of input views. This limitation\nis further pronounced in complex and dynamic scenes, where certain angles of\nobjects are never seen. In this paper, we propose to use video frame\ninterpolation as the data augmentation method for neural rendering.\nFurthermore, we design a lightweight yet high-quality video frame interpolation\nmodel, PS4PRO (Pixel-to-pixel Supervision for Photorealistic Rendering and\nOptimization). PS4PRO is trained on diverse video datasets, implicitly modeling\ncamera movement as well as real-world 3D geometry. Our model performs as an\nimplicit world prior, enriching the photo supervision for 3D reconstruction. By\nleveraging the proposed method, we effectively augment existing datasets for\nneural rendering methods. Our experimental results indicate that our method\nimproves the reconstruction performance on both static and dynamic scenes."}
{"id": "2505.22038", "pdf": "https://arxiv.org/pdf/2505.22038", "abs": "https://arxiv.org/abs/2505.22038", "authors": ["Kaiyuan Li", "Xiaoyue Chen", "Chen Gao", "Yong Li", "Xinlei Chen"], "title": "Balanced Token Pruning: Accelerating Vision Language Models Beyond Local Optimization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) have shown impressive performance across\nmulti-modal tasks by encoding images into thousands of tokens. However, the\nlarge number of image tokens results in significant computational overhead, and\nthe use of dynamic high-resolution inputs further increases this burden.\nPrevious approaches have attempted to reduce the number of image tokens through\ntoken pruning, typically by selecting tokens based on attention scores or image\ntoken diversity. Through empirical studies, we observe that existing methods\noften overlook the joint impact of pruning on both the current layer's output\n(local) and the outputs of subsequent layers (global), leading to suboptimal\npruning decisions. To address this challenge, we propose Balanced Token Pruning\n(BTP), a plug-and-play method for pruning vision tokens. Specifically, our\nmethod utilizes a small calibration set to divide the pruning process into\nmultiple stages. In the early stages, our method emphasizes the impact of\npruning on subsequent layers, whereas in the deeper stages, the focus shifts\ntoward preserving the consistency of local outputs. Extensive experiments\nacross various LVLMs demonstrate the broad effectiveness of our approach on\nmultiple benchmarks. Our method achieves a 78% compression rate while\npreserving 96.7% of the original models' performance on average."}
{"id": "2505.22636", "pdf": "https://arxiv.org/pdf/2505.22636", "abs": "https://arxiv.org/abs/2505.22636", "authors": ["Jixin Zhao", "Shangchen Zhou", "Zhouxia Wang", "Peiqing Yang", "Chen Change Loy"], "title": "ObjectClear: Complete Object Removal via Object-Effect Attention", "categories": ["cs.CV"], "comment": "Project page: https://zjx0101.github.io/projects/ObjectClear/", "summary": "Object removal requires eliminating not only the target object but also its\neffects, such as shadows and reflections. However, diffusion-based inpainting\nmethods often produce artifacts, hallucinate content, alter background, and\nstruggle to remove object effects accurately. To address this challenge, we\nintroduce a new dataset for OBject-Effect Removal, named OBER, which provides\npaired images with and without object effects, along with precise masks for\nboth objects and their associated visual artifacts. The dataset comprises\nhigh-quality captured and simulated data, covering diverse object categories\nand complex multi-object scenes. Building on OBER, we propose a novel\nframework, ObjectClear, which incorporates an object-effect attention mechanism\nto guide the model toward the foreground removal regions by learning attention\nmasks, effectively decoupling foreground removal from background\nreconstruction. Furthermore, the predicted attention map enables an\nattention-guided fusion strategy during inference, greatly preserving\nbackground details. Extensive experiments demonstrate that ObjectClear\noutperforms existing methods, achieving improved object-effect removal quality\nand background fidelity, especially in complex scenarios."}
{"id": "2505.22042", "pdf": "https://arxiv.org/pdf/2505.22042", "abs": "https://arxiv.org/abs/2505.22042", "authors": ["Hao Yang", "Haoxuan Li", "Mengyue Yang", "Xu Chen", "Mingming Gong"], "title": "Estimating the Effects of Sample Training Orders for Large Language Models without Retraining", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The order of training samples plays a crucial role in large language models\n(LLMs), significantly impacting both their external performance and internal\nlearning dynamics. Traditional methods for investigating this effect generally\nrequire retraining the model with various sample orders, which is\ncomputationally infeasible for LLMs. In this work, we improve traditional\nmethods by designing a retraining-free framework. By approximating Adam\noptimizer updates with first- and second-order Taylor expansions and utilizing\nrandom projection methods to store intermediate checkpoints, our framework can\nefficiently estimate model parameters for arbitrary training sample orders.\nNext, we apply our framework to two downstream research problems: (1) Training\ncurriculum design for LLMs -- we base our retraining-free framework to propose\na novel curriculum learning strategy that augments curriculum proposals with\nestimated model performances, enabling more informed sample scheduling. (2)\nLLMs' memorization and generalization effect analysis -- we use our\nretraining-free framework to estimate how the positions of training samples\ninfluence LLMs' capacity for memorization and generalization. We conduct\nextensive experiments to validate the effectiveness of our retraining-free\nframework in reproducing the true model performances, and further demonstrate\nits potential in optimizing LLM training curricula and analyzing the\nmemorization and generalization effects of LLMs."}
{"id": "2505.22643", "pdf": "https://arxiv.org/pdf/2505.22643", "abs": "https://arxiv.org/abs/2505.22643", "authors": ["Dekai Zhu", "Yixuan Hu", "Youquan Liu", "Dongyue Lu", "Lingdong Kong", "Slobodan Ilic"], "title": "SPIRAL: Semantic-Aware Progressive LiDAR Scene Generation", "categories": ["cs.CV"], "comment": null, "summary": "Leveraging recent diffusion models, LiDAR-based large-scale 3D scene\ngeneration has achieved great success. While recent voxel-based approaches can\ngenerate both geometric structures and semantic labels, existing range-view\nmethods are limited to producing unlabeled LiDAR scenes. Relying on pretrained\nsegmentation models to predict the semantic maps often results in suboptimal\ncross-modal consistency. To address this limitation while preserving the\nadvantages of range-view representations, such as computational efficiency and\nsimplified network design, we propose Spiral, a novel range-view LiDAR\ndiffusion model that simultaneously generates depth, reflectance images, and\nsemantic maps. Furthermore, we introduce novel semantic-aware metrics to\nevaluate the quality of the generated labeled range-view data. Experiments on\nthe SemanticKITTI and nuScenes datasets demonstrate that Spiral achieves\nstate-of-the-art performance with the smallest parameter size, outperforming\ntwo-step methods that combine the generative and segmentation models.\nAdditionally, we validate that range images generated by Spiral can be\neffectively used for synthetic data augmentation in the downstream segmentation\ntraining, significantly reducing the labeling effort on LiDAR data."}
{"id": "2505.22067", "pdf": "https://arxiv.org/pdf/2505.22067", "abs": "https://arxiv.org/abs/2505.22067", "authors": ["Xinyu Xia", "Xingjun Ma", "Yunfeng Hu", "Ting Qu", "Hong Chen", "Xun Gong"], "title": "From Failures to Fixes: LLM-Driven Scenario Repair for Self-Evolving Autonomous Driving", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "Ensuring robust and generalizable autonomous driving requires not only broad\nscenario coverage but also efficient repair of failure cases, particularly\nthose related to challenging and safety-critical scenarios. However, existing\nscenario generation and selection methods often lack adaptivity and semantic\nrelevance, limiting their impact on performance improvement. In this paper, we\npropose \\textbf{SERA}, an LLM-powered framework that enables autonomous driving\nsystems to self-evolve by repairing failure cases through targeted scenario\nrecommendation. By analyzing performance logs, SERA identifies failure patterns\nand dynamically retrieves semantically aligned scenarios from a structured\nbank. An LLM-based reflection mechanism further refines these recommendations\nto maximize relevance and diversity. The selected scenarios are used for\nfew-shot fine-tuning, enabling targeted adaptation with minimal data.\nExperiments on the benchmark show that SERA consistently improves key metrics\nacross multiple autonomous driving baselines, demonstrating its effectiveness\nand generalizability under safety-critical conditions."}
{"id": "2505.22647", "pdf": "https://arxiv.org/pdf/2505.22647", "abs": "https://arxiv.org/abs/2505.22647", "authors": ["Zhe Kong", "Feng Gao", "Yong Zhang", "Zhuoliang Kang", "Xiaoming Wei", "Xunliang Cai", "Guanying Chen", "Wenhan Luo"], "title": "Let Them Talk: Audio-Driven Multi-Person Conversational Video Generation", "categories": ["cs.CV"], "comment": "Homepage: https://meigen-ai.github.io/multi-talk Github:\n  https://github.com/MeiGen-AI/MultiTalk", "summary": "Audio-driven human animation methods, such as talking head and talking body\ngeneration, have made remarkable progress in generating synchronized facial\nmovements and appealing visual quality videos. However, existing methods\nprimarily focus on single human animation and struggle with multi-stream audio\ninputs, facing incorrect binding problems between audio and persons.\nAdditionally, they exhibit limitations in instruction-following capabilities.\nTo solve this problem, in this paper, we propose a novel task: Multi-Person\nConversational Video Generation, and introduce a new framework, MultiTalk, to\naddress the challenges during multi-person generation. Specifically, for audio\ninjection, we investigate several schemes and propose the Label Rotary Position\nEmbedding (L-RoPE) method to resolve the audio and person binding problem.\nFurthermore, during training, we observe that partial parameter training and\nmulti-task training are crucial for preserving the instruction-following\nability of the base model. MultiTalk achieves superior performance compared to\nother methods on several datasets, including talking head, talking body, and\nmulti-person datasets, demonstrating the powerful generation capabilities of\nour approach."}
{"id": "2505.22068", "pdf": "https://arxiv.org/pdf/2505.22068", "abs": "https://arxiv.org/abs/2505.22068", "authors": ["Ran Li", "Shimin Di", "Yuchen Liu", "Chen Jing", "Yu Qiu", "Lei Chen"], "title": "Beyond path selection: Better LLMs for Scientific Information Extraction with MimicSFT and Relevance and Rule-induced(R$^2$)GRPO", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Previous study suggest that powerful Large Language Models (LLMs) trained\nwith Reinforcement Learning with Verifiable Rewards (RLVR) only refines\nreasoning path without improving the reasoning capacity in math tasks while\nsupervised-finetuning(SFT) with distillation can. We study this from the view\nof Scientific information extraction (SciIE) where LLMs and reasoning LLMs\nunderperforms small Bert-based models. SciIE require both the reasoning and\nmemorization. We argue that both SFT and RLVR can refine the reasoning path and\nimprove reasoning capacity in a simple way based on SciIE. We propose two-stage\ntraining with 1. MimicSFT, using structured reasoning templates without needing\nhigh-quality chain-of-thought data, 2. R$^2$GRPO with relevance and\nrule-induced rewards. Experiments on scientific IE benchmarks show that both\nmethods can improve the reasoning capacity. R$^2$GRPO with mimicSFT surpasses\nbaseline LLMs and specialized supervised models in relation extraction. Our\ncode is available at https://github.com/ranlislz/R2GRPO."}
{"id": "2505.22651", "pdf": "https://arxiv.org/pdf/2505.22651", "abs": "https://arxiv.org/abs/2505.22651", "authors": ["Yi Ding", "Ruqi Zhang"], "title": "Sherlock: Self-Correcting Reasoning in Vision-Language Models", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "27 pages", "summary": "Reasoning Vision-Language Models (VLMs) have shown promising performance on\ncomplex multimodal tasks. However, they still face significant challenges: they\nare highly sensitive to reasoning errors, require large volumes of annotated\ndata or accurate verifiers, and struggle to generalize beyond specific domains.\nTo address these limitations, we explore self-correction as a strategy to\nenhance reasoning VLMs. We first conduct an in-depth analysis of reasoning\nVLMs' self-correction abilities and identify key gaps. Based on our findings,\nwe introduce Sherlock, a self-correction and self-improvement training\nframework. Sherlock introduces a trajectory-level self-correction objective, a\npreference data construction method based on visual perturbation, and a dynamic\n$\\beta$ for preference tuning. Once the model acquires self-correction\ncapabilities using only 20k randomly sampled annotated data, it continues to\nself-improve without external supervision. Built on the Llama3.2-Vision-11B\nmodel, Sherlock achieves remarkable results across eight benchmarks, reaching\nan average accuracy of 64.1 with direct generation and 65.4 after\nself-correction. It outperforms LLaVA-CoT (63.2), Mulberry (63.9), and\nLlamaV-o1 (63.4) while using less than 20% of the annotated data."}
{"id": "2505.22074", "pdf": "https://arxiv.org/pdf/2505.22074", "abs": "https://arxiv.org/abs/2505.22074", "authors": ["Coşku Can Horuz", "Geoffrey Kasenbacher", "Saya Higuchi", "Sebastian Kairat", "Jendrik Stoltz", "Moritz Pesl", "Bernhard A. Moser", "Christoph Linse", "Thomas Martinetz", "Sebastian Otte"], "title": "The Resurrection of the ReLU", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Modeling sophisticated activation functions within deep learning\narchitectures has evolved into a distinct research direction. Functions such as\nGELU, SELU, and SiLU offer smooth gradients and improved convergence\nproperties, making them popular choices in state-of-the-art models. Despite\nthis trend, the classical ReLU remains appealing due to its simplicity,\ninherent sparsity, and other advantageous topological characteristics. However,\nReLU units are prone to becoming irreversibly inactive - a phenomenon known as\nthe dying ReLU problem - which limits their overall effectiveness. In this\nwork, we introduce surrogate gradient learning for ReLU (SUGAR) as a novel,\nplug-and-play regularizer for deep architectures. SUGAR preserves the standard\nReLU function during the forward pass but replaces its derivative in the\nbackward pass with a smooth surrogate that avoids zeroing out gradients. We\ndemonstrate that SUGAR, when paired with a well-chosen surrogate function,\nsubstantially enhances generalization performance over convolutional network\narchitectures such as VGG-16 and ResNet-18, providing sparser activations while\neffectively resurrecting dead ReLUs. Moreover, we show that even in modern\narchitectures like Conv2NeXt and Swin Transformer - which typically employ GELU\n- substituting these with SUGAR yields competitive and even slightly superior\nperformance. These findings challenge the prevailing notion that advanced\nactivation functions are necessary for optimal performance. Instead, they\nsuggest that the conventional ReLU, particularly with appropriate gradient\nhandling, can serve as a strong, versatile revived classic across a broad range\nof deep learning vision models."}
{"id": "2505.22654", "pdf": "https://arxiv.org/pdf/2505.22654", "abs": "https://arxiv.org/abs/2505.22654", "authors": ["Ce Zhang", "Kaixin Ma", "Tianqing Fang", "Wenhao Yu", "Hongming Zhang", "Zhisong Zhang", "Yaqi Xie", "Katia Sycara", "Haitao Mi", "Dong Yu"], "title": "VScan: Rethinking Visual Token Reduction for Efficient Large Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Recent Large Vision-Language Models (LVLMs) have advanced multi-modal\nunderstanding by incorporating finer-grained visual perception and encoding.\nHowever, such methods incur significant computational costs due to longer\nvisual token sequences, posing challenges for real-time deployment. To mitigate\nthis, prior studies have explored pruning unimportant visual tokens either at\nthe output layer of the visual encoder or at the early layers of the language\nmodel. In this work, we revisit these design choices and reassess their\neffectiveness through comprehensive empirical studies of how visual tokens are\nprocessed throughout the visual encoding and language decoding stages. Guided\nby these insights, we propose VScan, a two-stage visual token reduction\nframework that addresses token redundancy by: (1) integrating complementary\nglobal and local scans with token merging during visual encoding, and (2)\nintroducing pruning at intermediate layers of the language model. Extensive\nexperimental results across four LVLMs validate the effectiveness of VScan in\naccelerating inference and demonstrate its superior performance over current\nstate-of-the-arts on sixteen benchmarks. Notably, when applied to\nLLaVA-NeXT-7B, VScan achieves a 2.91$\\times$ speedup in prefilling and a\n10$\\times$ reduction in FLOPs, while retaining 95.4% of the original\nperformance."}
{"id": "2505.22086", "pdf": "https://arxiv.org/pdf/2505.22086", "abs": "https://arxiv.org/abs/2505.22086", "authors": ["Runkai Li", "Jia Xiong", "Xi Wang"], "title": "iDSE: Navigating Design Space Exploration in High-Level Synthesis Using LLMs", "categories": ["cs.AR", "cs.AI"], "comment": null, "summary": "High-Level Synthesis (HLS) serves as an agile hardware development tool that\nstreamlines the circuit design by abstracting the register transfer level into\nbehavioral descriptions, while allowing designers to customize the generated\nmicroarchitectures through optimization directives. However, the combinatorial\nexplosion of possible directive configurations yields an intractable design\nspace. Traditional design space exploration (DSE) methods, despite adopting\nheuristics or constructing predictive models to accelerate Pareto-optimal\ndesign acquisition, still suffer from prohibitive exploration costs and\nsuboptimal results. Addressing these concerns, we introduce iDSE, the first\nLLM-aided DSE framework that leverages HLS design quality perception to\neffectively navigate the design space. iDSE intelligently pruns the design\nspace to guide LLMs in calibrating representative initial sampling designs,\nexpediting convergence toward the Pareto front. By exploiting the convergent\nand divergent thinking patterns inherent in LLMs for hardware optimization,\niDSE achieves multi-path refinement of the design quality and diversity.\nExtensive experiments demonstrate that iDSE outperforms heuristic-based DSE\nmethods by 5.1$\\times$$\\sim$16.6$\\times$ in proximity to the reference Pareto\nfront, matching NSGA-II with only 4.6% of the explored designs. Our work\ndemonstrates the transformative potential of LLMs in scalable and efficient HLS\ndesign optimization, offering new insights into multiobjective optimization\nchallenges."}
{"id": "2505.22657", "pdf": "https://arxiv.org/pdf/2505.22657", "abs": "https://arxiv.org/abs/2505.22657", "authors": ["Wenbo Hu", "Yining Hong", "Yanjun Wang", "Leison Gao", "Zibu Wei", "Xingcheng Yao", "Nanyun Peng", "Yonatan Bitton", "Idan Szpektor", "Kai-Wei Chang"], "title": "3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "demos at: https://3dllm-mem.github.io", "summary": "Humans excel at performing complex tasks by leveraging long-term memory\nacross temporal and spatial experiences. In contrast, current Large Language\nModels (LLMs) struggle to effectively plan and act in dynamic, multi-room 3D\nenvironments. We posit that part of this limitation is due to the lack of\nproper 3D spatial-temporal memory modeling in LLMs. To address this, we first\nintroduce 3DMem-Bench, a comprehensive benchmark comprising over 26,000\ntrajectories and 2,892 embodied tasks, question-answering and captioning,\ndesigned to evaluate an agent's ability to reason over long-term memory in 3D\nenvironments. Second, we propose 3DLLM-Mem, a novel dynamic memory management\nand fusion model for embodied spatial-temporal reasoning and actions in LLMs.\nOur model uses working memory tokens, which represents current observations, as\nqueries to selectively attend to and fuse the most useful spatial and temporal\nfeatures from episodic memory, which stores past observations and interactions.\nOur approach allows the agent to focus on task-relevant information while\nmaintaining memory efficiency in complex, long-horizon environments.\nExperimental results demonstrate that 3DLLM-Mem achieves state-of-the-art\nperformance across various tasks, outperforming the strongest baselines by\n16.5% in success rate on 3DMem-Bench's most challenging in-the-wild embodied\ntasks."}
{"id": "2505.22093", "pdf": "https://arxiv.org/pdf/2505.22093", "abs": "https://arxiv.org/abs/2505.22093", "authors": ["Santiago Berrezueta-Guzman", "Stephan Krusche", "Stefan Wagner"], "title": "From Coders to Critics: Empowering Students through Peer Assessment in the Age of AI Copilots", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "This is the authors' preprint version of a paper accepted at the 11th\n  International Symposium on Educational Technology, to be held in July 2025,\n  in Bangkok, Thailand. The final published version will be available via IEEE\n  Xplore Library", "summary": "The rapid adoption of AI powered coding assistants like ChatGPT and other\ncoding copilots is transforming programming education, raising questions about\nassessment practices, academic integrity, and skill development. As educators\nseek alternatives to traditional grading methods susceptible to AI enabled\nplagiarism, structured peer assessment could be a promising strategy. This\npaper presents an empirical study of a rubric based, anonymized peer review\nprocess implemented in a large introductory programming course.\n  Students evaluated each other's final projects (2D game), and their\nassessments were compared to instructor grades using correlation, mean absolute\nerror, and root mean square error (RMSE). Additionally, reflective surveys from\n47 teams captured student perceptions of fairness, grading behavior, and\npreferences regarding grade aggregation. Results show that peer review can\napproximate instructor evaluation with moderate accuracy and foster student\nengagement, evaluative thinking, and interest in providing good feedback to\ntheir peers. We discuss these findings for designing scalable, trustworthy peer\nassessment systems to face the age of AI assisted coding."}
{"id": "2505.22663", "pdf": "https://arxiv.org/pdf/2505.22663", "abs": "https://arxiv.org/abs/2505.22663", "authors": ["Aimon Rahman", "Kartik Narayan", "Vishal M. Patel"], "title": "Training Free Stylized Abstraction", "categories": ["cs.CV"], "comment": "Project Page: https://kartik-3004.github.io/TF-SA/", "summary": "Stylized abstraction synthesizes visually exaggerated yet semantically\nfaithful representations of subjects, balancing recognizability with perceptual\ndistortion. Unlike image-to-image translation, which prioritizes structural\nfidelity, stylized abstraction demands selective retention of identity cues\nwhile embracing stylistic divergence, especially challenging for\nout-of-distribution individuals. We propose a training-free framework that\ngenerates stylized abstractions from a single image using inference-time\nscaling in vision-language models (VLLMs) to extract identity-relevant\nfeatures, and a novel cross-domain rectified flow inversion strategy that\nreconstructs structure based on style-dependent priors. Our method adapts\nstructural restoration dynamically through style-aware temporal scheduling,\nenabling high-fidelity reconstructions that honor both subject and style. It\nsupports multi-round abstraction-aware generation without fine-tuning. To\nevaluate this task, we introduce StyleBench, a GPT-based human-aligned metric\nsuited for abstract styles where pixel-level similarity fails. Experiments\nacross diverse abstraction (e.g., LEGO, knitted dolls, South Park) show strong\ngeneralization to unseen identities and styles in a fully open-source setup."}
{"id": "2505.22096", "pdf": "https://arxiv.org/pdf/2505.22096", "abs": "https://arxiv.org/abs/2505.22096", "authors": ["Jinheon Baek", "Horst Samulowitz", "Oktie Hassanzadeh", "Dharmashankar Subramanian", "Sola Shirai", "Alfio Gliozzo", "Debarun Bhattacharjya"], "title": "Knowledge Base Construction for Knowledge-Augmented Text-to-SQL", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL Findings 2025", "summary": "Text-to-SQL aims to translate natural language queries into SQL statements,\nwhich is practical as it enables anyone to easily retrieve the desired\ninformation from databases. Recently, many existing approaches tackle this\nproblem with Large Language Models (LLMs), leveraging their strong capability\nin understanding user queries and generating corresponding SQL code. Yet, the\nparametric knowledge in LLMs might be limited to covering all the diverse and\ndomain-specific queries that require grounding in various database schemas,\nwhich makes generated SQLs less accurate oftentimes. To tackle this, we propose\nconstructing the knowledge base for text-to-SQL, a foundational source of\nknowledge, from which we retrieve and generate the necessary knowledge for\ngiven queries. In particular, unlike existing approaches that either manually\nannotate knowledge or generate only a few pieces of knowledge for each query,\nour knowledge base is comprehensive, which is constructed based on a\ncombination of all the available questions and their associated database\nschemas along with their relevant knowledge, and can be reused for unseen\ndatabases from different datasets and domains. We validate our approach on\nmultiple text-to-SQL datasets, considering both the overlapping and\nnon-overlapping database scenarios, where it outperforms relevant baselines\nsubstantially."}
{"id": "2505.22664", "pdf": "https://arxiv.org/pdf/2505.22664", "abs": "https://arxiv.org/abs/2505.22664", "authors": ["Kaiyu Yue", "Vasu Singla", "Menglin Jia", "John Kirchenbauer", "Rifaa Qadri", "Zikui Cai", "Abhinav Bhatele", "Furong Huang", "Tom Goldstein"], "title": "Zero-Shot Vision Encoder Grafting via LLM Surrogates", "categories": ["cs.CV"], "comment": "15 pages", "summary": "Vision language models (VLMs) typically pair a modestly sized vision encoder\nwith a large language model (LLM), e.g., Llama-70B, making the decoder the\nprimary computational burden during training. To reduce costs, a potential\npromising strategy is to first train the vision encoder using a small language\nmodel before transferring it to the large one. We construct small \"surrogate\nmodels\" that share the same embedding space and representation language as the\nlarge target LLM by directly inheriting its shallow layers. Vision encoders\ntrained on the surrogate can then be directly transferred to the larger model,\na process we call zero-shot grafting -- when plugged directly into the\nfull-size target LLM, the grafted pair surpasses the encoder-surrogate pair\nand, on some benchmarks, even performs on par with full decoder training with\nthe target LLM. Furthermore, our surrogate training approach reduces overall\nVLM training costs by ~45% when using Llama-70B as the decoder."}
{"id": "2505.22106", "pdf": "https://arxiv.org/pdf/2505.22106", "abs": "https://arxiv.org/abs/2505.22106", "authors": ["Junqi Zhao", "Jinzheng Zhao", "Haohe Liu", "Yun Chen", "Lu Han", "Xubo Liu", "Mark Plumbley", "Wenwu Wang"], "title": "AudioTurbo: Fast Text-to-Audio Generation with Rectified Diffusion", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "Diffusion models have significantly improved the quality and diversity of\naudio generation but are hindered by slow inference speed. Rectified flow\nenhances inference speed by learning straight-line ordinary differential\nequation (ODE) paths. However, this approach requires training a flow-matching\nmodel from scratch and tends to perform suboptimally, or even poorly, at low\nstep counts. To address the limitations of rectified flow while leveraging the\nadvantages of advanced pre-trained diffusion models, this study integrates\npre-trained models with the rectified diffusion method to improve the\nefficiency of text-to-audio (TTA) generation. Specifically, we propose\nAudioTurbo, which learns first-order ODE paths from deterministic noise sample\npairs generated by a pre-trained TTA model. Experiments on the AudioCaps\ndataset demonstrate that our model, with only 10 sampling steps, outperforms\nprior models and reduces inference to 3 steps compared to a flow-matching-based\nacceleration model."}
{"id": "2505.21523", "pdf": "https://arxiv.org/pdf/2505.21523", "abs": "https://arxiv.org/abs/2505.21523", "authors": ["Chengzhi Liu", "Zhongxing Xu", "Qingyue Wei", "Juncheng Wu", "James Zou", "Xin Eric Wang", "Yuyin Zhou", "Sheng Liu"], "title": "More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Test-time compute has empowered multimodal large language models to generate\nextended reasoning chains, yielding strong performance on tasks such as\nmultimodal math reasoning. However, this improved reasoning ability often comes\nwith increased hallucination: as generations become longer, models tend to\ndrift away from image-grounded content and rely more heavily on language\npriors. Attention analysis shows that longer reasoning chains lead to reduced\nfocus on visual inputs, which contributes to hallucination. To systematically\nstudy this phenomenon, we introduce RH-AUC, a metric that quantifies how a\nmodel's perception accuracy changes with reasoning length, allowing us to\nevaluate whether the model preserves visual grounding during reasoning. We also\nrelease RH-Bench, a diagnostic benchmark that spans a variety of multimodal\ntasks, designed to assess the trade-off between reasoning ability and\nhallucination. Our analysis reveals that (i) larger models typically achieve a\nbetter balance between reasoning and perception, and (ii) this balance is\ninfluenced more by the types and domains of training data than by its overall\nvolume. These findings underscore the importance of evaluation frameworks that\njointly consider both reasoning quality and perceptual fidelity."}
{"id": "2505.22108", "pdf": "https://arxiv.org/pdf/2505.22108", "abs": "https://arxiv.org/abs/2505.22108", "authors": ["Santhosh Parampottupadam", "Melih Coşğun", "Sarthak Pati", "Maximilian Zenk", "Saikat Roy", "Dimitrios Bounias", "Benjamin Hamm", "Sinem Sav", "Ralf Floca", "Klaus Maier-Hein"], "title": "Inclusive, Differentially Private Federated Learning for Clinical Data", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.DC"], "comment": null, "summary": "Federated Learning (FL) offers a promising approach for training clinical AI\nmodels without centralizing sensitive patient data. However, its real-world\nadoption is hindered by challenges related to privacy, resource constraints,\nand compliance. Existing Differential Privacy (DP) approaches often apply\nuniform noise, which disproportionately degrades model performance, even among\nwell-compliant institutions. In this work, we propose a novel compliance-aware\nFL framework that enhances DP by adaptively adjusting noise based on\nquantifiable client compliance scores. Additionally, we introduce a compliance\nscoring tool based on key healthcare and security standards to promote secure,\ninclusive, and equitable participation across diverse clinical settings.\nExtensive experiments on public datasets demonstrate that integrating\nunder-resourced, less compliant clinics with highly regulated institutions\nyields accuracy improvements of up to 15% over traditional FL. This work\nadvances FL by balancing privacy, compliance, and performance, making it a\nviable solution for real-world clinical workflows in global healthcare."}
{"id": "2505.21525", "pdf": "https://arxiv.org/pdf/2505.21525", "abs": "https://arxiv.org/abs/2505.21525", "authors": ["Peiliang Gong", "Yucheng Wang", "Min Wu", "Zhenghua Chen", "Xiaoli Li", "Daoqiang Zhang"], "title": "Temporal Restoration and Spatial Rewiring for Source-Free Multivariate Time Series Domain Adaptation", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained model from\nan annotated source domain to an unlabelled target domain without accessing the\nsource data, thereby preserving data privacy. While existing SFDA methods have\nproven effective in reducing reliance on source data, they struggle to perform\nwell on multivariate time series (MTS) due to their failure to consider the\nintrinsic spatial correlations inherent in MTS data. These spatial correlations\nare crucial for accurately representing MTS data and preserving invariant\ninformation across domains. To address this challenge, we propose Temporal\nRestoration and Spatial Rewiring (TERSE), a novel and concise SFDA method\ntailored for MTS data. Specifically, TERSE comprises a customized\nspatial-temporal feature encoder designed to capture the underlying\nspatial-temporal characteristics, coupled with both temporal restoration and\nspatial rewiring tasks to reinstate latent representations of the temporally\nmasked time series and the spatially masked correlated structures. During the\ntarget adaptation phase, the target encoder is guided to produce spatially and\ntemporally consistent features with the source domain by leveraging the source\npre-trained temporal restoration and spatial rewiring networks. Therefore,\nTERSE can effectively model and transfer spatial-temporal dependencies across\ndomains, facilitating implicit feature alignment. In addition, as the first\napproach to simultaneously consider spatial-temporal consistency in MTS-SFDA,\nTERSE can also be integrated as a versatile plug-and-play module into\nestablished SFDA methods. Extensive experiments on three real-world time series\ndatasets demonstrate the effectiveness and versatility of our approach."}
{"id": "2505.22109", "pdf": "https://arxiv.org/pdf/2505.22109", "abs": "https://arxiv.org/abs/2505.22109", "authors": ["Paul Krzakala", "Gabriel Melo", "Charlotte Laclau", "Florence d'Alché-Buc", "Rémi Flamary"], "title": "The quest for the GRAph Level autoEncoder (GRALE)", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Although graph-based learning has attracted a lot of attention, graph\nrepresentation learning is still a challenging task whose resolution may impact\nkey application fields such as chemistry or biology. To this end, we introduce\nGRALE, a novel graph autoencoder that encodes and decodes graphs of varying\nsizes into a shared embedding space. GRALE is trained using an Optimal\nTransport-inspired loss that compares the original and reconstructed graphs and\nleverages a differentiable node matching module, which is trained jointly with\nthe encoder and decoder. The proposed attention-based architecture relies on\nEvoformer, the core component of AlphaFold, which we extend to support both\ngraph encoding and decoding. We show, in numerical experiments on simulated and\nmolecular data, that GRALE enables a highly general form of pre-training,\napplicable to a wide range of downstream tasks, from classification and\nregression to more complex tasks such as graph interpolation, editing,\nmatching, and prediction."}
{"id": "2505.21530", "pdf": "https://arxiv.org/pdf/2505.21530", "abs": "https://arxiv.org/abs/2505.21530", "authors": ["Xuhang Chen", "Zhuo Li", "Yanyan Shen", "Mufti Mahmud", "Hieu Pham", "Chi-Man Pun", "Shuqiang Wang"], "title": "High-Fidelity Functional Ultrasound Reconstruction via A Visual Auto-Regressive Framework", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Functional ultrasound (fUS) imaging provides exceptional spatiotemporal\nresolution for neurovascular mapping, yet its practical application is\nsignificantly hampered by critical challenges. Foremost among these are data\nscarcity, arising from ethical considerations and signal degradation through\nthe cranium, which collectively limit dataset diversity and compromise the\nfairness of downstream machine learning models."}
{"id": "2505.22116", "pdf": "https://arxiv.org/pdf/2505.22116", "abs": "https://arxiv.org/abs/2505.22116", "authors": ["Jintao Zhang", "Zirui Liu", "Mingyue Cheng", "Shilong Zhang", "Tingyue Pan", "Qi Liu", "Yanhu Xie"], "title": "Multimodal Forecasting of Sparse Intraoperative Hypotension Events Powered by Language Model", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Intraoperative hypotension (IOH) frequently occurs under general anesthesia\nand is strongly linked to adverse outcomes such as myocardial injury and\nincreased mortality. Despite its significance, IOH prediction is hindered by\nevent sparsity and the challenge of integrating static and dynamic data across\ndiverse patients. In this paper, we propose \\textbf{IOHFuseLM}, a multimodal\nlanguage model framework. To accurately identify and differentiate sparse\nhypotensive events, we leverage a two-stage training strategy. The first stage\ninvolves domain adaptive pretraining on IOH physiological time series augmented\nthrough diffusion methods, thereby enhancing the model sensitivity to patterns\nassociated with hypotension. Subsequently, task fine-tuning is performed on the\noriginal clinical dataset to further enhance the ability to distinguish\nnormotensive from hypotensive states. To enable multimodal fusion for each\npatient, we align structured clinical descriptions with the corresponding\nphysiological time series at the token level. Such alignment enables the model\nto capture individualized temporal patterns alongside their corresponding\nclinical semantics. In addition, we convert static patient attributes into\nstructured text to enrich personalized information. Experimental evaluations on\ntwo intraoperative datasets demonstrate that IOHFuseLM outperforms established\nbaselines in accurately identifying IOH events, highlighting its applicability\nin clinical decision support scenarios. Our code is publicly available to\npromote reproducibility at https://github.com/zjt-gpu/IOHFuseLM."}
{"id": "2505.21546", "pdf": "https://arxiv.org/pdf/2505.21546", "abs": "https://arxiv.org/abs/2505.21546", "authors": ["Sajal Chakroborty", "Suddhasattwa Das"], "title": "Image denoising as a conditional expectation", "categories": ["eess.IV", "cs.CV", "math.OC"], "comment": null, "summary": "All techniques for denoising involve a notion of a true (noise-free) image,\nand a hypothesis space. The hypothesis space may reconstruct the image directly\nas a grayscale valued function, or indirectly by its Fourier or wavelet\nspectrum. Most common techniques estimate the true image as a projection to\nsome subspace. We propose an interpretation of a noisy image as a collection of\nsamples drawn from a certain probability space. Within this interpretation,\nprojection based approaches are not guaranteed to be unbiased and convergent.\nWe present a data-driven denoising method in which the true image is recovered\nas a conditional expectation. Although the probability space is unknown\napriori, integrals on this space can be estimated by kernel integral operators.\nThe true image is reformulated as the least squares solution to a linear\nequation in a reproducing kernel Hilbert space (RKHS), and involving various\nkernel integral operators as linear transforms. Assuming the true image to be a\ncontinuous function on a compact planar domain, the technique is shown to be\nconvergent as the number of pixels goes to infinity. We also show that for a\npicture with finite number of pixels, the convergence result can be used to\nchoose the various parameters for an optimum denoising result."}
{"id": "2505.22125", "pdf": "https://arxiv.org/pdf/2505.22125", "abs": "https://arxiv.org/abs/2505.22125", "authors": ["Melrose Tia", "Jezreel Sophia Lanuzo", "Lei Rigi Baltazar", "Marie Joy Lopez-Relente", "Diwa Malaya Quiñones", "Jason Albia"], "title": "Sentiment Simulation using Generative AI Agents", "categories": ["cs.MA", "cs.AI", "cs.CY", "I.2; I.6; J.4"], "comment": "18 pages, 10 figures", "summary": "Traditional sentiment analysis relies on surface-level linguistic patterns\nand retrospective data, limiting its ability to capture the psychological and\ncontextual drivers of human sentiment. These limitations constrain its\neffectiveness in applications that require predictive insight, such as policy\ntesting, narrative framing, and behavioral forecasting. We present a robust\nframework for sentiment simulation using generative AI agents embedded with\npsychologically rich profiles. Agents are instantiated from a nationally\nrepresentative survey of 2,485 Filipino respondents, combining sociodemographic\ninformation with validated constructs of personality traits, values, beliefs,\nand socio-political attitudes. The framework includes three stages: (1) agent\nembodiment via categorical or contextualized encodings, (2) exposure to\nreal-world political and economic scenarios, and (3) generation of sentiment\nratings accompanied by explanatory rationales. Using Quadratic Weighted\nAccuracy (QWA), we evaluated alignment between agent-generated and human\nresponses. Contextualized encoding achieved 92% alignment in replicating\noriginal survey responses. In sentiment simulation tasks, agents reached\n81%--86% accuracy against ground truth sentiment, with contextualized profile\nencodings significantly outperforming categorical (p < 0.0001, Cohen's d =\n0.70). Simulation results remained consistent across repeated trials\n(+/-0.2--0.5% SD) and resilient to variation in scenario framing (p = 0.9676,\nCohen's d = 0.02). Our findings establish a scalable framework for sentiment\nmodeling through psychographically grounded AI agents. This work signals a\nparadigm shift in sentiment analysis from retrospective classification to\nprospective and dynamic simulation grounded in psychology of sentiment\nformation."}
{"id": "2505.21581", "pdf": "https://arxiv.org/pdf/2505.21581", "abs": "https://arxiv.org/abs/2505.21581", "authors": ["Zhennan Wang", "Jianing Teng", "Canqun Xiang", "Kangliang Chen", "Xing Pan", "Lu Deng", "Weihao Gu"], "title": "CogAD: Cognitive-Hierarchy Guided End-to-End Autonomous Driving", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "While end-to-end autonomous driving has advanced significantly, prevailing\nmethods remain fundamentally misaligned with human cognitive principles in both\nperception and planning. In this paper, we propose CogAD, a novel end-to-end\nautonomous driving model that emulates the hierarchical cognition mechanisms of\nhuman drivers. CogAD implements dual hierarchical mechanisms: global-to-local\ncontext processing for human-like perception and intent-conditioned multi-mode\ntrajectory generation for cognitively-inspired planning. The proposed method\ndemonstrates three principal advantages: comprehensive environmental\nunderstanding through hierarchical perception, robust planning exploration\nenabled by multi-level planning, and diverse yet reasonable multi-modal\ntrajectory generation facilitated by dual-level uncertainty modeling. Extensive\nexperiments on nuScenes and Bench2Drive demonstrate that CogAD achieves\nstate-of-the-art performance in end-to-end planning, exhibiting particular\nsuperiority in long-tail scenarios and robust generalization to complex\nreal-world driving conditions."}
{"id": "2505.22126", "pdf": "https://arxiv.org/pdf/2505.22126", "abs": "https://arxiv.org/abs/2505.22126", "authors": ["Yifan Chang", "Yukang Feng", "Jianwen Sun", "Jiaxin Ai", "Chuanhao Li", "S. Kevin Zhou", "Kaipeng Zhang"], "title": "SridBench: Benchmark of Scientific Research Illustration Drawing of Image Generation Model", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent years have seen rapid advances in AI-driven image generation. Early\ndiffusion models emphasized perceptual quality, while newer multimodal models\nlike GPT-4o-image integrate high-level reasoning, improving semantic\nunderstanding and structural composition. Scientific illustration generation\nexemplifies this evolution: unlike general image synthesis, it demands accurate\ninterpretation of technical content and transformation of abstract ideas into\nclear, standardized visuals. This task is significantly more\nknowledge-intensive and laborious, often requiring hours of manual work and\nspecialized tools. Automating it in a controllable, intelligent manner would\nprovide substantial practical value. Yet, no benchmark currently exists to\nevaluate AI on this front. To fill this gap, we introduce SridBench, the first\nbenchmark for scientific figure generation. It comprises 1,120 instances\ncurated from leading scientific papers across 13 natural and computer science\ndisciplines, collected via human experts and MLLMs. Each sample is evaluated\nalong six dimensions, including semantic fidelity and structural accuracy.\nExperimental results reveal that even top-tier models like GPT-4o-image lag\nbehind human performance, with common issues in text/visual clarity and\nscientific correctness. These findings highlight the need for more advanced\nreasoning-driven visual generation capabilities."}
{"id": "2505.21592", "pdf": "https://arxiv.org/pdf/2505.21592", "abs": "https://arxiv.org/abs/2505.21592", "authors": ["Ze Chen", "Shaode Yu"], "title": "Taylor expansion-based Kolmogorov-Arnold network for blind image quality assessment", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "under review", "summary": "Kolmogorov-Arnold Network (KAN) has attracted growing interest for its strong\nfunction approximation capability. In our previous work, KAN and its variants\nwere explored in score regression for blind image quality assessment (BIQA).\nHowever, these models encounter challenges when processing high-dimensional\nfeatures, leading to limited performance gains and increased computational\ncost. To address these issues, we propose TaylorKAN that leverages the Taylor\nexpansions as learnable activation functions to enhance local approximation\ncapability. To improve the computational efficiency, network depth reduction\nand feature dimensionality compression are integrated into the TaylorKAN-based\nscore regression pipeline. On five databases (BID, CLIVE, KonIQ, SPAQ, and\nFLIVE) with authentic distortions, extensive experiments demonstrate that\nTaylorKAN consistently outperforms the other KAN-related models, indicating\nthat the local approximation via Taylor expansions is more effective than\nglobal approximation using orthogonal functions. Its generalization capacity is\nvalidated through inter-database experiments. The findings highlight the\npotential of TaylorKAN as an efficient and robust model for high-dimensional\nscore regression."}
{"id": "2505.22128", "pdf": "https://arxiv.org/pdf/2505.22128", "abs": "https://arxiv.org/abs/2505.22128", "authors": ["Alejandro D. Mousist"], "title": "Real-Time Blind Defocus Deblurring for Earth Observation: The IMAGIN-e Mission Approach", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This work addresses mechanical defocus in Earth observation images from the\nIMAGIN-e mission aboard the ISS, proposing a blind deblurring approach adapted\nto space-based edge computing constraints. Leveraging Sentinel-2 data, our\nmethod estimates the defocus kernel and trains a restoration model within a GAN\nframework, effectively operating without reference images.\n  On Sentinel-2 images with synthetic degradation, SSIM improved by 72.47% and\nPSNR by 25.00%, confirming the model's ability to recover lost details when the\noriginal clean image is known. On IMAGIN-e, where no reference images exist,\nperceptual quality metrics indicate a substantial enhancement, with NIQE\nimproving by 60.66% and BRISQUE by 48.38%, validating real-world onboard\nrestoration. The approach is currently deployed aboard the IMAGIN-e mission,\ndemonstrating its practical application in an operational space environment.\n  By efficiently handling high-resolution images under edge computing\nconstraints, the method enables applications such as water body segmentation\nand contour detection while maintaining processing viability despite resource\nlimitations."}
{"id": "2505.21597", "pdf": "https://arxiv.org/pdf/2505.21597", "abs": "https://arxiv.org/abs/2505.21597", "authors": ["Abdullah Al Mamun", "Pollob Chandra Ray", "Md Rahat Ul Nasib", "Akash Das", "Jia Uddin", "Md Nurul Absur"], "title": "Optimizing Deep Learning for Skin Cancer Classification: A Computationally Efficient CNN with Minimal Accuracy Trade-Off", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "6 pages, & 7 Images", "summary": "The rapid advancement of deep learning in medical image analysis has greatly\nenhanced the accuracy of skin cancer classification. However, current\nstate-of-the-art models, especially those based on transfer learning like\nResNet50, come with significant computational overhead, rendering them\nimpractical for deployment in resource-constrained environments. This study\nproposes a custom CNN model that achieves a 96.7\\% reduction in parameters\n(from 23.9 million in ResNet50 to 692,000) while maintaining a classification\naccuracy deviation of less than 0.022\\%. Our empirical analysis of the HAM10000\ndataset reveals that although transfer learning models provide a marginal\naccuracy improvement of approximately 0.022\\%, they result in a staggering\n13,216.76\\% increase in FLOPs, considerably raising computational costs and\ninference latency. In contrast, our lightweight CNN architecture, which\nencompasses only 30.04 million FLOPs compared to ResNet50's 4.00 billion,\nsignificantly reduces energy consumption, memory footprint, and inference time.\nThese findings underscore the trade-off between the complexity of deep models\nand their real-world feasibility, positioning our optimized CNN as a practical\nsolution for mobile and edge-based skin cancer diagnostics."}
{"id": "2505.22137", "pdf": "https://arxiv.org/pdf/2505.22137", "abs": "https://arxiv.org/abs/2505.22137", "authors": ["Marc Feger", "Katarina Boland", "Stefan Dietze"], "title": "Limited Generalizability in Argument Mining: State-Of-The-Art Models Learn Datasets, Not Arguments", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "This paper has been accepted to ACL 2025 and will be published after\n  27.07.2025", "summary": "Identifying arguments is a necessary prerequisite for various tasks in\nautomated discourse analysis, particularly within contexts such as political\ndebates, online discussions, and scientific reasoning. In addition to\ntheoretical advances in understanding the constitution of arguments, a\nsignificant body of research has emerged around practical argument mining,\nsupported by a growing number of publicly available datasets. On these\nbenchmarks, BERT-like transformers have consistently performed best,\nreinforcing the belief that such models are broadly applicable across diverse\ncontexts of debate. This study offers the first large-scale re-evaluation of\nsuch state-of-the-art models, with a specific focus on their ability to\ngeneralize in identifying arguments. We evaluate four transformers, three\nstandard and one enhanced with contrastive pre-training for better\ngeneralization, on 17 English sentence-level datasets as most relevant to the\ntask. Our findings show that, to varying degrees, these models tend to rely on\nlexical shortcuts tied to content words, suggesting that apparent progress may\noften be driven by dataset-specific cues rather than true task alignment. While\nthe models achieve strong results on familiar benchmarks, their performance\ndrops markedly when applied to unseen datasets. Nonetheless, incorporating both\ntask-specific pre-training and joint benchmark training proves effective in\nenhancing both robustness and generalization."}
{"id": "2505.21620", "pdf": "https://arxiv.org/pdf/2505.21620", "abs": "https://arxiv.org/abs/2505.21620", "authors": ["Zhengyuan Jiang", "Moyang Guo", "Kecen Li", "Yuepeng Hu", "Yupu Wang", "Zhicong Huang", "Cheng Hong", "Neil Zhenqiang Gong"], "title": "VideoMarkBench: Benchmarking Robustness of Video Watermarking", "categories": ["cs.CR", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "The rapid development of video generative models has led to a surge in highly\nrealistic synthetic videos, raising ethical concerns related to disinformation\nand copyright infringement. Recently, video watermarking has been proposed as a\nmitigation strategy by embedding invisible marks into AI-generated videos to\nenable subsequent detection. However, the robustness of existing video\nwatermarking methods against both common and adversarial perturbations remains\nunderexplored. In this work, we introduce VideoMarkBench, the first systematic\nbenchmark designed to evaluate the robustness of video watermarks under\nwatermark removal and watermark forgery attacks. Our study encompasses a\nunified dataset generated by three state-of-the-art video generative models,\nacross three video styles, incorporating four watermarking methods and seven\naggregation strategies used during detection. We comprehensively evaluate 12\ntypes of perturbations under white-box, black-box, and no-box threat models.\nOur findings reveal significant vulnerabilities in current watermarking\napproaches and highlight the urgent need for more robust solutions. Our code is\navailable at https://github.com/zhengyuan-jiang/VideoMarkBench."}
{"id": "2505.22141", "pdf": "https://arxiv.org/pdf/2505.22141", "abs": "https://arxiv.org/abs/2505.22141", "authors": ["Guanwen Feng", "Zhiyuan Ma", "Yunan Li", "Junwei Jing", "Jiahao Yang", "Qiguang Miao"], "title": "FaceEditTalker: Interactive Talking Head Generation with Facial Attribute Editing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in audio-driven talking head generation have achieved\nimpressive results in lip synchronization and emotional expression. However,\nthey largely overlook the crucial task of facial attribute editing. This\ncapability is crucial for achieving deep personalization and expanding the\nrange of practical applications, including user-tailored digital avatars,\nengaging online education content, and brand-specific digital customer service.\nIn these key domains, the flexible adjustment of visual attributes-such as\nhairstyle, accessories, and subtle facial features is essential for aligning\nwith user preferences, reflecting diverse brand identities, and adapting to\nvarying contextual demands. In this paper, we present FaceEditTalker, a unified\nframework that enables controllable facial attribute manipulation while\ngenerating high-quality, audio-synchronized talking head videos. Our method\nconsists of two key components: an image feature space editing module, which\nextracts semantic and detail features and allows flexible control over\nattributes like expression, hairstyle, and accessories; and an audio-driven\nvideo generation module, which fuses these edited features with audio-guided\nfacial landmarks to drive a diffusion-based generator. This design ensures\ntemporal coherence, visual fidelity, and identity preservation across frames.\nExtensive experiments on public datasets demonstrate that our method\noutperforms state-of-the-art approaches in lip-sync accuracy, video quality,\nand attribute controllability. Project page:\nhttps://peterfanfan.github.io/FaceEditTalker/"}
{"id": "2505.21634", "pdf": "https://arxiv.org/pdf/2505.21634", "abs": "https://arxiv.org/abs/2505.21634", "authors": ["Chengyu Yang", "Chengjun Liu"], "title": "Laparoscopic Image Desmoking Using the U-Net with New Loss Function and Integrated Differentiable Wiener Filter", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Laparoscopic surgeries often suffer from reduced visual clarity due to the\npresence of surgical smoke originated by surgical instruments, which poses\nsignificant challenges for both surgeons and vision based computer-assisted\ntechnologies. In order to remove the surgical smoke, a novel U-Net deep\nlearning with new loss function and integrated differentiable Wiener filter\n(ULW) method is presented. Specifically, the new loss function integrates the\npixel, structural, and perceptual properties. Thus, the new loss function,\nwhich combines the structural similarity index measure loss, the perceptual\nloss, as well as the mean squared error loss, is able to enhance the quality\nand realism of the reconstructed images. Furthermore, the learnable Wiener\nfilter is capable of effectively modelling the degradation process caused by\nthe surgical smoke. The effectiveness of the proposed ULW method is evaluated\nusing the publicly available paired laparoscopic smoke and smoke-free image\ndataset, which provides reliable benchmarking and quantitative comparisons.\nExperimental results show that the proposed ULW method excels in both visual\nclarity and metric-based evaluation. As a result, the proposed ULW method\noffers a promising solution for real-time enhancement of laparoscopic imagery.\nThe code is available at https://github.com/chengyuyang-njit/ImageDesmoke."}
{"id": "2505.22146", "pdf": "https://arxiv.org/pdf/2505.22146", "abs": "https://arxiv.org/abs/2505.22146", "authors": ["Guangfu Hao", "Haojie Wen", "Liangxuna Guo", "Yang Chen", "Yanchao Bi", "Shan Yu"], "title": "Flexible Tool Selection through Low-dimensional Attribute Alignment of Vision and Language", "categories": ["cs.CV", "cs.AI", "cs.CL", "q-bio.NC"], "comment": null, "summary": "Flexible tool selection reflects a complex cognitive ability that\ndistinguishes humans from other species, yet computational models that capture\nthis ability remain underdeveloped. We developed a framework using\nlow-dimensional attribute representations to bridge visual tool perception and\nlinguistic task understanding. We constructed a comprehensive dataset (ToolNet)\ncontaining 115 common tools labeled with 13 carefully designed attributes\nspanning physical, functional, and psychological properties, paired with\nnatural language scenarios describing tool usage. Visual encoders (ResNet or\nViT) extract attributes from tool images while fine-tuned language models\n(GPT-2, LLaMA, DeepSeek) derive required attributes from task descriptions. Our\napproach achieves 74% accuracy in tool selection tasks-significantly\noutperforming direct tool matching (20%) and smaller multimodal models\n(21%-58%), while approaching performance of much larger models like GPT-4o\n(73%) with substantially fewer parameters. Ablation studies revealed that\nmanipulation-related attributes (graspability, hand-relatedness, elongation)\nconsistently prove most critical across modalities. This work provides a\nparameter-efficient, interpretable solution that mimics human-like tool\ncognition, advancing both cognitive science understanding and practical\napplications in tool selection tasks."}
{"id": "2505.21686", "pdf": "https://arxiv.org/pdf/2505.21686", "abs": "https://arxiv.org/abs/2505.21686", "authors": ["Michele Gallo"], "title": "tenSVD algorithm for compression", "categories": ["stat.CO", "cs.CV", "cs.LG"], "comment": null, "summary": "Tensors provide a robust framework for managing high-dimensional data.\nConsequently, tensor analysis has emerged as an active research area in various\ndomains, including machine learning, signal processing, computer vision, graph\nanalysis, and data mining. This study introduces an efficient image storage\napproach utilizing tensors, aiming to minimize memory to store, bandwidth to\ntransmit and energy to processing. The proposed method organizes original data\ninto a higher-order tensor and applies the Tucker model for compression.\nImplemented in R, this method is compared to a baseline algorithm. The\nevaluation focuses on efficient of algorithm measured in term of computational\ntime and the quality of information preserved, using both simulated and real\ndatasets. A detailed analysis of the results is conducted, employing\nestablished quantitative metrics, with significant attention paid to\nsustainability in terms of energy consumption across algorithms."}
{"id": "2505.22165", "pdf": "https://arxiv.org/pdf/2505.22165", "abs": "https://arxiv.org/abs/2505.22165", "authors": ["Bocheng Li", "Zhujin Gao", "Linli Xu"], "title": "Unifying Continuous and Discrete Text Diffusion with Non-simultaneous Diffusion Processes", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Diffusion models have emerged as a promising approach for text generation,\nwith recent works falling into two main categories: discrete and continuous\ndiffusion models. Discrete diffusion models apply token corruption\nindependently using categorical distributions, allowing for different diffusion\nprogress across tokens but lacking fine-grained control. Continuous diffusion\nmodels map tokens to continuous spaces and apply fine-grained noise, but the\ndiffusion progress is uniform across tokens, limiting their ability to capture\nsemantic nuances. To address these limitations, we propose\n\\textbf{\\underline{N}}on-simultan\\textbf{\\underline{e}}ous\nC\\textbf{\\underline{o}}ntinuous \\textbf{\\underline{Diff}}usion Models\n(NeoDiff), a novel diffusion model that integrates the strengths of both\ndiscrete and continuous approaches. NeoDiff introduces a Poisson diffusion\nprocess for the forward process, enabling a flexible and fine-grained noising\nparadigm, and employs a time predictor for the reverse process to adaptively\nmodulate the denoising progress based on token semantics. Furthermore, NeoDiff\nutilizes an optimized schedule for inference to ensure more precise noise\ncontrol and improved performance. Our approach unifies the theories of discrete\nand continuous diffusion models, offering a more principled and effective\nframework for text generation. Experimental results on several text generation\ntasks demonstrate NeoDiff's superior performance compared to baselines of\nnon-autoregressive continuous and discrete diffusion models, iterative-based\nmethods and autoregressive diffusion-based methods. These results highlight\nNeoDiff's potential as a powerful tool for generating high-quality text and\nadvancing the field of diffusion-based text generation."}
{"id": "2505.21699", "pdf": "https://arxiv.org/pdf/2505.21699", "abs": "https://arxiv.org/abs/2505.21699", "authors": ["Zhengbo Zhou", "Dooman Arefan", "Margarita Zuley", "Jules Sumkin", "Shandong Wu"], "title": "STA-Risk: A Deep Dive of Spatio-Temporal Asymmetries for Breast Cancer Risk Prediction", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Predicting the risk of developing breast cancer is an important clinical tool\nto guide early intervention and tailoring personalized screening strategies.\nEarly risk models have limited performance and recently machine learning-based\nanalysis of mammogram images showed encouraging risk prediction effects. These\nmodels however are limited to the use of a single exam or tend to overlook\nnuanced breast tissue evolvement in spatial and temporal details of\nlongitudinal imaging exams that are indicative of breast cancer risk. In this\npaper, we propose STA-Risk (Spatial and Temporal Asymmetry-based Risk\nPrediction), a novel Transformer-based model that captures fine-grained\nmammographic imaging evolution simultaneously from bilateral and longitudinal\nasymmetries for breast cancer risk prediction. STA-Risk is innovative by the\nside encoding and temporal encoding to learn spatial-temporal asymmetries,\nregulated by a customized asymmetry loss. We performed extensive experiments\nwith two independent mammogram datasets and achieved superior performance than\nfour representative SOTA models for 1- to 5-year future risk prediction. Source\ncodes will be released upon publishing of the paper."}
{"id": "2505.22174", "pdf": "https://arxiv.org/pdf/2505.22174", "abs": "https://arxiv.org/abs/2505.22174", "authors": ["Georgios Amanatidis", "Alexandros Lolos", "Evangelos Markakis", "Victor Turmel"], "title": "Online Fair Division for Personalized $2$-Value Instances", "categories": ["cs.GT", "cs.AI", "cs.MA"], "comment": null, "summary": "We study an online fair division setting, where goods arrive one at a time\nand there is a fixed set of $n$ agents, each of whom has an additive valuation\nfunction over the goods. Once a good appears, the value each agent has for it\nis revealed and it must be allocated immediately and irrevocably to one of the\nagents. It is known that without any assumptions about the values being\nseverely restricted or coming from a distribution, very strong impossibility\nresults hold in this setting. To bypass the latter, we turn our attention to\ninstances where the valuation functions are restricted. In particular, we study\npersonalized $2$-value instances, where there are only two possible values each\nagent may have for each good, possibly different across agents, and we show how\nto obtain worst case guarantees with respect to well-known fairness notions,\nsuch as maximin share fairness and envy-freeness up to one (or two) good(s). We\nsuggest a deterministic algorithm that maintains a $1/(2n-1)$-MMS allocation at\nevery time step and show that this is the best possible any deterministic\nalgorithm can achieve if one cares about every single time step; nevertheless,\neventually the allocation constructed by our algorithm becomes a $1/4$-MMS\nallocation. To achieve this, the algorithm implicitly maintains a fragile\nsystem of priority levels for all agents. Further, we show that, by allowing\nsome limited access to future information, it is possible to have stronger\nresults with less involved approaches. By knowing the values of goods for $n-1$\ntime steps into the future, we design a matching-based algorithm that achieves\nan EF$1$ allocation every $n$ time steps, while always maintaining an EF$2$\nallocation. Finally, we show that our results allow us to get the first\nnontrivial guarantees for additive instances in which the ratio of the maximum\nover the minimum value an agent has for a good is bounded."}
{"id": "2505.21715", "pdf": "https://arxiv.org/pdf/2505.21715", "abs": "https://arxiv.org/abs/2505.21715", "authors": ["Md. Zahid Hossain", "Mustofa Ahmed", "Most. Sharmin Sultana Samu", "Md. Rakibul Islam"], "title": "Privacy-Preserving Chest X-ray Report Generation via Multimodal Federated Learning with ViT and GPT-2", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Preprint, manuscript under-review", "summary": "The automated generation of radiology reports from chest X-ray images holds\nsignificant promise in enhancing diagnostic workflows while preserving patient\nprivacy. Traditional centralized approaches often require sensitive data\ntransfer, posing privacy concerns. To address this, the study proposes a\nMultimodal Federated Learning framework for chest X-ray report generation using\nthe IU-Xray dataset. The system utilizes a Vision Transformer (ViT) as the\nencoder and GPT-2 as the report generator, enabling decentralized training\nwithout sharing raw data. Three Federated Learning (FL) aggregation strategies:\nFedAvg, Krum Aggregation and a novel Loss-aware Federated Averaging (L-FedAvg)\nwere evaluated. Among these, Krum Aggregation demonstrated superior performance\nacross lexical and semantic evaluation metrics such as ROUGE, BLEU, BERTScore\nand RaTEScore. The results show that FL can match or surpass centralized models\nin generating clinically relevant and semantically rich radiology reports. This\nlightweight and privacy-preserving framework paves the way for collaborative\nmedical AI development without compromising data confidentiality."}
{"id": "2505.22179", "pdf": "https://arxiv.org/pdf/2505.22179", "abs": "https://arxiv.org/abs/2505.22179", "authors": ["Yudi Zhang", "Weilin Zhao", "Xu Han", "Tiejun Zhao", "Wang Xu", "Hailong Cao", "Conghui Zhu"], "title": "Speculative Decoding Meets Quantization: Compatibility Evaluation and Hierarchical Framework Design", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "12 pages, 5 figures", "summary": "Speculative decoding and quantization effectively accelerate memory-bound\ninference of large language models. Speculative decoding mitigates the memory\nbandwidth bottleneck by verifying multiple tokens within a single forward pass,\nwhich increases computational effort. Quantization achieves this optimization\nby compressing weights and activations into lower bit-widths and also reduces\ncomputations via low-bit matrix multiplications. To further leverage their\nstrengths, we investigate the integration of these two techniques.\nSurprisingly, experiments applying the advanced speculative decoding method\nEAGLE-2 to various quantized models reveal that the memory benefits from 4-bit\nweight quantization are diminished by the computational load from speculative\ndecoding. Specifically, verifying a tree-style draft incurs significantly more\ntime overhead than a single-token forward pass on 4-bit weight quantized\nmodels. This finding led to our new speculative decoding design: a hierarchical\nframework that employs a small model as an intermediate stage to turn\ntree-style drafts into sequence drafts, leveraging the memory access benefits\nof the target quantized model. Experimental results show that our hierarchical\napproach achieves a 2.78$\\times$ speedup across various tasks for the 4-bit\nweight Llama-3-70B model on an A100 GPU, outperforming EAGLE-2 by 1.31$\\times$.\nCode available at https://github.com/AI9Stars/SpecMQuant."}
{"id": "2505.21874", "pdf": "https://arxiv.org/pdf/2505.21874", "abs": "https://arxiv.org/abs/2505.21874", "authors": ["Ruiguo Yu", "Yiyang Zhang", "Yuan Tian", "Yujie Diao", "Di Jin", "Witold Pedrycz"], "title": "MAMBO-NET: Multi-Causal Aware Modeling Backdoor-Intervention Optimization for Medical Image Segmentation Network", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Medical image segmentation methods generally assume that the process from\nmedical image to segmentation is unbiased, and use neural networks to establish\nconditional probability models to complete the segmentation task. This\nassumption does not consider confusion factors, which can affect medical\nimages, such as complex anatomical variations and imaging modality limitations.\nConfusion factors obfuscate the relevance and causality of medical image\nsegmentation, leading to unsatisfactory segmentation results. To address this\nissue, we propose a multi-causal aware modeling backdoor-intervention\noptimization (MAMBO-NET) network for medical image segmentation. Drawing\ninsights from causal inference, MAMBO-NET utilizes self-modeling with\nmulti-Gaussian distributions to fit the confusion factors and introduce causal\nintervention into the segmentation process. Moreover, we design appropriate\nposterior probability constraints to effectively train the distributions of\nconfusion factors. For the distributions to effectively guide the segmentation\nand mitigate and eliminate the Impact of confusion factors on the segmentation,\nwe introduce classical backdoor intervention techniques and analyze their\nfeasibility in the segmentation task. To evaluate the effectiveness of our\napproach, we conducted extensive experiments on five medical image datasets.\nThe results demonstrate that our method significantly reduces the influence of\nconfusion factors, leading to enhanced segmentation accuracy."}
{"id": "2505.22184", "pdf": "https://arxiv.org/pdf/2505.22184", "abs": "https://arxiv.org/abs/2505.22184", "authors": ["Xuchen Ma", "Jianxiang Yu", "Wenming Shao", "Bo Pang", "Xiang Li"], "title": "Breaking the Cloak! Unveiling Chinese Cloaked Toxicity with Homophone Graph and Toxic Lexicon", "categories": ["cs.CL", "cs.AI"], "comment": "25 pages, 5 figures, 9 tables", "summary": "Social media platforms have experienced a significant rise in toxic content,\nincluding abusive language and discriminatory remarks, presenting growing\nchallenges for content moderation. Some users evade censorship by deliberately\ndisguising toxic words through homophonic cloak, which necessitates the task of\nunveiling cloaked toxicity. Existing methods are mostly designed for English\ntexts, while Chinese cloaked toxicity unveiling has not been solved yet. To\ntackle the issue, we propose C$^2$TU, a novel training-free and prompt-free\nmethod for Chinese cloaked toxic content unveiling. It first employs substring\nmatching to identify candidate toxic words based on Chinese homo-graph and\ntoxic lexicon. Then it filters those candidates that are non-toxic and corrects\ncloaks to be their corresponding toxicities. Specifically, we develop two model\nvariants for filtering, which are based on BERT and LLMs, respectively. For\nLLMs, we address the auto-regressive limitation in computing word occurrence\nprobability and utilize the full semantic contexts of a text sequence to reveal\ncloaked toxic words. Extensive experiments demonstrate that C$^2$TU can achieve\nsuperior performance on two Chinese toxic datasets. In particular, our method\noutperforms the best competitor by up to 71% on the F1 score and 35% on\naccuracy, respectively."}
{"id": "2505.21906", "pdf": "https://arxiv.org/pdf/2505.21906", "abs": "https://arxiv.org/abs/2505.21906", "authors": ["Zhongyi Zhou", "Yichen Zhu", "Junjie Wen", "Chaomin Shen", "Yi Xu"], "title": "Vision-Language-Action Model with Open-World Embodied Reasoning from Pretrained Knowledge", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Project page: https://chatvla-2.github.io/", "summary": "Vision-language-action (VLA) models have emerged as the next generation of\nmodels in robotics. However, despite leveraging powerful pre-trained\nVision-Language Models (VLMs), existing end-to-end VLA systems often lose key\ncapabilities during fine-tuning as the model adapts to specific robotic tasks.\nWe argue that a generalizable VLA model should retain and expand upon the VLM's\ncore competencies: 1) Open-world embodied reasoning - the VLA should inherit\nthe knowledge from VLM, i.e., recognize anything that the VLM can recognize,\ncapable of solving math problems, possessing visual-spatial intelligence, 2)\nReasoning following - effectively translating the open-world reasoning into\nactionable steps for the robot. In this work, we introduce ChatVLA-2, a novel\nmixture-of-expert VLA model coupled with a specialized three-stage training\npipeline designed to preserve the VLM's original strengths while enabling\nactionable reasoning. To validate our approach, we design a math-matching task\nwherein a robot interprets math problems written on a whiteboard and picks\ncorresponding number cards from a table to solve equations. Remarkably, our\nmethod exhibits exceptional mathematical reasoning and OCR capabilities,\ndespite these abilities not being explicitly trained within the VLA.\nFurthermore, we demonstrate that the VLA possesses strong spatial reasoning\nskills, enabling it to interpret novel directional instructions involving\npreviously unseen objects. Overall, our method showcases reasoning and\ncomprehension abilities that significantly surpass state-of-the-art imitation\nlearning methods such as OpenVLA, DexVLA, and pi-zero. This work represents a\nsubstantial advancement toward developing truly generalizable robotic\nfoundation models endowed with robust reasoning capacities."}
{"id": "2505.22193", "pdf": "https://arxiv.org/pdf/2505.22193", "abs": "https://arxiv.org/abs/2505.22193", "authors": ["Marco Parigi", "Stefano Martina", "Francesco Aldo Venturelli", "Filippo Caruso"], "title": "Physics-inspired Generative AI models via real hardware-based noisy quantum diffusion", "categories": ["quant-ph", "cond-mat.dis-nn", "cs.AI", "cs.CV", "cs.LG", "81P68, 81P40, 81P47, 68Q12, 68T07,", "I.2.6; I.3.3; J.2"], "comment": "17 pages, 9 figures. Supplementary materials: 2 pages, 2 figures", "summary": "Quantum Diffusion Models (QDMs) are an emerging paradigm in Generative AI\nthat aims to use quantum properties to improve the performances of their\nclassical counterparts. However, existing algorithms are not easily scalable\ndue to the limitations of near-term quantum devices. Following our previous\nwork on QDMs, here we propose and implement two physics-inspired protocols. In\nthe first, we use the formalism of quantum stochastic walks, showing that a\nspecific interplay of quantum and classical dynamics in the forward process\nproduces statistically more robust models generating sets of MNIST images with\nlower Fr\\'echet Inception Distance (FID) than using totally classical dynamics.\nIn the second approach, we realize an algorithm to generate images by\nexploiting the intrinsic noise of real IBM quantum hardware with only four\nqubits. Our work could be a starting point to pave the way for new scenarios\nfor large-scale algorithms in quantum Generative AI, where quantum noise is\nneither mitigated nor corrected, but instead exploited as a useful resource."}
{"id": "2505.21910", "pdf": "https://arxiv.org/pdf/2505.21910", "abs": "https://arxiv.org/abs/2505.21910", "authors": ["Xianbiao Qi", "Yelin He", "Jiaquan Ye", "Chun-Guang Li", "Bojia Zi", "Xili Dai", "Qin Zou", "Rong Xiao"], "title": "Taming Transformer Without Using Learning Rate Warmup", "categories": ["cs.LG", "cs.CV"], "comment": "This paper is published as a conference paper at ICLR 2025", "summary": "Scaling Transformer to a large scale without using some technical tricks such\nas learning rate warump and using an obviously lower learning rate is an\nextremely challenging task, and is increasingly gaining more attention. In this\npaper, we provide a theoretical analysis for the process of training\nTransformer and reveal the rationale behind the model crash phenomenon in the\ntraining process, termed \\textit{spectral energy concentration} of\n${\\bW_q}^{\\top} \\bW_k$, which is the reason for a malignant entropy collapse,\nwhere ${\\bW_q}$ and $\\bW_k$ are the projection matrices for the query and the\nkey in Transformer, respectively. To remedy this problem, motivated by\n\\textit{Weyl's Inequality}, we present a novel optimization strategy, \\ie,\nmaking the weight updating in successive steps smooth -- if the ratio\n$\\frac{\\sigma_{1}(\\nabla \\bW_t)}{\\sigma_{1}(\\bW_{t-1})}$ is larger than a\nthreshold, we will automatically bound the learning rate to a weighted multiple\nof $\\frac{\\sigma_{1}(\\bW_{t-1})}{\\sigma_{1}(\\nabla \\bW_t)}$, where $\\nabla\n\\bW_t$ is the updating quantity in step $t$. Such an optimization strategy can\nprevent spectral energy concentration to only a few directions, and thus can\navoid malignant entropy collapse which will trigger the model crash. We conduct\nextensive experiments using ViT, Swin-Transformer and GPT, showing that our\noptimization strategy can effectively and stably train these Transformers\nwithout using learning rate warmup."}
{"id": "2505.22199", "pdf": "https://arxiv.org/pdf/2505.22199", "abs": "https://arxiv.org/abs/2505.22199", "authors": ["Xinyue Hu", "Zhibin Duan", "Bo Chen", "Mingyuan Zhou"], "title": "Enhancing Uncertainty Estimation and Interpretability via Bayesian Non-negative Decision Layer", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by The Thirteenth International Conference on Learning\n  Representations (ICLR 2025)", "summary": "Although deep neural networks have demonstrated significant success due to\ntheir powerful expressiveness, most models struggle to meet practical\nrequirements for uncertainty estimation. Concurrently, the entangled nature of\ndeep neural networks leads to a multifaceted problem, where various localized\nexplanation techniques reveal that multiple unrelated features influence the\ndecisions, thereby undermining interpretability. To address these challenges,\nwe develop a Bayesian Non-negative Decision Layer (BNDL), which reformulates\ndeep neural networks as a conditional Bayesian non-negative factor analysis. By\nleveraging stochastic latent variables, the BNDL can model complex dependencies\nand provide robust uncertainty estimation. Moreover, the sparsity and\nnon-negativity of the latent variables encourage the model to learn\ndisentangled representations and decision layers, thereby improving\ninterpretability. We also offer theoretical guarantees that BNDL can achieve\neffective disentangled learning. In addition, we developed a corresponding\nvariational inference method utilizing a Weibull variational inference network\nto approximate the posterior distribution of the latent variables. Our\nexperimental results demonstrate that with enhanced disentanglement\ncapabilities, BNDL not only improves the model's accuracy but also provides\nreliable uncertainty estimation and improved interpretability."}
{"id": "2505.21912", "pdf": "https://arxiv.org/pdf/2505.21912", "abs": "https://arxiv.org/abs/2505.21912", "authors": ["Marvin Limpijankit", "John Kender"], "title": "Detecting Cultural Differences in News Video Thumbnails via Computational Aesthetics", "categories": ["cs.CY", "cs.CV"], "comment": null, "summary": "We propose a two-step approach for detecting differences in the style of\nimages across sources of differing cultural affinity, where images are first\nclustered into finer visual themes based on content before their aesthetic\nfeatures are compared. We test this approach on 2,400 YouTube video thumbnails\ntaken equally from two U.S. and two Chinese YouTube channels, and relating\nequally to COVID-19 and the Ukraine conflict. Our results suggest that while\nChinese thumbnails are less formal and more candid, U.S. channels tend to use\nmore deliberate, proper photographs as thumbnails. In particular, U.S.\nthumbnails are less colorful, more saturated, darker, more finely detailed,\nless symmetric, sparser, less varied, and more up close and personal than\nChinese thumbnails. We suggest that most of these differences reflect cultural\npreferences, and that our methods and observations can serve as a baseline\nagainst which suspected visual propaganda can be computed and compared."}
{"id": "2505.22200", "pdf": "https://arxiv.org/pdf/2505.22200", "abs": "https://arxiv.org/abs/2505.22200", "authors": ["Darshana Saravanan", "Makarand Tapaswi", "Vineet Gandhi"], "title": "Investigating Mechanisms for In-Context Vision Language Binding", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to MIV at CVPRW 2025 (Oral)", "summary": "To understand a prompt, Vision-Language models (VLMs) must perceive the\nimage, comprehend the text, and build associations within and across both\nmodalities. For instance, given an 'image of a red toy car', the model should\nassociate this image to phrases like 'car', 'red toy', 'red object', etc. Feng\nand Steinhardt propose the Binding ID mechanism in LLMs, suggesting that the\nentity and its corresponding attribute tokens share a Binding ID in the model\nactivations. We investigate this for image-text binding in VLMs using a\nsynthetic dataset and task that requires models to associate 3D objects in an\nimage with their descriptions in the text. Our experiments demonstrate that\nVLMs assign a distinct Binding ID to an object's image tokens and its textual\nreferences, enabling in-context association."}
{"id": "2505.21925", "pdf": "https://arxiv.org/pdf/2505.21925", "abs": "https://arxiv.org/abs/2505.21925", "authors": ["Chong Zeng", "Yue Dong", "Pieter Peers", "Hongzhi Wu", "Xin Tong"], "title": "RenderFormer: Transformer-based Neural Rendering of Triangle Meshes with Global Illumination", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "Accepted to SIGGRAPH 2025. Project page:\n  https://microsoft.github.io/renderformer", "summary": "We present RenderFormer, a neural rendering pipeline that directly renders an\nimage from a triangle-based representation of a scene with full global\nillumination effects and that does not require per-scene training or\nfine-tuning. Instead of taking a physics-centric approach to rendering, we\nformulate rendering as a sequence-to-sequence transformation where a sequence\nof tokens representing triangles with reflectance properties is converted to a\nsequence of output tokens representing small patches of pixels. RenderFormer\nfollows a two stage pipeline: a view-independent stage that models\ntriangle-to-triangle light transport, and a view-dependent stage that\ntransforms a token representing a bundle of rays to the corresponding pixel\nvalues guided by the triangle-sequence from the view-independent stage. Both\nstages are based on the transformer architecture and are learned with minimal\nprior constraints. We demonstrate and evaluate RenderFormer on scenes with\nvarying complexity in shape and light transport."}
{"id": "2505.22202", "pdf": "https://arxiv.org/pdf/2505.22202", "abs": "https://arxiv.org/abs/2505.22202", "authors": ["Hyeonbin Hwang", "Byeongguk Jeon", "Seungone Kim", "Jiyeon Kim", "Hoyeon Chang", "Sohee Yang", "Seungpil Won", "Dohaeng Lee", "Youbin Ahn", "Minjoon Seo"], "title": "Let's Predict Sentence by Sentence", "categories": ["cs.CL", "cs.AI"], "comment": "Work In Progress", "summary": "Autoregressive language models (LMs) generate one token at a time, yet human\nreasoning operates over higher-level abstractions - sentences, propositions,\nand concepts. This contrast raises a central question- Can LMs likewise learn\nto reason over structured semantic units rather than raw token sequences? In\nthis work, we investigate whether pretrained LMs can be lifted into such\nabstract reasoning spaces by building on their learned representations. We\npresent a framework that adapts a pretrained token-level LM to operate in\nsentence space by autoregressively predicting continuous embeddings of next\nsentences. We explore two embedding paradigms inspired by classical\nrepresentation learning: 1) semantic embeddings, learned via autoencoding to\npreserve surface meaning; and 2) contextual embeddings, trained via\nnext-sentence prediction to encode anticipatory structure. We evaluate both\nunder two inference regimes: Discretized, which decodes each predicted\nembedding into text before re-encoding; and Continuous, which reasons entirely\nin embedding space for improved efficiency. Across four domains - mathematics,\nlogic, commonsense, and planning - contextual embeddings under continuous\ninference show competitive performance with Chain-of-Thought (CoT) while\nreducing inference-time FLOPs on average by half. We also present early signs\nof scalability and modular adaptation. Finally, to visualize latent\ntrajectories, we introduce SentenceLens, a diagnostic tool that decodes\nintermediate model states into interpretable sentences. Together, our results\nindicate that pretrained LMs can effectively transition to abstract, structured\nreasoning within latent embedding spaces."}
{"id": "2505.21928", "pdf": "https://arxiv.org/pdf/2505.21928", "abs": "https://arxiv.org/abs/2505.21928", "authors": ["Lianghui Zhu", "Xitong Ling", "Minxi Ouyang", "Xiaoping Liu", "Mingxi Fu", "Tian Guan", "Fanglei Fu", "Xuanyu Wang", "Maomao Zeng", "Mingxi Zhu", "Yibo Jin", "Liming Liu", "Song Duan", "Qiming He", "Yizhi Wang", "Luxi Xie", "Houqiang Li", "Yonghong He", "Sufang Tian"], "title": "Subspecialty-Specific Foundation Model for Intelligent Gastrointestinal Pathology", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Gastrointestinal (GI) diseases represent a clinically significant burden,\nnecessitating precise diagnostic approaches to optimize patient outcomes.\nConventional histopathological diagnosis, heavily reliant on the subjective\ninterpretation of pathologists, suffers from limited reproducibility and\ndiagnostic variability. To overcome these limitations and address the lack of\npathology-specific foundation models for GI diseases, we develop Digepath, a\nspecialized foundation model for GI pathology. Our framework introduces a\ndual-phase iterative optimization strategy combining pretraining with\nfine-screening, specifically designed to address the detection of sparsely\ndistributed lesion areas in whole-slide images. Digepath is pretrained on more\nthan 353 million image patches from over 200,000 hematoxylin and eosin-stained\nslides of GI diseases. It attains state-of-the-art performance on 33 out of 34\ntasks related to GI pathology, including pathological diagnosis, molecular\nprediction, gene mutation prediction, and prognosis evaluation, particularly in\ndiagnostically ambiguous cases and resolution-agnostic tissue classification.We\nfurther translate the intelligent screening module for early GI cancer and\nachieve near-perfect 99.6% sensitivity across 9 independent medical\ninstitutions nationwide. The outstanding performance of Digepath highlights its\npotential to bridge critical gaps in histopathological practice. This work not\nonly advances AI-driven precision pathology for GI diseases but also\nestablishes a transferable paradigm for other pathology subspecialties."}
{"id": "2505.22203", "pdf": "https://arxiv.org/pdf/2505.22203", "abs": "https://arxiv.org/abs/2505.22203", "authors": ["Yuzhen Huang", "Weihao Zeng", "Xingshan Zeng", "Qi Zhu", "Junxian He"], "title": "Pitfalls of Rule- and Model-based Verifiers -- A Case Study on Mathematical Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Trustworthy verifiers are essential for the success of reinforcement learning\nwith verifiable reward (RLVR), which is the core methodology behind various\nlarge reasoning models such as DeepSeek-R1. In complex domains like\nmathematical reasoning, rule-based verifiers have been widely adopted in\nprevious works to train strong reasoning models. However, the reliability of\nthese verifiers and their impact on the RL training process remain poorly\nunderstood. In this work, we take mathematical reasoning as a case study and\nconduct a comprehensive analysis of various verifiers in both static evaluation\nand RL training scenarios. First, we find that current open-source rule-based\nverifiers often fail to recognize equivalent answers presented in different\nformats across multiple commonly used mathematical datasets, resulting in\nnon-negligible false negative rates. This limitation adversely affects RL\ntraining performance and becomes more pronounced as the policy model gets\nstronger. Subsequently, we investigate model-based verifiers as a potential\nsolution to address these limitations. While the static evaluation shows that\nmodel-based verifiers achieve significantly higher verification accuracy,\nfurther analysis and RL training results imply that they are highly susceptible\nto hacking, where they misclassify certain patterns in responses as correct\n(i.e., false positives). This vulnerability is exploited during policy model\noptimization, leading to artificially inflated rewards. Our findings underscore\nthe unique risks inherent to both rule-based and model-based verifiers, aiming\nto offer valuable insights to develop more robust reward systems in\nreinforcement learning."}
{"id": "2505.21932", "pdf": "https://arxiv.org/pdf/2505.21932", "abs": "https://arxiv.org/abs/2505.21932", "authors": ["Adriana L. Duncan", "Joe Kileel"], "title": "Higher-Order Group Synchronization", "categories": ["stat.ML", "cs.CV", "cs.LG", "math.CO", "math.OC"], "comment": "40 pages", "summary": "Group synchronization is the problem of determining reliable global estimates\nfrom noisy local measurements on networks. The typical task for group\nsynchronization is to assign elements of a group to the nodes of a graph in a\nway that respects group elements given on the edges which encode information\nabout local pairwise relationships between the nodes. In this paper, we\nintroduce a novel higher-order group synchronization problem which operates on\na hypergraph and seeks to synchronize higher-order local measurements on the\nhyperedges to obtain global estimates on the nodes. Higher-order group\nsynchronization is motivated by applications to computer vision and image\nprocessing, among other computational problems. First, we define the problem of\nhigher-order group synchronization and discuss its mathematical foundations.\nSpecifically, we give necessary and sufficient synchronizability conditions\nwhich establish the importance of cycle consistency in higher-order group\nsynchronization. Then, we propose the first computational framework for general\nhigher-order group synchronization; it acts globally and directly on\nhigher-order measurements using a message passing algorithm. We discuss\ntheoretical guarantees for our framework, including convergence analyses under\noutliers and noise. Finally, we show potential advantages of our method through\nnumerical experiments. In particular, we show that in certain cases our\nhigher-order method applied to rotational and angular synchronization\noutperforms standard pairwise synchronization methods and is more robust to\noutliers. We also show that our method has comparable performance on simulated\ncryo-electron microscopy (cryo-EM) data compared to a standard cryo-EM\nreconstruction package."}
{"id": "2505.22224", "pdf": "https://arxiv.org/pdf/2505.22224", "abs": "https://arxiv.org/abs/2505.22224", "authors": ["Senne Berden", "Ali İrfan Mahmutoğulları", "Dimos Tsouros", "Tias Guns"], "title": "Solver-Free Decision-Focused Learning for Linear Optimization Problems", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Mathematical optimization is a fundamental tool for decision-making in a wide\nrange of applications. However, in many real-world scenarios, the parameters of\nthe optimization problem are not known a priori and must be predicted from\ncontextual features. This gives rise to predict-then-optimize problems, where a\nmachine learning model predicts problem parameters that are then used to make\ndecisions via optimization. A growing body of work on decision-focused learning\n(DFL) addresses this setting by training models specifically to produce\npredictions that maximize downstream decision quality, rather than accuracy.\nWhile effective, DFL is computationally expensive, because it requires solving\nthe optimization problem with the predicted parameters at each loss evaluation.\nIn this work, we address this computational bottleneck for linear optimization\nproblems, a common class of problems in both DFL literature and real-world\napplications. We propose a solver-free training method that exploits the\ngeometric structure of linear optimization to enable efficient training with\nminimal degradation in solution quality. Our method is based on the insight\nthat a solution is optimal if and only if it achieves an objective value that\nis at least as good as that of its adjacent vertices on the feasible polytope.\nBuilding on this, our method compares the estimated quality of the ground-truth\noptimal solution with that of its precomputed adjacent vertices, and uses this\nas loss function. Experiments demonstrate that our method significantly reduces\ncomputational cost while maintaining high decision quality."}
{"id": "2505.21981", "pdf": "https://arxiv.org/pdf/2505.21981", "abs": "https://arxiv.org/abs/2505.21981", "authors": ["Weiyu Liu", "Neil Nie", "Ruohan Zhang", "Jiayuan Mao", "Jiajun Wu"], "title": "Learning Compositional Behaviors from Demonstration and Language", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "comment": "Presented at CoRL 2024 and as an Oral Presentation at the 2024 CoRL\n  LEAP Workshop. The first two authors contributed equally. The last two\n  authors jointly advised the project. For videos and additional results,\n  visit: https://blade-bot.github.io/", "summary": "We introduce Behavior from Language and Demonstration (BLADE), a framework\nfor long-horizon robotic manipulation by integrating imitation learning and\nmodel-based planning. BLADE leverages language-annotated demonstrations,\nextracts abstract action knowledge from large language models (LLMs), and\nconstructs a library of structured, high-level action representations. These\nrepresentations include preconditions and effects grounded in visual perception\nfor each high-level action, along with corresponding controllers implemented as\nneural network-based policies. BLADE can recover such structured\nrepresentations automatically, without manually labeled states or symbolic\ndefinitions. BLADE shows significant capabilities in generalizing to novel\nsituations, including novel initial states, external state perturbations, and\nnovel goals. We validate the effectiveness of our approach both in simulation\nand on real robots with a diverse set of objects with articulated parts,\npartial observability, and geometric constraints."}
{"id": "2505.22232", "pdf": "https://arxiv.org/pdf/2505.22232", "abs": "https://arxiv.org/abs/2505.22232", "authors": ["Mehdi Ali", "Manuel Brack", "Max Lübbering", "Elias Wendt", "Abbas Goher Khan", "Richard Rutmann", "Alex Jude", "Maurice Kraus", "Alexander Arno Weber", "Felix Stollenwerk", "David Kaczér", "Florian Mai", "Lucie Flek", "Rafet Sifa", "Nicolas Flores-Herr", "Joachim Köhler", "Patrick Schramowski", "Michael Fromm", "Kristian Kersting"], "title": "Judging Quality Across Languages: A Multilingual Approach to Pretraining Data Filtering with Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Project page available at https://huggingface.co/spaces/Jackal-AI/JQL", "summary": "High-quality multilingual training data is essential for effectively\npretraining large language models (LLMs). Yet, the availability of suitable\nopen-source multilingual datasets remains limited. Existing state-of-the-art\ndatasets mostly rely on heuristic filtering methods, restricting both their\ncross-lingual transferability and scalability. Here, we introduce JQL, a\nsystematic approach that efficiently curates diverse and high-quality\nmultilingual data at scale while significantly reducing computational demands.\nJQL distills LLMs' annotation capabilities into lightweight annotators based on\npretrained multilingual embeddings. These models exhibit robust multilingual\nand cross-lingual performance, even for languages and scripts unseen during\ntraining. Evaluated empirically across 35 languages, the resulting annotation\npipeline substantially outperforms current heuristic filtering methods like\nFineweb2. JQL notably enhances downstream model training quality and increases\ndata retention rates. Our research provides practical insights and valuable\nresources for multilingual data curation, raising the standards of multilingual\ndataset development."}
{"id": "2505.22006", "pdf": "https://arxiv.org/pdf/2505.22006", "abs": "https://arxiv.org/abs/2505.22006", "authors": ["Changze Qiao", "Mingming Lu"], "title": "Efficiently Enhancing General Agents With Hierarchical-categorical Memory", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "With large language models (LLMs) demonstrating remarkable capabilities,\nthere has been a surge in research on leveraging LLMs to build general-purpose\nmulti-modal agents. However, existing approaches either rely on computationally\nexpensive end-to-end training using large-scale multi-modal data or adopt\ntool-use methods that lack the ability to continuously learn and adapt to new\nenvironments. In this paper, we introduce EHC, a general agent capable of\nlearning without parameter updates. EHC consists of a Hierarchical Memory\nRetrieval (HMR) module and a Task-Category Oriented Experience Learning (TOEL)\nmodule. The HMR module facilitates rapid retrieval of relevant memories and\ncontinuously stores new information without being constrained by memory\ncapacity. The TOEL module enhances the agent's comprehension of various task\ncharacteristics by classifying experiences and extracting patterns across\ndifferent categories. Extensive experiments conducted on multiple standard\ndatasets demonstrate that EHC outperforms existing methods, achieving\nstate-of-the-art performance and underscoring its effectiveness as a general\nagent for handling complex multi-modal tasks."}
{"id": "2505.22264", "pdf": "https://arxiv.org/pdf/2505.22264", "abs": "https://arxiv.org/abs/2505.22264", "authors": ["Maximiliano Hormazábal Lagos", "Álvaro Bueno Saez", "Héctor Cerezo-Costas", "Pedro Alonso Doval", "Jorge Alcalde Vesteiro"], "title": "MRT at SemEval-2025 Task 8: Maximizing Recovery from Tables with Multiple Steps", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "7 pages, 6 tables", "summary": "In this paper we expose our approach to solve the \\textit{SemEval 2025 Task\n8: Question-Answering over Tabular Data} challenge. Our strategy leverages\nPython code generation with LLMs to interact with the table and get the answer\nto the questions. The process is composed of multiple steps: understanding the\ncontent of the table, generating natural language instructions in the form of\nsteps to follow in order to get the answer, translating these instructions to\ncode, running it and handling potential errors or exceptions. These steps use\nopen source LLMs and fine grained optimized prompts for each task (step). With\nthis approach, we achieved a score of $70.50\\%$ for subtask 1."}
{"id": "2505.22019", "pdf": "https://arxiv.org/pdf/2505.22019", "abs": "https://arxiv.org/abs/2505.22019", "authors": ["Qiuchen Wang", "Ruixue Ding", "Yu Zeng", "Zehui Chen", "Lin Chen", "Shihang Wang", "Pengjun Xie", "Fei Huang", "Feng Zhao"], "title": "VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Effectively retrieving, reasoning and understanding visually rich information\nremains a challenge for RAG methods. Traditional text-based methods cannot\nhandle visual-related information. On the other hand, current vision-based RAG\napproaches are often limited by fixed pipelines and frequently struggle to\nreason effectively due to the insufficient activation of the fundamental\ncapabilities of models. As RL has been proven to be beneficial for model\nreasoning, we introduce VRAG-RL, a novel RL framework tailored for complex\nreasoning across visually rich information. With this framework, VLMs interact\nwith search engines, autonomously sampling single-turn or multi-turn reasoning\ntrajectories with the help of visual perception tokens and undergoing continual\noptimization based on these samples. Our approach highlights key limitations of\nRL in RAG domains: (i) Prior Multi-modal RAG approaches tend to merely\nincorporate images into the context, leading to insufficient reasoning token\nallocation and neglecting visual-specific perception; and (ii) When models\ninteract with search engines, their queries often fail to retrieve relevant\ninformation due to the inability to articulate requirements, thereby leading to\nsuboptimal performance. To address these challenges, we define an action space\ntailored for visually rich inputs, with actions including cropping and scaling,\nallowing the model to gather information from a coarse-to-fine perspective.\nFurthermore, to bridge the gap between users' original inquiries and the\nretriever, we employ a simple yet effective reward that integrates query\nrewriting and retrieval performance with a model-based reward. Our VRAG-RL\noptimizes VLMs for RAG tasks using specially designed RL strategies, aligning\nthe model with real-world applications. The code is available at\n\\hyperlink{https://github.com/Alibaba-NLP/VRAG}{https://github.com/Alibaba-NLP/VRAG}."}
{"id": "2505.22271", "pdf": "https://arxiv.org/pdf/2505.22271", "abs": "https://arxiv.org/abs/2505.22271", "authors": ["Yongcan Yu", "Yanbo Wang", "Ran He", "Jian Liang"], "title": "Test-Time Immunization: A Universal Defense Framework Against Jailbreaks for (Multimodal) Large Language Models", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": "Under Review", "summary": "While (multimodal) large language models (LLMs) have attracted widespread\nattention due to their exceptional capabilities, they remain vulnerable to\njailbreak attacks. Various defense methods are proposed to defend against\njailbreak attacks, however, they are often tailored to specific types of\njailbreak attacks, limiting their effectiveness against diverse adversarial\nstrategies. For instance, rephrasing-based defenses are effective against text\nadversarial jailbreaks but fail to counteract image-based attacks. To overcome\nthese limitations, we propose a universal defense framework, termed Test-time\nIMmunization (TIM), which can adaptively defend against various jailbreak\nattacks in a self-evolving way. Specifically, TIM initially trains a gist token\nfor efficient detection, which it subsequently applies to detect jailbreak\nactivities during inference. When jailbreak attempts are identified, TIM\nimplements safety fine-tuning using the detected jailbreak instructions paired\nwith refusal answers. Furthermore, to mitigate potential performance\ndegradation in the detector caused by parameter updates during safety\nfine-tuning, we decouple the fine-tuning process from the detection module.\nExtensive experiments on both LLMs and multimodal LLMs demonstrate the efficacy\nof TIM."}
{"id": "2505.22024", "pdf": "https://arxiv.org/pdf/2505.22024", "abs": "https://arxiv.org/abs/2505.22024", "authors": ["Long-Khanh Pham", "Thanh V. T. Tran", "Minh-Tan Pham", "Van Nguyen"], "title": "RESOUND: Speech Reconstruction from Silent Videos via Acoustic-Semantic Decomposed Modeling", "categories": ["cs.SD", "cs.CV", "eess.AS"], "comment": "accepted in Interspeech 2025", "summary": "Lip-to-speech (L2S) synthesis, which reconstructs speech from visual cues,\nfaces challenges in accuracy and naturalness due to limited supervision in\ncapturing linguistic content, accents, and prosody. In this paper, we propose\nRESOUND, a novel L2S system that generates intelligible and expressive speech\nfrom silent talking face videos. Leveraging source-filter theory, our method\ninvolves two components: an acoustic path to predict prosody and a semantic\npath to extract linguistic features. This separation simplifies learning,\nallowing independent optimization of each representation. Additionally, we\nenhance performance by integrating speech units, a proven unsupervised speech\nrepresentation technique, into waveform generation alongside mel-spectrograms.\nThis allows RESOUND to synthesize prosodic speech while preserving content and\nspeaker identity. Experiments conducted on two standard L2S benchmarks confirm\nthe effectiveness of the proposed method across various metrics."}
{"id": "2505.22280", "pdf": "https://arxiv.org/pdf/2505.22280", "abs": "https://arxiv.org/abs/2505.22280", "authors": ["Zihan Xu", "Haotian Ma", "Gongbo Zhang", "Yihao Ding", "Chunhua Weng", "Yifan Peng"], "title": "Natural Language Processing in Support of Evidence-based Medicine: A Scoping Review", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025 Findings", "summary": "Evidence-based medicine (EBM) is at the forefront of modern healthcare,\nemphasizing the use of the best available scientific evidence to guide clinical\ndecisions. Due to the sheer volume and rapid growth of medical literature and\nthe high cost of curation, there is a critical need to investigate Natural\nLanguage Processing (NLP) methods to identify, appraise, synthesize, summarize,\nand disseminate evidence in EBM. This survey presents an in-depth review of 129\nresearch studies on leveraging NLP for EBM, illustrating its pivotal role in\nenhancing clinical decision-making processes. The paper systematically explores\nhow NLP supports the five fundamental steps of EBM -- Ask, Acquire, Appraise,\nApply, and Assess. The review not only identifies current limitations within\nthe field but also proposes directions for future research, emphasizing the\npotential for NLP to revolutionize EBM by refining evidence extraction,\nevidence synthesis, appraisal, summarization, enhancing data comprehensibility,\nand facilitating a more efficient clinical workflow."}
{"id": "2505.22045", "pdf": "https://arxiv.org/pdf/2505.22045", "abs": "https://arxiv.org/abs/2505.22045", "authors": ["Le Xu", "Chenxing Li", "Yong Ren", "Yujie Chen", "Yu Gu", "Ruibo Fu", "Shan Yang", "Dong Yu"], "title": "Mitigating Audiovisual Mismatch in Visual-Guide Audio Captioning", "categories": ["cs.MM", "cs.CV", "cs.SD", "eess.AS"], "comment": "Accepted by INTERSPEECH 2025", "summary": "Current vision-guided audio captioning systems frequently fail to address\naudiovisual misalignment in real-world scenarios, such as dubbed content or\noff-screen sounds. To bridge this critical gap, we present an entropy-aware\ngated fusion framework that dynamically modulates visual information flow\nthrough cross-modal uncertainty quantification. Our novel approach employs\nattention entropy analysis in cross-attention layers to automatically identify\nand suppress misleading visual cues during modal fusion. Complementing this\narchitecture, we develop a batch-wise audiovisual shuffling technique that\ngenerates synthetic mismatched training pairs, greatly enhancing model\nresilience against alignment noise. Evaluations on the AudioCaps benchmark\ndemonstrate our system's superior performance over existing baselines,\nespecially in mismatched modality scenarios. Furthermore, our solution\ndemonstrates an approximately 6x improvement in inference speed compared to the\nbaseline."}
{"id": "2505.22287", "pdf": "https://arxiv.org/pdf/2505.22287", "abs": "https://arxiv.org/abs/2505.22287", "authors": ["Daniel McDuff", "Tim Korjakow", "Kevin Klyman", "Danish Contractor"], "title": "New Tools are Needed for Tracking Adherence to AI Model Behavioral Use Clauses", "categories": ["cs.CY", "cs.AI"], "comment": "Preprint", "summary": "Foundation models have had a transformative impact on AI. A combination of\nlarge investments in research and development, growing sources of digital data\nfor training, and architectures that scale with data and compute has led to\nmodels with powerful capabilities. Releasing assets is fundamental to\nscientific advancement and commercial enterprise. However, concerns over\nnegligent or malicious uses of AI have led to the design of mechanisms to limit\nthe risks of the technology. The result has been a proliferation of licenses\nwith behavioral-use clauses and acceptable-use-policies that are increasingly\nbeing adopted by commonly used families of models (Llama, Gemma, Deepseek) and\na myriad of smaller projects. We created and deployed a custom AI licenses\ngenerator to facilitate license creation and have quantitatively and\nqualitatively analyzed over 300 customized licenses created with this tool.\nAlongside this we analyzed 1.7 million models licenses on the HuggingFace model\nhub. Our results show increasing adoption of these licenses, interest in tools\nthat support their creation and a convergence on common clause configurations.\nIn this paper we take the position that tools for tracking adoption of, and\nadherence to, these licenses is the natural next step and urgently needed in\norder to ensure they have the desired impact of ensuring responsible use."}
{"id": "2505.22159", "pdf": "https://arxiv.org/pdf/2505.22159", "abs": "https://arxiv.org/abs/2505.22159", "authors": ["Jiawen Yu", "Hairuo Liu", "Qiaojun Yu", "Jieji Ren", "Ce Hao", "Haitong Ding", "Guangyu Huang", "Guofan Huang", "Yan Song", "Panpan Cai", "Cewu Lu", "Wenqiang Zhang"], "title": "ForceVLA: Enhancing VLA Models with a Force-aware MoE for Contact-rich Manipulation", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Vision-Language-Action (VLA) models have advanced general-purpose robotic\nmanipulation by leveraging pretrained visual and linguistic representations.\nHowever, they struggle with contact-rich tasks that require fine-grained\ncontrol involving force, especially under visual occlusion or dynamic\nuncertainty. To address these limitations, we propose \\textbf{ForceVLA}, a\nnovel end-to-end manipulation framework that treats external force sensing as a\nfirst-class modality within VLA systems. ForceVLA introduces \\textbf{FVLMoE}, a\nforce-aware Mixture-of-Experts fusion module that dynamically integrates\npretrained visual-language embeddings with real-time 6-axis force feedback\nduring action decoding. This enables context-aware routing across\nmodality-specific experts, enhancing the robot's ability to adapt to subtle\ncontact dynamics. We also introduce \\textbf{ForceVLA-Data}, a new dataset\ncomprising synchronized vision, proprioception, and force-torque signals across\nfive contact-rich manipulation tasks. ForceVLA improves average task success by\n23.2\\% over strong $\\pi_0$-based baselines, achieving up to 80\\% success in\ntasks such as plug insertion. Our approach highlights the importance of\nmultimodal integration for dexterous manipulation and sets a new benchmark for\nphysically intelligent robotic control. Code and data will be released at\nhttps://sites.google.com/view/forcevla2025."}
{"id": "2505.22291", "pdf": "https://arxiv.org/pdf/2505.22291", "abs": "https://arxiv.org/abs/2505.22291", "authors": ["Saptarshi Neil Sinha", "P. Julius Kuehn", "Johannes Koppe", "Arjan Kuijper", "Michael Weinmann"], "title": "Neural Restoration of Greening Defects in Historical Autochrome Photographs Based on Purely Synthetic Data", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The preservation of early visual arts, particularly color photographs, is\nchallenged by deterioration caused by aging and improper storage, leading to\nissues like blurring, scratches, color bleeding, and fading defects. In this\npaper, we present the first approach for the automatic removal of greening\ncolor defects in digitized autochrome photographs. Our main contributions\ninclude a method based on synthetic dataset generation and the use of\ngenerative AI with a carefully designed loss function for the restoration of\nvisual arts. To address the lack of suitable training datasets for analyzing\ngreening defects in damaged autochromes, we introduce a novel approach for\naccurately simulating such defects in synthetic data. We also propose a\nmodified weighted loss function for the ChaIR method to account for color\nimbalances between defected and non-defected areas. While existing methods\nstruggle with accurately reproducing original colors and may require\nsignificant manual effort, our method allows for efficient restoration with\nreduced time requirements."}
{"id": "2505.22193", "pdf": "https://arxiv.org/pdf/2505.22193", "abs": "https://arxiv.org/abs/2505.22193", "authors": ["Marco Parigi", "Stefano Martina", "Francesco Aldo Venturelli", "Filippo Caruso"], "title": "Physics-inspired Generative AI models via real hardware-based noisy quantum diffusion", "categories": ["quant-ph", "cond-mat.dis-nn", "cs.AI", "cs.CV", "cs.LG", "81P68, 81P40, 81P47, 68Q12, 68T07,", "I.2.6; I.3.3; J.2"], "comment": "17 pages, 9 figures. Supplementary materials: 2 pages, 2 figures", "summary": "Quantum Diffusion Models (QDMs) are an emerging paradigm in Generative AI\nthat aims to use quantum properties to improve the performances of their\nclassical counterparts. However, existing algorithms are not easily scalable\ndue to the limitations of near-term quantum devices. Following our previous\nwork on QDMs, here we propose and implement two physics-inspired protocols. In\nthe first, we use the formalism of quantum stochastic walks, showing that a\nspecific interplay of quantum and classical dynamics in the forward process\nproduces statistically more robust models generating sets of MNIST images with\nlower Fr\\'echet Inception Distance (FID) than using totally classical dynamics.\nIn the second approach, we realize an algorithm to generate images by\nexploiting the intrinsic noise of real IBM quantum hardware with only four\nqubits. Our work could be a starting point to pave the way for new scenarios\nfor large-scale algorithms in quantum Generative AI, where quantum noise is\nneither mitigated nor corrected, but instead exploited as a useful resource."}
{"id": "2505.22303", "pdf": "https://arxiv.org/pdf/2505.22303", "abs": "https://arxiv.org/abs/2505.22303", "authors": ["Grzegorz Wolny", "Michał Szczerbak"], "title": "Voice CMS: updating the knowledge base of a digital assistant through conversation", "categories": ["cs.HC", "cs.AI", "cs.MA"], "comment": null, "summary": "In this study, we propose a solution based on a multi-agent LLM architecture\nand a voice user interface (VUI) designed to update the knowledge base of a\ndigital assistant. Its usability is evaluated in comparison to a more\ntraditional graphical content management system (CMS), with a focus on\nunderstanding the relationship between user preferences and the complexity of\nthe information being provided. The findings demonstrate that, while the\noverall usability of the VUI is rated lower than the graphical interface, it is\nalready preferred by users for less complex tasks. Furthermore, the quality of\ncontent entered through the VUI is comparable to that achieved with the\ngraphical interface, even for highly complex tasks. Obtained qualitative\nresults suggest that a hybrid interface combining the strengths of both\napproaches could address the key challenges identified during the experiment,\nsuch as reducing cognitive load through graphical feedback while maintaining\nthe intuitive nature of voice-based interactions. This work highlights the\npotential of conversational interfaces as a viable and effective method for\nknowledge management in specific business contexts."}
{"id": "2505.22258", "pdf": "https://arxiv.org/pdf/2505.22258", "abs": "https://arxiv.org/abs/2505.22258", "authors": ["Benjamin Serfling", "Hannes Reichert", "Lorenzo Bayerlein", "Konrad Doll", "Kati Radkhah-Lens"], "title": "LiDAR Based Semantic Perception for Forklifts in Outdoor Environments", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": null, "summary": "In this study, we present a novel LiDAR-based semantic segmentation framework\ntailored for autonomous forklifts operating in complex outdoor environments.\nCentral to our approach is the integration of a dual LiDAR system, which\ncombines forward-facing and downward-angled LiDAR sensors to enable\ncomprehensive scene understanding, specifically tailored for industrial\nmaterial handling tasks. The dual configuration improves the detection and\nsegmentation of dynamic and static obstacles with high spatial precision. Using\nhigh-resolution 3D point clouds captured from two sensors, our method employs a\nlightweight yet robust approach that segments the point clouds into\nsafety-critical instance classes such as pedestrians, vehicles, and forklifts,\nas well as environmental classes such as driveable ground, lanes, and\nbuildings. Experimental validation demonstrates that our approach achieves high\nsegmentation accuracy while satisfying strict runtime requirements,\nestablishing its viability for safety-aware, fully autonomous forklift\nnavigation in dynamic warehouse and yard environments."}
{"id": "2505.22306", "pdf": "https://arxiv.org/pdf/2505.22306", "abs": "https://arxiv.org/abs/2505.22306", "authors": ["Zehua Chen", "Yuyang Miao", "Liyuan Wang", "Luyun Fan", "Danilo P. Mandic", "Jun Zhu"], "title": "Versatile Cardiovascular Signal Generation with a Unified Diffusion Transformer", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Cardiovascular signals such as photoplethysmography (PPG),\nelectrocardiography (ECG), and blood pressure (BP) are inherently correlated\nand complementary, together reflecting the health of cardiovascular system.\nHowever, their joint utilization in real-time monitoring is severely limited by\ndiverse acquisition challenges from noisy wearable recordings to burdened\ninvasive procedures. Here we propose UniCardio, a multi-modal diffusion\ntransformer that reconstructs low-quality signals and synthesizes unrecorded\nsignals in a unified generative framework. Its key innovations include a\nspecialized model architecture to manage the signal modalities involved in\ngeneration tasks and a continual learning paradigm to incorporate varying\nmodality combinations. By exploiting the complementary nature of cardiovascular\nsignals, UniCardio clearly outperforms recent task-specific baselines in signal\ndenoising, imputation, and translation. The generated signals match the\nperformance of ground-truth signals in detecting abnormal health conditions and\nestimating vital signs, even in unseen domains, while ensuring interpretability\nfor human experts. These advantages position UniCardio as a promising avenue\nfor advancing AI-assisted healthcare."}
{"id": "2505.22310", "pdf": "https://arxiv.org/pdf/2505.22310", "abs": "https://arxiv.org/abs/2505.22310", "authors": ["Shoaib Ahmed Siddiqui", "Adrian Weller", "David Krueger", "Gintare Karolina Dziugaite", "Michael Curtis Mozer", "Eleni Triantafillou"], "title": "From Dormant to Deleted: Tamper-Resistant Unlearning Through Weight-Space Regularization", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Recent unlearning methods for LLMs are vulnerable to relearning attacks:\nknowledge believed-to-be-unlearned re-emerges by fine-tuning on a small set of\n(even seemingly-unrelated) examples. We study this phenomenon in a controlled\nsetting for example-level unlearning in vision classifiers. We make the\nsurprising discovery that forget-set accuracy can recover from around 50%\npost-unlearning to nearly 100% with fine-tuning on just the retain set -- i.e.,\nzero examples of the forget set. We observe this effect across a wide variety\nof unlearning methods, whereas for a model retrained from scratch excluding the\nforget set (gold standard), the accuracy remains at 50%. We observe that\nresistance to relearning attacks can be predicted by weight-space properties,\nspecifically, $L_2$-distance and linear mode connectivity between the original\nand the unlearned model. Leveraging this insight, we propose a new class of\nmethods that achieve state-of-the-art resistance to relearning attacks."}
{"id": "2505.22310", "pdf": "https://arxiv.org/pdf/2505.22310", "abs": "https://arxiv.org/abs/2505.22310", "authors": ["Shoaib Ahmed Siddiqui", "Adrian Weller", "David Krueger", "Gintare Karolina Dziugaite", "Michael Curtis Mozer", "Eleni Triantafillou"], "title": "From Dormant to Deleted: Tamper-Resistant Unlearning Through Weight-Space Regularization", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Recent unlearning methods for LLMs are vulnerable to relearning attacks:\nknowledge believed-to-be-unlearned re-emerges by fine-tuning on a small set of\n(even seemingly-unrelated) examples. We study this phenomenon in a controlled\nsetting for example-level unlearning in vision classifiers. We make the\nsurprising discovery that forget-set accuracy can recover from around 50%\npost-unlearning to nearly 100% with fine-tuning on just the retain set -- i.e.,\nzero examples of the forget set. We observe this effect across a wide variety\nof unlearning methods, whereas for a model retrained from scratch excluding the\nforget set (gold standard), the accuracy remains at 50%. We observe that\nresistance to relearning attacks can be predicted by weight-space properties,\nspecifically, $L_2$-distance and linear mode connectivity between the original\nand the unlearned model. Leveraging this insight, we propose a new class of\nmethods that achieve state-of-the-art resistance to relearning attacks."}
{"id": "2505.22313", "pdf": "https://arxiv.org/pdf/2505.22313", "abs": "https://arxiv.org/abs/2505.22313", "authors": ["Kaixuan Wei", "Hector A. Jimenez-Romero", "Hadi Amata", "Jipeng Sun", "Qiang Fu", "Felix Heide", "Wolfgang Heidrich"], "title": "Large-Area Fabrication-aware Computational Diffractive Optics", "categories": ["physics.optics", "cs.CV", "cs.ET", "cs.GR"], "comment": null, "summary": "Differentiable optics, as an emerging paradigm that jointly optimizes optics\nand (optional) image processing algorithms, has made innovative optical designs\npossible across a broad range of applications. Many of these systems utilize\ndiffractive optical components (DOEs) for holography, PSF engineering, or\nwavefront shaping. Existing approaches have, however, mostly remained limited\nto laboratory prototypes, owing to a large quality gap between simulation and\nmanufactured devices. We aim at lifting the fundamental technical barriers to\nthe practical use of learned diffractive optical systems. To this end, we\npropose a fabrication-aware design pipeline for diffractive optics fabricated\nby direct-write grayscale lithography followed by nano-imprinting replication,\nwhich is directly suited for inexpensive mass production of large area designs.\nWe propose a super-resolved neural lithography model that can accurately\npredict the 3D geometry generated by the fabrication process. This model can be\nseamlessly integrated into existing differentiable optics frameworks, enabling\nfabrication-aware, end-to-end optimization of computational optical systems. To\ntackle the computational challenges, we also devise tensor-parallel compute\nframework centered on distributing large-scale FFT computation across many\nGPUs. As such, we demonstrate large scale diffractive optics designs up to\n32.16 mm $\\times$ 21.44 mm, simulated on grids of up to 128,640 by 85,760\nfeature points. We find adequate agreement between simulation and fabricated\nprototypes for applications such as holography and PSF engineering. We also\nachieve high image quality from an imaging system comprised only of a single\nDOE, with images processed only by a Wiener filter utilizing the simulation\nPSF. We believe our findings lift the fabrication limitations for real-world\napplications of diffractive optics and differentiable optical design."}
{"id": "2505.22312", "pdf": "https://arxiv.org/pdf/2505.22312", "abs": "https://arxiv.org/abs/2505.22312", "authors": ["Jujie He", "Jiacai Liu", "Chris Yuhao Liu", "Rui Yan", "Chaojie Wang", "Peng Cheng", "Xiaoyu Zhang", "Fuxiang Zhang", "Jiacheng Xu", "Wei Shen", "Siyuan Li", "Liang Zeng", "Tianwen Wei", "Cheng Cheng", "Bo An", "Yang Liu", "Yahui Zhou"], "title": "Skywork Open Reasoner 1 Technical Report", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The success of DeepSeek-R1 underscores the significant role of reinforcement\nlearning (RL) in enhancing the reasoning capabilities of large language models\n(LLMs). In this work, we present Skywork-OR1, an effective and scalable RL\nimplementation for long Chain-of-Thought (CoT) models. Building on the\nDeepSeek-R1-Distill model series, our RL approach achieves notable performance\ngains, increasing average accuracy across AIME24, AIME25, and LiveCodeBench\nfrom 57.8% to 72.8% (+15.0%) for the 32B model and from 43.6% to 57.5% (+13.9%)\nfor the 7B model. Our Skywork-OR1-32B model surpasses both DeepSeek-R1 and\nQwen3-32B on the AIME24 and AIME25 benchmarks, while achieving comparable\nresults on LiveCodeBench. The Skywork-OR1-7B and Skywork-OR1-Math-7B models\ndemonstrate competitive reasoning capabilities among models of similar size. We\nperform comprehensive ablation studies on the core components of our training\npipeline to validate their effectiveness. Additionally, we thoroughly\ninvestigate the phenomenon of entropy collapse, identify key factors affecting\nentropy dynamics, and demonstrate that mitigating premature entropy collapse is\ncritical for improved test performance. To support community research, we fully\nopen-source our model weights, training code, and training datasets."}
{"id": "2505.22334", "pdf": "https://arxiv.org/pdf/2505.22334", "abs": "https://arxiv.org/abs/2505.22334", "authors": ["Lai Wei", "Yuting Li", "Kaipeng Zheng", "Chen Wang", "Yue Wang", "Linghe Kong", "Lichao Sun", "Weiran Huang"], "title": "Advancing Multimodal Reasoning via Reinforcement Learning with Cold Start", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have demonstrated\nimpressive chain-of-thought reasoning capabilities, with reinforcement learning\n(RL) playing a crucial role in this progress. While \"aha moment\"\npatterns--where models exhibit self-correction through reflection--are often\nattributed to emergent properties from RL, we first demonstrate that these\npatterns exist in multimodal LLMs (MLLMs) prior to RL training but may not\nnecessarily correlate with improved reasoning performance. Building on these\ninsights, we present a comprehensive study on enhancing multimodal reasoning\nthrough a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start\nwith structured chain-of-thought reasoning patterns, followed by (2)\nreinforcement learning via GRPO to further refine these capabilities. Our\nextensive experiments show that this combined approach consistently outperforms\nboth SFT-only and RL-only methods across challenging multimodal reasoning\nbenchmarks. The resulting models achieve state-of-the-art performance among\nopen-source MLLMs at both 3B and 7B scales, with our 7B model showing\nsubstantial improvements over base models (e.g., 66.3 %$\\rightarrow$73.4 % on\nMathVista, 62.9 %$\\rightarrow$70.4 % on We-Math) and our 3B model achieving\nperformance competitive with several 7B models. Overall, this work provides\npractical guidance for building advanced multimodal reasoning models. Our code\nis available at https://github.com/waltonfuture/RL-with-Cold-Start."}
{"id": "2505.22334", "pdf": "https://arxiv.org/pdf/2505.22334", "abs": "https://arxiv.org/abs/2505.22334", "authors": ["Lai Wei", "Yuting Li", "Kaipeng Zheng", "Chen Wang", "Yue Wang", "Linghe Kong", "Lichao Sun", "Weiran Huang"], "title": "Advancing Multimodal Reasoning via Reinforcement Learning with Cold Start", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have demonstrated\nimpressive chain-of-thought reasoning capabilities, with reinforcement learning\n(RL) playing a crucial role in this progress. While \"aha moment\"\npatterns--where models exhibit self-correction through reflection--are often\nattributed to emergent properties from RL, we first demonstrate that these\npatterns exist in multimodal LLMs (MLLMs) prior to RL training but may not\nnecessarily correlate with improved reasoning performance. Building on these\ninsights, we present a comprehensive study on enhancing multimodal reasoning\nthrough a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start\nwith structured chain-of-thought reasoning patterns, followed by (2)\nreinforcement learning via GRPO to further refine these capabilities. Our\nextensive experiments show that this combined approach consistently outperforms\nboth SFT-only and RL-only methods across challenging multimodal reasoning\nbenchmarks. The resulting models achieve state-of-the-art performance among\nopen-source MLLMs at both 3B and 7B scales, with our 7B model showing\nsubstantial improvements over base models (e.g., 66.3 %$\\rightarrow$73.4 % on\nMathVista, 62.9 %$\\rightarrow$70.4 % on We-Math) and our 3B model achieving\nperformance competitive with several 7B models. Overall, this work provides\npractical guidance for building advanced multimodal reasoning models. Our code\nis available at https://github.com/waltonfuture/RL-with-Cold-Start."}
{"id": "2505.22335", "pdf": "https://arxiv.org/pdf/2505.22335", "abs": "https://arxiv.org/abs/2505.22335", "authors": ["Wancai Zheng", "Linlin Ou", "Jiajie He", "Libo Zhou", "Xinyi Yu", "Yan Wei"], "title": "UP-SLAM: Adaptively Structured Gaussian SLAM with Uncertainty Prediction in Dynamic Environments", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Recent 3D Gaussian Splatting (3DGS) techniques for Visual Simultaneous\nLocalization and Mapping (SLAM) have significantly progressed in tracking and\nhigh-fidelity mapping. However, their sequential optimization framework and\nsensitivity to dynamic objects limit real-time performance and robustness in\nreal-world scenarios. We present UP-SLAM, a real-time RGB-D SLAM system for\ndynamic environments that decouples tracking and mapping through a parallelized\nframework. A probabilistic octree is employed to manage Gaussian primitives\nadaptively, enabling efficient initialization and pruning without hand-crafted\nthresholds. To robustly filter dynamic regions during tracking, we propose a\ntraining-free uncertainty estimator that fuses multi-modal residuals to\nestimate per-pixel motion uncertainty, achieving open-set dynamic object\nhandling without reliance on semantic labels. Furthermore, a temporal encoder\nis designed to enhance rendering quality. Concurrently, low-dimensional\nfeatures are efficiently transformed via a shallow multilayer perceptron to\nconstruct DINO features, which are then employed to enrich the Gaussian field\nand improve the robustness of uncertainty prediction. Extensive experiments on\nmultiple challenging datasets suggest that UP-SLAM outperforms state-of-the-art\nmethods in both localization accuracy (by 59.8%) and rendering quality (by 4.57\ndB PSNR), while maintaining real-time performance and producing reusable,\nartifact-free static maps in dynamic environments.The project:\nhttps://aczheng-cai.github.io/up_slam.github.io/"}
{"id": "2505.22338", "pdf": "https://arxiv.org/pdf/2505.22338", "abs": "https://arxiv.org/abs/2505.22338", "authors": ["Hanyang Wang", "Lu Wang", "Chaoyun Zhang", "Tianjun Mao", "Si Qin", "Qingwei Lin", "Saravan Rajmohan", "Dongmei Zhang"], "title": "Text2Grad: Reinforcement Learning from Natural Language Feedback", "categories": ["cs.CL", "cs.AI"], "comment": "The code for our method is available at\n  https://github.com/microsoft/Text2Grad", "summary": "Traditional RLHF optimizes language models with coarse, scalar rewards that\nmask the fine-grained reasons behind success or failure, leading to slow and\nopaque learning. Recent work augments RL with textual critiques through\nprompting or reflection, improving interpretability but leaving model\nparameters untouched. We introduce Text2Grad, a reinforcement-learning paradigm\nthat turns free-form textual feedback into span-level gradients. Given human\n(or programmatic) critiques, Text2Grad aligns each feedback phrase with the\nrelevant token spans, converts these alignments into differentiable reward\nsignals, and performs gradient updates that directly refine the offending\nportions of the model's policy. This yields precise, feedback-conditioned\nadjustments instead of global nudges. Text2Grad is realized through three\ncomponents: (1) a high-quality feedback-annotation pipeline that pairs\ncritiques with token spans; (2) a fine-grained reward model that predicts\nspan-level reward on answer while generating explanatory critiques; and (3) a\nspan-level policy optimizer that back-propagates natural-language gradients.\nAcross summarization, code generation, and question answering, Text2Grad\nconsistently surpasses scalar-reward RL and prompt-only baselines, providing\nboth higher task metrics and richer interpretability. Our results demonstrate\nthat natural-language feedback, when converted to gradients, is a powerful\nsignal for fine-grained policy optimization. The code for our method is\navailable at https://github.com/microsoft/Text2Grad"}
{"id": "2505.22400", "pdf": "https://arxiv.org/pdf/2505.22400", "abs": "https://arxiv.org/abs/2505.22400", "authors": ["Zehao Li", "Hao Jiang", "Yujun Cai", "Jianing Chen", "Baolong Bi", "Shuqin Gao", "Honglong Zhao", "Yiwei Wang", "Tianlu Mao", "Zhaoqi Wang"], "title": "STDR: Spatio-Temporal Decoupling for Real-Time Dynamic Scene Rendering", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Although dynamic scene reconstruction has long been a fundamental challenge\nin 3D vision, the recent emergence of 3D Gaussian Splatting (3DGS) offers a\npromising direction by enabling high-quality, real-time rendering through\nexplicit Gaussian primitives. However, existing 3DGS-based methods for dynamic\nreconstruction often suffer from \\textit{spatio-temporal incoherence} during\ninitialization, where canonical Gaussians are constructed by aggregating\nobservations from multiple frames without temporal distinction. This results in\nspatio-temporally entangled representations, making it difficult to model\ndynamic motion accurately. To overcome this limitation, we propose\n\\textbf{STDR} (Spatio-Temporal Decoupling for Real-time rendering), a\nplug-and-play module that learns spatio-temporal probability distributions for\neach Gaussian. STDR introduces a spatio-temporal mask, a separated deformation\nfield, and a consistency regularization to jointly disentangle spatial and\ntemporal patterns. Extensive experiments demonstrate that incorporating our\nmodule into existing 3DGS-based dynamic scene reconstruction frameworks leads\nto notable improvements in both reconstruction quality and spatio-temporal\nconsistency across synthetic and real-world benchmarks."}
{"id": "2505.22343", "pdf": "https://arxiv.org/pdf/2505.22343", "abs": "https://arxiv.org/abs/2505.22343", "authors": ["Zhonghao Lyu", "Yulan Gao", "Junting Chen", "Hongyang Du", "Jie Xu", "Kaibin Huang", "Dong In Kim"], "title": "Empowering Intelligent Low-altitude Economy with Large AI Model Deployment", "categories": ["eess.SP", "cs.AI"], "comment": null, "summary": "Low-altitude economy (LAE) represents an emerging economic paradigm that\nredefines commercial and social aerial activities. Large artificial\nintelligence models (LAIMs) offer transformative potential to further enhance\nthe intelligence of LAE services. However, deploying LAIMs in LAE poses several\nchallenges, including the significant gap between their computational/storage\ndemands and the limited onboard resources of LAE entities, the mismatch between\nlab-trained LAIMs and dynamic physical environments, and the inefficiencies of\ntraditional decoupled designs for sensing, communication, and computation. To\naddress these issues, we first propose a hierarchical system architecture\ntailored for LAIM deployment and present representative LAE application\nscenarios. Next, we explore key enabling techniques that facilitate the mutual\nco-evolution of LAIMs and low-altitude systems, and introduce a task-oriented\nexecution pipeline for scalable and adaptive service delivery. Then, the\nproposed framework is validated through real-world case studies. Finally, we\noutline open challenges to inspire future research."}
{"id": "2505.22416", "pdf": "https://arxiv.org/pdf/2505.22416", "abs": "https://arxiv.org/abs/2505.22416", "authors": ["Sihun Cha", "Serin Yoon", "Kwanggyoon Seo", "Junyong Noh"], "title": "Neural Face Skinning for Mesh-agnostic Facial Expression Cloning", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Accurately retargeting facial expressions to a face mesh while enabling\nmanipulation is a key challenge in facial animation retargeting. Recent\ndeep-learning methods address this by encoding facial expressions into a global\nlatent code, but they often fail to capture fine-grained details in local\nregions. While some methods improve local accuracy by transferring deformations\nlocally, this often complicates overall control of the facial expression. To\naddress this, we propose a method that combines the strengths of both global\nand local deformation models. Our approach enables intuitive control and\ndetailed expression cloning across diverse face meshes, regardless of their\nunderlying structures. The core idea is to localize the influence of the global\nlatent code on the target mesh. Our model learns to predict skinning weights\nfor each vertex of the target face mesh through indirect supervision from\npredefined segmentation labels. These predicted weights localize the global\nlatent code, enabling precise and region-specific deformations even for meshes\nwith unseen shapes. We supervise the latent code using Facial Action Coding\nSystem (FACS)-based blendshapes to ensure interpretability and allow\nstraightforward editing of the generated animation. Through extensive\nexperiments, we demonstrate improved performance over state-of-the-art methods\nin terms of expression fidelity, deformation transfer accuracy, and\nadaptability across diverse mesh structures."}
{"id": "2505.22349", "pdf": "https://arxiv.org/pdf/2505.22349", "abs": "https://arxiv.org/abs/2505.22349", "authors": ["Anjie Xu", "Ruiqing Ding", "Leye Wang"], "title": "ChatPD: An LLM-driven Paper-Dataset Networking System", "categories": ["cs.DB", "cs.AI", "cs.IR"], "comment": "Accepted by KDD Applied Data Science Track 2025", "summary": "Scientific research heavily depends on suitable datasets for method\nvalidation, but existing academic platforms with dataset management like\nPapersWithCode suffer from inefficiencies in their manual workflow. To overcome\nthis bottleneck, we present a system, called ChatPD, that utilizes Large\nLanguage Models (LLMs) to automate dataset information extraction from academic\npapers and construct a structured paper-dataset network. Our system consists of\nthree key modules: \\textit{paper collection}, \\textit{dataset information\nextraction}, and \\textit{dataset entity resolution} to construct paper-dataset\nnetworks. Specifically, we propose a \\textit{Graph Completion and Inference}\nstrategy to map dataset descriptions to their corresponding entities. Through\nextensive experiments, we demonstrate that ChatPD not only outperforms the\nexisting platform PapersWithCode in dataset usage extraction but also achieves\nabout 90\\% precision and recall in entity resolution tasks. Moreover, we have\ndeployed ChatPD to continuously extract which datasets are used in papers, and\nprovide a dataset discovery service, such as task-specific dataset queries and\nsimilar dataset recommendations. We open source ChatPD and the current\npaper-dataset network on this [GitHub\nrepository]{https://github.com/ChatPD-web/ChatPD}."}
{"id": "2505.22438", "pdf": "https://arxiv.org/pdf/2505.22438", "abs": "https://arxiv.org/abs/2505.22438", "authors": ["Zijian Liang", "Kai Niu", "Changshuo Wang", "Jin Xu", "Ping Zhang"], "title": "Synonymous Variational Inference for Perceptual Image Compression", "categories": ["cs.IT", "cs.AI", "cs.CV", "cs.LG", "eess.IV", "math.IT"], "comment": "31 pages, 20 figures. This paper is accepted by Proceedings of the\n  42nd International Conference on Machine Learning (ICML 2025) Poster", "summary": "Recent contributions of semantic information theory reveal the set-element\nrelationship between semantic and syntactic information, represented as\nsynonymous relationships. In this paper, we propose a synonymous variational\ninference (SVI) method based on this synonymity viewpoint to re-analyze the\nperceptual image compression problem. It takes perceptual similarity as a\ntypical synonymous criterion to build an ideal synonymous set (Synset), and\napproximate the posterior of its latent synonymous representation with a\nparametric density by minimizing a partial semantic KL divergence. This\nanalysis theoretically proves that the optimization direction of perception\nimage compression follows a triple tradeoff that can cover the existing\nrate-distortion-perception schemes. Additionally, we introduce synonymous image\ncompression (SIC), a new image compression scheme that corresponds to the\nanalytical process of SVI, and implement a progressive SIC codec to fully\nleverage the model's capabilities. Experimental results demonstrate comparable\nrate-distortion-perception performance using a single progressive SIC codec,\nthus verifying the effectiveness of our proposed analysis method."}
{"id": "2505.22353", "pdf": "https://arxiv.org/pdf/2505.22353", "abs": "https://arxiv.org/abs/2505.22353", "authors": ["Noora Al-Emadi", "Ingmar Weber", "Yin Yang", "Ferda Ofli"], "title": "VME: A Satellite Imagery Dataset and Benchmark for Detecting Vehicles in the Middle East and Beyond", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Detecting vehicles in satellite images is crucial for traffic management,\nurban planning, and disaster response. However, current models struggle with\nreal-world diversity, particularly across different regions. This challenge is\namplified by geographic bias in existing datasets, which often focus on\nspecific areas and overlook regions like the Middle East. To address this gap,\nwe present the Vehicles in the Middle East (VME) dataset, designed explicitly\nfor vehicle detection in high-resolution satellite images from Middle Eastern\ncountries. Sourced from Maxar, the VME dataset spans 54 cities across 12\ncountries, comprising over 4,000 image tiles and more than 100,000 vehicles,\nannotated using both manual and semi-automated methods. Additionally, we\nintroduce the largest benchmark dataset for Car Detection in Satellite Imagery\n(CDSI), combining images from multiple sources to enhance global car detection.\nOur experiments demonstrate that models trained on existing datasets perform\npoorly on Middle Eastern images, while the VME dataset significantly improves\ndetection accuracy in this region. Moreover, state-of-the-art models trained on\nCDSI achieve substantial improvements in global car detection."}
{"id": "2505.22453", "pdf": "https://arxiv.org/pdf/2505.22453", "abs": "https://arxiv.org/abs/2505.22453", "authors": ["Lai Wei", "Yuting Li", "Chen Wang", "Yue Wang", "Linghe Kong", "Weiran Huang", "Lichao Sun"], "title": "Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Improving Multi-modal Large Language Models (MLLMs) in the post-training\nstage typically relies on supervised fine-tuning (SFT) or reinforcement\nlearning (RL). However, these supervised methods require expensive and manually\nannotated multi-modal data--an ultimately unsustainable resource. While recent\nefforts have explored unsupervised post-training, their methods are complex and\ndifficult to iterate. In this work, we are the first to investigate the use of\nGRPO, a stable and scalable online RL algorithm, for enabling continual\nself-improvement without any external supervision. We propose MM-UPT, a simple\nyet effective framework for unsupervised post-training of MLLMs. MM-UPT builds\nupon GRPO, replacing traditional reward signals with a self-rewarding mechanism\nbased on majority voting over multiple sampled responses. Our experiments\ndemonstrate that MM-UPT significantly improves the reasoning ability of\nQwen2.5-VL-7B (e.g., 66.3 %$\\rightarrow$72.9 % on MathVista, 62.9\n%$\\rightarrow$68.7 % on We-Math), using standard dataset without ground truth\nlabels. MM-UPT also outperforms prior unsupervised baselines and even\napproaches the results of supervised GRPO. Furthermore, we show that\nincorporating synthetic questions, generated solely by MLLM itself, can boost\nperformance as well, highlighting a promising approach for scalable\nself-improvement. Overall, MM-UPT offers a new paradigm for continual,\nautonomous enhancement of MLLMs in the absence of external supervision. Our\ncode is available at https://github.com/waltonfuture/MM-UPT."}
{"id": "2505.22356", "pdf": "https://arxiv.org/pdf/2505.22356", "abs": "https://arxiv.org/abs/2505.22356", "authors": ["Angéline Pouget", "Mohammad Yaghini", "Stephan Rabanser", "Nicolas Papernot"], "title": "Suitability Filter: A Statistical Framework for Classifier Evaluation in Real-World Deployment Settings", "categories": ["cs.LG", "cs.AI", "cs.CY", "stat.ML"], "comment": "Accepted to ICML 2025", "summary": "Deploying machine learning models in safety-critical domains poses a key\nchallenge: ensuring reliable model performance on downstream user data without\naccess to ground truth labels for direct validation. We propose the suitability\nfilter, a novel framework designed to detect performance deterioration by\nutilizing suitability signals -- model output features that are sensitive to\ncovariate shifts and indicative of potential prediction errors. The suitability\nfilter evaluates whether classifier accuracy on unlabeled user data shows\nsignificant degradation compared to the accuracy measured on the labeled test\ndataset. Specifically, it ensures that this degradation does not exceed a\npre-specified margin, which represents the maximum acceptable drop in accuracy.\nTo achieve reliable performance evaluation, we aggregate suitability signals\nfor both test and user data and compare these empirical distributions using\nstatistical hypothesis testing, thus providing insights into decision\nuncertainty. Our modular method adapts to various models and domains. Empirical\nevaluations across different classification tasks demonstrate that the\nsuitability filter reliably detects performance deviations due to covariate\nshift. This enables proactive mitigation of potential failures in high-stakes\napplications."}
{"id": "2505.22483", "pdf": "https://arxiv.org/pdf/2505.22483", "abs": "https://arxiv.org/abs/2505.22483", "authors": ["Abhra Chaudhuri", "Anjan Dutta", "Tu Bui", "Serban Georgescu"], "title": "A Closer Look at Multimodal Representation Collapse", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "International Conference on Machine Learning (ICML) 2025 (Spotlight)", "summary": "We aim to develop a fundamental understanding of modality collapse, a\nrecently observed empirical phenomenon wherein models trained for multimodal\nfusion tend to rely only on a subset of the modalities, ignoring the rest. We\nshow that modality collapse happens when noisy features from one modality are\nentangled, via a shared set of neurons in the fusion head, with predictive\nfeatures from another, effectively masking out positive contributions from the\npredictive features of the former modality and leading to its collapse. We\nfurther prove that cross-modal knowledge distillation implicitly disentangles\nsuch representations by freeing up rank bottlenecks in the student encoder,\ndenoising the fusion-head outputs without negatively impacting the predictive\nfeatures from either modality. Based on the above findings, we propose an\nalgorithm that prevents modality collapse through explicit basis reallocation,\nwith applications in dealing with missing modalities. Extensive experiments on\nmultiple multimodal benchmarks validate our theoretical claims. Project page:\nhttps://abhrac.github.io/mmcollapse/."}
{"id": "2505.22358", "pdf": "https://arxiv.org/pdf/2505.22358", "abs": "https://arxiv.org/abs/2505.22358", "authors": ["Zhiyi Wan", "Wanrou Du", "Liang Li", "Miao Pan", "Xiaoqi Qin"], "title": "Budget-Adaptive Adapter Tuning in Orthogonal Subspaces for Continual Learning in LLMs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) often suffer from catastrophic forgetting in\ncontinual learning (CL) scenarios, where performance on previously learned\ntasks degrades severely while training on sequentially arriving tasks. Although\npioneering CL approaches using orthogonal subspaces can mitigate task\ninterference, they typically employ fixed budget allocation, neglecting the\nvarying complexity across tasks and layers. Besides, recent budget-adaptive\ntuning methods for LLMs often adopt multi-stage paradigms that decouple\noptimization and budget allocation. Such decoupling results in potential\nmisalignment, which hinders those approaches' practical application in CL\nscenarios. To address these limitations, we propose OA-Adapter, a novel\nparameter-efficient approach for continual learning in LLMs that unifies\ndynamic budget adaptation with orthogonal subspace learning in a single\nend-to-end training stage. Specifically, OA-Adapter introduces a dynamic\nbottleneck dimension adaptation mechanism that simultaneously allocates an\nefficient parameter budget and optimizes task objectives without misalignment.\nTo effectively preserve previously acquired knowledge while coordinating with\nthe dynamic budget allocation, orthogonal constraints are applied specifically\nbetween the parameter subspace of the current task and the dynamically\nallocated parameter subspaces of historical tasks. Experimental results on\ncontinual learning benchmarks demonstrate that OA-Adapter outperforms\nstate-of-the-art methods in both accuracy and parameter efficiency, achieving\nhigher average accuracy while using 58.5% fewer parameters on the standard CL\nbenchmark."}
{"id": "2505.22486", "pdf": "https://arxiv.org/pdf/2505.22486", "abs": "https://arxiv.org/abs/2505.22486", "authors": ["Mujtaba Hussain Mirza", "Maria Rosaria Briglia", "Filippo Bartolucci", "Senad Beadini", "Giuseppe Lisanti", "Iacopo Masi"], "title": "Understanding Adversarial Training with Energy-based Models", "categories": ["cs.LG", "cs.CV"], "comment": "Under review for TPAMI", "summary": "We aim at using Energy-based Model (EBM) framework to better understand\nadversarial training (AT) in classifiers, and additionally to analyze the\nintrinsic generative capabilities of robust classifiers. By viewing standard\nclassifiers through an energy lens, we begin by analyzing how the energies of\nadversarial examples, generated by various attacks, differ from those of the\nnatural samples. The central focus of our work is to understand the critical\nphenomena of Catastrophic Overfitting (CO) and Robust Overfitting (RO) in AT\nfrom an energy perspective. We analyze the impact of existing AT approaches on\nthe energy of samples during training and observe that the behavior of the\n``delta energy' -- change in energy between original sample and its adversarial\ncounterpart -- diverges significantly when CO or RO occurs. After a thorough\nanalysis of these energy dynamics and their relationship with overfitting, we\npropose a novel regularizer, the Delta Energy Regularizer (DER), designed to\nsmoothen the energy landscape during training. We demonstrate that DER is\neffective in mitigating both CO and RO across multiple benchmarks. We further\nshow that robust classifiers, when being used as generative models, have limits\nin handling trade-off between image quality and variability. We propose an\nimproved technique based on a local class-wise principal component analysis\n(PCA) and energy-based guidance for better class-specific initialization and\nadaptive stopping, enhancing sample diversity and generation quality.\nConsidering that we do not explicitly train for generative modeling, we achieve\na competitive Inception Score (IS) and Fr\\'echet inception distance (FID)\ncompared to hybrid discriminative-generative models."}
{"id": "2505.22370", "pdf": "https://arxiv.org/pdf/2505.22370", "abs": "https://arxiv.org/abs/2505.22370", "authors": ["Haomiao Qiu", "Miao Zhang", "Ziyue Qiao", "Weili Guan", "Min Zhang", "Liqiang Nie"], "title": "SplitLoRA: Balancing Stability and Plasticity in Continual Learning Through Gradient Space Splitting", "categories": ["cs.LG", "cs.AI"], "comment": "18 pages, 4 figures", "summary": "Continual Learning requires a model to learn multiple tasks in sequence while\nmaintaining both stability:preserving knowledge from previously learned tasks,\nand plasticity:effectively learning new tasks. Gradient projection has emerged\nas an effective and popular paradigm in CL, where it partitions the gradient\nspace of previously learned tasks into two orthogonal subspaces: a primary\nsubspace and a minor subspace. New tasks are learned effectively within the\nminor subspace, thereby reducing interference with previously acquired\nknowledge. However, existing Gradient Projection methods struggle to achieve an\noptimal balance between plasticity and stability, as it is hard to\nappropriately partition the gradient space. In this work, we consider a\ncontinual learning paradigm based on Low-Rank Adaptation, which has gained\nconsiderable attention due to its efficiency and wide applicability, and\npropose a novel approach for continual learning, called SplitLoRA. We first\nprovide a theoretical analysis of how subspace partitioning affects model\nstability and plasticity. Informed by this analysis, we then introduce an\neffective method that derives the optimal partition of the gradient space for\npreviously learned tasks. This approach effectively balances stability and\nplasticity in continual learning. Experimental results on multiple datasets\ndemonstrate that the proposed method achieves state-of-the-art performance."}
{"id": "2505.22489", "pdf": "https://arxiv.org/pdf/2505.22489", "abs": "https://arxiv.org/abs/2505.22489", "authors": ["Siyeop Yoon", "Sifan Song", "Pengfei Jin", "Matthew Tivnan", "Yujin Oh", "Sekeun Kim", "Dufan Wu", "Xiang Li", "Quanzheng Li"], "title": "Cascaded 3D Diffusion Models for Whole-body 3D 18-F FDG PET/CT synthesis from Demographics", "categories": ["eess.IV", "cs.CV", "cs.GR"], "comment": "MICCAI2025 Submitted version", "summary": "We propose a cascaded 3D diffusion model framework to synthesize\nhigh-fidelity 3D PET/CT volumes directly from demographic variables, addressing\nthe growing need for realistic digital twins in oncologic imaging, virtual\ntrials, and AI-driven data augmentation. Unlike deterministic phantoms, which\nrely on predefined anatomical and metabolic templates, our method employs a\ntwo-stage generative process. An initial score-based diffusion model\nsynthesizes low-resolution PET/CT volumes from demographic variables alone,\nproviding global anatomical structures and approximate metabolic activity. This\nis followed by a super-resolution residual diffusion model that refines spatial\nresolution. Our framework was trained on 18-F FDG PET/CT scans from the AutoPET\ndataset and evaluated using organ-wise volume and standardized uptake value\n(SUV) distributions, comparing synthetic and real data between demographic\nsubgroups. The organ-wise comparison demonstrated strong concordance between\nsynthetic and real images. In particular, most deviations in metabolic uptake\nvalues remained within 3-5% of the ground truth in subgroup analysis. These\nfindings highlight the potential of cascaded 3D diffusion models to generate\nanatomically and metabolically accurate PET/CT images, offering a robust\nalternative to traditional phantoms and enabling scalable, population-informed\nsynthetic imaging for clinical and research applications."}
{"id": "2505.22384", "pdf": "https://arxiv.org/pdf/2505.22384", "abs": "https://arxiv.org/abs/2505.22384", "authors": ["Foivos Fioravantes", "Harmender Gahlawat", "Nikolaos Melissinos"], "title": "Exact Algorithms and Lower Bounds for Forming Coalitions of Constrained Maximum Size", "categories": ["cs.DS", "cs.AI"], "comment": "a preliminary version appeared in AAAI 2025", "summary": "Imagine we want to split a group of agents into teams in the most\n\\emph{efficient} way, considering that each agent has their own preferences\nabout their teammates. This scenario is modeled by the extensively studied\n\\textsc{Coalition Formation} problem. Here, we study a version of this problem\nwhere each team must additionally be of bounded size.\n  We conduct a systematic algorithmic study, providing several intractability\nresults as well as multiple exact algorithms that scale well as the input grows\n(FPT), which could prove useful in practice.\n  Our main contribution is an algorithm that deals efficiently with tree-like\nstructures (bounded \\emph{treewidth}) for ``small'' teams. We complement this\nresult by proving that our algorithm is asymptotically optimal. Particularly,\nthere can be no algorithm that vastly outperforms the one we present, under\nreasonable theoretical assumptions, even when considering star-like structures\n(bounded \\emph{vertex cover number})."}
{"id": "2505.22496", "pdf": "https://arxiv.org/pdf/2505.22496", "abs": "https://arxiv.org/abs/2505.22496", "authors": ["Long Hui"], "title": "Risk-Sensitive Conformal Prediction for Catheter Placement Detection in Chest X-rays", "categories": ["eess.IV", "cs.CV", "stat.AP"], "comment": null, "summary": "This paper presents a novel approach to catheter and line position detection\nin chest X-rays, combining multi-task learning with risk-sensitive conformal\nprediction to address critical clinical requirements. Our model simultaneously\nperforms classification, segmentation, and landmark detection, leveraging the\nsynergistic relationship between these tasks to improve overall performance. We\nfurther enhance clinical reliability through risk-sensitive conformal\nprediction, which provides statistically guaranteed prediction sets with higher\nreliability for clinically critical findings. Experimental results demonstrate\nexcellent performance with 90.68\\% overall empirical coverage and 99.29\\%\ncoverage for critical conditions, while maintaining remarkable precision in\nprediction sets. Most importantly, our risk-sensitive approach achieves zero\nhigh-risk mispredictions (cases where the system dangerously declares\nproblematic tubes as confidently normal), making the system particularly\nsuitable for clinical deployment. This work offers both accurate predictions\nand reliably quantified uncertainty -- essential features for life-critical\nmedical applications."}
{"id": "2505.22387", "pdf": "https://arxiv.org/pdf/2505.22387", "abs": "https://arxiv.org/abs/2505.22387", "authors": ["Jaehyun Choi", "Gyojin Han", "Dong-Jae Lee", "Sunghyun Baek", "Junmo Kim"], "title": "DAM: Domain-Aware Module for Multi-Domain Dataset Condensation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Dataset Condensation (DC) has emerged as a promising solution to mitigate the\ncomputational and storage burdens associated with training deep learning\nmodels. However, existing DC methods largely overlook the multi-domain nature\nof modern datasets, which are increasingly composed of heterogeneous images\nspanning multiple domains. In this paper, we extend DC and introduce\nMulti-Domain Dataset Condensation (MDDC), which aims to condense data that\ngeneralizes across both single-domain and multi-domain settings. To this end,\nwe propose the Domain-Aware Module (DAM), a training-time module that embeds\ndomain-related features into each synthetic image via learnable spatial masks.\nAs explicit domain labels are mostly unavailable in real-world datasets, we\nemploy frequency-based pseudo-domain labeling, which leverages low-frequency\namplitude statistics. DAM is only active during the condensation process, thus\npreserving the same images per class (IPC) with prior methods. Experiments show\nthat DAM consistently improves in-domain, out-of-domain, and cross-architecture\nperformance over baseline dataset condensation methods."}
{"id": "2505.22511", "pdf": "https://arxiv.org/pdf/2505.22511", "abs": "https://arxiv.org/abs/2505.22511", "authors": ["Siyeop Yoon", "Yujin Oh", "Pengfei Jin", "Sifan Song", "Matthew Tivnan", "Dufan Wu", "Xiang Li", "Quanzheng Li"], "title": "Surf2CT: Cascaded 3D Flow Matching Models for Torso 3D CT Synthesis from Skin Surface", "categories": ["eess.IV", "cs.CV"], "comment": "Neurips 2025 submitted", "summary": "We present Surf2CT, a novel cascaded flow matching framework that synthesizes\nfull 3D computed tomography (CT) volumes of the human torso from external\nsurface scans and simple demographic data (age, sex, height, weight). This is\nthe first approach capable of generating realistic volumetric internal anatomy\nimages solely based on external body shape and demographics, without any\ninternal imaging. Surf2CT proceeds through three sequential stages: (1) Surface\nCompletion, reconstructing a complete signed distance function (SDF) from\npartial torso scans using conditional 3D flow matching; (2) Coarse CT\nSynthesis, generating a low-resolution CT volume from the completed SDF and\ndemographic information; and (3) CT Super-Resolution, refining the coarse\nvolume into a high-resolution CT via a patch-wise conditional flow model. Each\nstage utilizes a 3D-adapted EDM2 backbone trained via flow matching. We trained\nour model on a combined dataset of 3,198 torso CT scans (approximately 1.13\nmillion axial slices) sourced from Massachusetts General Hospital (MGH) and the\nAutoPET challenge. Evaluation on 700 paired torso surface-CT cases demonstrated\nstrong anatomical fidelity: organ volumes exhibited small mean percentage\ndifferences (range from -11.1% to 4.4%), and muscle/fat body composition\nmetrics matched ground truth with strong correlation (range from 0.67 to 0.96).\nLung localization had minimal bias (mean difference -2.5 mm), and surface\ncompletion significantly improved metrics (Chamfer distance: from 521.8 mm to\n2.7 mm; Intersection-over-Union: from 0.87 to 0.98). Surf2CT establishes a new\nparadigm for non-invasive internal anatomical imaging using only external data,\nopening opportunities for home-based healthcare, preventive medicine, and\npersonalized clinical assessments without the risks associated with\nconventional imaging techniques."}
{"id": "2505.22389", "pdf": "https://arxiv.org/pdf/2505.22389", "abs": "https://arxiv.org/abs/2505.22389", "authors": ["Haomiao Qiu", "Miao Zhang", "Ziyue Qiao", "Liqiang Nie"], "title": "Train with Perturbation, Infer after Merging: A Two-Stage Framework for Continual Learning", "categories": ["cs.LG", "cs.AI"], "comment": "17 pages, 3 figures", "summary": "Continual Learning (CL) aims to enable models to continuously acquire new\nknowledge from a sequence of tasks with avoiding the forgetting of learned\ninformation. However, existing CL methods only rely on the parameters of the\nmost recent task for inference, which makes them susceptible to catastrophic\nforgetting. Inspired by the recent success of model merging techniques, we\npropose \\textbf{Perturb-and-Merge (P\\&M)}, a novel continual learning framework\nthat integrates model merging into the CL paradigm to mitigate forgetting.\nSpecifically, after training on each task, P\\&M constructs a new model by\nforming a convex combination of the previous model and the newly trained\ntask-specific model. Through theoretical analysis, we minimize the total loss\nincrease across all tasks and derive an analytical solution for the optimal\nmerging coefficient. To further improve the performance of the merged model, we\nobserve that the degradation introduced during merging can be alleviated by a\nregularization term composed of the task vector and the Hessian matrix of the\nloss function. Interestingly, we show that this term can be efficiently\napproximated using second-order symmetric finite differences, and a stochastic\nperturbation strategy along the task vector direction is accordingly devised\nwhich incurs no additional forward or backward passes while providing an\neffective approximation of the regularization term. Finally, we combine P\\&M\nwith LoRA, a parameter-efficient fine-tuning method, to reduce memory overhead.\nOur proposed approach achieves state-of-the-art performance on several\ncontinual learning benchmark datasets."}
{"id": "2505.22568", "pdf": "https://arxiv.org/pdf/2505.22568", "abs": "https://arxiv.org/abs/2505.22568", "authors": ["Aravind R. Krishnan", "Thomas Z. Li", "Lucas W. Remedios", "Michael E. Kim", "Chenyu Gao", "Gaurav Rudravaram", "Elyssa M. McMaster", "Adam M. Saunders", "Shunxing Bao", "Kaiwen Xu", "Lianrui Zuo", "Kim L. Sandler", "Fabien Maldonado", "Yuankai Huo", "Bennett A. Landman"], "title": "Multipath cycleGAN for harmonization of paired and unpaired low-dose lung computed tomography reconstruction kernels", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Reconstruction kernels in computed tomography (CT) affect spatial resolution\nand noise characteristics, introducing systematic variability in quantitative\nimaging measurements such as emphysema quantification. Choosing an appropriate\nkernel is therefore essential for consistent quantitative analysis. We propose\na multipath cycleGAN model for CT kernel harmonization, trained on a mixture of\npaired and unpaired data from a low-dose lung cancer screening cohort. The\nmodel features domain-specific encoders and decoders with a shared latent space\nand uses discriminators tailored for each domain.We train the model on 42\nkernel combinations using 100 scans each from seven representative kernels in\nthe National Lung Screening Trial (NLST) dataset. To evaluate performance, 240\nscans from each kernel are harmonized to a reference soft kernel, and emphysema\nis quantified before and after harmonization. A general linear model assesses\nthe impact of age, sex, smoking status, and kernel on emphysema. We also\nevaluate harmonization from soft kernels to a reference hard kernel. To assess\nanatomical consistency, we compare segmentations of lung vessels, muscle, and\nsubcutaneous adipose tissue generated by TotalSegmentator between harmonized\nand original images. Our model is benchmarked against traditional and\nswitchable cycleGANs. For paired kernels, our approach reduces bias in\nemphysema scores, as seen in Bland-Altman plots (p<0.05). For unpaired kernels,\nharmonization eliminates confounding differences in emphysema (p>0.05). High\nDice scores confirm preservation of muscle and fat anatomy, while lung vessel\noverlap remains reasonable. Overall, our shared latent space multipath cycleGAN\nenables robust harmonization across paired and unpaired CT kernels, improving\nemphysema quantification and preserving anatomical fidelity."}
{"id": "2505.22391", "pdf": "https://arxiv.org/pdf/2505.22391", "abs": "https://arxiv.org/abs/2505.22391", "authors": ["Yi Zhang", "Difan Zou"], "title": "Physics-Informed Distillation of Diffusion Models for PDE-Constrained Generation", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.NA", "math.NA"], "comment": "23 pages, 5 figures, 4 tables", "summary": "Modeling physical systems in a generative manner offers several advantages,\nincluding the ability to handle partial observations, generate diverse\nsolutions, and address both forward and inverse problems. Recently, diffusion\nmodels have gained increasing attention in the modeling of physical systems,\nparticularly those governed by partial differential equations (PDEs). However,\ndiffusion models only access noisy data $\\boldsymbol{x}_t$ at intermediate\nsteps, making it infeasible to directly enforce constraints on the clean sample\n$\\boldsymbol{x}_0$ at each noisy level. As a workaround, constraints are\ntypically applied to the expectation of clean samples\n$\\mathbb{E}[\\boldsymbol{x}_0|\\boldsymbol{x}_t]$, which is estimated using the\nlearned score network. However, imposing PDE constraints on the expectation\ndoes not strictly represent the one on the true clean data, known as Jensen's\nGap. This gap creates a trade-off: enforcing PDE constraints may come at the\ncost of reduced accuracy in generative modeling. To address this, we propose a\nsimple yet effective post-hoc distillation approach, where PDE constraints are\nnot injected directly into the diffusion process, but instead enforced during a\npost-hoc distillation stage. We term our method as Physics-Informed\nDistillation of Diffusion Models (PIDDM). This distillation not only\nfacilitates single-step generation with improved PDE satisfaction, but also\nsupport both forward and inverse problem solving and reconstruction from\nrandomly partial observation. Extensive experiments across various PDE\nbenchmarks demonstrate that PIDDM significantly improves PDE satisfaction over\nseveral recent and competitive baselines, such as PIDM, DiffusionPDE, and\nECI-sampling, with less computation overhead. Our approach can shed light on\nmore efficient and effective strategies for incorporating physical constraints\ninto diffusion models."}
{"id": "2505.22592", "pdf": "https://arxiv.org/pdf/2505.22592", "abs": "https://arxiv.org/abs/2505.22592", "authors": ["Yiheng Li", "Francisco Carrillo-Perez", "Mohammed Alawad", "Olivier Gevaert"], "title": "Comparative Analysis of Machine Learning Models for Lung Cancer Mutation Detection and Staging Using 3D CT Scans", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Lung cancer is the leading cause of cancer mortality worldwide, and\nnon-invasive methods for detecting key mutations and staging are essential for\nimproving patient outcomes. Here, we compare the performance of two machine\nlearning models - FMCIB+XGBoost, a supervised model with domain-specific\npretraining, and Dinov2+ABMIL, a self-supervised model with attention-based\nmultiple-instance learning - on 3D lung nodule data from the Stanford\nRadiogenomics and Lung-CT-PT-Dx cohorts. In the task of KRAS and EGFR mutation\ndetection, FMCIB+XGBoost consistently outperformed Dinov2+ABMIL, achieving\naccuracies of 0.846 and 0.883 for KRAS and EGFR mutations, respectively. In\ncancer staging, Dinov2+ABMIL demonstrated competitive generalization, achieving\nan accuracy of 0.797 for T-stage prediction in the Lung-CT-PT-Dx cohort,\nsuggesting SSL's adaptability across diverse datasets. Our results emphasize\nthe clinical utility of supervised models in mutation detection and highlight\nthe potential of SSL to improve staging generalization, while identifying areas\nfor enhancement in mutation sensitivity."}
{"id": "2505.22411", "pdf": "https://arxiv.org/pdf/2505.22411", "abs": "https://arxiv.org/abs/2505.22411", "authors": ["Yao Huang", "Huanran Chen", "Shouwei Ruan", "Yichi Zhang", "Xingxing Wei", "Yinpeng Dong"], "title": "Mitigating Overthinking in Large Reasoning Models via Manifold Steering", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "17 pages, 7 figures", "summary": "Recent advances in Large Reasoning Models (LRMs) have demonstrated remarkable\ncapabilities in solving complex tasks such as mathematics and coding. However,\nthese models frequently exhibit a phenomenon known as overthinking during\ninference, characterized by excessive validation loops and redundant\ndeliberation, leading to substantial computational overheads. In this paper, we\naim to mitigate overthinking by investigating the underlying mechanisms from\nthe perspective of mechanistic interpretability. We first showcase that the\ntendency of overthinking can be effectively captured by a single direction in\nthe model's activation space and the issue can be eased by intervening the\nactivations along this direction. However, this efficacy soon reaches a plateau\nand even deteriorates as the intervention strength increases. We therefore\nsystematically explore the activation space and find that the overthinking\nphenomenon is actually tied to a low-dimensional manifold, which indicates that\nthe limited effect stems from the noises introduced by the high-dimensional\nsteering direction. Based on this insight, we propose Manifold Steering, a\nnovel approach that elegantly projects the steering direction onto the\nlow-dimensional activation manifold given the theoretical approximation of the\ninterference noise. Extensive experiments on DeepSeek-R1 distilled models\nvalidate that our method reduces output tokens by up to 71% while maintaining\nand even improving the accuracy on several mathematical benchmarks. Our method\nalso exhibits robust cross-domain transferability, delivering consistent token\nreduction performance in code generation and knowledge-based QA tasks. Code is\navailable at: https://github.com/Aries-iai/Manifold_Steering."}
{"id": "2505.22609", "pdf": "https://arxiv.org/pdf/2505.22609", "abs": "https://arxiv.org/abs/2505.22609", "authors": ["Alanna Hazlett", "Naomi Ohashi", "Timothy Rodriguez", "Sodiq Adewole"], "title": "Chest Disease Detection In X-Ray Images Using Deep Learning Classification Method", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "In this work, we investigate the performance across multiple classification\nmodels to classify chest X-ray images into four categories of COVID-19,\npneumonia, tuberculosis (TB), and normal cases. We leveraged transfer learning\ntechniques with state-of-the-art pre-trained Convolutional Neural Networks\n(CNNs) models. We fine-tuned these pre-trained architectures on a labeled\nmedical x-ray images. The initial results are promising with high accuracy and\nstrong performance in key classification metrics such as precision, recall, and\nF1 score. We applied Gradient-weighted Class Activation Mapping (Grad-CAM) for\nmodel interpretability to provide visual explanations for classification\ndecisions, improving trust and transparency in clinical applications."}
{"id": "2505.22425", "pdf": "https://arxiv.org/pdf/2505.22425", "abs": "https://arxiv.org/abs/2505.22425", "authors": ["Xueliang Zhao", "Wei Wu", "Lingpeng Kong"], "title": "Scaling Reasoning without Attention", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "preprint", "summary": "Large language models (LLMs) have made significant advances in complex\nreasoning tasks, yet they remain bottlenecked by two core challenges:\narchitectural inefficiency due to reliance on Transformers, and a lack of\nstructured fine-tuning for high-difficulty domains. We introduce \\ourmodel, an\nattention-free language model that addresses both issues through architectural\nand data-centric innovations. Built on the state space dual (SSD) layers of\nMamba-2, our model eliminates the need for self-attention and key-value\ncaching, enabling fixed-memory, constant-time inference. To train it for\ncomplex reasoning, we propose a two-phase curriculum fine-tuning strategy based\non the \\textsc{PromptCoT} synthesis paradigm, which generates pedagogically\nstructured problems via abstract concept selection and rationale-guided\ngeneration. On benchmark evaluations, \\ourmodel-7B outperforms strong\nTransformer and hybrid models of comparable scale, and even surpasses the much\nlarger Gemma3-27B by 2.6\\% on AIME 24, 0.6\\% on AIME 25, and 3.0\\% on\nLivecodebench. These results highlight the potential of state space models as\nefficient and scalable alternatives to attention-based architectures for\nhigh-capacity reasoning."}
{"id": "2505.22627", "pdf": "https://arxiv.org/pdf/2505.22627", "abs": "https://arxiv.org/abs/2505.22627", "authors": ["Yijun Shen", "Delong Chen", "Fan Liu", "Xingyu Wang", "Chuanyi Zhang", "Liang Yao", "Yuhui Zheng"], "title": "Chain-of-Talkers (CoTalk): Fast Human Annotation of Dense Image Captions", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "While densely annotated image captions significantly facilitate the learning\nof robust vision-language alignment, methodologies for systematically\noptimizing human annotation efforts remain underexplored. We introduce\nChain-of-Talkers (CoTalk), an AI-in-the-loop methodology designed to maximize\nthe number of annotated samples and improve their comprehensiveness under fixed\nbudget constraints (e.g., total human annotation time). The framework is built\nupon two key insights. First, sequential annotation reduces redundant workload\ncompared to conventional parallel annotation, as subsequent annotators only\nneed to annotate the ``residual'' -- the missing visual information that\nprevious annotations have not covered. Second, humans process textual input\nfaster by reading while outputting annotations with much higher throughput via\ntalking; thus a multimodal interface enables optimized efficiency. We evaluate\nour framework from two aspects: intrinsic evaluations that assess the\ncomprehensiveness of semantic units, obtained by parsing detailed captions into\nobject-attribute trees and analyzing their effective connections; extrinsic\nevaluation measures the practical usage of the annotated captions in\nfacilitating vision-language alignment. Experiments with eight participants\nshow our Chain-of-Talkers (CoTalk) improves annotation speed (0.42 vs. 0.30\nunits/sec) and retrieval performance (41.13\\% vs. 40.52\\%) over the parallel\nmethod."}
{"id": "2505.22438", "pdf": "https://arxiv.org/pdf/2505.22438", "abs": "https://arxiv.org/abs/2505.22438", "authors": ["Zijian Liang", "Kai Niu", "Changshuo Wang", "Jin Xu", "Ping Zhang"], "title": "Synonymous Variational Inference for Perceptual Image Compression", "categories": ["cs.IT", "cs.AI", "cs.CV", "cs.LG", "eess.IV", "math.IT"], "comment": "31 pages, 20 figures. This paper is accepted by Proceedings of the\n  42nd International Conference on Machine Learning (ICML 2025) Poster", "summary": "Recent contributions of semantic information theory reveal the set-element\nrelationship between semantic and syntactic information, represented as\nsynonymous relationships. In this paper, we propose a synonymous variational\ninference (SVI) method based on this synonymity viewpoint to re-analyze the\nperceptual image compression problem. It takes perceptual similarity as a\ntypical synonymous criterion to build an ideal synonymous set (Synset), and\napproximate the posterior of its latent synonymous representation with a\nparametric density by minimizing a partial semantic KL divergence. This\nanalysis theoretically proves that the optimization direction of perception\nimage compression follows a triple tradeoff that can cover the existing\nrate-distortion-perception schemes. Additionally, we introduce synonymous image\ncompression (SIC), a new image compression scheme that corresponds to the\nanalytical process of SVI, and implement a progressive SIC codec to fully\nleverage the model's capabilities. Experimental results demonstrate comparable\nrate-distortion-perception performance using a single progressive SIC codec,\nthus verifying the effectiveness of our proposed analysis method."}
{"id": "2505.22633", "pdf": "https://arxiv.org/pdf/2505.22633", "abs": "https://arxiv.org/abs/2505.22633", "authors": ["Yida Xue", "Zhen Bi", "Jinnan Yang", "Jungang Lou", "Huajun Chen", "Ningyu Zhang"], "title": "Spatial Knowledge Graph-Guided Multimodal Synthesis", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "comment": "Ongoing work", "summary": "Recent advances in multimodal large language models (MLLMs) have\nsignificantly enhanced their capabilities; however, their spatial perception\nabilities remain a notable limitation. To address this challenge, multimodal\ndata synthesis offers a promising solution. Yet, ensuring that synthesized data\nadhere to spatial common sense is a non-trivial task. In this work, we\nintroduce SKG2Data, a novel multimodal synthesis approach guided by spatial\nknowledge graphs, grounded in the concept of knowledge-to-data generation.\nSKG2Data automatically constructs a Spatial Knowledge Graph (SKG) to emulate\nhuman-like perception of spatial directions and distances, which is\nsubsequently utilized to guide multimodal data synthesis. Extensive experiments\ndemonstrate that data synthesized from diverse types of spatial knowledge,\nincluding direction and distance, not only enhance the spatial perception and\nreasoning abilities of MLLMs but also exhibit strong generalization\ncapabilities. We hope that the idea of knowledge-based data synthesis can\nadvance the development of spatial intelligence."}
{"id": "2505.22441", "pdf": "https://arxiv.org/pdf/2505.22441", "abs": "https://arxiv.org/abs/2505.22441", "authors": ["Chaitanya Amballa", "Sattwik Basu", "Yu-Lin Wei", "Zhijian Yang", "Mehmet Ergezer", "Romit Roy Choudhury"], "title": "Can NeRFs See without Cameras?", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Neural Radiance Fields (NeRFs) have been remarkably successful at\nsynthesizing novel views of 3D scenes by optimizing a volumetric scene\nfunction. This scene function models how optical rays bring color information\nfrom a 3D object to the camera pixels. Radio frequency (RF) or audio signals\ncan also be viewed as a vehicle for delivering information about the\nenvironment to a sensor. However, unlike camera pixels, an RF/audio sensor\nreceives a mixture of signals that contain many environmental reflections (also\ncalled \"multipath\"). Is it still possible to infer the environment using such\nmultipath signals? We show that with redesign, NeRFs can be taught to learn\nfrom multipath signals, and thereby \"see\" the environment. As a grounding\napplication, we aim to infer the indoor floorplan of a home from sparse WiFi\nmeasurements made at multiple locations inside the home. Although a difficult\ninverse problem, our implicitly learnt floorplans look promising, and enables\nforward applications, such as indoor signal prediction and basic ray tracing."}
{"id": "2505.22442", "pdf": "https://arxiv.org/pdf/2505.22442", "abs": "https://arxiv.org/abs/2505.22442", "authors": ["Mattie Fellows", "Clarisse Wibault", "Uljad Berdica", "Johannes Forkel", "Jakob N. Foerster", "Michael A. Osborne"], "title": "SOReL and TOReL: Two Methods for Fully Offline Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Sample efficiency remains a major obstacle for real world adoption of\nreinforcement learning (RL): success has been limited to settings where\nsimulators provide access to essentially unlimited environment interactions,\nwhich in reality are typically costly or dangerous to obtain. Offline RL in\nprinciple offers a solution by exploiting offline data to learn a near-optimal\npolicy before deployment. In practice, however, current offline RL methods rely\non extensive online interactions for hyperparameter tuning, and have no\nreliable bound on their initial online performance. To address these two\nissues, we introduce two algorithms. Firstly, SOReL: an algorithm for safe\noffline reinforcement learning. Using only offline data, our Bayesian approach\ninfers a posterior over environment dynamics to obtain a reliable estimate of\nthe online performance via the posterior predictive uncertainty. Crucially, all\nhyperparameters are also tuned fully offline. Secondly, we introduce TOReL: a\ntuning for offline reinforcement learning algorithm that extends our\ninformation rate based offline hyperparameter tuning methods to general offline\nRL approaches. Our empirical evaluation confirms SOReL's ability to accurately\nestimate regret in the Bayesian setting whilst TOReL's offline hyperparameter\ntuning achieves competitive performance with the best online hyperparameter\ntuning methods using only offline data. Thus, SOReL and TOReL make a\nsignificant step towards safe and reliable offline RL, unlocking the potential\nfor RL in the real world. Our implementations are publicly available:\nhttps://github.com/CWibault/sorel\\_torel."}
{"id": "2505.22445", "pdf": "https://arxiv.org/pdf/2505.22445", "abs": "https://arxiv.org/abs/2505.22445", "authors": ["Puhua Jiang", "Zhangquan Chen", "Mingze Sun", "Ruqi Huang"], "title": "NFR: Neural Feature-Guided Non-Rigid Shape Registration", "categories": ["cs.CV", "cs.AI", "I.4.m; I.2.6"], "comment": "20 pages, 9 figures. arXiv admin note: substantial text overlap with\n  arXiv:2311.04494", "summary": "In this paper, we propose a novel learning-based framework for 3D shape\nregistration, which overcomes the challenges of significant non-rigid\ndeformation and partiality undergoing among input shapes, and, remarkably,\nrequires no correspondence annotation during training. Our key insight is to\nincorporate neural features learned by deep learning-based shape matching\nnetworks into an iterative, geometric shape registration pipeline. The\nadvantage of our approach is two-fold -- On one hand, neural features provide\nmore accurate and semantically meaningful correspondence estimation than\nspatial features (e.g., coordinates), which is critical in the presence of\nlarge non-rigid deformations; On the other hand, the correspondences are\ndynamically updated according to the intermediate registrations and filtered by\nconsistency prior, which prominently robustify the overall pipeline. Empirical\nresults show that, with as few as dozens of training shapes of limited\nvariability, our pipeline achieves state-of-the-art results on several\nbenchmarks of non-rigid point cloud matching and partial shape matching across\nvarying settings, but also delivers high-quality correspondences between unseen\nchallenging shape pairs that undergo both significant extrinsic and intrinsic\ndeformations, in which case neither traditional registration methods nor\nintrinsic methods work."}
{"id": "2505.22453", "pdf": "https://arxiv.org/pdf/2505.22453", "abs": "https://arxiv.org/abs/2505.22453", "authors": ["Lai Wei", "Yuting Li", "Chen Wang", "Yue Wang", "Linghe Kong", "Weiran Huang", "Lichao Sun"], "title": "Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Improving Multi-modal Large Language Models (MLLMs) in the post-training\nstage typically relies on supervised fine-tuning (SFT) or reinforcement\nlearning (RL). However, these supervised methods require expensive and manually\nannotated multi-modal data--an ultimately unsustainable resource. While recent\nefforts have explored unsupervised post-training, their methods are complex and\ndifficult to iterate. In this work, we are the first to investigate the use of\nGRPO, a stable and scalable online RL algorithm, for enabling continual\nself-improvement without any external supervision. We propose MM-UPT, a simple\nyet effective framework for unsupervised post-training of MLLMs. MM-UPT builds\nupon GRPO, replacing traditional reward signals with a self-rewarding mechanism\nbased on majority voting over multiple sampled responses. Our experiments\ndemonstrate that MM-UPT significantly improves the reasoning ability of\nQwen2.5-VL-7B (e.g., 66.3 %$\\rightarrow$72.9 % on MathVista, 62.9\n%$\\rightarrow$68.7 % on We-Math), using standard dataset without ground truth\nlabels. MM-UPT also outperforms prior unsupervised baselines and even\napproaches the results of supervised GRPO. Furthermore, we show that\nincorporating synthetic questions, generated solely by MLLM itself, can boost\nperformance as well, highlighting a promising approach for scalable\nself-improvement. Overall, MM-UPT offers a new paradigm for continual,\nautonomous enhancement of MLLMs in the absence of external supervision. Our\ncode is available at https://github.com/waltonfuture/MM-UPT."}
{"id": "2505.22457", "pdf": "https://arxiv.org/pdf/2505.22457", "abs": "https://arxiv.org/abs/2505.22457", "authors": ["Haonan Wang", "Hongfu Liu", "Xiangyan Liu", "Chao Du", "Kenji Kawaguchi", "Ye Wang", "Tianyu Pang"], "title": "Fostering Video Reasoning via Next-Event Prediction", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Next-token prediction serves as the foundational learning task enabling\nreasoning in LLMs. But what should the learning task be when aiming to equip\nMLLMs with temporal reasoning capabilities over video inputs? Existing tasks\nsuch as video question answering often rely on annotations from humans or much\nstronger MLLMs, while video captioning tends to entangle temporal reasoning\nwith spatial information. To address this gap, we propose next-event prediction\n(NEP), a learning task that harnesses future video segments as a rich,\nself-supervised signal to foster temporal reasoning. We segment each video into\npast and future frames: the MLLM takes the past frames as input and predicts a\nsummary of events derived from the future frames, thereby encouraging the model\nto reason temporally in order to complete the task. To support this task, we\ncurate V1-33K, a dataset comprising 33,000 automatically extracted video\nsegments spanning diverse real-world scenarios. We further explore a range of\nvideo instruction-tuning strategies to study their effects on temporal\nreasoning. To evaluate progress, we introduce FutureBench to assess coherence\nin predicting unseen future events. Experiments validate that NEP offers a\nscalable and effective training paradigm for fostering temporal reasoning in\nMLLMs."}
{"id": "2505.22467", "pdf": "https://arxiv.org/pdf/2505.22467", "abs": "https://arxiv.org/abs/2505.22467", "authors": ["Jiaxi Yang", "Mengqi Zhang", "Yiqiao Jin", "Hao Chen", "Qingsong Wen", "Lu Lin", "Yi He", "Weijie Xu", "James Evans", "Jindong Wang"], "title": "Topological Structure Learning Should Be A Research Priority for LLM-Based Multi-Agent Systems", "categories": ["cs.MA", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Model-based Multi-Agent Systems (MASs) have emerged as a\npowerful paradigm for tackling complex tasks through collaborative\nintelligence. Nevertheless, the question of how agents should be structurally\norganized for optimal cooperation remains largely unexplored. In this position\npaper, we aim to gently redirect the focus of the MAS research community toward\nthis critical dimension: develop topology-aware MASs for specific tasks.\nSpecifically, the system consists of three core components - agents,\ncommunication links, and communication patterns - that collectively shape its\ncoordination performance and efficiency. To this end, we introduce a\nsystematic, three-stage framework: agent selection, structure profiling, and\ntopology synthesis. Each stage would trigger new research opportunities in\nareas such as language models, reinforcement learning, graph learning, and\ngenerative modeling; together, they could unleash the full potential of MASs in\ncomplicated real-world applications. Then, we discuss the potential challenges\nand opportunities in the evaluation of multiple systems. We hope our\nperspective and framework can offer critical new insights in the era of agentic\nAI."}
{"id": "2505.22477", "pdf": "https://arxiv.org/pdf/2505.22477", "abs": "https://arxiv.org/abs/2505.22477", "authors": ["Qi Gao", "Wei Xu", "Hanxi Pan", "Mowei Shen", "Zaifeng Gao"], "title": "Human-Centered Human-AI Collaboration (HCHAC)", "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": "This article is a chapter from the upcoming book Handbook of\n  Human-Centered Artificial Intelligence", "summary": "In the intelligent era, the interaction between humans and intelligent\nsystems fundamentally involves collaboration with autonomous intelligent\nagents. Human-AI Collaboration (HAC) represents a novel type of human-machine\nrelationship facilitated by autonomous intelligent machines equipped with AI\ntechnologies. In this paradigm, AI agents serve not only as auxiliary tools but\nalso as active teammates, partnering with humans to accomplish tasks\ncollaboratively. Human-centered AI (HCAI) emphasizes that humans play critical\nleadership roles in the collaboration. This human-led collaboration imparts new\ndimensions to the human-machine relationship, necessitating innovative research\nperspectives, paradigms, and agenda to address the unique challenges posed by\nHAC. This chapter delves into the essence of HAC from the human-centered\nperspective, outlining its core concepts and distinguishing features. It\nreviews the current research methodologies and research agenda within the HAC\nfield from the HCAI perspective, highlighting advancements and ongoing studies.\nFurthermore, a framework for human-centered HAC (HCHAC) is proposed by\nintegrating these reviews and analyses. A case study of HAC in the context of\nautonomous vehicles is provided, illustrating practical applications and the\nsynergistic interactions between humans and AI agents. Finally, it identifies\npotential future research directions aimed at enhancing the effectiveness,\nreliability, and ethical integration of human-centered HAC systems in diverse\ndomains."}
{"id": "2505.22483", "pdf": "https://arxiv.org/pdf/2505.22483", "abs": "https://arxiv.org/abs/2505.22483", "authors": ["Abhra Chaudhuri", "Anjan Dutta", "Tu Bui", "Serban Georgescu"], "title": "A Closer Look at Multimodal Representation Collapse", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "International Conference on Machine Learning (ICML) 2025 (Spotlight)", "summary": "We aim to develop a fundamental understanding of modality collapse, a\nrecently observed empirical phenomenon wherein models trained for multimodal\nfusion tend to rely only on a subset of the modalities, ignoring the rest. We\nshow that modality collapse happens when noisy features from one modality are\nentangled, via a shared set of neurons in the fusion head, with predictive\nfeatures from another, effectively masking out positive contributions from the\npredictive features of the former modality and leading to its collapse. We\nfurther prove that cross-modal knowledge distillation implicitly disentangles\nsuch representations by freeing up rank bottlenecks in the student encoder,\ndenoising the fusion-head outputs without negatively impacting the predictive\nfeatures from either modality. Based on the above findings, we propose an\nalgorithm that prevents modality collapse through explicit basis reallocation,\nwith applications in dealing with missing modalities. Extensive experiments on\nmultiple multimodal benchmarks validate our theoretical claims. Project page:\nhttps://abhrac.github.io/mmcollapse/."}
{"id": "2505.22491", "pdf": "https://arxiv.org/pdf/2505.22491", "abs": "https://arxiv.org/abs/2505.22491", "authors": ["Moritz Haas", "Sebastian Bordt", "Ulrike von Luxburg", "Leena Chennuru Vankadara"], "title": "On the Surprising Effectiveness of Large Learning Rates under Standard Width Scaling", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "The dominant paradigm for training large-scale vision and language models is\nHe initialization and a single global learning rate (\\textit{standard\nparameterization}, SP). Despite its practical success, standard parametrization\nremains poorly understood from a theoretical perspective: Existing\ninfinite-width theory would predict instability under large learning rates and\nvanishing feature learning under stable learning rates. However, empirically\noptimal learning rates consistently decay much slower than theoretically\npredicted. By carefully studying neural network training dynamics, we\ndemonstrate that this discrepancy is not fully explained by finite-width\nphenomena such as catapult effects or a lack of alignment between weights and\nincoming activations. We instead show that the apparent contradiction can be\nfundamentally resolved by taking the loss function into account: In contrast to\nMean Squared Error (MSE) loss, we prove that under cross-entropy (CE) loss, an\nintermediate \\textit{controlled divergence} regime emerges, where logits\ndiverge but loss, gradients, and activations remain stable. Stable training\nunder large learning rates enables persistent feature evolution at scale in all\nhidden layers, which is crucial for the practical success of SP. In experiments\nacross optimizers (SGD, Adam), architectures (MLPs, GPT) and data modalities\n(vision, language), we validate that neural networks operate in this controlled\ndivergence regime under CE loss but not under MSE loss. Our empirical evidence\nsuggests that width-scaling considerations are surprisingly useful for\npredicting empirically optimal learning rate exponents. Finally, our analysis\nclarifies the effectiveness and limitations of recently proposed layerwise\nlearning rate scalings for standard initialization."}
{"id": "2505.22492", "pdf": "https://arxiv.org/pdf/2505.22492", "abs": "https://arxiv.org/abs/2505.22492", "authors": ["Hongyi Zhou", "Josiah P. Hanna", "Jin Zhu", "Ying Yang", "Chengchun Shi"], "title": "Demystifying the Paradox of Importance Sampling with an Estimated History-Dependent Behavior Policy in Off-Policy Evaluation", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "Accepted by ICML 2025", "summary": "This paper studies off-policy evaluation (OPE) in reinforcement learning with\na focus on behavior policy estimation for importance sampling. Prior work has\nshown empirically that estimating a history-dependent behavior policy can lead\nto lower mean squared error (MSE) even when the true behavior policy is\nMarkovian. However, the question of why the use of history should lower MSE\nremains open. In this paper, we theoretically demystify this paradox by\nderiving a bias-variance decomposition of the MSE of ordinary importance\nsampling (IS) estimators, demonstrating that history-dependent behavior policy\nestimation decreases their asymptotic variances while increasing their\nfinite-sample biases. Additionally, as the estimated behavior policy conditions\non a longer history, we show a consistent decrease in variance. We extend these\nfindings to a range of other OPE estimators, including the sequential IS\nestimator, the doubly robust estimator and the marginalized IS estimator, with\nthe behavior policy estimated either parametrically or non-parametrically."}
{"id": "2505.22503", "pdf": "https://arxiv.org/pdf/2505.22503", "abs": "https://arxiv.org/abs/2505.22503", "authors": ["Yuanfei Wang", "Xinju Huang", "Fangwei Zhong", "Yaodong Yang", "Yizhou Wang", "Yuanpei Chen", "Hao Dong"], "title": "From Strangers to Assistants: Fast Desire Alignment for Embodied Agent-User Adaptation", "categories": ["cs.RO", "cs.AI", "cs.MA"], "comment": null, "summary": "While embodied agents have made significant progress in performing complex\nphysical tasks, real-world applications demand more than pure task execution.\nThe agents must collaborate with unfamiliar agents and human users, whose goals\nare often vague and implicit. In such settings, interpreting ambiguous\ninstructions and uncovering underlying desires is essential for effective\nassistance. Therefore, fast and accurate desire alignment becomes a critical\ncapability for embodied agents. In this work, we first develop a home\nassistance simulation environment HA-Desire that integrates an LLM-driven human\nuser agent exhibiting realistic value-driven goal selection and communication.\nThe ego agent must interact with this proxy user to infer and adapt to the\nuser's latent desires. To achieve this, we present a novel framework FAMER for\nfast desire alignment, which introduces a desire-based mental reasoning\nmechanism to identify user intent and filter desire-irrelevant actions. We\nfurther design a reflection-based communication module that reduces redundant\ninquiries, and incorporate goal-relevant information extraction with memory\npersistence to improve information reuse and reduce unnecessary exploration.\nExtensive experiments demonstrate that our framework significantly enhances\nboth task execution and communication efficiency, enabling embodied agents to\nquickly adapt to user-specific desires in complex embodied environments."}
{"id": "2505.22513", "pdf": "https://arxiv.org/pdf/2505.22513", "abs": "https://arxiv.org/abs/2505.22513", "authors": ["Bradley Phillips", "Edith Elkind", "Nicholas Teh", "Tomasz Wąs"], "title": "Strengthening Proportionality in Temporal Voting", "categories": ["cs.GT", "cs.AI"], "comment": null, "summary": "We study proportional representation in the framework of temporal voting with\napproval ballots. Prior work adapted basic proportional representation concepts\n-- justified representation (JR), proportional JR (PJR), and extended JR (EJR)\n-- from the multiwinner setting to the temporal setting. Our work introduces\nand examines ways of going beyond EJR. Specifically, we consider stronger\nvariants of JR, PJR, and EJR, and introduce temporal adaptations of more\ndemanding multiwinner axioms, such as EJR+, full JR (FJR), full proportional JR\n(FPJR), and the Core. For each of these concepts, we investigate its existence\nand study its relationship to existing notions, thereby establishing a rich\nhierarchy of proportionality concepts. Notably, we show that two of our\nproposed axioms -- EJR+ and FJR -- strengthen EJR while remaining satisfiable\nin every temporal election."}
{"id": "2505.22521", "pdf": "https://arxiv.org/pdf/2505.22521", "abs": "https://arxiv.org/abs/2505.22521", "authors": ["Chao Wang", "Chuanhao Nie", "Yunbo Liu"], "title": "Evaluating Supervised Learning Models for Fraud Detection: A Comparative Study of Classical and Deep Architectures on Imbalanced Transaction Data", "categories": ["cs.LG", "cs.AI"], "comment": "5 pages. Chao Wang, Chuanhao Nie, and Yunbo Liu contributed equally\n  to this work. Corresponding author: Yunbo Liu (yunbo.liu954@duke.edu).\n  Submitted to the 3rd International Conference on Management Innovation and\n  Economy Development (MIED 2025), Chongqing, China", "summary": "Fraud detection remains a critical task in high-stakes domains such as\nfinance and e-commerce, where undetected fraudulent transactions can lead to\nsignificant economic losses. In this study, we systematically compare the\nperformance of four supervised learning models - Logistic Regression, Random\nForest, Light Gradient Boosting Machine (LightGBM), and a Gated Recurrent Unit\n(GRU) network - on a large-scale, highly imbalanced online transaction dataset.\nWhile ensemble methods such as Random Forest and LightGBM demonstrated superior\nperformance in both overall and class-specific metrics, Logistic Regression\noffered a reliable and interpretable baseline. The GRU model showed strong\nrecall for the minority fraud class, though at the cost of precision,\nhighlighting a trade-off relevant for real-world deployment. Our evaluation\nemphasizes not only weighted averages but also per-class precision, recall, and\nF1-scores, providing a nuanced view of each model's effectiveness in detecting\nrare but consequential fraudulent activity. The findings underscore the\nimportance of choosing models based on the specific risk tolerance and\noperational needs of fraud detection systems."}
{"id": "2505.22525", "pdf": "https://arxiv.org/pdf/2505.22525", "abs": "https://arxiv.org/abs/2505.22525", "authors": ["Ethan Chern", "Zhulin Hu", "Steffi Chern", "Siqi Kou", "Jiadi Su", "Yan Ma", "Zhijie Deng", "Pengfei Liu"], "title": "Thinking with Generated Images", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "We present Thinking with Generated Images, a novel paradigm that\nfundamentally transforms how large multimodal models (LMMs) engage with visual\nreasoning by enabling them to natively think across text and vision modalities\nthrough spontaneous generation of intermediate visual thinking steps. Current\nvisual reasoning with LMMs is constrained to either processing fixed\nuser-provided images or reasoning solely through text-based chain-of-thought\n(CoT). Thinking with Generated Images unlocks a new dimension of cognitive\ncapability where models can actively construct intermediate visual thoughts,\ncritique their own visual hypotheses, and refine them as integral components of\ntheir reasoning process. We demonstrate the effectiveness of our approach\nthrough two complementary mechanisms: (1) vision generation with intermediate\nvisual subgoals, where models decompose complex visual tasks into manageable\ncomponents that are generated and integrated progressively, and (2) vision\ngeneration with self-critique, where models generate an initial visual\nhypothesis, analyze its shortcomings through textual reasoning, and produce\nrefined outputs based on their own critiques. Our experiments on vision\ngeneration benchmarks show substantial improvements over baseline approaches,\nwith our models achieving up to 50% (from 38% to 57%) relative improvement in\nhandling complex multi-object scenarios. From biochemists exploring novel\nprotein structures, and architects iterating on spatial designs, to forensic\nanalysts reconstructing crime scenes, and basketball players envisioning\nstrategic plays, our approach enables AI models to engage in the kind of visual\nimagination and iterative refinement that characterizes human creative,\nanalytical, and strategic thinking. We release our open-source suite at\nhttps://github.com/GAIR-NLP/thinking-with-generated-images."}
{"id": "2505.22531", "pdf": "https://arxiv.org/pdf/2505.22531", "abs": "https://arxiv.org/abs/2505.22531", "authors": ["Andres Molina-Markham", "Luis Robaina", "Sean Steinle", "Akash Trivedi", "Derek Tsui", "Nicholas Potteiger", "Lauren Brandt", "Ransom Winder", "Ahmed Ridley"], "title": "Training RL Agents for Multi-Objective Network Defense Tasks", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Open-ended learning (OEL) -- which emphasizes training agents that achieve\nbroad capability over narrow competency -- is emerging as a paradigm to develop\nartificial intelligence (AI) agents to achieve robustness and generalization.\nHowever, despite promising results that demonstrate the benefits of OEL,\napplying OEL to develop autonomous agents for real-world cybersecurity\napplications remains a challenge.\n  We propose a training approach, inspired by OEL, to develop autonomous\nnetwork defenders. Our results demonstrate that like in other domains, OEL\nprinciples can translate into more robust and generalizable agents for cyber\ndefense. To apply OEL to network defense, it is necessary to address several\ntechnical challenges. Most importantly, it is critical to provide a task\nrepresentation approach over a broad universe of tasks that maintains a\nconsistent interface over goals, rewards and action spaces. This way, the\nlearning agent can train with varying network conditions, attacker behaviors,\nand defender goals while being able to build on previously gained knowledge.\n  With our tools and results, we aim to fundamentally impact research that\napplies AI to solve cybersecurity problems. Specifically, as researchers\ndevelop gyms and benchmarks for cyber defense, it is paramount that they\nconsider diverse tasks with consistent representations, such as those we\npropose in our work."}
{"id": "2505.22533", "pdf": "https://arxiv.org/pdf/2505.22533", "abs": "https://arxiv.org/abs/2505.22533", "authors": ["Pallavi Bhardwaj", "Caitlin Jones", "Lasse Dierich", "Aleksandar Vučković"], "title": "TabularQGAN: A Quantum Generative Model for Tabular Data", "categories": ["cs.LG", "cs.AI", "quant-ph"], "comment": "18 pages,8 figures and 4 tables", "summary": "In this paper, we introduce a novel quantum generative model for synthesizing\ntabular data. Synthetic data is valuable in scenarios where real-world data is\nscarce or private, it can be used to augment or replace existing datasets.\nReal-world enterprise data is predominantly tabular and heterogeneous, often\ncomprising a mixture of categorical and numerical features, making it highly\nrelevant across various industries such as healthcare, finance, and software.\nWe propose a quantum generative adversarial network architecture with flexible\ndata encoding and a novel quantum circuit ansatz to effectively model tabular\ndata. The proposed approach is tested on the MIMIC III healthcare and Adult\nCensus datasets, with extensive benchmarking against leading classical models,\nCTGAN, and CopulaGAN. Experimental results demonstrate that our quantum model\noutperforms classical models by an average of 8.5% with respect to an overall\nsimilarity score from SDMetrics, while using only 0.072% of the parameters of\nthe classical models. Additionally, we evaluate the generalization capabilities\nof the models using two custom-designed metrics that demonstrate the ability of\nthe proposed quantum model to generate useful and novel samples. To our\nknowledge, this is one of the first demonstrations of a successful quantum\ngenerative model for handling tabular data, indicating that this task could be\nwell-suited to quantum computers."}
{"id": "2505.22543", "pdf": "https://arxiv.org/pdf/2505.22543", "abs": "https://arxiv.org/abs/2505.22543", "authors": ["Ziheng Jia", "Zicheng Zhang", "Zeyu Zhang", "Yingji Liang", "Xiaorong Zhu", "Chunyi Li", "Jinliang Han", "Haoning Wu", "Bin Wang", "Haoran Zhang", "Guanyu Zhu", "Qiyong Zhao", "Xiaohong Liu", "Guangtao Zhai", "Xiongkuo Min"], "title": "Scaling-up Perceptual Video Quality Assessment", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The data scaling law has been shown to significantly enhance the performance\nof large multi-modal models (LMMs) across various downstream tasks. However, in\nthe domain of perceptual video quality assessment (VQA), the potential of\nscaling law remains unprecedented due to the scarcity of labeled resources and\nthe insufficient scale of datasets. To address this, we propose\n\\textbf{OmniVQA}, an efficient framework designed to efficiently build\nhigh-quality, human-in-the-loop VQA multi-modal instruction databases (MIDBs).\nWe then scale up to create \\textbf{OmniVQA-Chat-400K}, the largest MIDB in the\nVQA field concurrently. Our focus is on the technical and aesthetic quality\ndimensions, with abundant in-context instruction data to provide fine-grained\nVQA knowledge. Additionally, we have built the \\textbf{OmniVQA-MOS-20K} dataset\nto enhance the model's quantitative quality rating capabilities. We then\nintroduce a \\textbf{complementary} training strategy that effectively leverages\nthe knowledge from datasets for quality understanding and quality rating tasks.\nFurthermore, we propose the \\textbf{OmniVQA-FG (fine-grain)-Benchmark} to\nevaluate the fine-grained performance of the models. Our results demonstrate\nthat our models achieve state-of-the-art performance in both quality\nunderstanding and rating tasks."}
{"id": "2505.22552", "pdf": "https://arxiv.org/pdf/2505.22552", "abs": "https://arxiv.org/abs/2505.22552", "authors": ["Hoang Pham", "Thanh-Do Nguyen", "Khac-Hoai Nam Bui"], "title": "ClaimPKG: Enhancing Claim Verification via Pseudo-Subgraph Generation with Lightweight Specialized LLM", "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": "Accepted by ACL 2025 findings", "summary": "Integrating knowledge graphs (KGs) to enhance the reasoning capabilities of\nlarge language models (LLMs) is an emerging research challenge in claim\nverification. While KGs provide structured, semantically rich representations\nwell-suited for reasoning, most existing verification methods rely on\nunstructured text corpora, limiting their ability to effectively leverage KGs.\nAdditionally, despite possessing strong reasoning abilities, modern LLMs\nstruggle with multi-step modular pipelines and reasoning over KGs without\nadaptation. To address these challenges, we propose ClaimPKG, an end-to-end\nframework that seamlessly integrates LLM reasoning with structured knowledge\nfrom KGs. Specifically, the main idea of ClaimPKG is to employ a lightweight,\nspecialized LLM to represent the input claim as pseudo-subgraphs, guiding a\ndedicated subgraph retrieval module to identify relevant KG subgraphs. These\nretrieved subgraphs are then processed by a general-purpose LLM to produce the\nfinal verdict and justification. Extensive experiments on the FactKG dataset\ndemonstrate that ClaimPKG achieves state-of-the-art performance, outperforming\nstrong baselines in this research field by 9%-12% accuracy points across\nmultiple categories. Furthermore, ClaimPKG exhibits zero-shot generalizability\nto unstructured datasets such as HoVer and FEVEROUS, effectively combining\nstructured knowledge from KGs with LLM reasoning across various LLM backbones."}
{"id": "2505.22564", "pdf": "https://arxiv.org/pdf/2505.22564", "abs": "https://arxiv.org/abs/2505.22564", "authors": ["Jaehyun Choi", "Jiwan Hur", "Gyojin Han", "Jaemyung Yu", "Junmo Kim"], "title": "PRISM: Video Dataset Condensation with Progressive Refinement and Insertion for Sparse Motion", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Video dataset condensation has emerged as a critical technique for addressing\nthe computational challenges associated with large-scale video data processing\nin deep learning applications. While significant progress has been made in\nimage dataset condensation, the video domain presents unique challenges due to\nthe complex interplay between spatial content and temporal dynamics. This paper\nintroduces PRISM, Progressive Refinement and Insertion for Sparse Motion, for\nvideo dataset condensation, a novel approach that fundamentally reconsiders how\nvideo data should be condensed. Unlike the previous method that separates\nstatic content from dynamic motion, our method preserves the essential\ninterdependence between these elements. Our approach progressively refines and\ninserts frames to fully accommodate the motion in an action while achieving\nbetter performance but less storage, considering the relation of gradients for\neach frame. Extensive experiments across standard video action recognition\nbenchmarks demonstrate that PRISM outperforms existing disentangled approaches\nwhile maintaining compact representations suitable for resource-constrained\nenvironments."}
{"id": "2505.22566", "pdf": "https://arxiv.org/pdf/2505.22566", "abs": "https://arxiv.org/abs/2505.22566", "authors": ["Yifan Xie", "Mingyang Li", "Shoujie Li", "Xingting Li", "Guangyu Chen", "Fei Ma", "Fei Richard Yu", "Wenbo Ding"], "title": "Universal Visuo-Tactile Video Understanding for Embodied Interaction", "categories": ["cs.CV", "cs.AI"], "comment": "13 pages, 5 figures", "summary": "Tactile perception is essential for embodied agents to understand physical\nattributes of objects that cannot be determined through visual inspection\nalone. While existing approaches have made progress in visual and language\nmodalities for physical understanding, they fail to effectively incorporate\ntactile information that provides crucial haptic feedback for real-world\ninteraction. In this paper, we present VTV-LLM, the first multi-modal large\nlanguage model for universal Visuo-Tactile Video (VTV) understanding that\nbridges the gap between tactile perception and natural language. To address the\nchallenges of cross-sensor and cross-modal integration, we contribute VTV150K,\na comprehensive dataset comprising 150,000 video frames from 100 diverse\nobjects captured across three different tactile sensors (GelSight Mini, DIGIT,\nand Tac3D), annotated with four fundamental tactile attributes (hardness,\nprotrusion, elasticity, and friction). We develop a novel three-stage training\nparadigm that includes VTV enhancement for robust visuo-tactile representation,\nVTV-text alignment for cross-modal correspondence, and text prompt finetuning\nfor natural language generation. Our framework enables sophisticated tactile\nreasoning capabilities including feature assessment, comparative analysis,\nscenario-based decision making and so on. Experimental evaluations demonstrate\nthat VTV-LLM achieves superior performance in tactile video understanding\ntasks, establishing a foundation for more intuitive human-machine interaction\nin tactile domains."}
{"id": "2505.22571", "pdf": "https://arxiv.org/pdf/2505.22571", "abs": "https://arxiv.org/abs/2505.22571", "authors": ["Hoang Pham", "Khac-Hoai Nam Bui"], "title": "Agent-UniRAG: A Trainable Open-Source LLM Agent Framework for Unified Retrieval-Augmented Generation Systems", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR"], "comment": null, "summary": "This paper presents a novel approach for unified retrieval-augmented\ngeneration (RAG) systems using the recent emerging large language model (LLM)\nagent concept. Specifically, Agent LLM, which utilizes LLM as fundamental\ncontrollers, has become a promising approach to enable the interpretability of\nRAG tasks, especially for complex reasoning question-answering systems (e.g.,\nmulti-hop queries). Nonetheless, previous works mainly focus on solving RAG\nsystems with either single-hop or multi-hop approaches separately, which limits\nthe application of those approaches to real-world applications. In this study,\nwe propose a trainable agent framework called Agent-UniRAG for unified\nretrieval-augmented LLM systems, which enhances the effectiveness and\ninterpretability of RAG systems. The main idea is to design an LLM agent\nframework to solve RAG tasks step-by-step based on the complexity of the\ninputs, simultaneously including single-hop and multi-hop queries in an\nend-to-end manner. Furthermore, we introduce SynAgent-RAG, a synthetic dataset\nto enable the proposed agent framework for small open-source LLMs (e.g.,\nLlama-3-8B). The results show comparable performances with closed-source and\nlarger open-source LLMs across various RAG benchmarks. Our source code and\ndataset are publicly available for further exploitation."}
{"id": "2505.22572", "pdf": "https://arxiv.org/pdf/2505.22572", "abs": "https://arxiv.org/abs/2505.22572", "authors": ["Waldemar Chang", "Alhassan Yasin"], "title": "Fusion Steering: Prompt-Specific Activation Control", "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 4 figures, 2 tables", "summary": "We present Fusion Steering, an activation steering methodology that improves\nfactual accuracy in large language models (LLMs) for question-answering (QA)\ntasks. This approach introduces flexible steering configurations, including\nfull-layer steering and segmented steering. Unlike traditional methods\nconstrained to single-layer or fixed-layer operations, Fusion Steering employs\ndynamic injection of prompt-specific activation deltas across all transformer\nlayers. These activation deltas are derived from reference completions that\ncombine the ground-truth answer with a model-generated explanation to\nfacilitate semantically enriched, example-specific steering. The injection\nweights are optimized per prompt using Optuna, targeting a joint objective that\nbalances token overlap (factual alignment) and perplexity (fluency proxy).\nEvaluation employs a composite score integrating token overlap and LLM-graded\nquality, encompassing factual accuracy, coherence, and relevance. Empirical\nresults on 260 SimpleQA prompts (selected from 500 where the baseline failed)\nshowcase the efficacy of segmented steering. Using Gemma-2-2B-IT with 8-bit\nquantization, segmented steering achieves an accuracy of 25.4% (outputs scoring\n$\\geq 0.6$), outperforming the baseline at 3.5% and full-layer steering at\n16.2%. Under the stricter SimpleQA rubric, segmented steering boosts fully\ncorrect responses from 0.0% to 13.1%. These findings highlight the strengths of\nsegmented, dynamic intervention strategies and the promise of per-prompt,\nfull-network activation control. Fusion Steering is also amenable to sparse\nrepresentations, such as Neuronpedia or sparse crosscoders, suggesting a\npromising direction for interpretable and scalable activation-level control in\nLLMs."}
{"id": "2505.22581", "pdf": "https://arxiv.org/pdf/2505.22581", "abs": "https://arxiv.org/abs/2505.22581", "authors": ["Kartik Kuckreja", "Parul Gupta", "Injy Hamed", "Thamar Solorio", "Muhammad Haris Khan", "Abhinav Dhall"], "title": "Tell me Habibi, is it Real or Fake?", "categories": ["cs.CV", "cs.AI"], "comment": "9 pages, 2 figures, 12 tables", "summary": "Deepfake generation methods are evolving fast, making fake media harder to\ndetect and raising serious societal concerns. Most deepfake detection and\ndataset creation research focuses on monolingual content, often overlooking the\nchallenges of multilingual and code-switched speech, where multiple languages\nare mixed within the same discourse. Code-switching, especially between Arabic\nand English, is common in the Arab world and is widely used in digital\ncommunication. This linguistic mixing poses extra challenges for deepfake\ndetection, as it can confuse models trained mostly on monolingual data. To\naddress this, we introduce \\textbf{ArEnAV}, the first large-scale\nArabic-English audio-visual deepfake dataset featuring intra-utterance\ncode-switching, dialectal variation, and monolingual Arabic content. It\n\\textbf{contains 387k videos and over 765 hours of real and fake videos}. Our\ndataset is generated using a novel pipeline integrating four Text-To-Speech and\ntwo lip-sync models, enabling comprehensive analysis of multilingual multimodal\ndeepfake detection. We benchmark our dataset against existing monolingual and\nmultilingual datasets, state-of-the-art deepfake detection models, and a human\nevaluation, highlighting its potential to advance deepfake research. The\ndataset can be accessed\n\\href{https://huggingface.co/datasets/kartik060702/ArEnAV-Full}{here}."}
{"id": "2505.22583", "pdf": "https://arxiv.org/pdf/2505.22583", "abs": "https://arxiv.org/abs/2505.22583", "authors": ["Tobias Lindenbauer", "Egor Bogomolov", "Yaroslav Zharov"], "title": "GitGoodBench: A Novel Benchmark For Evaluating Agentic Performance On Git", "categories": ["cs.SE", "cs.AI"], "comment": "Short Paper, 5 pages", "summary": "Benchmarks for Software Engineering (SE) AI agents, most notably SWE-bench,\nhave catalyzed progress in programming capabilities of AI agents. However, they\noverlook critical developer workflows such as Version Control System (VCS)\noperations. To address this issue, we present GitGoodBench, a novel benchmark\nfor evaluating AI agent performance on VCS tasks. GitGoodBench covers three\ncore Git scenarios extracted from permissive open-source Python, Java, and\nKotlin repositories. Our benchmark provides three datasets: a comprehensive\nevaluation suite (900 samples), a rapid prototyping version (120 samples), and\na training corpus (17,469 samples). We establish baseline performance on the\nprototyping version of our benchmark using GPT-4o equipped with custom tools,\nachieving a 21.11% solve rate overall. We expect GitGoodBench to serve as a\ncrucial stepping stone toward truly comprehensive SE agents that go beyond mere\nprogramming."}
{"id": "2505.22591", "pdf": "https://arxiv.org/pdf/2505.22591", "abs": "https://arxiv.org/abs/2505.22591", "authors": ["Erxin Yu", "Jing Li", "Ming Liao", "Qi Zhu", "Boyang Xue", "Minghui Xu", "Baojun Wang", "Lanqing Hong", "Fei Mi", "Lifeng Shang"], "title": "Self-Error-Instruct: Generalizing from Errors for LLMs Mathematical Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "16 pages, 9 figures", "summary": "Although large language models demonstrate strong performance across various\ndomains, they still struggle with numerous bad cases in mathematical reasoning.\nPrevious approaches to learning from errors synthesize training data by solely\nextrapolating from isolated bad cases, thereby failing to generalize the\nextensive patterns inherent within these cases. This paper presents\nSelf-Error-Instruct (SEI), a framework that addresses these model weaknesses\nand synthesizes more generalized targeted training data. Specifically, we\nexplore a target model on two mathematical datasets, GSM8K and MATH, to\npinpoint bad cases. Then, we generate error keyphrases for these cases based on\nthe instructor model's (GPT-4o) analysis and identify error types by clustering\nthese keyphrases. Next, we sample a few bad cases during each generation for\neach identified error type and input them into the instructor model, which\nsynthesizes additional training data using a self-instruct approach. This new\ndata is refined through a one-shot learning process to ensure that only the\nmost effective examples are kept. Finally, we use these curated data to\nfine-tune the target model, iteratively repeating the process to enhance\nperformance. We apply our framework to various models and observe improvements\nin their reasoning abilities across both in-domain and out-of-domain\nmathematics datasets. These results demonstrate the effectiveness of self-error\ninstruction in improving LLMs' mathematical reasoning through error\ngeneralization."}
{"id": "2505.22598", "pdf": "https://arxiv.org/pdf/2505.22598", "abs": "https://arxiv.org/abs/2505.22598", "authors": ["Luca Maria Del Bono", "Federico Ricci-Tersenghi", "Francesco Zamponi"], "title": "On the performance of machine-learning assisted Monte Carlo in sampling from simple statistical physics models", "categories": ["cond-mat.dis-nn", "cond-mat.stat-mech", "cs.AI", "cs.LG", "physics.comp-ph"], "comment": "16 pages, 9 figures", "summary": "Recent years have seen a rise in the application of machine learning\ntechniques to aid the simulation of hard-to-sample systems that cannot be\nstudied using traditional methods. Despite the introduction of many different\narchitectures and procedures, a wide theoretical understanding is still\nlacking, with the risk of suboptimal implementations. As a first step to\naddress this gap, we provide here a complete analytic study of the widely-used\nSequential Tempering procedure applied to a shallow MADE architecture for the\nCurie-Weiss model. The contribution of this work is twofold: firstly, we give a\ndescription of the optimal weights and of the training under Gradient Descent\noptimization. Secondly, we compare what happens in Sequential Tempering with\nand without the addition of local Metropolis Monte Carlo steps. We are thus\nable to give theoretical predictions on the best procedure to apply in this\ncase. This work establishes a clear theoretical basis for the integration of\nmachine learning techniques into Monte Carlo sampling and optimization."}
{"id": "2505.22601", "pdf": "https://arxiv.org/pdf/2505.22601", "abs": "https://arxiv.org/abs/2505.22601", "authors": ["Jacob L. Block", "Aryan Mokhtari", "Sanjay Shakkottai"], "title": "Machine Unlearning under Overparameterization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Machine unlearning algorithms aim to remove the influence of specific\ntraining samples, ideally recovering the model that would have resulted from\ntraining on the remaining data alone. We study unlearning in the\noverparameterized setting, where many models interpolate the data, and defining\nthe unlearning solution as any loss minimizer over the retained\nset$\\unicode{x2013}$as in prior work in the underparameterized\nsetting$\\unicode{x2013}$is inadequate, since the original model may already\ninterpolate the retained data and satisfy this condition. In this regime, loss\ngradients vanish, rendering prior methods based on gradient perturbations\nineffective, motivating both new unlearning definitions and algorithms. For\nthis setting, we define the unlearning solution as the minimum-complexity\ninterpolator over the retained data and propose a new algorithmic framework\nthat only requires access to model gradients on the retained set at the\noriginal solution. We minimize a regularized objective over perturbations\nconstrained to be orthogonal to these model gradients, a first-order relaxation\nof the interpolation condition. For different model classes, we provide exact\nand approximate unlearning guarantees, and we demonstrate that an\nimplementation of our framework outperforms existing baselines across various\nunlearning experiments."}
{"id": "2505.22602", "pdf": "https://arxiv.org/pdf/2505.22602", "abs": "https://arxiv.org/abs/2505.22602", "authors": ["Mahtab Alizadeh Vandchali", "Fangshuo", "Liao", "Anastasios Kyrillidis"], "title": "One Rank at a Time: Cascading Error Dynamics in Sequential Learning", "categories": ["cs.LG", "cs.AI", "math.OC"], "comment": "36 pages", "summary": "Sequential learning -- where complex tasks are broken down into simpler,\nhierarchical components -- has emerged as a paradigm in AI. This paper views\nsequential learning through the lens of low-rank linear regression, focusing\nspecifically on how errors propagate when learning rank-1 subspaces\nsequentially. We present an analysis framework that decomposes the learning\nprocess into a series of rank-1 estimation problems, where each subsequent\nestimation depends on the accuracy of previous steps. Our contribution is a\ncharacterization of the error propagation in this sequential process,\nestablishing bounds on how errors -- e.g., due to limited computational budgets\nand finite precision -- affect the overall model accuracy. We prove that these\nerrors compound in predictable ways, with implications for both algorithmic\ndesign and stability guarantees."}
{"id": "2505.22608", "pdf": "https://arxiv.org/pdf/2505.22608", "abs": "https://arxiv.org/abs/2505.22608", "authors": ["Haoning Xu", "Zhaoqing Li", "Youjun Chen", "Huimeng Wang", "Guinan Li", "Mengzhe Geng", "Chengxi Deng", "Xunying Liu"], "title": "Effective and Efficient One-pass Compression of Speech Foundation Models Using Sparsity-aware Self-pinching Gates", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "Submitted to Interspeech 2025", "summary": "This paper presents a novel approach for speech foundation models compression\nthat tightly integrates model pruning and parameter update into a single stage.\nHighly compact layer-level tied self-pinching gates each containing only a\nsingle learnable threshold are jointly trained with uncompressed models and\nused in fine-grained neuron level pruning. Experiments conducted on the\nLibriSpeech-100hr corpus suggest that our approach reduces the number of\nparameters of wav2vec2.0-base and HuBERT-large models by 65% and 60%\nrespectively, while incurring no statistically significant word error rate\n(WER) increase on the test-clean dataset. Compared to previously published\nmethods on the same task, our approach not only achieves the lowest WER of\n7.05% on the test-clean dataset under a comparable model compression ratio of\n4.26x, but also operates with at least 25% less model compression time."}
{"id": "2505.22613", "pdf": "https://arxiv.org/pdf/2505.22613", "abs": "https://arxiv.org/abs/2505.22613", "authors": ["Yuchi Wang", "Yishuo Cai", "Shuhuai Ren", "Sihan Yang", "Linli Yao", "Yuanxin Liu", "Yuanxing Zhang", "Pengfei Wan", "Xu Sun"], "title": "RICO: Improving Accuracy and Completeness in Image Recaptioning via Visual Reconstruction", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "code: https://github.com/wangyuchi369/RICO", "summary": "Image recaptioning is widely used to generate training datasets with enhanced\nquality for various multimodal tasks. Existing recaptioning methods typically\nrely on powerful multimodal large language models (MLLMs) to enhance textual\ndescriptions, but often suffer from inaccuracies due to hallucinations and\nincompleteness caused by missing fine-grained details. To address these\nlimitations, we propose RICO, a novel framework that refines captions through\nvisual reconstruction. Specifically, we leverage a text-to-image model to\nreconstruct a caption into a reference image, and prompt an MLLM to identify\ndiscrepancies between the original and reconstructed images to refine the\ncaption. This process is performed iteratively, further progressively promoting\nthe generation of more faithful and comprehensive descriptions. To mitigate the\nadditional computational cost induced by the iterative process, we introduce\nRICO-Flash, which learns to generate captions like RICO using DPO. Extensive\nexperiments demonstrate that our approach significantly improves caption\naccuracy and completeness, outperforms most baselines by approximately 10% on\nboth CapsBench and CompreCap. Code released at\nhttps://github.com/wangyuchi369/RICO."}
{"id": "2505.22617", "pdf": "https://arxiv.org/pdf/2505.22617", "abs": "https://arxiv.org/abs/2505.22617", "authors": ["Ganqu Cui", "Yuchen Zhang", "Jiacheng Chen", "Lifan Yuan", "Zhi Wang", "Yuxin Zuo", "Haozhan Li", "Yuchen Fan", "Huayu Chen", "Weize Chen", "Zhiyuan Liu", "Hao Peng", "Lei Bai", "Wanli Ouyang", "Yu Cheng", "Bowen Zhou", "Ning Ding"], "title": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "This paper aims to overcome a major obstacle in scaling RL for reasoning with\nLLMs, namely the collapse of policy entropy. Such phenomenon is consistently\nobserved across vast RL runs without entropy intervention, where the policy\nentropy dropped sharply at the early training stage, this diminished\nexploratory ability is always accompanied with the saturation of policy\nperformance. In practice, we establish a transformation equation R=-a*e^H+b\nbetween entropy H and downstream performance R. This empirical law strongly\nindicates that, the policy performance is traded from policy entropy, thus\nbottlenecked by its exhaustion, and the ceiling is fully predictable H=0,\nR=-a+b. Our finding necessitates entropy management for continuous exploration\ntoward scaling compute for RL. To this end, we investigate entropy dynamics\nboth theoretically and empirically. Our derivation highlights that, the change\nin policy entropy is driven by the covariance between action probability and\nthe change in logits, which is proportional to its advantage when using Policy\nGradient-like algorithms. Empirical study shows that, the values of covariance\nterm and entropy differences matched exactly, supporting the theoretical\nconclusion. Moreover, the covariance term stays mostly positive throughout\ntraining, further explaining why policy entropy would decrease monotonically.\nThrough understanding the mechanism behind entropy dynamics, we motivate to\ncontrol entropy by restricting the update of high-covariance tokens.\nSpecifically, we propose two simple yet effective techniques, namely Clip-Cov\nand KL-Cov, which clip and apply KL penalty to tokens with high covariances\nrespectively. Experiments show that these methods encourage exploration, thus\nhelping policy escape entropy collapse and achieve better downstream\nperformance."}
{"id": "2505.22626", "pdf": "https://arxiv.org/pdf/2505.22626", "abs": "https://arxiv.org/abs/2505.22626", "authors": ["Yu Zhang", "Yuqi Xie", "Huihan Liu", "Rutav Shah", "Michael Wan", "Linxi Fan", "Yuke Zhu"], "title": "SCIZOR: A Self-Supervised Approach to Data Curation for Large-Scale Imitation Learning", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Imitation learning advances robot capabilities by enabling the acquisition of\ndiverse behaviors from human demonstrations. However, large-scale datasets used\nfor policy training often introduce substantial variability in quality, which\ncan negatively impact performance. As a result, automatically curating datasets\nby filtering low-quality samples to improve quality becomes essential. Existing\nrobotic curation approaches rely on costly manual annotations and perform\ncuration at a coarse granularity, such as the dataset or trajectory level,\nfailing to account for the quality of individual state-action pairs. To address\nthis, we introduce SCIZOR, a self-supervised data curation framework that\nfilters out low-quality state-action pairs to improve the performance of\nimitation learning policies. SCIZOR targets two complementary sources of\nlow-quality data: suboptimal data, which hinders learning with undesirable\nactions, and redundant data, which dilutes training with repetitive patterns.\nSCIZOR leverages a self-supervised task progress predictor for suboptimal data\nto remove samples lacking task progression, and a deduplication module\noperating on joint state-action representation for samples with redundant\npatterns. Empirically, we show that SCIZOR enables imitation learning policies\nto achieve higher performance with less data, yielding an average improvement\nof 15.4% across multiple benchmarks. More information is available at:\nhttps://ut-austin-rpl.github.io/SCIZOR/"}
{"id": "2505.22633", "pdf": "https://arxiv.org/pdf/2505.22633", "abs": "https://arxiv.org/abs/2505.22633", "authors": ["Yida Xue", "Zhen Bi", "Jinnan Yang", "Jungang Lou", "Huajun Chen", "Ningyu Zhang"], "title": "Spatial Knowledge Graph-Guided Multimodal Synthesis", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "comment": "Ongoing work", "summary": "Recent advances in multimodal large language models (MLLMs) have\nsignificantly enhanced their capabilities; however, their spatial perception\nabilities remain a notable limitation. To address this challenge, multimodal\ndata synthesis offers a promising solution. Yet, ensuring that synthesized data\nadhere to spatial common sense is a non-trivial task. In this work, we\nintroduce SKG2Data, a novel multimodal synthesis approach guided by spatial\nknowledge graphs, grounded in the concept of knowledge-to-data generation.\nSKG2Data automatically constructs a Spatial Knowledge Graph (SKG) to emulate\nhuman-like perception of spatial directions and distances, which is\nsubsequently utilized to guide multimodal data synthesis. Extensive experiments\ndemonstrate that data synthesized from diverse types of spatial knowledge,\nincluding direction and distance, not only enhance the spatial perception and\nreasoning abilities of MLLMs but also exhibit strong generalization\ncapabilities. We hope that the idea of knowledge-based data synthesis can\nadvance the development of spatial intelligence."}
{"id": "2505.22635", "pdf": "https://arxiv.org/pdf/2505.22635", "abs": "https://arxiv.org/abs/2505.22635", "authors": ["Fangcong Yin", "Zeyu Leo Liu", "Liu Leqi", "Xi Ye", "Greg Durrett"], "title": "Learning Composable Chains-of-Thought", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "A common approach for teaching large language models (LLMs) to reason is to\ntrain on chain-of-thought (CoT) traces of in-distribution reasoning problems,\nbut such annotated data is costly to obtain for every problem of interest. We\nwant reasoning models to generalize beyond their training distribution, and\nideally to generalize compositionally: combine atomic reasoning skills to solve\nharder, unseen reasoning tasks. We take a step towards compositional\ngeneralization of reasoning skills when addressing a target compositional task\nthat has no labeled CoT data. We find that simply training models on CoT data\nof atomic tasks leads to limited generalization, but minimally modifying CoT\nformats of constituent atomic tasks to be composable can lead to improvements.\nWe can train \"atomic CoT\" models on the atomic tasks with Composable CoT data\nand combine them with multitask learning or model merging for better zero-shot\nperformance on the target compositional task. Such a combined model can be\nfurther bootstrapped on a small amount of compositional data using rejection\nsampling fine-tuning (RFT). Results on string operations and natural language\nskill compositions show that training LLMs on Composable CoT outperforms\nmultitask learning and continued fine-tuning baselines within a given training\ndata budget."}
{"id": "2505.22642", "pdf": "https://arxiv.org/pdf/2505.22642", "abs": "https://arxiv.org/abs/2505.22642", "authors": ["Younggyo Seo", "Carmelo Sferrazza", "Haoran Geng", "Michal Nauman", "Zhao-Heng Yin", "Pieter Abbeel"], "title": "FastTD3: Simple, Fast, and Capable Reinforcement Learning for Humanoid Control", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Project webpage: https://younggyo.me/fast_td3", "summary": "Reinforcement learning (RL) has driven significant progress in robotics, but\nits complexity and long training times remain major bottlenecks. In this\nreport, we introduce FastTD3, a simple, fast, and capable RL algorithm that\nsignificantly speeds up training for humanoid robots in popular suites such as\nHumanoidBench, IsaacLab, and MuJoCo Playground. Our recipe is remarkably\nsimple: we train an off-policy TD3 agent with several modifications -- parallel\nsimulation, large-batch updates, a distributional critic, and carefully tuned\nhyperparameters. FastTD3 solves a range of HumanoidBench tasks in under 3 hours\non a single A100 GPU, while remaining stable during training. We also provide a\nlightweight and easy-to-use implementation of FastTD3 to accelerate RL research\nin robotics."}
{"id": "2505.22649", "pdf": "https://arxiv.org/pdf/2505.22649", "abs": "https://arxiv.org/abs/2505.22649", "authors": ["Guoxuan Chen", "Lianghao Xia", "Chao Huang"], "title": "Pre-training for Recommendation Unlearning", "categories": ["cs.IR", "cs.AI", "cs.LG"], "comment": "Accepted to SIGIR 2025 Oral", "summary": "Modern recommender systems powered by Graph Neural Networks (GNNs) excel at\nmodeling complex user-item interactions, yet increasingly face scenarios\nrequiring selective forgetting of training data. Beyond user requests to remove\nspecific interactions due to privacy concerns or preference changes, regulatory\nframeworks mandate recommender systems' ability to eliminate the influence of\ncertain user data from models. This recommendation unlearning challenge\npresents unique difficulties as removing connections within interaction graphs\ncreates ripple effects throughout the model, potentially impacting\nrecommendations for numerous users. Traditional approaches suffer from\nsignificant drawbacks: fragmentation methods damage graph structure and\ndiminish performance, while influence function techniques make assumptions that\nmay not hold in complex GNNs, particularly with self-supervised or random\narchitectures. To address these limitations, we propose a novel model-agnostic\npre-training paradigm UnlearnRec that prepares systems for efficient unlearning\noperations. Our Influence Encoder takes unlearning requests together with\nexisting model parameters and directly produces updated parameters of unlearned\nmodel with little fine-tuning, avoiding complete retraining while preserving\nmodel performance characteristics. Extensive evaluation on public benchmarks\ndemonstrates that our method delivers exceptional unlearning effectiveness\nwhile providing more than 10x speedup compared to retraining approaches. We\nrelease our method implementation at: https://github.com/HKUDS/UnlearnRec."}
{"id": "2505.22655", "pdf": "https://arxiv.org/pdf/2505.22655", "abs": "https://arxiv.org/abs/2505.22655", "authors": ["Michael Kirchhof", "Gjergji Kasneci", "Enkelejda Kasneci"], "title": "Position: Uncertainty Quantification Needs Reassessment for Large-language Model Agents", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted at ICML 2025", "summary": "Large-language models (LLMs) and chatbot agents are known to provide wrong\noutputs at times, and it was recently found that this can never be fully\nprevented. Hence, uncertainty quantification plays a crucial role, aiming to\nquantify the level of ambiguity in either one overall number or two numbers for\naleatoric and epistemic uncertainty. This position paper argues that this\ntraditional dichotomy of uncertainties is too limited for the open and\ninteractive setup that LLM agents operate in when communicating with a user,\nand that we need to research avenues that enrich uncertainties in this novel\nscenario. We review the literature and find that popular definitions of\naleatoric and epistemic uncertainties directly contradict each other and lose\ntheir meaning in interactive LLM agent settings. Hence, we propose three novel\nresearch directions that focus on uncertainties in such human-computer\ninteractions: Underspecification uncertainties, for when users do not provide\nall information or define the exact task at the first go, interactive learning,\nto ask follow-up questions and reduce the uncertainty about the current\ncontext, and output uncertainties, to utilize the rich language and speech\nspace to express uncertainties as more than mere numbers. We expect that these\nnew ways of dealing with and communicating uncertainties will lead to LLM agent\ninteractions that are more transparent, trustworthy, and intuitive."}
{"id": "2505.22657", "pdf": "https://arxiv.org/pdf/2505.22657", "abs": "https://arxiv.org/abs/2505.22657", "authors": ["Wenbo Hu", "Yining Hong", "Yanjun Wang", "Leison Gao", "Zibu Wei", "Xingcheng Yao", "Nanyun Peng", "Yonatan Bitton", "Idan Szpektor", "Kai-Wei Chang"], "title": "3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "demos at: https://3dllm-mem.github.io", "summary": "Humans excel at performing complex tasks by leveraging long-term memory\nacross temporal and spatial experiences. In contrast, current Large Language\nModels (LLMs) struggle to effectively plan and act in dynamic, multi-room 3D\nenvironments. We posit that part of this limitation is due to the lack of\nproper 3D spatial-temporal memory modeling in LLMs. To address this, we first\nintroduce 3DMem-Bench, a comprehensive benchmark comprising over 26,000\ntrajectories and 2,892 embodied tasks, question-answering and captioning,\ndesigned to evaluate an agent's ability to reason over long-term memory in 3D\nenvironments. Second, we propose 3DLLM-Mem, a novel dynamic memory management\nand fusion model for embodied spatial-temporal reasoning and actions in LLMs.\nOur model uses working memory tokens, which represents current observations, as\nqueries to selectively attend to and fuse the most useful spatial and temporal\nfeatures from episodic memory, which stores past observations and interactions.\nOur approach allows the agent to focus on task-relevant information while\nmaintaining memory efficiency in complex, long-horizon environments.\nExperimental results demonstrate that 3DLLM-Mem achieves state-of-the-art\nperformance across various tasks, outperforming the strongest baselines by\n16.5% in success rate on 3DMem-Bench's most challenging in-the-wild embodied\ntasks."}
{"id": "2505.22660", "pdf": "https://arxiv.org/pdf/2505.22660", "abs": "https://arxiv.org/abs/2505.22660", "authors": ["Mihir Prabhudesai", "Lili Chen", "Alex Ippoliti", "Katerina Fragkiadaki", "Hao Liu", "Deepak Pathak"], "title": "Maximizing Confidence Alone Improves Reasoning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement learning (RL) has enabled machine learning models to achieve\nsignificant advances in many fields. Most recently, RL has empowered frontier\nlanguage models to solve challenging math, science, and coding problems.\nHowever, central to any RL algorithm is the reward function, and reward\nengineering is a notoriously difficult problem in any domain. In this paper, we\npropose RENT: Reinforcement Learning via Entropy Minimization -- a fully\nunsupervised RL method that requires no external reward or ground-truth\nanswers, and instead uses the model's entropy of its underlying distribution as\nan intrinsic reward. We find that by reinforcing the chains of thought that\nyield high model confidence on its generated answers, the model improves its\nreasoning ability. In our experiments, we showcase these improvements on an\nextensive suite of commonly-used reasoning benchmarks, including GSM8K,\nMATH500, AMC, AIME, and GPQA, and models of varying sizes from the Qwen and\nMistral families. The generality of our unsupervised learning method lends\nitself to applicability in a wide range of domains where external supervision\nis limited or unavailable."}
