{"id": "2504.18572", "pdf": "https://arxiv.org/pdf/2504.18572", "abs": "https://arxiv.org/abs/2504.18572", "authors": ["Syed Quiser Ahmed", "Bharathi Vokkaliga Ganesh", "Jagadish Babu P", "Karthick Selvaraj", "ReddySiva Naga Parvathi Devi", "Sravya Kappala"], "title": "BELL: Benchmarking the Explainability of Large Language Models", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models have demonstrated remarkable capabilities in natural\nlanguage processing, yet their decision-making processes often lack\ntransparency. This opaqueness raises significant concerns regarding trust,\nbias, and model performance. To address these issues, understanding and\nevaluating the interpretability of LLMs is crucial. This paper introduces a\nstandardised benchmarking technique, Benchmarking the Explainability of Large\nLanguage Models, designed to evaluate the explainability of large language\nmodels."}
{"id": "2504.18600", "pdf": "https://arxiv.org/pdf/2504.18600", "abs": "https://arxiv.org/abs/2504.18600", "authors": ["Saizhuo Wang", "Hao Kong", "Jiadong Guo", "Fengrui Hua", "Yiyan Qi", "Wanyun Zhou", "Jiahao Zheng", "Xinyu Wang", "Lionel M. Ni", "Jian Guo"], "title": "QuantBench: Benchmarking AI Methods for Quantitative Investment", "categories": ["q-fin.CP", "cs.AI", "cs.CE"], "comment": null, "summary": "The field of artificial intelligence (AI) in quantitative investment has seen\nsignificant advancements, yet it lacks a standardized benchmark aligned with\nindustry practices. This gap hinders research progress and limits the practical\napplication of academic innovations. We present QuantBench, an industrial-grade\nbenchmark platform designed to address this critical need. QuantBench offers\nthree key strengths: (1) standardization that aligns with quantitative\ninvestment industry practices, (2) flexibility to integrate various AI\nalgorithms, and (3) full-pipeline coverage of the entire quantitative\ninvestment process. Our empirical studies using QuantBench reveal some critical\nresearch directions, including the need for continual learning to address\ndistribution shifts, improved methods for modeling relational financial data,\nand more robust approaches to mitigate overfitting in low signal-to-noise\nenvironments. By providing a common ground for evaluation and fostering\ncollaboration between researchers and practitioners, QuantBench aims to\naccelerate progress in AI for quantitative investment, similar to the impact of\nbenchmark platforms in computer vision and natural language processing."}
{"id": "2504.18604", "pdf": "https://arxiv.org/pdf/2504.18604", "abs": "https://arxiv.org/abs/2504.18604", "authors": ["Xingyu Xiao", "Peng Chen", "Jiejuan Tong", "Shunshun Liu", "Hongru Zhao", "Jun Zhao", "Qianqian Jia", "Jingang Liang", "Haitao Wang"], "title": "A Cognitive-Mechanistic Human Reliability Analysis Framework: A Nuclear Power Plant Case Study", "categories": ["cs.AI"], "comment": null, "summary": "Traditional human reliability analysis (HRA) methods, such as IDHEAS-ECA,\nrely on expert judgment and empirical rules that often overlook the cognitive\nunderpinnings of human error. Moreover, conducting human-in-the-loop\nexperiments for advanced nuclear power plants is increasingly impractical due\nto novel interfaces and limited operational data. This study proposes a\ncognitive-mechanistic framework (COGMIF) that enhances the IDHEAS-ECA\nmethodology by integrating an ACT-R-based human digital twin (HDT) with\nTimeGAN-augmented simulation. The ACT-R model simulates operator cognition,\nincluding memory retrieval, goal-directed procedural reasoning, and\nperceptual-motor execution, under high-fidelity scenarios derived from a\nhigh-temperature gas-cooled reactor (HTGR) simulator. To overcome the resource\nconstraints of large-scale cognitive modeling, TimeGAN is trained on\nACT-R-generated time-series data to produce high-fidelity synthetic operator\nbehavior datasets. These simulations are then used to drive IDHEAS-ECA\nassessments, enabling scalable, mechanism-informed estimation of human error\nprobabilities (HEPs). Comparative analyses with SPAR-H and sensitivity\nassessments demonstrate the robustness and practical advantages of the proposed\nCOGMIF. Finally, procedural features are mapped onto a Bayesian network to\nquantify the influence of contributing factors, revealing key drivers of\noperational risk. This work offers a credible and computationally efficient\npathway to integrate cognitive theory into industrial HRA practices."}
{"id": "2504.18631", "pdf": "https://arxiv.org/pdf/2504.18631", "abs": "https://arxiv.org/abs/2504.18631", "authors": ["Dingxin Lu", "Shurui Wu", "Xinyi Huang"], "title": "Research on Personalized Medical Intervention Strategy Generation System based on Group Relative Policy Optimization and Time-Series Data Fusion", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "With the timely formation of personalized intervention plans based on\nhigh-dimensional heterogeneous time series information becoming an important\nchallenge in the medical field today, electronic medical records, wearables,\nand other multi-source medical data are increasingly generated and diversified.\nIn this work, we develop a system to generate personalized medical intervention\nstrategies based on Group Relative Policy Optimization (GRPO) and Time-Series\nData Fusion. First, by incorporating relative policy constraints among the\ngroups during policy gradient updates, we adaptively balance individual and\ngroup gains. To improve the robustness and interpretability of decision-making,\na multi-layer neural network structure is employed to group-code patient\ncharacteristics. Second, for the rapid multi-modal fusion of multi-source\nheterogeneous time series, a multi-channel neural network combined with a\nself-attention mechanism is used for dynamic feature extraction. Key feature\nscreening and aggregation are achieved through a differentiable gating network.\nFinally, a collaborative search process combining a genetic algorithm and Monte\nCarlo tree search is proposed to find the ideal intervention strategy,\nachieving global optimization. Experimental results show significant\nimprovements in accuracy, coverage, and decision-making benefits compared with\nexisting methods."}
{"id": "2504.18560", "pdf": "https://arxiv.org/pdf/2504.18560", "abs": "https://arxiv.org/abs/2504.18560", "authors": ["Alessio Buscemi", "Cédric Lothritz", "Sergio Morales", "Marcos Gomez-Vazquez", "Robert Clarisó", "Jordi Cabot", "German Castignani"], "title": "Mind the Language Gap: Automated and Augmented Evaluation of Bias in LLMs for High- and Low-Resource Languages", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have exhibited impressive natural language\nprocessing capabilities but often perpetuate social biases inherent in their\ntraining data. To address this, we introduce MultiLingual Augmented Bias\nTesting (MLA-BiTe), a framework that improves prior bias evaluation methods by\nenabling systematic multilingual bias testing. MLA-BiTe leverages automated\ntranslation and paraphrasing techniques to support comprehensive assessments\nacross diverse linguistic settings. In this study, we evaluate the\neffectiveness of MLA-BiTe by testing four state-of-the-art LLMs in six\nlanguages -- including two low-resource languages -- focusing on seven\nsensitive categories of discrimination."}
{"id": "2504.18586", "pdf": "https://arxiv.org/pdf/2504.18586", "abs": "https://arxiv.org/abs/2504.18586", "authors": ["Leo Thomas Ramos", "Angel D. Sappa"], "title": "A Decade of You Only Look Once (YOLO) for Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "This review marks the tenth anniversary of You Only Look Once (YOLO), one of\nthe most influential frameworks in real-time object detection. Over the past\ndecade, YOLO has evolved from a streamlined detector into a diverse family of\narchitectures characterized by efficient design, modular scalability, and\ncross-domain adaptability. The paper presents a technical overview of the main\nversions, highlights key architectural trends, and surveys the principal\napplication areas in which YOLO has been adopted. It also addresses evaluation\npractices, ethical considerations, and potential future directions for the\nframework's continued development. The analysis aims to provide a comprehensive\nand critical perspective on YOLO's trajectory and ongoing transformation."}
{"id": "2504.18651", "pdf": "https://arxiv.org/pdf/2504.18651", "abs": "https://arxiv.org/abs/2504.18651", "authors": ["Filipi Miranda Soares", "Antonio Mauro Saraiva", "Luís Ferreira Pires", "Luiz Olavo Bonino da Silva Santos", "Dilvan de Abreu Moreira", "Fernando Elias Corrêa", "Kelly Rosa Braghetto", "Debora Pignatari Drucker", "Alexandre Cláudio Botazzo Delbem"], "title": "Exploring a Large Language Model for Transforming Taxonomic Data into OWL: Lessons Learned and Implications for Ontology Development", "categories": ["cs.AI"], "comment": "31 pages, 6 Figures, accepted for publication in Data Intelligence", "summary": "Managing scientific names in ontologies that represent species taxonomies is\nchallenging due to the ever-evolving nature of these taxonomies. Manually\nmaintaining these names becomes increasingly difficult when dealing with\nthousands of scientific names. To address this issue, this paper investigates\nthe use of ChatGPT-4 to automate the development of the :Organism module in the\nAgricultural Product Types Ontology (APTO) for species classification. Our\nmethodology involved leveraging ChatGPT-4 to extract data from the GBIF\nBackbone API and generate OWL files for further integration in APTO. Two\nalternative approaches were explored: (1) issuing a series of prompts for\nChatGPT-4 to execute tasks via the BrowserOP plugin and (2) directing ChatGPT-4\nto design a Python algorithm to perform analogous tasks. Both approaches rely\non a prompting method where we provide instructions, context, input data, and\nan output indicator. The first approach showed scalability limitations, while\nthe second approach used the Python algorithm to overcome these challenges, but\nit struggled with typographical errors in data handling. This study highlights\nthe potential of Large language models like ChatGPT-4 to streamline the\nmanagement of species names in ontologies. Despite certain limitations, these\ntools offer promising advancements in automating taxonomy-related tasks and\nimproving the efficiency of ontology development."}
{"id": "2504.18639", "pdf": "https://arxiv.org/pdf/2504.18639", "abs": "https://arxiv.org/abs/2504.18639", "authors": ["Passant Elchafei", "Mervet Abu-Elkheir"], "title": "Span-Level Hallucination Detection for LLM-Generated Answers", "categories": ["cs.CL"], "comment": null, "summary": "Detecting spans of hallucination in LLM-generated answers is crucial for\nimproving factual consistency. This paper presents a span-level hallucination\ndetection framework for the SemEval-2025 Shared Task, focusing on English and\nArabic texts. Our approach integrates Semantic Role Labeling (SRL) to decompose\nthe answer into atomic roles, which are then compared with a retrieved\nreference context obtained via question-based LLM prompting. Using a\nDeBERTa-based textual entailment model, we evaluate each role semantic\nalignment with the retrieved context. The entailment scores are further refined\nthrough token-level confidence measures derived from output logits, and the\ncombined scores are used to detect hallucinated spans. Experiments on the\nMu-SHROOM dataset demonstrate competitive performance. Additionally,\nhallucinated spans have been verified through fact-checking by prompting GPT-4\nand LLaMA. Our findings contribute to improving hallucination detection in\nLLM-generated responses."}
{"id": "2504.18589", "pdf": "https://arxiv.org/pdf/2504.18589", "abs": "https://arxiv.org/abs/2504.18589", "authors": ["Zhikai Wang", "Jiashuo Sun", "Wenqi Zhang", "Zhiqiang Hu", "Xin Li", "Fan Wang", "Deli Zhao"], "title": "Benchmarking Multimodal Mathematical Reasoning with Explicit Visual Dependency", "categories": ["cs.CV"], "comment": "Home page: https://alibaba-damo-academy.github.io/VCBench/", "summary": "Recent advancements in Large Vision-Language Models (LVLMs) have\nsignificantly enhanced their ability to integrate visual and linguistic\ninformation, achieving near-human proficiency in tasks like object recognition,\ncaptioning, and visual question answering. However, current benchmarks\ntypically focus on knowledge-centric evaluations that assess domain-specific\nexpertise, often neglecting the core ability to reason about fundamental\nmathematical elements and visual concepts. We identify a gap in evaluating\nelementary-level math problems, which rely on explicit visual\ndependencies-requiring models to discern, integrate, and reason across multiple\nimages while incorporating commonsense knowledge, all of which are crucial for\nadvancing toward broader AGI capabilities. To address this gap, we introduce\nVCBENCH, a comprehensive benchmark for multimodal mathematical reasoning with\nexplicit visual dependencies. VCBENCH includes 1,720 problems across six\ncognitive domains, featuring 6,697 images (averaging 3.9 per question) to\nensure multi-image reasoning. We evaluate 26 state-of-the-art LVLMs on VCBENCH,\nrevealing substantial performance disparities, with even the top models unable\nto exceed 50% accuracy. Our findings highlight the ongoing challenges in\nvisual-mathematical integration and suggest avenues for future LVLM\nadvancements."}
{"id": "2504.18671", "pdf": "https://arxiv.org/pdf/2504.18671", "abs": "https://arxiv.org/abs/2504.18671", "authors": ["Ross Gore", "Eranga Bandara", "Sachin Shetty", "Alberto E. Musto", "Pratip Rana", "Ambrosio Valencia-Romero", "Christopher Rhea", "Lobat Tayebi", "Heather Richter", "Atmaram Yarlagadda", "Donna Edmonds", "Steven Wallace", "Donna Broshek"], "title": "Proof-of-TBI -- Fine-Tuned Vision Language Model Consortium and OpenAI-o3 Reasoning LLM-Based Medical Diagnosis Support System for Mild Traumatic Brain Injury (TBI) Prediction", "categories": ["cs.AI"], "comment": null, "summary": "Mild Traumatic Brain Injury (TBI) detection presents significant challenges\ndue to the subtle and often ambiguous presentation of symptoms in medical\nimaging, making accurate diagnosis a complex task. To address these challenges,\nwe propose Proof-of-TBI, a medical diagnosis support system that integrates\nmultiple fine-tuned vision-language models with the OpenAI-o3 reasoning large\nlanguage model (LLM). Our approach fine-tunes multiple vision-language models\nusing a labeled dataset of TBI MRI scans, training them to diagnose TBI\nsymptoms effectively. The predictions from these models are aggregated through\na consensus-based decision-making process. The system evaluates the predictions\nfrom all fine-tuned vision language models using the OpenAI-o3 reasoning LLM, a\nmodel that has demonstrated remarkable reasoning performance, to produce the\nmost accurate final diagnosis. The LLM Agents orchestrates interactions between\nthe vision-language models and the reasoning LLM, managing the final\ndecision-making process with transparency, reliability, and automation. This\nend-to-end decision-making workflow combines the vision-language model\nconsortium with the OpenAI-o3 reasoning LLM, enabled by custom prompt\nengineering by the LLM agents. The prototype for the proposed platform was\ndeveloped in collaboration with the U.S. Army Medical Research team in Newport\nNews, Virginia, incorporating five fine-tuned vision-language models. The\nresults demonstrate the transformative potential of combining fine-tuned\nvision-language model inputs with the OpenAI-o3 reasoning LLM to create a\nrobust, secure, and highly accurate diagnostic system for mild TBI prediction.\nTo the best of our knowledge, this research represents the first application of\nfine-tuned vision-language models integrated with a reasoning LLM for TBI\nprediction tasks."}
{"id": "2504.18673", "pdf": "https://arxiv.org/pdf/2504.18673", "abs": "https://arxiv.org/abs/2504.18673", "authors": ["Jiayi Li", "Yingfan Zhou", "Pranav Narayanan Venkit", "Halima Binte Islam", "Sneha Arya", "Shomir Wilson", "Sarah Rajtmajer"], "title": "Can Third-parties Read Our Emotions?", "categories": ["cs.CL"], "comment": null, "summary": "Natural Language Processing tasks that aim to infer an author's private\nstates, e.g., emotions and opinions, from their written text, typically rely on\ndatasets annotated by third-party annotators. However, the assumption that\nthird-party annotators can accurately capture authors' private states remains\nlargely unexamined. In this study, we present human subjects experiments on\nemotion recognition tasks that directly compare third-party annotations with\nfirst-party (author-provided) emotion labels. Our findings reveal significant\nlimitations in third-party annotations-whether provided by human annotators or\nlarge language models (LLMs)-in faithfully representing authors' private\nstates. However, LLMs outperform human annotators nearly across the board. We\nfurther explore methods to improve third-party annotation quality. We find that\ndemographic similarity between first-party authors and third-party human\nannotators enhances annotation performance. While incorporating first-party\ndemographic information into prompts leads to a marginal but statistically\nsignificant improvement in LLMs' performance. We introduce a framework for\nevaluating the limitations of third-party annotations and call for refined\nannotation practices to accurately represent and model authors' private states."}
{"id": "2504.18666", "pdf": "https://arxiv.org/pdf/2504.18666", "abs": "https://arxiv.org/abs/2504.18666", "authors": ["David Aparco-Cardenas", "Jancarlo F. Gomes", "Alexandre X. Falcão", "Pedro J. de Rezende"], "title": "Co-Training with Active Contrastive Learning and Meta-Pseudo-Labeling on 2D Projections for Deep Semi-Supervised Learning", "categories": ["cs.CV", "68T07", "I.4.10; I.5.1"], "comment": "Submitted to Journal of the Brazilian Computer Society (JBCS)\n  [https://journals-sol.sbc.org.br]", "summary": "A major challenge that prevents the training of DL models is the limited\navailability of accurately labeled data. This shortcoming is highlighted in\nareas where data annotation becomes a time-consuming and error-prone task. In\nthis regard, SSL tackles this challenge by capitalizing on scarce labeled and\nabundant unlabeled data; however, SoTA methods typically depend on pre-trained\nfeatures and large validation sets to learn effective representations for\nclassification tasks. In addition, the reduced set of labeled data is often\nrandomly sampled, neglecting the selection of more informative samples. Here,\nwe present active-DeepFA, a method that effectively combines CL,\nteacher-student-based meta-pseudo-labeling and AL to train non-pretrained CNN\narchitectures for image classification in scenarios of scarcity of labeled and\nabundance of unlabeled data. It integrates DeepFA into a co-training setup that\nimplements two cooperative networks to mitigate confirmation bias from\npseudo-labels. The method starts with a reduced set of labeled samples by\nwarming up the networks with supervised CL. Afterward and at regular epoch\nintervals, label propagation is performed on the 2D projections of the\nnetworks' deep features. Next, the most reliable pseudo-labels are exchanged\nbetween networks in a cross-training fashion, while the most meaningful samples\nare annotated and added into the labeled set. The networks independently\nminimize an objective loss function comprising supervised contrastive,\nsupervised and semi-supervised loss components, enhancing the representations\ntowards image classification. Our approach is evaluated on three challenging\nbiological image datasets using only 5% of labeled samples, improving baselines\nand outperforming six other SoTA methods. In addition, it reduces annotation\neffort by achieving comparable results to those of its counterparts with only\n3% of labeled data."}
{"id": "2504.18687", "pdf": "https://arxiv.org/pdf/2504.18687", "abs": "https://arxiv.org/abs/2504.18687", "authors": ["Samuel Schapiro", "Jonah Black", "Lav R. Varshney"], "title": "Transformational Creativity in Science: A Graphical Theory", "categories": ["cs.AI"], "comment": null, "summary": "Creative processes are typically divided into three types: combinatorial,\nexploratory, and transformational. Here, we provide a graphical theory of\ntransformational scientific creativity, synthesizing Boden's insight that\ntransformational creativity arises from changes in the \"enabling constraints\"\nof a conceptual space and Kuhn's structure of scientific revolutions as\nresulting from paradigm shifts. We prove that modifications made to axioms of\nour graphical model have the most transformative potential and then illustrate\nhow several historical instances of transformational creativity can be captured\nby our framework."}
{"id": "2504.18715", "pdf": "https://arxiv.org/pdf/2504.18715", "abs": "https://arxiv.org/abs/2504.18715", "authors": ["Tuochao Chen", "Qirui Wang", "Runlin He", "Shyam Gollakota"], "title": "Spatial Speech Translation: Translating Across Space With Binaural Hearables", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted by CHI2025", "summary": "Imagine being in a crowded space where people speak a different language and\nhaving hearables that transform the auditory space into your native language,\nwhile preserving the spatial cues for all speakers. We introduce spatial speech\ntranslation, a novel concept for hearables that translate speakers in the\nwearer's environment, while maintaining the direction and unique voice\ncharacteristics of each speaker in the binaural output. To achieve this, we\ntackle several technical challenges spanning blind source separation,\nlocalization, real-time expressive translation, and binaural rendering to\npreserve the speaker directions in the translated audio, while achieving\nreal-time inference on the Apple M2 silicon. Our proof-of-concept evaluation\nwith a prototype binaural headset shows that, unlike existing models, which\nfail in the presence of interference, we achieve a BLEU score of up to 22.01\nwhen translating between languages, despite strong interference from other\nspeakers in the environment. User studies further confirm the system's\neffectiveness in spatially rendering the translated speech in previously unseen\nreal-world reverberant environments. Taking a step back, this work marks the\nfirst step towards integrating spatial perception into speech translation."}
{"id": "2504.18684", "pdf": "https://arxiv.org/pdf/2504.18684", "abs": "https://arxiv.org/abs/2504.18684", "authors": ["Nader Zantout", "Haochen Zhang", "Pujith Kachana", "Jinkai Qiu", "Ji Zhang", "Wenshan Wang"], "title": "SORT3D: Spatial Object-centric Reasoning Toolbox for Zero-Shot 3D Grounding Using Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "7 pages, 6 figures, submitted to IROS 2025", "summary": "Interpreting object-referential language and grounding objects in 3D with\nspatial relations and attributes is essential for robots operating alongside\nhumans. However, this task is often challenging due to the diversity of scenes,\nlarge number of fine-grained objects, and complex free-form nature of language\nreferences. Furthermore, in the 3D domain, obtaining large amounts of natural\nlanguage training data is difficult. Thus, it is important for methods to learn\nfrom little data and zero-shot generalize to new environments. To address these\nchallenges, we propose SORT3D, an approach that utilizes rich object attributes\nfrom 2D data and merges a heuristics-based spatial reasoning toolbox with the\nability of large language models (LLMs) to perform sequential reasoning.\nImportantly, our method does not require text-to-3D data for training and can\nbe applied zero-shot to unseen environments. We show that SORT3D achieves\nstate-of-the-art performance on complex view-dependent grounding tasks on two\nbenchmarks. We also implement the pipeline to run real-time on an autonomous\nvehicle and demonstrate that our approach can be used for object-goal\nnavigation on previously unseen real-world environments. All source code for\nthe system pipeline is publicly released at https://github.com/nzantout/SORT3D ."}
{"id": "2504.18765", "pdf": "https://arxiv.org/pdf/2504.18765", "abs": "https://arxiv.org/abs/2504.18765", "authors": ["Chengwei Liu", "Chong Wang", "Jiayue Cao", "Jingquan Ge", "Kun Wang", "Lvye Zhang", "Ming-Ming Cheng", "Penghai Zhao", "Tianlin Li", "Xiaojun Jia", "Xiang Li", "Xinfeng Li", "Yang Liu", "Yebo Feng", "Yihao Huang", "Yijia Xu", "Yuqiang Sun", "Zhenhong Zhou", "Zhengzi Xu"], "title": "A Vision for Auto Research with LLM Agents", "categories": ["cs.AI"], "comment": null, "summary": "This paper introduces Agent-Based Auto Research, a structured multi-agent\nframework designed to automate, coordinate, and optimize the full lifecycle of\nscientific research. Leveraging the capabilities of large language models\n(LLMs) and modular agent collaboration, the system spans all major research\nphases, including literature review, ideation, methodology planning,\nexperimentation, paper writing, peer review response, and dissemination. By\naddressing issues such as fragmented workflows, uneven methodological\nexpertise, and cognitive overload, the framework offers a systematic and\nscalable approach to scientific inquiry. Preliminary explorations demonstrate\nthe feasibility and potential of Auto Research as a promising paradigm for\nself-improving, AI-driven research processes."}
{"id": "2504.18718", "pdf": "https://arxiv.org/pdf/2504.18718", "abs": "https://arxiv.org/abs/2504.18718", "authors": ["Lauren Levine", "Junghyun Min", "Amir Zeldes"], "title": "Building UD Cairo for Old English in the Classroom", "categories": ["cs.CL"], "comment": "7 pages, 2 figures", "summary": "In this paper we present a sample treebank for Old English based on the UD\nCairo sentences, collected and annotated as part of a classroom curriculum in\nHistorical Linguistics. To collect the data, a sample of 20 sentences\nillustrating a range of syntactic constructions in the world's languages, we\nemploy a combination of LLM prompting and searches in authentic Old English\ndata. For annotation we assigned sentences to multiple students with limited\nprior exposure to UD, whose annotations we compare and adjudicate. Our results\nsuggest that while current LLM outputs in Old English do not reflect authentic\nsyntax, this can be mitigated by post-editing, and that although beginner\nannotators do not possess enough background to complete the task perfectly,\ntaken together they can produce good results and learn from the experience. We\nalso conduct preliminary parsing experiments using Modern English training\ndata, and find that although performance on Old English is poor, parsing on\nannotated features (lemma, hyperlemma, gloss) leads to improved performance."}
{"id": "2504.18689", "pdf": "https://arxiv.org/pdf/2504.18689", "abs": "https://arxiv.org/abs/2504.18689", "authors": ["Apoorva Beedu", "Irfan Essa"], "title": "HierSum: A Global and Local Attention Mechanism for Video Summarization", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Video summarization creates an abridged version (i.e., a summary) that\nprovides a quick overview of the video while retaining pertinent information.\nIn this work, we focus on summarizing instructional videos and propose a method\nfor breaking down a video into meaningful segments, each corresponding to\nessential steps in the video. We propose \\textbf{HierSum}, a hierarchical\napproach that integrates fine-grained local cues from subtitles with global\ncontextual information provided by video-level instructions. Our approach\nutilizes the ``most replayed\" statistic as a supervisory signal to identify\ncritical segments, thereby improving the effectiveness of the summary. We\nevaluate on benchmark datasets such as TVSum, BLiSS, Mr.HiSum, and the WikiHow\ntest set, and show that HierSum consistently outperforms existing methods in\nkey metrics such as F1-score and rank correlation. We also curate a new\nmulti-modal dataset using WikiHow and EHow videos and associated articles\ncontaining step-by-step instructions. Through extensive ablation studies, we\ndemonstrate that training on this dataset significantly enhances summarization\non the target datasets."}
{"id": "2504.18777", "pdf": "https://arxiv.org/pdf/2504.18777", "abs": "https://arxiv.org/abs/2504.18777", "authors": ["Diana Febrita"], "title": "Evaluating AI-Driven Automated Map Digitization in QGIS", "categories": ["cs.AI"], "comment": "Submitted to 2025 Indiana Geographic Information Council (IGIC)\n  Conference", "summary": "Map digitization is an important process that converts maps into digital\nformats that can be used for further analysis. This process typically requires\na deep human involvement because of the need for interpretation and\ndecision-making when translating complex features. With the advancement of\nartificial intelligence, there is an alternative to conducting map digitization\nwith the help of machine learning techniques. Deepness, or Deep Neural Remote\nSensing, is an advanced AI-driven tool designed and integrated as a plugin in\nQGIS application. This research focuses on assessing the effectiveness of\nDeepness in automated digitization. This study analyses AI-generated\ndigitization results from Google Earth imagery and compares them with digitized\noutputs from OpenStreetMap (OSM) to evaluate performance."}
{"id": "2504.18736", "pdf": "https://arxiv.org/pdf/2504.18736", "abs": "https://arxiv.org/abs/2504.18736", "authors": ["Jianyou Wang", "Weili Cao", "Kaicheng Wang", "Xiaoyue Wang", "Ashish Dalvi", "Gino Prasad", "Qishan Liang", "Hsuan-lin Her", "Ming Wang", "Qin Yang", "Gene W. Yeo", "David E. Neal", "Maxim Khan", "Christopher D. Rosin", "Ramamohan Paturi", "Leon Bergen"], "title": "EvidenceBench: A Benchmark for Extracting Evidence from Biomedical Papers", "categories": ["cs.CL"], "comment": null, "summary": "We study the task of automatically finding evidence relevant to hypotheses in\nbiomedical papers. Finding relevant evidence is an important step when\nresearchers investigate scientific hypotheses. We introduce EvidenceBench to\nmeasure models performance on this task, which is created by a novel pipeline\nthat consists of hypothesis generation and sentence-by-sentence annotation of\nbiomedical papers for relevant evidence, completely guided by and faithfully\nfollowing existing human experts judgment. We demonstrate the pipeline's\nvalidity and accuracy with multiple sets of human-expert annotations. We\nevaluated a diverse set of language models and retrieval systems on the\nbenchmark and found that model performances still fall significantly short of\nthe expert level on this task. To show the scalability of our proposed\npipeline, we create a larger EvidenceBench-100k with 107,461 fully annotated\npapers with hypotheses to facilitate model training and development. Both\ndatasets are available at https://github.com/EvidenceBench/EvidenceBench"}
{"id": "2504.18738", "pdf": "https://arxiv.org/pdf/2504.18738", "abs": "https://arxiv.org/abs/2504.18738", "authors": ["Ranjan Sapkota", "Konstantinos I Roumeliotis", "Rahul Harsha Cheppally", "Marco Flores Calero", "Manoj Karkee"], "title": "A Review of 3D Object Detection with Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "This review provides a systematic analysis of comprehensive survey of 3D\nobject detection with vision-language models(VLMs) , a rapidly advancing area\nat the intersection of 3D vision and multimodal AI. By examining over 100\nresearch papers, we provide the first systematic analysis dedicated to 3D\nobject detection with vision-language models. We begin by outlining the unique\nchallenges of 3D object detection with vision-language models, emphasizing\ndifferences from 2D detection in spatial reasoning and data complexity.\nTraditional approaches using point clouds and voxel grids are compared to\nmodern vision-language frameworks like CLIP and 3D LLMs, which enable\nopen-vocabulary detection and zero-shot generalization. We review key\narchitectures, pretraining strategies, and prompt engineering methods that\nalign textual and 3D features for effective 3D object detection with\nvision-language models. Visualization examples and evaluation benchmarks are\ndiscussed to illustrate performance and behavior. Finally, we highlight current\nchallenges, such as limited 3D-language datasets and computational demands, and\npropose future research directions to advance 3D object detection with\nvision-language models. >Object Detection, Vision-Language Models, Agents,\nVLMs, LLMs, AI"}
{"id": "2504.18794", "pdf": "https://arxiv.org/pdf/2504.18794", "abs": "https://arxiv.org/abs/2504.18794", "authors": ["Brendon Johnson", "Alfredo Weitzenfeld"], "title": "Hierarchical Reinforcement Learning in Multi-Goal Spatial Navigation with Autonomous Mobile Robots", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "Hierarchical reinforcement learning (HRL) is hypothesized to be able to take\nadvantage of the inherent hierarchy in robot learning tasks with sparse reward\nschemes, in contrast to more traditional reinforcement learning algorithms. In\nthis research, hierarchical reinforcement learning is evaluated and contrasted\nwith standard reinforcement learning in complex navigation tasks. We evaluate\nunique characteristics of HRL, including their ability to create sub-goals and\nthe termination function. We constructed experiments to test the differences\nbetween PPO and HRL, different ways of creating sub-goals, manual vs automatic\nsub-goal creation, and the effects of the frequency of termination on\nperformance. These experiments highlight the advantages of HRL and how it\nachieves these advantages."}
{"id": "2504.18762", "pdf": "https://arxiv.org/pdf/2504.18762", "abs": "https://arxiv.org/abs/2504.18762", "authors": ["Ojasw Upadhyay", "Abishek Saravankumar", "Ayman Ismail"], "title": "SynLexLM: Scaling Legal LLMs with Synthetic Data and Curriculum Learning", "categories": ["cs.CL", "cs.LG"], "comment": "9 pages, 4 figures, 4 tables", "summary": "Large Language Models (LLMs) are powerful but often require extensive\nfine-tuning and large datasets for specialized domains like law.\nGeneral-purpose pre-training may not capture legal nuances, and acquiring\nsufficient legal data is challenging. We introduce SynLexLM, a novel approach\nto efficiently pre-train a legal LLM. Our method employs curriculum learning,\nprogressing from simple to complex legal texts and queries, combined with\nsynthetic data augmentation using models like Gemini Pro to address data\nscarcity. We aim to achieve improved performance on legal benchmarks\n(BigLaw-Bench, EUR-Lex-Sum) compared to traditional models and fine-tuned\nversions. Preliminary work involves generating synthetic QA pairs reflecting\nlegal reasoning. This work aims to enhance legal document analysis and research\ntools, potentially democratizing access to advanced legal AI."}
{"id": "2504.18746", "pdf": "https://arxiv.org/pdf/2504.18746", "abs": "https://arxiv.org/abs/2504.18746", "authors": ["Brian K. S. Isaac-Medina", "Toby P. Breckon"], "title": "Dream-Box: Object-wise Outlier Generation for Out-of-Distribution Detection", "categories": ["cs.CV"], "comment": "9 pages, 6 figures, 2 tables, LatinX in AI CVPR 2025 Workshop", "summary": "Deep neural networks have demonstrated great generalization capabilities for\ntasks whose training and test sets are drawn from the same distribution.\nNevertheless, out-of-distribution (OOD) detection remains a challenging task\nthat has received significant attention in recent years. Specifically, OOD\ndetection refers to the detection of instances that do not belong to the\ntraining distribution, while still having good performance on the\nin-distribution task (e.g., classification or object detection). Recent work\nhas focused on generating synthetic outliers and using them to train an outlier\ndetector, generally achieving improved OOD detection than traditional OOD\nmethods. In this regard, outliers can be generated either in feature or pixel\nspace. Feature space driven methods have shown strong performance on both the\nclassification and object detection tasks, at the expense that the\nvisualization of training outliers remains unknown, making further analysis on\nOOD failure modes challenging. On the other hand, pixel space outlier\ngeneration techniques enabled by diffusion models have been used for image\nclassification using, providing improved OOD detection performance and outlier\nvisualization, although their adaption to the object detection task is as yet\nunexplored. We therefore introduce Dream-Box, a method that provides a link to\nobject-wise outlier generation in the pixel space for OOD detection.\nSpecifically, we use diffusion models to generate object-wise outliers that are\nused to train an object detector for an in-distribution task and OOD detection.\nOur method achieves comparable performance to previous traditional methods\nwhile being the first technique to provide concrete visualization of generated\nOOD objects."}
{"id": "2504.18875", "pdf": "https://arxiv.org/pdf/2504.18875", "abs": "https://arxiv.org/abs/2504.18875", "authors": ["Johannes Schneider"], "title": "Generative to Agentic AI: Survey, Conceptualization, and Challenges", "categories": ["cs.AI"], "comment": null, "summary": "Agentic Artificial Intelligence (AI) builds upon Generative AI (GenAI). It\nconstitutes the next major step in the evolution of AI with much stronger\nreasoning and interaction capabilities that enable more autonomous behavior to\ntackle complex tasks. Since the initial release of ChatGPT (3.5), Generative AI\nhas seen widespread adoption, giving users firsthand experience. However, the\ndistinction between Agentic AI and GenAI remains less well understood. To\naddress this gap, our survey is structured in two parts. In the first part, we\ncompare GenAI and Agentic AI using existing literature, discussing their key\ncharacteristics, how Agentic AI remedies limitations of GenAI, and the major\nsteps in GenAI's evolution toward Agentic AI. This section is intended for a\nbroad audience, including academics in both social sciences and engineering, as\nwell as industry professionals. It provides the necessary insights to\ncomprehend novel applications that are possible with Agentic AI but not with\nGenAI. In the second part, we deep dive into novel aspects of Agentic AI,\nincluding recent developments and practical concerns such as defining agents.\nFinally, we discuss several challenges that could serve as a future research\nagenda, while cautioning against risks that can emerge when exceeding human\nintelligence."}
{"id": "2504.18805", "pdf": "https://arxiv.org/pdf/2504.18805", "abs": "https://arxiv.org/abs/2504.18805", "authors": ["Jong Inn Park", "Maanas Taneja", "Qianwen Wang", "Dongyeop Kang"], "title": "Stealing Creator's Workflow: A Creator-Inspired Agentic Framework with Iterative Feedback Loop for Improved Scientific Short-form Generation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Project page: https://minnesotanlp.github.io/scitalk-project-page/", "summary": "Generating engaging, accurate short-form videos from scientific papers is\nchallenging due to content complexity and the gap between expert authors and\nreaders. Existing end-to-end methods often suffer from factual inaccuracies and\nvisual artifacts, limiting their utility for scientific dissemination. To\naddress these issues, we propose SciTalk, a novel multi-LLM agentic framework,\ngrounding videos in various sources, such as text, figures, visual styles, and\navatars. Inspired by content creators' workflows, SciTalk uses specialized\nagents for content summarization, visual scene planning, and text and layout\nediting, and incorporates an iterative feedback mechanism where video agents\nsimulate user roles to give feedback on generated videos from previous\niterations and refine generation prompts. Experimental evaluations show that\nSciTalk outperforms simple prompting methods in generating scientifically\naccurate and engaging content over the refined loop of video generation.\nAlthough preliminary results are still not yet matching human creators'\nquality, our framework provides valuable insights into the challenges and\nbenefits of feedback-driven video generation. Our code, data, and generated\nvideos will be publicly available."}
{"id": "2504.18756", "pdf": "https://arxiv.org/pdf/2504.18756", "abs": "https://arxiv.org/abs/2504.18756", "authors": ["Rezowan Shuvo", "M S Mekala", "Eyad Elyan"], "title": "Multi-Stage Boundary-Aware Transformer Network for Action Segmentation in Untrimmed Surgical Videos", "categories": ["cs.CV"], "comment": null, "summary": "Understanding actions within surgical workflows is essential for evaluating\npost-operative outcomes. However, capturing long sequences of actions performed\nin surgical settings poses challenges, as individual surgeons have their unique\napproaches shaped by their expertise, leading to significant variability. To\ntackle this complex problem, we focused on segmentation with precise\nboundaries, a demanding task due to the inherent variability in action\ndurations and the subtle transitions often observed in untrimmed videos. These\ntransitions, marked by ambiguous starting and ending points, complicate the\nsegmentation process. Traditional models, such as MS-TCN, which depend on large\nreceptive fields, frequently face challenges of over-segmentation (resulting in\nfragmented segments) or under-segmentation (merging distinct actions). Both of\nthese issues negatively impact the quality of segmentation. To overcome these\nchallenges, we present the Multi-Stage Boundary-Aware Transformer Network\n(MSBATN) with hierarchical sliding window attention, designed to enhance action\nsegmentation. Our proposed approach incorporates a novel unified loss function\nthat treats action classification and boundary detection as distinct yet\ninterdependent tasks. Unlike traditional binary boundary detection methods, our\nboundary voting mechanism accurately identifies start and end points by\nleveraging contextual information. Extensive experiments using three\nchallenging surgical datasets demonstrate the superior performance of the\nproposed method, achieving state-of-the-art results in F1 scores at thresholds\nof 25% and 50%, while also delivering comparable performance in other metrics."}
{"id": "2504.18880", "pdf": "https://arxiv.org/pdf/2504.18880", "abs": "https://arxiv.org/abs/2504.18880", "authors": ["Zuhong Lin", "Daoyuan Ren", "Kai Ran", "Sun Jing", "Xiaotiang Huang", "Haiyang He", "Pengxu Pan", "Xiaohang Zhang", "Ying Fang", "Tianying Wang", "Minli Wu", "Zhanglin Li", "Xiaochuan Zhang", "Haipu Li", "Jingjing Yao"], "title": "Reshaping MOFs Text Mining with a Dynamic Multi-Agent Framework of Large Language Agents", "categories": ["cs.AI", "cond-mat.mtrl-sci"], "comment": null, "summary": "The mining of synthesis conditions for metal-organic frameworks (MOFs) is a\nsignificant focus in materials science. However, identifying the precise\nsynthesis conditions for specific MOFs within the vast array of possibilities\npresents a considerable challenge. Large Language Models (LLMs) offer a\npromising solution to this problem. We leveraged the capabilities of LLMs,\nspecifically gpt-4o-mini, as core agents to integrate various MOF-related\nagents, including synthesis, attribute, and chemical information agents. This\nintegration culminated in the development of MOFh6, an LLM tool designed to\nstreamline the MOF synthesis process. MOFh6 allows users to query in multiple\nformats, such as submitting scientific literature, or inquiring about specific\nMOF codes or structural properties. The tool analyzes these queries to provide\noptimal synthesis conditions and generates model files for density functional\ntheory pre modeling. We believe MOFh6 will enhance efficiency in the MOF\nsynthesis of all researchers."}
{"id": "2504.18838", "pdf": "https://arxiv.org/pdf/2504.18838", "abs": "https://arxiv.org/abs/2504.18838", "authors": ["Yixin Cao", "Shibo Hong", "Xinze Li", "Jiahao Ying", "Yubo Ma", "Haiyuan Liang", "Yantao Liu", "Zijun Yao", "Xiaozhi Wang", "Dan Huang", "Wenxuan Zhang", "Lifu Huang", "Muhao Chen", "Lei Hou", "Qianru Sun", "Xingjun Ma", "Zuxuan Wu", "Min-Yen Kan", "David Lo", "Qi Zhang", "Heng Ji", "Jing Jiang", "Juanzi Li", "Aixin Sun", "Xuanjing Huang", "Tat-Seng Chua", "Yu-Gang Jiang"], "title": "Toward Generalizable Evaluation in the LLM Era: A Survey Beyond Benchmarks", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are advancing at an amazing speed and have\nbecome indispensable across academia, industry, and daily applications. To keep\npace with the status quo, this survey probes the core challenges that the rise\nof LLMs poses for evaluation. We identify and analyze two pivotal transitions:\n(i) from task-specific to capability-based evaluation, which reorganizes\nbenchmarks around core competencies such as knowledge, reasoning, instruction\nfollowing, multi-modal understanding, and safety; and (ii) from manual to\nautomated evaluation, encompassing dynamic dataset curation and\n\"LLM-as-a-judge\" scoring.\n  Yet, even with these transitions, a crucial obstacle persists: the evaluation\ngeneralization issue. Bounded test sets cannot scale alongside models whose\nabilities grow seemingly without limit. We will dissect this issue, along with\nthe core challenges of the above two transitions, from the perspectives of\nmethods, datasets, evaluators, and metrics. Due to the fast evolving of this\nfield, we will maintain a living GitHub repository (links are in each section)\nto crowd-source updates and corrections, and warmly invite contributors and\ncollaborators."}
{"id": "2504.18770", "pdf": "https://arxiv.org/pdf/2504.18770", "abs": "https://arxiv.org/abs/2504.18770", "authors": ["Manuel Weber", "Carly Beneke"], "title": "PyViT-FUSE: A Foundation Model for Multi-Sensor Earth Observation Data", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "11 pages, 13 figures, Published at ICLR 2025 - Machine Learning for\n  Remote Sensing (ML4RS) Workshop", "summary": "We propose PyViT-FUSE, a foundation model for earth observation data\nexplicitly designed to handle multi-modal imagery by learning to fuse an\narbitrary number of mixed-resolution input bands into a single representation\nthrough an attention mechanism. The learned patch tokens are further processed\nby a stack of vision transformers with a novel pyramidal structure. We train\nthe model on a globally sampled dataset in a self-supervised manner, leveraging\ncore concepts of the SwAV algorithm. We show the interpretability of the fusion\nmechanism by visualization of the attention scores and the models applicability\nto downstream tasks."}
{"id": "2504.18948", "pdf": "https://arxiv.org/pdf/2504.18948", "abs": "https://arxiv.org/abs/2504.18948", "authors": ["Devesh Pant", "Dibyendu Talukder", "Deepak Kumar", "Rachit Pandey", "Aaditeshwar Seth", "Chetan Arora"], "title": "Use of Metric Learning for the Recognition of Handwritten Digits, and its Application to Increase the Outreach of Voice-based Communication Platforms", "categories": ["cs.AI", "cs.CV"], "comment": "10 Pages, 7 Figures, ACM COMPASS 2022", "summary": "Initiation, monitoring, and evaluation of development programmes can involve\nfield-based data collection about project activities. This data collection\nthrough digital devices may not always be feasible though, for reasons such as\nunaffordability of smartphones and tablets by field-based cadre, or shortfalls\nin their training and capacity building. Paper-based data collection has been\nargued to be more appropriate in several contexts, with automated digitization\nof the paper forms through OCR (Optical Character Recognition) and OMR (Optical\nMark Recognition) techniques. We contribute with providing a large dataset of\nhandwritten digits, and deep learning based models and methods built using this\ndata, that are effective in real-world environments. We demonstrate the\ndeployment of these tools in the context of a maternal and child health and\nnutrition awareness project, which uses IVR (Interactive Voice Response)\nsystems to provide awareness information to rural women SHG (Self Help Group)\nmembers in north India. Paper forms were used to collect phone numbers of the\nSHG members at scale, which were digitized using the OCR tools developed by us,\nand used to push almost 4 million phone calls. The data, model, and code have\nbeen released in the open-source domain."}
{"id": "2504.18839", "pdf": "https://arxiv.org/pdf/2504.18839", "abs": "https://arxiv.org/abs/2504.18839", "authors": ["Abdellah Ghassel", "Xianzhi Li", "Xiaodan Zhu"], "title": "Towards Robust Dialogue Breakdown Detection: Addressing Disruptors in Large Language Models with Self-Guided Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are rapidly changing various domains. However,\ntheir capabilities in handling conversational breakdowns still require an\nin-depth exploration. This paper addresses the challenge of detecting and\nmitigating dialogue breakdowns within LLM-driven conversational systems. While\npowerful models from OpenAI and Anthropic excel in many dialogue tasks, they\ncan still produce incoherent or contradictory responses, commonly referred to\nas breakdowns, which undermine user trust. To tackle this, we propose an\napproach that combines specialized fine-tuning with advanced prompting\nstrategies, including few-shot learning, chain-of-thought reasoning, and\nanalogical prompting. In particular, we fine-tune a small 8B model and\ndemonstrate its robust classification and calibration capabilities in English\nand Japanese dialogue. We also validate its generalization on the BETOLD\ndataset, achieving a 7\\% accuracy improvement over its base model. Furthermore,\nwe introduce a real-time deployment architecture that selectively escalates\nsuspicious responses to more resource-intensive frontier models only when\nbreakdowns are detected, significantly cutting operational expenses and energy\nconsumption. Experimental results show our method surpasses prior\nstate-of-the-art specialized classifiers while also narrowing performance gaps\nbetween smaller open-source models and large proprietary ones. Our approach\noffers a scalable solution for robust conversational AI in high-impact domains\nby combining efficiency, interpretability, and reliability."}
{"id": "2504.18773", "pdf": "https://arxiv.org/pdf/2504.18773", "abs": "https://arxiv.org/abs/2504.18773", "authors": ["Zhiheng Tu", "Xinjian Huang", "Yong He", "Ruiyang Zhou", "Bo Du", "Weitao Wu"], "title": "Depth as Points: Center Point-based Depth Estimation", "categories": ["cs.CV"], "comment": "Depth Esitimation, Key-points, Virtual Datasets, Autonomous Driving", "summary": "The perception of vehicles and pedestrians in urban scenarios is crucial for\nautonomous driving. This process typically involves complicated data\ncollection, imposes high computational and hardware demands. To address these\nlimitations, we first develop a highly efficient method for generating virtual\ndatasets, which enables the creation of task- and scenario-specific datasets in\na short time. Leveraging this method, we construct the virtual depth estimation\ndataset VirDepth, a large-scale, multi-task autonomous driving dataset.\nSubsequently, we propose CenterDepth, a lightweight architecture for monocular\ndepth estimation that ensures high operational efficiency and exhibits superior\nperformance in depth estimation tasks with highly imbalanced height-scale\ndistributions. CenterDepth integrates global semantic information through the\ninnovative Center FC-CRFs algorithm, aggregates multi-scale features based on\nobject key points, and enables detection-based depth estimation of targets.\nExperiments demonstrate that our proposed method achieves superior performance\nin terms of both computational speed and prediction accuracy."}
{"id": "2504.19017", "pdf": "https://arxiv.org/pdf/2504.19017", "abs": "https://arxiv.org/abs/2504.19017", "authors": ["Alireza Ghafarollahi", "Markus J. Buehler"], "title": "Sparks: Multi-Agent Artificial Intelligence Model Discovers Protein Design Principles", "categories": ["cs.AI", "cond-mat.mtrl-sci", "cond-mat.soft", "cs.LG", "q-bio.BM"], "comment": null, "summary": "Advances in artificial intelligence (AI) promise autonomous discovery, yet\nmost systems still resurface knowledge latent in their training data. We\npresent Sparks, a multi-modal multi-agent AI model that executes the entire\ndiscovery cycle that includes hypothesis generation, experiment design and\niterative refinement to develop generalizable principles and a report without\nhuman intervention. Applied to protein science, Sparks uncovered two previously\nunknown phenomena: (i) a length-dependent mechanical crossover whereby\nbeta-sheet-biased peptides surpass alpha-helical ones in unfolding force beyond\n~80 residues, establishing a new design principle for peptide mechanics; and\n(ii) a chain-length/secondary-structure stability map revealing unexpectedly\nrobust beta-sheet-rich architectures and a \"frustration zone\" of high variance\nin mixed alpha/beta folds. These findings emerged from fully self-directed\nreasoning cycles that combined generative sequence design, high-accuracy\nstructure prediction and physics-aware property models, with paired\ngeneration-and-reflection agents enforcing self-correction and reproducibility.\nThe key result is that Sparks can independently conduct rigorous scientific\ninquiry and identify previously unknown scientific principles."}
{"id": "2504.18851", "pdf": "https://arxiv.org/pdf/2504.18851", "abs": "https://arxiv.org/abs/2504.18851", "authors": ["Hayley Ross", "Ameya Sunil Mahabaleshwarkar", "Yoshi Suhara"], "title": "When2Call: When (not) to Call Tools", "categories": ["cs.CL"], "comment": "NAACL 2025", "summary": "Leveraging external tools is a key feature for modern Language Models (LMs)\nto expand their capabilities and integrate them into existing systems. However,\nexisting benchmarks primarily focus on the accuracy of tool calling -- whether\nthe correct tool is called with the correct parameters -- and less on\nevaluating when LMs should (not) call tools. We develop a new benchmark,\nWhen2Call, which evaluates tool-calling decision-making: when to generate a\ntool call, when to ask follow-up questions and when to admit the question can't\nbe answered with the tools provided. We find that state-of-the-art tool-calling\nLMs show significant room for improvement on When2Call, indicating the\nimportance of this benchmark. We also develop a training set for When2Call and\nleverage the multiple-choice nature of the benchmark to develop a preference\noptimization training regime, which shows considerably more improvement than\ntraditional fine-tuning. We release the benchmark and training data as well as\nevaluation scripts at https://github.com/NVIDIA/When2Call."}
{"id": "2504.18781", "pdf": "https://arxiv.org/pdf/2504.18781", "abs": "https://arxiv.org/abs/2504.18781", "authors": ["Hassan Wasswa", "Timothy Lynar", "Aziida Nanyonga", "Hussein Abbass"], "title": "IoT Botnet Detection: Application of Vision Transformer to Classification of Network Flow Traffic", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Despite the demonstrated effectiveness of transformer models in NLP, and\nimage and video classification, the available tools for extracting features\nfrom captured IoT network flow packets fail to capture sequential patterns in\naddition to the absence of spatial patterns consequently limiting transformer\nmodel application. This work introduces a novel preprocessing method to adapt\ntransformer models, the vision transformer (ViT) in particular, for IoT botnet\nattack detection using network flow packets. The approach involves feature\nextraction from .pcap files and transforming each instance into a 1-channel 2D\nimage shape, enabling ViT-based classification. Also, the ViT model was\nenhanced to allow use any classifier besides Multilayer Perceptron (MLP) that\nwas deployed in the initial ViT paper. Models including the conventional feed\nforward Deep Neural Network (DNN), LSTM and Bidirectional-LSTM (BLSTM)\ndemonstrated competitive performance in terms of precision, recall, and\nF1-score for multiclass-based attack detection when evaluated on two IoT attack\ndatasets."}
{"id": "2504.19023", "pdf": "https://arxiv.org/pdf/2504.19023", "abs": "https://arxiv.org/abs/2504.19023", "authors": ["Justin Mücke", "Ansgar Scherp"], "title": "GLaMoR: Consistency Checking of OWL Ontologies using Graph Language Models", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Semantic reasoning aims to infer new knowledge from existing knowledge, with\nOWL ontologies serving as a standardized framework for organizing information.\nA key challenge in semantic reasoning is verifying ontology consistency.\nHowever, state-of-the-art reasoners are computationally expensive, and their\nefficiency decreases as ontology sizes grow. While classical machine learning\nmodels have been explored for consistency checking, they struggle to capture\ncomplex relationships within ontologies. Large language models (LLMs) have\nshown promising results for simple reasoning tasks but perform poorly on\nstructured reasoning. The recently introduced Graph Language Model (GLM) offers\na way to simultaneously process graph-structured data and text. This paper\nproposes GLaMoR (Graph Language Model for Reasoning), a reasoning pipeline that\ntransforms OWL ontologies into graph-structured data and adapts the GLM\narchitecture for consistency checking. We evaluate GLaMoR on ontologies from\nthe NCBO BioPortal repository, converting them into triples suitable for model\ninput. Our results show that the GLM outperforms all baseline models, achieving\n$95\\%$ accuracy while being 20 times faster than classical reasoners.\n  The Code is accessible under: https://github.com/JustinMuecke/GLaMoR"}
{"id": "2504.18857", "pdf": "https://arxiv.org/pdf/2504.18857", "abs": "https://arxiv.org/abs/2504.18857", "authors": ["Yi Lu", "Wanxu Zhao", "Xin Zhou", "Chenxin An", "Chenglong Wang", "Shuo Li", "Yuming Yang", "Jun Zhao", "Tao Ji", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "Effective Length Extrapolation via Dimension-Wise Positional Embeddings Manipulation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) often struggle to process and generate coherent\ncontext when the number of input tokens exceeds the pre-trained length. Recent\nadvancements in long-context extension have significantly expanded the context\nwindow of LLMs but require expensive overhead to train the large-scale models\nwith longer context. In this work, we propose Dimension-Wise Positional\nEmbeddings Manipulation (DPE), a training-free framework to extrapolate the\ncontext window of LLMs by diving into RoPE's different hidden dimensions.\nInstead of manipulating all dimensions equally, DPE detects the effective\nlength for every dimension and finds the key dimensions for context extension.\nWe reuse the original position indices with their embeddings from the\npre-trained model and manipulate the key dimensions' position indices to their\nmost effective lengths. In this way, DPE adjusts the pre-trained models with\nminimal modifications while ensuring that each dimension reaches its optimal\nstate for extrapolation. DPE significantly surpasses well-known baselines such\nas YaRN and Self-Extend. DPE enables Llama3-8k 8B to support context windows of\n128k tokens without continual training and integrates seamlessly with Flash\nAttention 2. In addition to its impressive extrapolation capability, DPE also\ndramatically improves the models' performance within training length, such as\nLlama3.1 70B, by over 18 points on popular long-context benchmarks RULER. When\ncompared with commercial models, Llama 3.1 70B with DPE even achieves better\nperformance than GPT-4-128K."}
{"id": "2504.18782", "pdf": "https://arxiv.org/pdf/2504.18782", "abs": "https://arxiv.org/abs/2504.18782", "authors": ["Hang Yu", "Jiahao Wen", "Zhedong Zheng"], "title": "CAMeL: Cross-modality Adaptive Meta-Learning for Text-based Person Retrieval", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Text-based person retrieval aims to identify specific individuals within an\nimage database using textual descriptions. Due to the high cost of annotation\nand privacy protection, researchers resort to synthesized data for the paradigm\nof pretraining and fine-tuning. However, these generated data often exhibit\ndomain biases in both images and textual annotations, which largely compromise\nthe scalability of the pre-trained model. Therefore, we introduce a\ndomain-agnostic pretraining framework based on Cross-modality Adaptive\nMeta-Learning (CAMeL) to enhance the model generalization capability during\npretraining to facilitate the subsequent downstream tasks. In particular, we\ndevelop a series of tasks that reflect the diversity and complexity of\nreal-world scenarios, and introduce a dynamic error sample memory unit to\nmemorize the history for errors encountered within multiple tasks. To further\nensure multi-task adaptation, we also adopt an adaptive dual-speed update\nstrategy, balancing fast adaptation to new tasks and slow weight updates for\nhistorical tasks. Albeit simple, our proposed model not only surpasses existing\nstate-of-the-art methods on real-world benchmarks, including CUHK-PEDES,\nICFG-PEDES, and RSTPReid, but also showcases robustness and scalability in\nhandling biased synthetic images and noisy text annotations. Our code is\navailable at https://github.com/Jahawn-Wen/CAMeL-reID."}
{"id": "2504.19027", "pdf": "https://arxiv.org/pdf/2504.19027", "abs": "https://arxiv.org/abs/2504.19027", "authors": ["Volkan Bakir", "Polat Goktas", "Sureyya Akyuz"], "title": "DiCE-Extended: A Robust Approach to Counterfactual Explanations in Machine Learning", "categories": ["cs.AI", "cs.LG", "cs.NE", "I.2; K.4; H.4"], "comment": "MCO 2025, 5th International Conference on Modelling, Computation and\n  Optimization in Information Systems and Management Sciences", "summary": "Explainable artificial intelligence (XAI) has become increasingly important\nin decision-critical domains such as healthcare, finance, and law.\nCounterfactual (CF) explanations, a key approach in XAI, provide users with\nactionable insights by suggesting minimal modifications to input features that\nlead to different model outcomes. Despite significant advancements, existing CF\ngeneration methods often struggle to balance proximity, diversity, and\nrobustness, limiting their real-world applicability. A widely adopted\nframework, Diverse Counterfactual Explanations (DiCE), emphasizes diversity but\nlacks robustness, making CF explanations sensitive to perturbations and domain\nconstraints. To address these challenges, we introduce DiCE-Extended, an\nenhanced CF explanation framework that integrates multi-objective optimization\ntechniques to improve robustness while maintaining interpretability. Our\napproach introduces a novel robustness metric based on the Dice-Sorensen\ncoefficient, ensuring stability under small input variations. Additionally, we\nrefine CF generation using weighted loss components (lambda_p, lambda_d,\nlambda_r) to balance proximity, diversity, and robustness. We empirically\nvalidate DiCE-Extended on benchmark datasets (COMPAS, Lending Club, German\nCredit, Adult Income) across multiple ML backends (Scikit-learn, PyTorch,\nTensorFlow). Results demonstrate improved CF validity, stability, and alignment\nwith decision boundaries compared to standard DiCE-generated explanations. Our\nfindings highlight the potential of DiCE-Extended in generating more reliable\nand interpretable CFs for high-stakes applications. Future work will explore\nadaptive optimization techniques and domain-specific constraints to further\nenhance CF generation in real-world scenarios."}
{"id": "2504.18872", "pdf": "https://arxiv.org/pdf/2504.18872", "abs": "https://arxiv.org/abs/2504.18872", "authors": ["Alexandra Abbas", "Nora Petrova", "Helios Ael Lyons", "Natalia Perez-Campanero"], "title": "Latent Adversarial Training Improves the Representation of Refusal", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Recent work has shown that language models' refusal behavior is primarily\nencoded in a single direction in their latent space, making it vulnerable to\ntargeted attacks. Although Latent Adversarial Training (LAT) attempts to\nimprove robustness by introducing noise during training, a key question\nremains: How does this noise-based training affect the underlying\nrepresentation of refusal behavior? Understanding this encoding is crucial for\nevaluating LAT's effectiveness and limitations, just as the discovery of linear\nrefusal directions revealed vulnerabilities in traditional supervised safety\nfine-tuning (SSFT).\n  Through the analysis of Llama 2 7B, we examine how LAT reorganizes the\nrefusal behavior in the model's latent space compared to SSFT and embedding\nspace adversarial training (AT). By computing activation differences between\nharmful and harmless instruction pairs and applying Singular Value\nDecomposition (SVD), we find that LAT significantly alters the refusal\nrepresentation, concentrating it in the first two SVD components which explain\napproximately 75 percent of the activation differences variance - significantly\nhigher than in reference models. This concentrated representation leads to more\neffective and transferable refusal vectors for ablation attacks: LAT models\nshow improved robustness when attacked with vectors from reference models but\nbecome more vulnerable to self-generated vectors compared to SSFT and AT. Our\nfindings suggest that LAT's training perturbations enable a more comprehensive\nrepresentation of refusal behavior, highlighting both its potential strengths\nand vulnerabilities for improving model safety."}
{"id": "2504.18800", "pdf": "https://arxiv.org/pdf/2504.18800", "abs": "https://arxiv.org/abs/2504.18800", "authors": ["Ryo Takizawa", "Satoshi Kodera", "Tempei Kabayama", "Ryo Matsuoka", "Yuta Ando", "Yuto Nakamura", "Haruki Settai", "Norihiko Takeda"], "title": "Video CLIP Model for Multi-View Echocardiography Interpretation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Echocardiography involves recording videos of the heart using ultrasound,\nenabling clinicians to evaluate its condition. Recent advances in large-scale\nvision-language models (VLMs) have garnered attention for automating the\ninterpretation of echocardiographic videos. However, most existing VLMs\nproposed for medical interpretation thus far rely on single-frame (i.e., image)\ninputs. Consequently, these image-based models often exhibit lower diagnostic\naccuracy for conditions identifiable through cardiac motion. Moreover,\nechocardiographic videos are recorded from various views that depend on the\ndirection of ultrasound emission, and certain views are more suitable than\nothers for interpreting specific conditions. Incorporating multiple views could\npotentially yield further improvements in accuracy. In this study, we developed\na video-language model that takes five different views and full video sequences\nas input, training it on pairs of echocardiographic videos and clinical reports\nfrom 60,747 cases. Our experiments demonstrate that this expanded approach\nachieves higher interpretation accuracy than models trained with only\nsingle-view videos or with still images."}
{"id": "2504.19144", "pdf": "https://arxiv.org/pdf/2504.19144", "abs": "https://arxiv.org/abs/2504.19144", "authors": ["Bowei Wang", "Jiaran Gao", "Yelai Feng", "Renzhi Chen", "Shanshan Li", "Lei Wang"], "title": "ChiseLLM: Unleashing the Power of Reasoning LLMs for Chisel Agile Hardware Development", "categories": ["cs.AI", "cs.AR", "cs.SE"], "comment": null, "summary": "The growing demand for Domain-Specific Architecture (DSA) has driven the\ndevelopment of Agile Hardware Development Methodology (AHDM). Hardware\nConstruction Language (HCL) like Chisel offers high-level abstraction features,\nmaking it an ideal language for HCL-Based AHDM. While Large Language Models\n(LLMs) excel in code generation tasks, they still face challenges with Chisel\ngeneration, particularly regarding syntax correctness and design variability.\nRecent reasoning models have significantly enhanced code generation\ncapabilities through test-time scaling techniques. However, we found that\nreasoning models without domain adaptation cannot bring substantial benefits to\nChisel code generation tasks. This paper presents ChiseLLM, a solution\ncomprising data processing and transformation, prompt-guided reasoning trace\nsynthesis, and domain-adapted model training. We constructed high-quality\ndatasets from public RTL code resources and guided the model to adopt\nstructured thinking patterns through prompt enhancement methods. Experiments\ndemonstrate that our ChiseLLM-7B and ChiseLLM-32B models improved syntax\ncorrectness by 18.85% and 26.32% respectively over base models, while\nincreasing variability design ability by 47.58% compared to baseline reasoning\nmodels. Our datasets and models are publicly available, providing\nhigh-performance, cost-effective models for HCL-Based AHDM, and offering an\neffective baseline for future research. Github repository:\nhttps://github.com/observerw/ChiseLLM"}
{"id": "2504.18884", "pdf": "https://arxiv.org/pdf/2504.18884", "abs": "https://arxiv.org/abs/2504.18884", "authors": ["Junichiro Niimi"], "title": "A Simple Ensemble Strategy for LLM Inference: Towards More Stable Text Classification", "categories": ["cs.CL", "cs.AI"], "comment": "This manuscript has been accepted for the 30th International\n  Conference on Natural Language & Information Systems (NLDB 2025). The final\n  version will appear in the Springer LNCS proceedings. arXiv admin note: text\n  overlap with arXiv:2407.13069", "summary": "With the advance of large language models (LLMs), LLMs have been utilized for\nthe various tasks. However, the issues of variability and reproducibility of\nresults from each trial of LLMs have been largely overlooked in existing\nliterature while actual human annotation uses majority voting to resolve\ndisagreements among annotators. Therefore, this study introduces the\nstraightforward ensemble strategy to a sentiment analysis using LLMs. As the\nresults, we demonstrate that the ensemble of multiple inference using\nmedium-sized LLMs produces more robust and accurate results than using a large\nmodel with a single attempt with reducing RMSE by 18.6%."}
{"id": "2504.18810", "pdf": "https://arxiv.org/pdf/2504.18810", "abs": "https://arxiv.org/abs/2504.18810", "authors": ["Yifan Xie", "Fei Ma", "Yi Bin", "Ying He", "Fei Yu"], "title": "Audio-Driven Talking Face Video Generation with Joint Uncertainty Learning", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 7 figures", "summary": "Talking face video generation with arbitrary speech audio is a significant\nchallenge within the realm of digital human technology. The previous studies\nhave emphasized the significance of audio-lip synchronization and visual\nquality. Currently, limited attention has been given to the learning of visual\nuncertainty, which creates several issues in existing systems, including\ninconsistent visual quality and unreliable performance across different input\nconditions. To address the problem, we propose a Joint Uncertainty Learning\nNetwork (JULNet) for high-quality talking face video generation, which\nincorporates a representation of uncertainty that is directly related to visual\nerror. Specifically, we first design an uncertainty module to individually\npredict the error map and uncertainty map after obtaining the generated image.\nThe error map represents the difference between the generated image and the\nground truth image, while the uncertainty map is used to predict the\nprobability of incorrect estimates. Furthermore, to match the uncertainty\ndistribution with the error distribution through a KL divergence term, we\nintroduce a histogram technique to approximate the distributions. By jointly\noptimizing error and uncertainty, the performance and robustness of our model\ncan be enhanced. Extensive experiments demonstrate that our method achieves\nsuperior high-fidelity and audio-lip synchronization in talking face video\ngeneration compared to previous methods."}
{"id": "2504.19148", "pdf": "https://arxiv.org/pdf/2504.19148", "abs": "https://arxiv.org/abs/2504.19148", "authors": ["Ke Liu", "Jing Ma", "Edmund M-K Lai"], "title": "A Dynamic Fuzzy Rule and Attribute Management Framework for Fuzzy Inference Systems in High-Dimensional Data", "categories": ["cs.AI"], "comment": null, "summary": "This paper presents an Adaptive Dynamic Attribute and Rule (ADAR) framework\ndesigned to address the challenges posed by high-dimensional data in\nneuro-fuzzy inference systems. By integrating dual weighting\nmechanisms-assigning adaptive importance to both attributes and rules-together\nwith automated growth and pruning strategies, ADAR adaptively streamlines\ncomplex fuzzy models without sacrificing performance or interpretability.\nExperimental evaluations on four diverse datasets - Auto MPG (7 variables),\nBeijing PM2.5 (10 variables), Boston Housing (13 variables), and Appliances\nEnergy Consumption (27 variables) show that ADAR-based models achieve\nconsistently lower Root Mean Square Error (RMSE) compared to state-of-the-art\nbaselines. On the Beijing PM2.5 dataset, for instance, ADAR-SOFENN attained an\nRMSE of 56.87 with nine rules, surpassing traditional ANFIS [12] and SOFENN\n[16] models. Similarly, on the high-dimensional Appliances Energy dataset,\nADAR-ANFIS reached an RMSE of 83.25 with nine rules, outperforming established\nfuzzy logic approaches and interpretability-focused methods such as APLR.\nAblation studies further reveal that combining rule-level and attribute-level\nweight assignment significantly reduces model overlap while preserving\nessential features, thereby enhancing explainability. These results highlight\nADAR's effectiveness in dynamically balancing rule complexity and feature\nimportance, paving the way for scalable, high-accuracy, and transparent\nneuro-fuzzy systems applicable to a range of real-world scenarios."}
{"id": "2504.18938", "pdf": "https://arxiv.org/pdf/2504.18938", "abs": "https://arxiv.org/abs/2504.18938", "authors": ["Junhong Liang", "Yu Zhou"], "title": "MTCSC: Retrieval-Augmented Iterative Refinement for Chinese Spelling Correction", "categories": ["cs.CL"], "comment": "12 pages, 2 figures", "summary": "Chinese Spelling Correction (CSC) aims to detect and correct erroneous tokens\nin sentences. While Large Language Models (LLMs) have shown remarkable success\nin identifying and rectifying potential errors, they often struggle with\nmaintaining consistent output lengths and adapting to domain-specific\ncorrections. Furthermore, existing CSC task impose rigid constraints requiring\ninput and output lengths to be identical, limiting their applicability. In this\nwork, we extend traditional CSC to variable-length correction scenarios,\nincluding Chinese Splitting Error Correction (CSEC) and ASR N-best Error\nCorrection. To address domain adaptation and length consistency, we propose\nMTCSC (Multi-Turn CSC) framework based on RAG enhanced with a length reflection\nmechanism. Our approach constructs a retrieval database from domain-specific\ntraining data and dictionaries, fine-tuning retrievers to optimize performance\nfor error-containing inputs. Additionally, we introduce a multi-source\ncombination strategy with iterative length reflection to ensure output length\nfidelity. Experiments across diverse domain datasets demonstrate that our\nmethod significantly outperforms current approaches in correction quality,\nparticularly in handling domain-specific and variable-length error correction\ntasks."}
{"id": "2504.18856", "pdf": "https://arxiv.org/pdf/2504.18856", "abs": "https://arxiv.org/abs/2504.18856", "authors": ["Shahad Albastaki", "Anabia Sohail", "Iyyakutti Iyappan Ganapathi", "Basit Alawode", "Asim Khan", "Sajid Javed", "Naoufel Werghi", "Mohammed Bennamoun", "Arif Mahmood"], "title": "Multi-Resolution Pathology-Language Pre-training Model with Text-Guided Visual Representation", "categories": ["cs.CV"], "comment": null, "summary": "In Computational Pathology (CPath), the introduction of Vision-Language\nModels (VLMs) has opened new avenues for research, focusing primarily on\naligning image-text pairs at a single magnification level. However, this\napproach might not be sufficient for tasks like cancer subtype classification,\ntissue phenotyping, and survival analysis due to the limited level of detail\nthat a single-resolution image can provide. Addressing this, we propose a novel\nmulti-resolution paradigm leveraging Whole Slide Images (WSIs) to extract\nhistology patches at multiple resolutions and generate corresponding textual\ndescriptions through advanced CPath VLM. We introduce visual-textual alignment\nat multiple resolutions as well as cross-resolution alignment to establish more\neffective text-guided visual representations. Cross-resolution alignment using\na multimodal encoder enhances the model's ability to capture context from\nmultiple resolutions in histology images. Our model aims to capture a broader\nrange of information, supported by novel loss functions, enriches feature\nrepresentation, improves discriminative ability, and enhances generalization\nacross different resolutions. Pre-trained on a comprehensive TCGA dataset with\n34 million image-language pairs at various resolutions, our fine-tuned model\noutperforms state-of-the-art (SOTA) counterparts across multiple datasets and\ntasks, demonstrating its effectiveness in CPath. The code is available on\nGitHub at: https://github.com/BasitAlawode/MR-PLIP"}
{"id": "2504.19179", "pdf": "https://arxiv.org/pdf/2504.19179", "abs": "https://arxiv.org/abs/2504.19179", "authors": ["Pedro A. Moreno-Sánchez", "Javier Del Ser", "Mark van Gils", "Jussi Hernesniemi"], "title": "A Design Framework for operationalizing Trustworthy Artificial Intelligence in Healthcare: Requirements, Tradeoffs and Challenges for its Clinical Adoption", "categories": ["cs.AI"], "comment": null, "summary": "Artificial Intelligence (AI) holds great promise for transforming healthcare,\nparticularly in disease diagnosis, prognosis, and patient care. The increasing\navailability of digital medical data, such as images, omics, biosignals, and\nelectronic health records, combined with advances in computing, has enabled AI\nmodels to approach expert-level performance. However, widespread clinical\nadoption remains limited, primarily due to challenges beyond technical\nperformance, including ethical concerns, regulatory barriers, and lack of\ntrust. To address these issues, AI systems must align with the principles of\nTrustworthy AI (TAI), which emphasize human agency and oversight, algorithmic\nrobustness, privacy and data governance, transparency, bias and discrimination\navoidance, and accountability. Yet, the complexity of healthcare processes\n(e.g., screening, diagnosis, prognosis, and treatment) and the diversity of\nstakeholders (clinicians, patients, providers, regulators) complicate the\nintegration of TAI principles. To bridge the gap between TAI theory and\npractical implementation, this paper proposes a design framework to support\ndevelopers in embedding TAI principles into medical AI systems. Thus, for each\nstakeholder identified across various healthcare processes, we propose a\ndisease-agnostic collection of requirements that medical AI systems should\nincorporate to adhere to the principles of TAI. Additionally, we examine the\nchallenges and tradeoffs that may arise when applying these principles in\npractice. To ground the discussion, we focus on cardiovascular diseases, a\nfield marked by both high prevalence and active AI innovation, and demonstrate\nhow TAI principles have been applied and where key obstacles persist."}
{"id": "2504.18942", "pdf": "https://arxiv.org/pdf/2504.18942", "abs": "https://arxiv.org/abs/2504.18942", "authors": ["Debarati Das", "Khanh Chi Le", "Ritik Sachin Parkar", "Karin De Langis", "Brendan Madson", "Chad M. Berryman", "Robin M. Willis", "Daniel H. Moses", "Brett McDonnell", "Daniel Schwarcz", "Dongyeop Kang"], "title": "LawFlow : Collecting and Simulating Lawyers' Thought Processes", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "submitted to COLM 2025", "summary": "Legal practitioners, particularly those early in their careers, face complex,\nhigh-stakes tasks that require adaptive, context-sensitive reasoning. While AI\nholds promise in supporting legal work, current datasets and models are\nnarrowly focused on isolated subtasks and fail to capture the end-to-end\ndecision-making required in real-world practice. To address this gap, we\nintroduce LawFlow, a dataset of complete end-to-end legal workflows collected\nfrom trained law students, grounded in real-world business entity formation\nscenarios. Unlike prior datasets focused on input-output pairs or linear chains\nof thought, LawFlow captures dynamic, modular, and iterative reasoning\nprocesses that reflect the ambiguity, revision, and client-adaptive strategies\nof legal practice. Using LawFlow, we compare human and LLM-generated workflows,\nrevealing systematic differences in structure, reasoning flexibility, and plan\nexecution. Human workflows tend to be modular and adaptive, while LLM workflows\nare more sequential, exhaustive, and less sensitive to downstream implications.\nOur findings also suggest that legal professionals prefer AI to carry out\nsupportive roles, such as brainstorming, identifying blind spots, and surfacing\nalternatives, rather than executing complex workflows end-to-end. Building on\nthese findings, we propose a set of design suggestions, rooted in empirical\nobservations, that align AI assistance with human goals of clarity,\ncompleteness, creativity, and efficiency, through hybrid planning, adaptive\nexecution, and decision-point support. Our results highlight both the current\nlimitations of LLMs in supporting complex legal workflows and opportunities for\ndeveloping more collaborative, reasoning-aware legal AI systems. All data and\ncode are available on our project page\n(https://minnesotanlp.github.io/LawFlow-website/)."}
{"id": "2504.18864", "pdf": "https://arxiv.org/pdf/2504.18864", "abs": "https://arxiv.org/abs/2504.18864", "authors": ["Yunzhong Zhang", "Bo Xiong", "You Zhou", "Changqing Su", "Zhen Cheng", "Zhaofei Yu", "Xun Cao", "Tiejun Huang"], "title": "Spike Imaging Velocimetry: Dense Motion Estimation of Fluids Using Spike Cameras", "categories": ["cs.CV"], "comment": null, "summary": "The need for accurate and non-intrusive flow measurement methods has led to\nthe widespread adoption of Particle Image Velocimetry (PIV), a powerful\ndiagnostic tool in fluid motion estimation. This study investigates the\ntremendous potential of spike cameras (a type of ultra-high-speed,\nhigh-dynamic-range camera) in PIV. We propose a deep learning framework, Spike\nImaging Velocimetry (SIV), designed specifically for highly turbulent and\nintricate flow fields. To aggregate motion features from the spike stream while\nminimizing information loss, we incorporate a Detail-Preserving Hierarchical\nTransform (DPHT) module. Additionally, we introduce a Graph Encoder (GE) to\nextract contextual features from highly complex fluid flows. Furthermore, we\npresent a spike-based PIV dataset, Particle Scenes with Spike and Displacement\n(PSSD), which provides labeled data for three challenging fluid dynamics\nscenarios. Our proposed method achieves superior performance compared to\nexisting baseline methods on PSSD. The datasets and our implementation of SIV\nare open-sourced in the supplementary materials."}
{"id": "2504.19255", "pdf": "https://arxiv.org/pdf/2504.19255", "abs": "https://arxiv.org/abs/2504.19255", "authors": ["Chad Coleman", "W. Russell Neuman", "Ali Dasdan", "Safinah Ali", "Manan Shah"], "title": "The Convergent Ethics of AI? Analyzing Moral Foundation Priorities in Large Language Models with a Multi-Framework Approach", "categories": ["cs.AI", "cs.CY"], "comment": "25 pages, 8 figures", "summary": "As large language models (LLMs) are increasingly deployed in consequential\ndecision-making contexts, systematically assessing their ethical reasoning\ncapabilities becomes a critical imperative. This paper introduces the\nPriorities in Reasoning and Intrinsic Moral Evaluation (PRIME) framework--a\ncomprehensive methodology for analyzing moral priorities across foundational\nethical dimensions including consequentialist-deontological reasoning, moral\nfoundations theory, and Kohlberg's developmental stages. We apply this\nframework to six leading LLMs through a dual-protocol approach combining direct\nquestioning and response analysis to established ethical dilemmas. Our analysis\nreveals striking patterns of convergence: all evaluated models demonstrate\nstrong prioritization of care/harm and fairness/cheating foundations while\nconsistently underweighting authority, loyalty, and sanctity dimensions.\nThrough detailed examination of confidence metrics, response reluctance\npatterns, and reasoning consistency, we establish that contemporary LLMs (1)\nproduce decisive ethical judgments, (2) demonstrate notable cross-model\nalignment in moral decision-making, and (3) generally correspond with\nempirically established human moral preferences. This research contributes a\nscalable, extensible methodology for ethical benchmarking while highlighting\nboth the promising capabilities and systematic limitations in current AI moral\nreasoning architectures--insights critical for responsible development as these\nsystems assume increasingly significant societal roles."}
{"id": "2504.18992", "pdf": "https://arxiv.org/pdf/2504.18992", "abs": "https://arxiv.org/abs/2504.18992", "authors": ["Sanwoo Lee", "Jiahao Liu", "Qifan Wang", "Jingang Wang", "Xunliang Cai", "Yunfang Wu"], "title": "Dynamic Fisher-weighted Model Merging via Bayesian Optimization", "categories": ["cs.CL"], "comment": null, "summary": "The fine-tuning of pre-trained language models has resulted in the widespread\navailability of task-specific models. Model merging offers an efficient way to\ncreate multi-task models by combining these fine-tuned models at the parameter\nlevel, without the need for training data or joint training on multiple\ndatasets. Existing merging approaches typically involve scaling the parameters\nmodel-wise or integrating parameter importance parameter-wise. Both approaches\nexhibit their own weaknesses, leading to a notable performance gap compared to\nmulti-task fine-tuning. In this paper, we unify these seemingly distinct\nstrategies into a more general merging framework, and introduce Dynamic\nFisher-weighted Merging (DF-Merge). Specifically, candidate models are\nassociated with a set of coefficients that linearly scale their fine-tuned\nparameters. Bayesian optimization is applied to dynamically adjust these\ncoefficients, aiming to maximize overall performance on validation sets. Each\niteration of this process integrates parameter importance based on the Fisher\ninformation conditioned by the coefficients. Experimental results show that\nDF-Merge outperforms strong baselines across models of different sizes and a\nvariety of tasks. Our analysis shows that the effectiveness of DF-Merge arises\nfrom the unified view of merging and that near-optimal performance is\nachievable in a few iterations, even with minimal validation data."}
{"id": "2504.18866", "pdf": "https://arxiv.org/pdf/2504.18866", "abs": "https://arxiv.org/abs/2504.18866", "authors": ["Jiaxu Leng", "Zhanjie Wu", "Mingpi Tan", "Mengjingcheng Mo", "Jiankang Zheng", "Qingqing Li", "Ji Gan", "Xinbo Gao"], "title": "PiercingEye: Dual-Space Video Violence Detection with Hyperbolic Vision-Language Guidance", "categories": ["cs.CV"], "comment": "Submitted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence", "summary": "Existing weakly supervised video violence detection (VVD) methods primarily\nrely on Euclidean representation learning, which often struggles to distinguish\nvisually similar yet semantically distinct events due to limited hierarchical\nmodeling and insufficient ambiguous training samples. To address this\nchallenge, we propose PiercingEye, a novel dual-space learning framework that\nsynergizes Euclidean and hyperbolic geometries to enhance discriminative\nfeature representation. Specifically, PiercingEye introduces a layer-sensitive\nhyperbolic aggregation strategy with hyperbolic Dirichlet energy constraints to\nprogressively model event hierarchies, and a cross-space attention mechanism to\nfacilitate complementary feature interactions between Euclidean and hyperbolic\nspaces. Furthermore, to mitigate the scarcity of ambiguous samples, we leverage\nlarge language models to generate logic-guided ambiguous event descriptions,\nenabling explicit supervision through a hyperbolic vision-language contrastive\nloss that prioritizes high-confusion samples via dynamic similarity-aware\nweighting. Extensive experiments on XD-Violence and UCF-Crime benchmarks\ndemonstrate that PiercingEye achieves state-of-the-art performance, with\nparticularly strong results on a newly curated ambiguous event subset,\nvalidating its superior capability in fine-grained violence detection."}
{"id": "2504.19277", "pdf": "https://arxiv.org/pdf/2504.19277", "abs": "https://arxiv.org/abs/2504.19277", "authors": ["Ishan Kavathekar", "Raghav Donakanti", "Ponnurangam Kumaraguru", "Karthik Vaidhyanathan"], "title": "Small Models, Big Tasks: An Exploratory Empirical Study on Small Language Models for Function Calling", "categories": ["cs.AI", "cs.SE"], "comment": "Accepted at EASE 2025 AI Models and Data Evaluation track", "summary": "Function calling is a complex task with widespread applications in domains\nsuch as information retrieval, software engineering and automation. For\nexample, a query to book the shortest flight from New York to London on January\n15 requires identifying the correct parameters to generate accurate function\ncalls. Large Language Models (LLMs) can automate this process but are\ncomputationally expensive and impractical in resource-constrained settings. In\ncontrast, Small Language Models (SLMs) can operate efficiently, offering faster\nresponse times, and lower computational demands, making them potential\ncandidates for function calling on edge devices. In this exploratory empirical\nstudy, we evaluate the efficacy of SLMs in generating function calls across\ndiverse domains using zero-shot, few-shot, and fine-tuning approaches, both\nwith and without prompt injection, while also providing the finetuned models to\nfacilitate future applications. Furthermore, we analyze the model responses\nacross a range of metrics, capturing various aspects of function call\ngeneration. Additionally, we perform experiments on an edge device to evaluate\ntheir performance in terms of latency and memory usage, providing useful\ninsights into their practical applicability. Our findings show that while SLMs\nimprove from zero-shot to few-shot and perform best with fine-tuning, they\nstruggle significantly with adhering to the given output format. Prompt\ninjection experiments further indicate that the models are generally robust and\nexhibit only a slight decline in performance. While SLMs demonstrate potential\nfor the function call generation task, our results also highlight areas that\nneed further refinement for real-time functioning."}
{"id": "2504.19019", "pdf": "https://arxiv.org/pdf/2504.19019", "abs": "https://arxiv.org/abs/2504.19019", "authors": ["Mohammad Akbar-Tajari", "Mohammad Taher Pilehvar", "Mohammad Mahmoody"], "title": "Graph of Attacks: Improved Black-Box and Interpretable Jailbreaks for LLMs", "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": "19 pages, 1 figure, 6 tables", "summary": "The challenge of ensuring Large Language Models (LLMs) align with societal\nstandards is of increasing interest, as these models are still prone to\nadversarial jailbreaks that bypass their safety mechanisms. Identifying these\nvulnerabilities is crucial for enhancing the robustness of LLMs against such\nexploits. We propose Graph of ATtacks (GoAT), a method for generating\nadversarial prompts to test the robustness of LLM alignment using the Graph of\nThoughts framework [Besta et al., 2024]. GoAT excels at generating highly\neffective jailbreak prompts with fewer queries to the victim model than\nstate-of-the-art attacks, achieving up to five times better jailbreak success\nrate against robust models like Llama. Notably, GoAT creates high-quality,\nhuman-readable prompts without requiring access to the targeted model's\nparameters, making it a black-box attack. Unlike approaches constrained by\ntree-based reasoning, GoAT's reasoning is based on a more intricate graph\nstructure. By making simultaneous attack paths aware of each other's progress,\nthis dynamic framework allows a deeper integration and refinement of reasoning\npaths, significantly enhancing the collaborative exploration of adversarial\nvulnerabilities in LLMs. At a technical level, GoAT starts with a graph\nstructure and iteratively refines it by combining and improving thoughts,\nenabling synergy between different thought paths. The code for our\nimplementation can be found at: https://github.com/GoAT-pydev/Graph_of_Attacks."}
{"id": "2504.18870", "pdf": "https://arxiv.org/pdf/2504.18870", "abs": "https://arxiv.org/abs/2504.18870", "authors": ["Guodong Sun", "Mingjing Li", "Dingjie Liu", "Mingxuan Liu", "Bo Wu", "Yang Zhang"], "title": "WLTCL: Wide Field-of-View 3-D LiDAR Truck Compartment Automatic Localization System", "categories": ["cs.CV", "cs.RO", "eess.IV"], "comment": "To appear in IEEE TIM", "summary": "As an essential component of logistics automation, the automated loading\nsystem is becoming a critical technology for enhancing operational efficiency\nand safety. Precise automatic positioning of the truck compartment, which\nserves as the loading area, is the primary step in automated loading. However,\nexisting methods have difficulty adapting to truck compartments of various\nsizes, do not establish a unified coordinate system for LiDAR and mobile\nmanipulators, and often exhibit reliability issues in cluttered environments.\nTo address these limitations, our study focuses on achieving precise automatic\npositioning of key points in large, medium, and small fence-style truck\ncompartments in cluttered scenarios. We propose an innovative wide\nfield-of-view 3-D LiDAR vehicle compartment automatic localization system. For\nvehicles of various sizes, this system leverages the LiDAR to generate\nhigh-density point clouds within an extensive field-of-view range. By\nincorporating parking area constraints, our vehicle point cloud segmentation\nmethod more effectively segments vehicle point clouds within the scene. Our\ncompartment key point positioning algorithm utilizes the geometric features of\nthe compartments to accurately locate the corner points, providing stackable\nspatial regions. Extensive experiments on our collected data and public\ndatasets demonstrate that this system offers reliable positioning accuracy and\nreduced computational resource consumption, leading to its application and\npromotion in relevant fields."}
{"id": "2504.19320", "pdf": "https://arxiv.org/pdf/2504.19320", "abs": "https://arxiv.org/abs/2504.19320", "authors": ["Ralph Wojtowicz"], "title": "Logic-Based Artificial Intelligence Algorithms Supporting Categorical Semantics", "categories": ["cs.AI", "03, 18", "I.1.2"], "comment": "31 pages", "summary": "This paper seeks to apply categorical logic to the design of artificial\nintelligent agents that reason symbolically about objects more richly\nstructured than sets. Using Johnstone's sequent calculus of terms- and\nformulae-in-context, we develop forward chaining and normal form algorithms for\nreasoning about objects in cartesian categories with the rules for Horn logic.\nWe also adapt first-order unification to support multi-sorted theories,\ncontexts, and fragments of first-order logic. The significance of these\nreformulations rests in the fact that they can be applied to reasoning about\nobjects in semantic categories that do not support classical logic or even all\nits connectives."}
{"id": "2504.19021", "pdf": "https://arxiv.org/pdf/2504.19021", "abs": "https://arxiv.org/abs/2504.19021", "authors": ["Zhyar Rzgar K Rostam", "Gábor Kertész"], "title": "Advancing Scientific Text Classification: Fine-Tuned Models with Dataset Expansion and Hard-Voting", "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 1 figure, 8 tables", "summary": "Efficient text classification is essential for handling the increasing volume\nof academic publications. This study explores the use of pre-trained language\nmodels (PLMs), including BERT, SciBERT, BioBERT, and BlueBERT, fine-tuned on\nthe Web of Science (WoS-46985) dataset for scientific text classification. To\nenhance performance, we augment the dataset by executing seven targeted queries\nin the WoS database, retrieving 1,000 articles per category aligned with\nWoS-46985's main classes. PLMs predict labels for this unlabeled data, and a\nhard-voting strategy combines predictions for improved accuracy and confidence.\nFine-tuning on the expanded dataset with dynamic learning rates and early\nstopping significantly boosts classification accuracy, especially in\nspecialized domains. Domain-specific models like SciBERT and BioBERT\nconsistently outperform general-purpose models such as BERT. These findings\nunderscore the efficacy of dataset augmentation, inference-driven label\nprediction, hard-voting, and fine-tuning techniques in creating robust and\nscalable solutions for automated academic text classification."}
{"id": "2504.18886", "pdf": "https://arxiv.org/pdf/2504.18886", "abs": "https://arxiv.org/abs/2504.18886", "authors": ["Simone Maurizio La Cava", "Roberto Casula", "Sara Concas", "Giulia Orrù", "Ruben Tolosana", "Martin Drahansky", "Julian Fierrez", "Gian Luca Marcialis"], "title": "Exploiting Multiple Representations: 3D Face Biometrics Fusion with Application to Surveillance", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "3D face reconstruction (3DFR) algorithms are based on specific assumptions\ntailored to the limits and characteristics of the different application\nscenarios. In this study, we investigate how multiple state-of-the-art 3DFR\nalgorithms can be used to generate a better representation of subjects, with\nthe final goal of improving the performance of face recognition systems in\nchallenging uncontrolled scenarios. We also explore how different parametric\nand non-parametric score-level fusion methods can exploit the unique strengths\nof multiple 3DFR algorithms to enhance biometric recognition robustness. With\nthis goal, we propose a comprehensive analysis of several face recognition\nsystems across diverse conditions, such as varying distances and camera setups,\nintra-dataset and cross-dataset, to assess the robustness of the proposed\nensemble method. The results demonstrate that the distinct information provided\nby different 3DFR algorithms can alleviate the problem of generalizing over\nmultiple application scenarios. In addition, the present study highlights the\npotential of advanced fusion strategies to enhance the reliability of\n3DFR-based face recognition systems, providing the research community with key\ninsights to exploit them in real-world applications effectively. Although the\nexperiments are carried out in a specific face verification setup, our proposed\nfusion-based 3DFR methods may be applied to other tasks around face biometrics\nthat are not strictly related to identity recognition."}
{"id": "2504.19354", "pdf": "https://arxiv.org/pdf/2504.19354", "abs": "https://arxiv.org/abs/2504.19354", "authors": ["Erkan Karabulut", "Paul Groth", "Victoria Degeler"], "title": "Neurosymbolic Association Rule Mining from Tabular Data", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Association Rule Mining (ARM) is the task of mining patterns among data\nfeatures in the form of logical rules, with applications across a myriad of\ndomains. However, high-dimensional datasets often result in an excessive number\nof rules, increasing execution time and negatively impacting downstream task\nperformance. Managing this rule explosion remains a central challenge in ARM\nresearch. To address this, we introduce Aerial+, a novel neurosymbolic ARM\nmethod. Aerial+ leverages an under-complete autoencoder to create a neural\nrepresentation of the data, capturing associations between features. It\nextracts rules from this neural representation by exploiting the model's\nreconstruction mechanism. Extensive evaluations on five datasets against seven\nbaselines demonstrate that Aerial+ achieves state-of-the-art results by\nlearning more concise, high-quality rule sets with full data coverage. When\nintegrated into rule-based interpretable machine learning models, Aerial+\nsignificantly reduces execution time while maintaining or improving accuracy."}
{"id": "2504.19024", "pdf": "https://arxiv.org/pdf/2504.19024", "abs": "https://arxiv.org/abs/2504.19024", "authors": ["Jiabin Fan", "Guoqing Luo", "Michael Bowling", "Lili Mou"], "title": "KETCHUP: K-Step Return Estimation for Sequential Knowledge Distillation", "categories": ["cs.CL"], "comment": null, "summary": "We propose a novel k-step return estimation method (called KETCHUP) for\nReinforcement Learning(RL)-based knowledge distillation (KD) in text generation\ntasks. Our idea is to induce a K-step return by using the Bellman Optimality\nEquation for multiple steps. Theoretical analysis shows that this K-step\nformulation reduces the variance of the gradient estimates, thus leading to\nimproved RL optimization especially when the student model size is large.\nEmpirical evaluation on three text generation tasks demonstrates that our\napproach yields superior performance in both standard task metrics and large\nlanguage model (LLM)-based evaluation. These results suggest that our K-step\nreturn induction offers a promising direction for enhancing RL-based KD in LLM\nresearch."}
{"id": "2504.18906", "pdf": "https://arxiv.org/pdf/2504.18906", "abs": "https://arxiv.org/abs/2504.18906", "authors": ["Yufeng Wu", "Xin Liao", "Baowei Wang", "Han Fang", "Xiaoshuai Wu", "Guiling Wang"], "title": "Sim-to-Real: An Unsupervised Noise Layer for Screen-Camera Watermarking Robustness", "categories": ["cs.CV"], "comment": null, "summary": "Unauthorized screen capturing and dissemination pose severe security threats\nsuch as data leakage and information theft. Several studies propose robust\nwatermarking methods to track the copyright of Screen-Camera (SC) images,\nfacilitating post-hoc certification against infringement. These techniques\ntypically employ heuristic mathematical modeling or supervised neural network\nfitting as the noise layer, to enhance watermarking robustness against SC.\nHowever, both strategies cannot fundamentally achieve an effective\napproximation of SC noise. Mathematical simulation suffers from biased\napproximations due to the incomplete decomposition of the noise and the absence\nof interdependence among the noise components. Supervised networks require\npaired data to train the noise-fitting model, and it is difficult for the model\nto learn all the features of the noise. To address the above issues, we propose\nSimulation-to-Real (S2R). Specifically, an unsupervised noise layer employs\nunpaired data to learn the discrepancy between the modeling simulated noise\ndistribution and the real-world SC noise distribution, rather than directly\nlearning the mapping from sharp images to real-world images. Learning this\ntransformation from simulation to reality is inherently simpler, as it\nprimarily involves bridging the gap in noise distributions, instead of the\ncomplex task of reconstructing fine-grained image details. Extensive\nexperimental results validate the efficacy of the proposed method,\ndemonstrating superior watermark robustness and generalization compared to\nthose of state-of-the-art methods."}
{"id": "2504.19499", "pdf": "https://arxiv.org/pdf/2504.19499", "abs": "https://arxiv.org/abs/2504.19499", "authors": ["Omid Semiari", "Hosein Nikopour", "Shilpa Talwar"], "title": "Graph Reinforcement Learning for QoS-Aware Load Balancing in Open Radio Access Networks", "categories": ["cs.AI", "cs.IT", "cs.LG", "cs.NI", "eess.SP", "math.IT"], "comment": "To be published in the proceedings of the 2025 IEEE International\n  Conference on Communications (ICC), Seventh Workshop on Data Driven\n  Intelligence for Networks and Systems (DDINS)", "summary": "Next-generation wireless cellular networks are expected to provide\nunparalleled Quality-of-Service (QoS) for emerging wireless applications,\nnecessitating strict performance guarantees, e.g., in terms of link-level data\nrates. A critical challenge in meeting these QoS requirements is the prevention\nof cell congestion, which involves balancing the load to ensure sufficient\nradio resources are available for each cell to serve its designated User\nEquipments (UEs). In this work, a novel QoS-aware Load Balancing (LB) approach\nis developed to optimize the performance of Guaranteed Bit Rate (GBR) and Best\nEffort (BE) traffic in a multi-band Open Radio Access Network (O-RAN) under QoS\nand resource constraints. The proposed solution builds on Graph Reinforcement\nLearning (GRL), a powerful framework at the intersection of Graph Neural\nNetwork (GNN) and RL. The QoS-aware LB is modeled as a Markov Decision Process,\nwith states represented as graphs. QoS consideration are integrated into both\nstate representations and reward signal design. The LB agent is then trained\nusing an off-policy dueling Deep Q Network (DQN) that leverages a GNN-based\narchitecture. This design ensures the LB policy is invariant to the ordering of\nnodes (UE or cell), flexible in handling various network sizes, and capable of\naccounting for spatial node dependencies in LB decisions. Performance of the\nGRL-based solution is compared with two baseline methods. Results show\nsubstantial performance gains, including a $53\\%$ reduction in QoS violations\nand a fourfold increase in the 5th percentile rate for BE traffic."}
{"id": "2504.19044", "pdf": "https://arxiv.org/pdf/2504.19044", "abs": "https://arxiv.org/abs/2504.19044", "authors": ["Di Wu", "Yibin Lei", "Christof Monz"], "title": "Calibrating Translation Decoding with Quality Estimation on LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Neural machine translation (NMT) systems typically employ maximum a\nposteriori (MAP) decoding to select the highest-scoring translation from the\ndistribution mass. However, recent evidence highlights the inadequacy of MAP\ndecoding, often resulting in low-quality or even pathological hypotheses -- the\ndecoding objective is not aligned with real-world translation quality. This\npaper proposes calibrating hypothesis likelihoods with translation quality from\na distribution view by directly optimizing their Pearson correlation -- thereby\nenhancing the effectiveness of translation decoding. With our method,\ntranslation on large language models (LLMs) improves substantially after\nlimited training (2K instances per direction). This improvement is orthogonal\nto those achieved through supervised fine-tuning, leading to substantial gains\nacross a broad range of metrics and human evaluations -- even when applied to\ntop-performing translation-specialized LLMs fine-tuned on high-quality\ntranslation data, such as Tower, or when compared to recent preference\noptimization methods, like CPO. Moreover, the calibrated translation likelihood\ncan directly serve as a strong proxy for translation quality, closely\napproximating or even surpassing some state-of-the-art translation quality\nestimation models, like CometKiwi. Lastly, our in-depth analysis demonstrates\nthat calibration enhances the effectiveness of MAP decoding, thereby enabling\ngreater efficiency in real-world deployment. The resulting state-of-the-art\ntranslation model, which covers 10 languages, along with the accompanying code\nand human evaluation data, has been released to the community:\nhttps://github.com/moore3930/calibrating-llm-mt."}
{"id": "2504.18910", "pdf": "https://arxiv.org/pdf/2504.18910", "abs": "https://arxiv.org/abs/2504.18910", "authors": ["Ali Nazari", "Mohsen Ebrahimi Moghaddam", "Omidreza Borzoei"], "title": "Kinship Verification through a Forest Neural Network", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Early methods used face representations in kinship verification, which are\nless accurate than joint representations of parents' and children's facial\nimages learned from scratch. We propose an approach featuring graph neural\nnetwork concepts to utilize face representations and have comparable results to\njoint representation algorithms. Moreover, we designed the structure of the\nclassification module and introduced a new combination of losses to engage the\ncenter loss gradually in training our network. Additionally, we conducted\nexperiments on KinFaceW-I and II, demonstrating the effectiveness of our\napproach. We achieved the best result on KinFaceW-II, an average improvement of\nnearly 1.6 for all kinship types, and we were near the best on KinFaceW-I. The\ncode is available at https://github.com/ali-nazari/Kinship-Verification"}
{"id": "2504.19599", "pdf": "https://arxiv.org/pdf/2504.19599", "abs": "https://arxiv.org/abs/2504.19599", "authors": ["Kaichen Zhang", "Yuzhong Hong", "Junwei Bao", "Hongfei Jiang", "Yang Song", "Dingqian Hong", "Hui Xiong"], "title": "GVPO: Group Variance Policy Optimization for Large Language Model Post-Training", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Post-training plays a crucial role in refining and aligning large language\nmodels to meet specific tasks and human preferences. While recent advancements\nin post-training techniques, such as Group Relative Policy Optimization (GRPO),\nleverage increased sampling with relative reward scoring to achieve superior\nperformance, these methods often suffer from training instability that limits\ntheir practical adoption. To address this challenge, we present Group Variance\nPolicy Optimization (GVPO). GVPO incorporates the analytical solution to\nKL-constrained reward maximization directly into its gradient weights, ensuring\nalignment with the optimal policy. The method provides intuitive physical\ninterpretations: its gradient mirrors the mean squared error between the\ncentral distance of implicit rewards and that of actual rewards. GVPO offers\ntwo key advantages: (1) it guarantees a unique optimal solution, exactly the\nKL-constrained reward maximization objective, (2) it supports flexible sampling\ndistributions that avoids on-policy and importance sampling limitations. By\nunifying theoretical guarantees with practical adaptability, GVPO establishes a\nnew paradigm for reliable and versatile LLM post-training."}
{"id": "2504.19061", "pdf": "https://arxiv.org/pdf/2504.19061", "abs": "https://arxiv.org/abs/2504.19061", "authors": ["Anindya Bijoy Das", "Shibbir Ahmed", "Shahnewaz Karim Sakib"], "title": "Hallucinations and Key Information Extraction in Medical Texts: A Comprehensive Assessment of Open-Source Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Clinical summarization is crucial in healthcare as it distills complex\nmedical data into digestible information, enhancing patient understanding and\ncare management. Large language models (LLMs) have shown significant potential\nin automating and improving the accuracy of such summarizations due to their\nadvanced natural language understanding capabilities. These models are\nparticularly applicable in the context of summarizing medical/clinical texts,\nwhere precise and concise information transfer is essential. In this paper, we\ninvestigate the effectiveness of open-source LLMs in extracting key events from\ndischarge reports, such as reasons for hospital admission, significant\nin-hospital events, and critical follow-up actions. In addition, we also assess\nthe prevalence of various types of hallucinations in the summaries produced by\nthese models. Detecting hallucinations is vital as it directly influences the\nreliability of the information, potentially affecting patient care and\ntreatment outcomes. We conduct comprehensive numerical simulations to\nrigorously evaluate the performance of these models, further probing the\naccuracy and fidelity of the extracted content in clinical summarization."}
{"id": "2504.18959", "pdf": "https://arxiv.org/pdf/2504.18959", "abs": "https://arxiv.org/abs/2504.18959", "authors": ["Kamirul Kamirul", "Odysseas Pappas", "Alin Achim"], "title": "R-Sparse R-CNN: SAR Ship Detection Based on Background-Aware Sparse Learnable Proposals", "categories": ["cs.CV"], "comment": "Submitted to IEEE Journal of Selected Topics in Applied Earth\n  Observations and Remote Sensing", "summary": "We introduce R-Sparse R-CNN, a novel pipeline for oriented ship detection in\nSynthetic Aperture Radar (SAR) images that leverages sparse learnable proposals\nenriched with background contextual information, termed background-aware\nproposals (BAPs). The adoption of sparse proposals streamlines the pipeline by\neliminating the need for proposal generators and post-processing for\noverlapping predictions. The proposed BAPs enrich object representation by\nintegrating ship and background features, allowing the model to learn their\ncontextual relationships for more accurate distinction of ships in complex\nenvironments. To complement BAPs, we propose Dual-Context Pooling (DCP), a\nnovel strategy that jointly extracts ship and background features in a single\nunified operation. This unified design improves efficiency by eliminating\nredundant computation inherent in separate pooling. Moreover, by ensuring that\nship and background features are pooled from the same feature map level, DCP\nprovides aligned features that improve contextual relationship learning.\nFinally, as a core component of contextual relationship learning in R-Sparse\nR-CNN, we design a dedicated transformer-based Interaction Module. This module\ninteracts pooled ship and background features with corresponding proposal\nfeatures and models their relationships. Experimental results show that\nR-Sparse R-CNN delivers outstanding accuracy, surpassing state-of-the-art\nmodels by margins of up to 12.8% and 11.9% on SSDD and RSDD-SAR inshore\ndatasets, respectively. These results demonstrate the effectiveness and\ncompetitiveness of R-Sparse R-CNN as a robust framework for oriented ship\ndetection in SAR imagery. The code is available at:\nwww.github.com/ka-mirul/R-Sparse-R-CNN."}
{"id": "2504.19622", "pdf": "https://arxiv.org/pdf/2504.19622", "abs": "https://arxiv.org/abs/2504.19622", "authors": ["Minsu Kim", "Sangryul Kim", "James Thorne"], "title": "From Evidence to Belief: A Bayesian Epistemology Approach to Language Models", "categories": ["cs.AI"], "comment": null, "summary": "This paper investigates the knowledge of language models from the perspective\nof Bayesian epistemology. We explore how language models adjust their\nconfidence and responses when presented with evidence with varying levels of\ninformativeness and reliability. To study these properties, we create a dataset\nwith various types of evidence and analyze language models' responses and\nconfidence using verbalized confidence, token probability, and sampling. We\nobserved that language models do not consistently follow Bayesian epistemology:\nlanguage models follow the Bayesian confirmation assumption well with true\nevidence but fail to adhere to other Bayesian assumptions when encountering\ndifferent evidence types. Also, we demonstrated that language models can\nexhibit high confidence when given strong evidence, but this does not always\nguarantee high accuracy. Our analysis also reveals that language models are\nbiased toward golden evidence and show varying performance depending on the\ndegree of irrelevance, helping explain why they deviate from Bayesian\nassumptions."}
{"id": "2504.19066", "pdf": "https://arxiv.org/pdf/2504.19066", "abs": "https://arxiv.org/abs/2504.19066", "authors": ["Deeksha Varshney", "Keane Ong", "Rui Mao", "Erik Cambria", "Gianmarco Mengaldo"], "title": "ClimaEmpact: Domain-Aligned Small Language Models and Datasets for Extreme Weather Analytics", "categories": ["cs.CL", "cs.AI", "cs.LG", "physics.ao-ph"], "comment": null, "summary": "Accurate assessments of extreme weather events are vital for research and\npolicy, yet localized and granular data remain scarce in many parts of the\nworld. This data gap limits our ability to analyze potential outcomes and\nimplications of extreme weather events, hindering effective decision-making.\nLarge Language Models (LLMs) can process vast amounts of unstructured text\ndata, extract meaningful insights, and generate detailed assessments by\nsynthesizing information from multiple sources. Furthermore, LLMs can\nseamlessly transfer their general language understanding to smaller models,\nenabling these models to retain key knowledge while being fine-tuned for\nspecific tasks. In this paper, we propose Extreme Weather Reasoning-Aware\nAlignment (EWRA), a method that enhances small language models (SLMs) by\nincorporating structured reasoning paths derived from LLMs, and\nExtremeWeatherNews, a large dataset of extreme weather event-related news\narticles. EWRA and ExtremeWeatherNews together form the overall framework,\nClimaEmpact, that focuses on addressing three critical extreme-weather tasks:\ncategorization of tangible vulnerabilities/impacts, topic labeling, and emotion\nanalysis. By aligning SLMs with advanced reasoning strategies on\nExtremeWeatherNews (and its derived dataset ExtremeAlign used specifically for\nSLM alignment), EWRA improves the SLMs' ability to generate well-grounded and\ndomain-specific responses for extreme weather analytics. Our results show that\nthe approach proposed guides SLMs to output domain-aligned responses,\nsurpassing the performance of task-specific models and offering enhanced\nreal-world applicability for extreme weather analytics."}
{"id": "2504.18977", "pdf": "https://arxiv.org/pdf/2504.18977", "abs": "https://arxiv.org/abs/2504.18977", "authors": ["Ihsan Ullah", "Alfredo Petrosino"], "title": "3DPyranet Features Fusion for Spatio-temporal Feature Learning", "categories": ["cs.CV"], "comment": null, "summary": "Convolutional neural network (CNN) slides a kernel over the whole image to\nproduce an output map. This kernel scheme reduces the number of parameters with\nrespect to a fully connected neural network (NN). While CNN has proven to be an\neffective model in recognition of handwritten characters and traffic signal\nsign boards, etc. recently, its deep variants have proven to be effective in\nsimilar as well as more challenging applications like object, scene and action\nrecognition. Deep CNN add more layers and kernels to the classical CNN,\nincreasing the number of parameters, and partly reducing the main advantage of\nCNN which is less parameters. In this paper, a 3D pyramidal neural network\ncalled 3DPyraNet and a discriminative approach for spatio-temporal feature\nlearning based on it, called 3DPyraNet-F, are proposed. 3DPyraNet introduces a\nnew weighting scheme which learns features from both spatial and temporal\ndimensions analyzing multiple adjacent frames and keeping a biological\nplausible structure. It keeps the spatial topology of the input image and\npresents fewer parameters and lower computational and memory costs compared to\nboth fully connected NNs and recent deep CNNs. 3DPyraNet-F extract the features\nmaps of the highest layer of the learned network, fuse them in a single vector,\nand provide it as input in such a way to a linear-SVM classifier that enhances\nthe recognition of human actions and dynamic scenes from the videos.\nEncouraging results are reported with 3DPyraNet in real-world environments,\nespecially in the presence of camera induced motion. Further, 3DPyraNet-F\nclearly outperforms the state-of-the-art on three benchmark datasets and shows\ncomparable result for the fourth."}
{"id": "2504.19636", "pdf": "https://arxiv.org/pdf/2504.19636", "abs": "https://arxiv.org/abs/2504.19636", "authors": ["Fei Liu", "Qingfu Zhang", "Xialiang Tong", "Mingxuan Yuan", "Kun Mao"], "title": "Fitness Landscape of Large Language Model-Assisted Automated Algorithm Search", "categories": ["cs.AI", "cs.NE"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated significant potential in\nalgorithm design. However, when integrated into search frameworks for iterative\nalgorithm search, the underlying fitness landscape--critical for understanding\nsearch behaviou--remains underexplored. In this paper, we illustrate and\nanalyze the fitness landscape of LLM-assisted Algorithm Search (LAS) using a\ngraph-based approach, where nodes represent algorithms and edges denote\ntransitions between them. We conduct extensive evaluations across six algorithm\ndesign tasks and six commonly used LLMs. Our findings reveal that LAS\nlandscapes are highly multimodal and rugged, particularly in combinatorial\noptimization tasks, with distinct structural variations across tasks and LLMs.\nFor instance, heuristic design tasks exhibit dense clusters of high-performing\nalgorithms, while symbolic regression tasks show sparse, scattered\ndistributions. Additionally, we demonstrate how population size influences\nexploration-exploitation trade-offs and the evolving trajectory of elite\nalgorithms. These insights not only advance our understanding of LAS landscapes\nbut also provide practical guidance for designing more effective LAS methods."}
{"id": "2504.19070", "pdf": "https://arxiv.org/pdf/2504.19070", "abs": "https://arxiv.org/abs/2504.19070", "authors": ["Sakshi Singh", "Abhinav Prakash", "Aakriti Shah", "Chaitanya Sachdeva", "Sanjana Dumpala"], "title": "Sample-Efficient Language Model for Hinglish Conversational AI", "categories": ["cs.CL", "I.2.7; I.2.6; H.5.2"], "comment": "5 pages, 2 tables, 2 figures", "summary": "This paper presents our process for developing a sample-efficient language\nmodel for a conversational Hinglish chatbot. Hinglish, a code-mixed language\nthat combines Hindi and English, presents a unique computational challenge due\nto inconsistent spelling, lack of standardization, and limited quality of\nconversational data. This work evaluates multiple pre-trained cross-lingual\nlanguage models, including Gemma3-4B and Qwen2.5-7B, and employs fine-tuning\ntechniques to improve performance on Hinglish conversational tasks. The\nproposed approach integrates synthetically generated dialogues with insights\nfrom existing Hinglish datasets to address data scarcity. Experimental results\ndemonstrate that models with fewer parameters, when appropriately fine-tuned on\nhigh-quality code-mixed data, can achieve competitive performance for Hinglish\nconversation generation while maintaining computational efficiency."}
{"id": "2504.18983", "pdf": "https://arxiv.org/pdf/2504.18983", "abs": "https://arxiv.org/abs/2504.18983", "authors": ["Xuyin Qi", "Zeyu Zhang", "Canxuan Gang", "Hao Zhang", "Lei Zhang", "Zhiwei Zhang", "Yang Zhao"], "title": "MediAug: Exploring Visual Augmentation in Medical Imaging", "categories": ["cs.CV"], "comment": null, "summary": "Data augmentation is essential in medical imaging for improving\nclassification accuracy, lesion detection, and organ segmentation under limited\ndata conditions. However, two significant challenges remain. First, a\npronounced domain gap between natural photographs and medical images can\ndistort critical disease features. Second, augmentation studies in medical\nimaging are fragmented and limited to single tasks or architectures, leaving\nthe benefits of advanced mix-based strategies unclear. To address these\nchallenges, we propose a unified evaluation framework with six mix-based\naugmentation methods integrated with both convolutional and transformer\nbackbones on brain tumour MRI and eye disease fundus datasets. Our\ncontributions are threefold. (1) We introduce MediAug, a comprehensive and\nreproducible benchmark for advanced data augmentation in medical imaging. (2)\nWe systematically evaluate MixUp, YOCO, CropMix, CutMix, AugMix, and SnapMix\nwith ResNet-50 and ViT-B backbones. (3) We demonstrate through extensive\nexperiments that MixUp yields the greatest improvement on the brain tumor\nclassification task for ResNet-50 with 79.19% accuracy and SnapMix yields the\ngreatest improvement for ViT-B with 99.44% accuracy, and that YOCO yields the\ngreatest improvement on the eye disease classification task for ResNet-50 with\n91.60% accuracy and CutMix yields the greatest improvement for ViT-B with\n97.94% accuracy. Code will be available at\nhttps://github.com/AIGeeksGroup/MediAug."}
{"id": "2504.19678", "pdf": "https://arxiv.org/pdf/2504.19678", "abs": "https://arxiv.org/abs/2504.19678", "authors": ["Mohamed Amine Ferrag", "Norbert Tihanyi", "Merouane Debbah"], "title": "From LLM Reasoning to Autonomous AI Agents: A Comprehensive Review", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Large language models and autonomous AI agents have evolved rapidly,\nresulting in a diverse array of evaluation benchmarks, frameworks, and\ncollaboration protocols. However, the landscape remains fragmented and lacks a\nunified taxonomy or comprehensive survey. Therefore, we present a side-by-side\ncomparison of benchmarks developed between 2019 and 2025 that evaluate these\nmodels and agents across multiple domains. In addition, we propose a taxonomy\nof approximately 60 benchmarks that cover general and academic knowledge\nreasoning, mathematical problem-solving, code generation and software\nengineering, factual grounding and retrieval, domain-specific evaluations,\nmultimodal and embodied tasks, task orchestration, and interactive assessments.\nFurthermore, we review AI-agent frameworks introduced between 2023 and 2025\nthat integrate large language models with modular toolkits to enable autonomous\ndecision-making and multi-step reasoning. Moreover, we present real-world\napplications of autonomous AI agents in materials science, biomedical research,\nacademic ideation, software engineering, synthetic data generation, chemical\nreasoning, mathematical problem-solving, geographic information systems,\nmultimedia, healthcare, and finance. We then survey key agent-to-agent\ncollaboration protocols, namely the Agent Communication Protocol (ACP), the\nModel Context Protocol (MCP), and the Agent-to-Agent Protocol (A2A). Finally,\nwe discuss recommendations for future research, focusing on advanced reasoning\nstrategies, failure modes in multi-agent LLM systems, automated scientific\ndiscovery, dynamic tool integration via reinforcement learning, integrated\nsearch capabilities, and security vulnerabilities in agent protocols."}
{"id": "2504.19095", "pdf": "https://arxiv.org/pdf/2504.19095", "abs": "https://arxiv.org/abs/2504.19095", "authors": ["Jikai Wang", "Juntao Li", "Lijun Wu", "Min Zhang"], "title": "Efficient Reasoning for LLMs through Speculative Chain-of-Thought", "categories": ["cs.CL"], "comment": null, "summary": "Large reasoning language models such as OpenAI-o1 and Deepseek-R1 have\nrecently attracted widespread attention due to their impressive task-solving\nabilities. However, the enormous model size and the generation of lengthy\nthought chains introduce significant reasoning costs and response latency.\nExisting methods for efficient reasoning mainly focus on reducing the number of\nmodel parameters or shortening the chain-of-thought length. In this paper, we\nintroduce Speculative Chain-of-Thought (SCoT), which reduces reasoning latency\nfrom another perspective by accelerated average reasoning speed through large\nand small model collaboration. SCoT conducts thought-level drafting using a\nlightweight draft model. Then it selects the best CoT draft and corrects the\nerror cases with the target model. The proposed thinking behavior alignment\nimproves the efficiency of drafting and the draft selection strategy maintains\nthe prediction accuracy for complex problems. Experimental results on GSM8K,\nMATH, GaoKao, CollegeMath and Olympiad datasets show that SCoT reduces\nreasoning latency by 48\\%$\\sim$66\\% for Deepseek-R1-Distill-Qwen-32B while\nachieving near-target-model-level performance. Our code is available at\nhttps://github.com/Jikai0Wang/Speculative_CoT."}
{"id": "2504.19032", "pdf": "https://arxiv.org/pdf/2504.19032", "abs": "https://arxiv.org/abs/2504.19032", "authors": ["Niaz Ahmad", "Youngmoon Lee", "Guanghui Wang"], "title": "VISUALCENT: Visual Human Analysis using Dynamic Centroid Representation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We introduce VISUALCENT, a unified human pose and instance segmentation\nframework to address generalizability and scalability limitations to multi\nperson visual human analysis. VISUALCENT leverages centroid based bottom up\nkeypoint detection paradigm and uses Keypoint Heatmap incorporating Disk\nRepresentation and KeyCentroid to identify the optimal keypoint coordinates.\nFor the unified segmentation task, an explicit keypoint is defined as a dynamic\ncentroid called MaskCentroid to swiftly cluster pixels to specific human\ninstance during rapid changes in human body movement or significantly occluded\nenvironment. Experimental results on COCO and OCHuman datasets demonstrate\nVISUALCENTs accuracy and real time performance advantages, outperforming\nexisting methods in mAP scores and execution frame rate per second. The\nimplementation is available on the project page."}
{"id": "2504.19738", "pdf": "https://arxiv.org/pdf/2504.19738", "abs": "https://arxiv.org/abs/2504.19738", "authors": ["Yingbin Bai", "Sylvie Thiebaux", "Felipe Trevizan"], "title": "Learning Efficiency Meets Symmetry Breaking", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Learning-based planners leveraging Graph Neural Networks can learn search\nguidance applicable to large search spaces, yet their potential to address\nsymmetries remains largely unexplored. In this paper, we introduce a graph\nrepresentation of planning problems allying learning efficiency with the\nability to detect symmetries, along with two pruning methods, action pruning\nand state pruning, designed to manage symmetries during search. The integration\nof these techniques into Fast Downward achieves a first-time success over LAMA\non the latest IPC learning track dataset. Code is released at:\nhttps://github.com/bybeye/Distincter."}
{"id": "2504.19101", "pdf": "https://arxiv.org/pdf/2504.19101", "abs": "https://arxiv.org/abs/2504.19101", "authors": ["Qianren Mao", "Qili Zhang", "Hanwen Hao", "Zhentao Han", "Runhua Xu", "Weifeng Jiang", "Qi Hu", "Zhijun Chen", "Tyler Zhou", "Bo Li", "Yangqiu Song", "Jin Dong", "Jianxin Li", "Philip S. Yu"], "title": "Privacy-Preserving Federated Embedding Learning for Localized Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has recently emerged as a promising\nsolution for enhancing the accuracy and credibility of Large Language Models\n(LLMs), particularly in Question & Answer tasks. This is achieved by\nincorporating proprietary and private data from integrated databases. However,\nprivate RAG systems face significant challenges due to the scarcity of private\ndomain data and critical data privacy issues. These obstacles impede the\ndeployment of private RAG systems, as developing privacy-preserving RAG systems\nrequires a delicate balance between data security and data availability. To\naddress these challenges, we regard federated learning (FL) as a highly\npromising technology for privacy-preserving RAG services. We propose a novel\nframework called Federated Retrieval-Augmented Generation (FedE4RAG). This\nframework facilitates collaborative training of client-side RAG retrieval\nmodels. The parameters of these models are aggregated and distributed on a\ncentral-server, ensuring data privacy without direct sharing of raw data. In\nFedE4RAG, knowledge distillation is employed for communication between the\nserver and client models. This technique improves the generalization of local\nRAG retrievers during the federated learning process. Additionally, we apply\nhomomorphic encryption within federated learning to safeguard model parameters\nand mitigate concerns related to data leakage. Extensive experiments conducted\non the real-world dataset have validated the effectiveness of FedE4RAG. The\nresults demonstrate that our proposed framework can markedly enhance the\nperformance of private RAG systems while maintaining robust data privacy\nprotection."}
{"id": "2504.19056", "pdf": "https://arxiv.org/pdf/2504.19056", "abs": "https://arxiv.org/abs/2504.19056", "authors": ["Mohammad Mahdi Abootorabi", "Omid Ghahroodi", "Pardis Sadat Zahraei", "Hossein Behzadasl", "Alireza Mirrokni", "Mobina Salimipanah", "Arash Rasouli", "Bahar Behzadipour", "Sara Azarnoush", "Benyamin Maleki", "Erfan Sadraiye", "Kiarash Kiani Feriz", "Mahdi Teymouri Nahad", "Ali Moghadasi", "Abolfazl Eshagh Abianeh", "Nizi Nazar", "Hamid R. Rabiee", "Mahdieh Soleymani Baghshah", "Meisam Ahmadi", "Ehsaneddin Asgari"], "title": "Generative AI for Character Animation: A Comprehensive Survey of Techniques, Applications, and Future Directions", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "50 main pages, 30 pages appendix, 21 figures, 8 tables, GitHub\n  Repository:\n  https://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey", "summary": "Generative AI is reshaping art, gaming, and most notably animation. Recent\nbreakthroughs in foundation and diffusion models have reduced the time and cost\nof producing animated content. Characters are central animation components,\ninvolving motion, emotions, gestures, and facial expressions. The pace and\nbreadth of advances in recent months make it difficult to maintain a coherent\nview of the field, motivating the need for an integrative review. Unlike\nearlier overviews that treat avatars, gestures, or facial animation in\nisolation, this survey offers a single, comprehensive perspective on all the\nmain generative AI applications for character animation. We begin by examining\nthe state-of-the-art in facial animation, expression rendering, image\nsynthesis, avatar creation, gesture modeling, motion synthesis, object\ngeneration, and texture synthesis. We highlight leading research, practical\ndeployments, commonly used datasets, and emerging trends for each area. To\nsupport newcomers, we also provide a comprehensive background section that\nintroduces foundational models and evaluation metrics, equipping readers with\nthe knowledge needed to enter the field. We discuss open challenges and map\nfuture research directions, providing a roadmap to advance AI-driven\ncharacter-animation technologies. This survey is intended as a resource for\nresearchers and developers entering the field of generative AI animation or\nadjacent fields. Resources are available at:\nhttps://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey."}
{"id": "2504.19912", "pdf": "https://arxiv.org/pdf/2504.19912", "abs": "https://arxiv.org/abs/2504.19912", "authors": ["Khachik Smbatyan", "Tsolak Ghukasyan", "Tigran Aghajanyan", "Hovhannes Dabaghyan", "Sergey Adamyan", "Aram Bughdaryan", "Vahagn Altunyan", "Gagik Navasardyan", "Aram Davtyan", "Anush Hakobyan", "Aram Gharibyan", "Arman Fahradyan", "Artur Hakobyan", "Hasmik Mnatsakanyan", "Narek Ginoyan", "Garik Petrosyan"], "title": "Can AI Agents Design and Implement Drug Discovery Pipelines?", "categories": ["cs.AI", "cs.MA"], "comment": null, "summary": "The rapid advancement of artificial intelligence, particularly autonomous\nagentic systems based on Large Language Models (LLMs), presents new\nopportunities to accelerate drug discovery by improving in-silico modeling and\nreducing dependence on costly experimental trials. Current AI agent-based\nsystems demonstrate proficiency in solving programming challenges and\nconducting research, indicating an emerging potential to develop software\ncapable of addressing complex problems such as pharmaceutical design and drug\ndiscovery. This paper introduces DO Challenge, a benchmark designed to evaluate\nthe decision-making abilities of AI agents in a single, complex problem\nresembling virtual screening scenarios. The benchmark challenges systems to\nindependently develop, implement, and execute efficient strategies for\nidentifying promising molecular structures from extensive datasets, while\nnavigating chemical space, selecting models, and managing limited resources in\na multi-objective context. We also discuss insights from the DO Challenge 2025,\na competition based on the proposed benchmark, which showcased diverse\nstrategies explored by human participants. Furthermore, we present the Deep\nThought multi-agent system, which demonstrated strong performance on the\nbenchmark, outperforming most human teams. Among the language models tested,\nClaude 3.7 Sonnet, Gemini 2.5 Pro and o3 performed best in primary agent roles,\nand GPT-4o, Gemini 2.0 Flash were effective in auxiliary roles. While\npromising, the system's performance still fell short of expert-designed\nsolutions and showed high instability, highlighting both the potential and\ncurrent limitations of AI-driven methodologies in transforming drug discovery\nand broader scientific research."}
{"id": "2504.19110", "pdf": "https://arxiv.org/pdf/2504.19110", "abs": "https://arxiv.org/abs/2504.19110", "authors": ["Huajian Xin", "Luming Li", "Xiaoran Jin", "Jacques Fleuriot", "Wenda Li"], "title": "APE-Bench I: Towards File-level Automated Proof Engineering of Formal Math Libraries", "categories": ["cs.CL"], "comment": null, "summary": "Recent progress in large language models (LLMs) has shown promise in formal\ntheorem proving, yet existing benchmarks remain limited to isolated, static\nproof tasks, failing to capture the iterative, engineering-intensive workflows\nof real-world formal mathematics libraries. Motivated by analogous advances in\nsoftware engineering, we introduce the paradigm of Automated Proof Engineering\n(APE), which aims to automate proof engineering tasks such as feature addition,\nproof refactoring, and bug fixing using LLMs. To facilitate research in this\ndirection, we present APE-Bench I, the first realistic benchmark built from\nreal-world commit histories of Mathlib4, featuring diverse file-level tasks\ndescribed in natural language and verified via a hybrid approach combining the\nLean compiler and LLM-as-a-Judge. We further develop Eleanstic, a scalable\nparallel verification infrastructure optimized for proof checking across\nmultiple versions of Mathlib. Empirical results on state-of-the-art LLMs\ndemonstrate strong performance on localized edits but substantial degradation\non handling complex proof engineering. This work lays the foundation for\ndeveloping agentic workflows in proof engineering, with future benchmarks\ntargeting multi-file coordination, project-scale verification, and autonomous\nagents capable of planning, editing, and repairing formal libraries."}
{"id": "2504.19074", "pdf": "https://arxiv.org/pdf/2504.19074", "abs": "https://arxiv.org/abs/2504.19074", "authors": ["Anyong Qin", "Chaoqi Yuan", "Qiang Li", "Feng Yang", "Tiecheng Song", "Chenqiang Gao"], "title": "Dual-Branch Residual Network for Cross-Domain Few-Shot Hyperspectral Image Classification with Refined Prototype", "categories": ["cs.CV", "cs.LG"], "comment": "5 pages, 2 figures. IEEE Geoscience and Remote Sensing Letters (2025)", "summary": "Convolutional neural networks (CNNs) are effective for hyperspectral image\n(HSI) classification, but their 3D convolutional structures introduce high\ncomputational costs and limited generalization in few-shot scenarios. Domain\nshifts caused by sensor differences and environmental variations further hinder\ncross-dataset adaptability. Metric-based few-shot learning (FSL) prototype\nnetworks mitigate this problem, yet their performance is sensitive to prototype\nquality, especially with limited samples. To overcome these challenges, a\ndual-branch residual network that integrates spatial and spectral features via\nparallel branches is proposed in this letter. Additionally, more robust refined\nprototypes are obtained through a regulation term. Furthermore, a kernel\nprobability matching strategy aligns source and target domain features,\nalleviating domain shift. Experiments on four publicly available HSI datasets\nillustrate that the proposal achieves superior performance compared to other\nmethods."}
{"id": "2504.19933", "pdf": "https://arxiv.org/pdf/2504.19933", "abs": "https://arxiv.org/abs/2504.19933", "authors": ["Riccardo Lo Bianco", "Willem van Jaarsveld", "Jeroen Middelhuis", "Luca Begnardi", "Remco Dijkman"], "title": "Automated decision-making for dynamic task assignment at scale", "categories": ["cs.AI", "cs.LG", "math.OC"], "comment": null, "summary": "The Dynamic Task Assignment Problem (DTAP) concerns matching resources to\ntasks in real time while minimizing some objectives, like resource costs or\ntask cycle time. In this work, we consider a DTAP variant where every task is a\ncase composed of a stochastic sequence of activities. The DTAP, in this case,\ninvolves the decision of which employee to assign to which activity to process\nrequests as quickly as possible. In recent years, Deep Reinforcement Learning\n(DRL) has emerged as a promising tool for tackling this DTAP variant, but most\nresearch is limited to solving small-scale, synthetic problems, neglecting the\nchallenges posed by real-world use cases. To bridge this gap, this work\nproposes a DRL-based Decision Support System (DSS) for real-world scale DTAPS.\nTo this end, we introduce a DRL agent with two novel elements: a graph\nstructure for observations and actions that can effectively represent any DTAP\nand a reward function that is provably equivalent to the objective of\nminimizing the average cycle time of tasks. The combination of these two\nnovelties allows the agent to learn effective and generalizable assignment\npolicies for real-world scale DTAPs. The proposed DSS is evaluated on five DTAP\ninstances whose parameters are extracted from real-world logs through process\nmining. The experimental evaluation shows how the proposed DRL agent matches or\noutperforms the best baseline in all DTAP instances and generalizes on\ndifferent time horizons and across instances."}
{"id": "2504.19162", "pdf": "https://arxiv.org/pdf/2504.19162", "abs": "https://arxiv.org/abs/2504.19162", "authors": ["Jiaqi Chen", "Bang Zhang", "Ruotian Ma", "Peisong Wang", "Xiaodan Liang", "Zhaopeng Tu", "Xiaolong Li", "Kwan-Yee K. Wong"], "title": "SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Project: https://chen-judge.github.io/SPC/", "summary": "Evaluating the step-by-step reliability of large language model (LLM)\nreasoning, such as Chain-of-Thought, remains challenging due to the difficulty\nand cost of obtaining high-quality step-level supervision. In this paper, we\nintroduce Self-Play Critic (SPC), a novel approach where a critic model evolves\nits ability to assess reasoning steps through adversarial self-play games,\neliminating the need for manual step-level annotation. SPC involves fine-tuning\ntwo copies of a base model to play two roles, namely a \"sneaky generator\" that\ndeliberately produces erroneous steps designed to be difficult to detect, and a\n\"critic\" that analyzes the correctness of reasoning steps. These two models\nengage in an adversarial game in which the generator aims to fool the critic,\nwhile the critic model seeks to identify the generator's errors. Using\nreinforcement learning based on the game outcomes, the models iteratively\nimprove; the winner of each confrontation receives a positive reward and the\nloser receives a negative reward, driving continuous self-evolution.\nExperiments on three reasoning process benchmarks (ProcessBench, PRM800K,\nDeltaBench) demonstrate that our SPC progressively enhances its error detection\ncapabilities (e.g., accuracy increases from 70.8% to 77.7% on ProcessBench) and\nsurpasses strong baselines, including distilled R1 model. Furthermore, applying\nSPC to guide the test-time search of diverse LLMs significantly improves their\nmathematical reasoning performance on MATH500 and AIME2024, outperforming\nstate-of-the-art process reward models."}
{"id": "2504.19075", "pdf": "https://arxiv.org/pdf/2504.19075", "abs": "https://arxiv.org/abs/2504.19075", "authors": ["Qiuhui Chen", "Jintao Wang", "Gang Wang", "Yi Hong"], "title": "HoloDx: Knowledge- and Data-Driven Multimodal Diagnosis of Alzheimer's Disease", "categories": ["cs.CV"], "comment": null, "summary": "Accurate diagnosis of Alzheimer's disease (AD) requires effectively\nintegrating multimodal data and clinical expertise. However, existing methods\noften struggle to fully utilize multimodal information and lack structured\nmechanisms to incorporate dynamic domain knowledge. To address these\nlimitations, we propose HoloDx, a knowledge- and data-driven framework that\nenhances AD diagnosis by aligning domain knowledge with multimodal clinical\ndata. HoloDx incorporates a knowledge injection module with a knowledge-aware\ngated cross-attention, allowing the model to dynamically integrate\ndomain-specific insights from both large language models (LLMs) and clinical\nexpertise. Also, a memory injection module with a designed prototypical memory\nattention enables the model to retain and retrieve subject-specific\ninformation, ensuring consistency in decision-making. By jointly leveraging\nthese mechanisms, HoloDx enhances interpretability, improves robustness, and\neffectively aligns prior knowledge with current subject data. Evaluations on\nfive AD datasets demonstrate that HoloDx outperforms state-of-the-art methods,\nachieving superior diagnostic accuracy and strong generalization across diverse\ncohorts. The source code will be released upon publication acceptance."}
{"id": "2504.19968", "pdf": "https://arxiv.org/pdf/2504.19968", "abs": "https://arxiv.org/abs/2504.19968", "authors": ["John Beverley", "Regina Hurley"], "title": "How Group Lives Go Well", "categories": ["cs.AI", "cs.GT"], "comment": null, "summary": "This paper explores the ontological space of group well being, proposing a\nframework for representing collective welfare, group functions, and long term\ncontributions within an ontology engineering context. Traditional well being\ntheories focus on individual states, often relying on hedonistic, desire\nsatisfaction, or objective list models. Such approaches struggle to account for\ncases where individual sacrifices contribute to broader social progress, a\ncritical challenge in modeling group flourishing. To address this, the paper\nrefines and extends the Counterfactual Account (CT) of well being, which\nevaluates goodness of an event by comparing an individual's actual well being\nwith a hypothetical counterpart in a nearby possible world. While useful, this\nframework is insufficient for group level ontologies, where well being depends\non functional persistence, institutional roles, and historical impact rather\nthan immediate individual outcomes. Drawing on Basic Formal Ontology (BFO), the\npaper introduces a model in which group flourishing is evaluated in terms of\ngroup functional, where members bear roles and exhibit persistence conditions\nakin to biological systems or designed artifacts. This approach enables\nsemantic interoperability for modeling longitudinal social contributions,\nallowing for structured reasoning about group welfare, social institutions, and\ngroup flourishing over time."}
{"id": "2504.19191", "pdf": "https://arxiv.org/pdf/2504.19191", "abs": "https://arxiv.org/abs/2504.19191", "authors": ["Liu Xiao", "Li Zhiyuan", "Lin Yueyu"], "title": "WuNeng: Hybrid State with Attention", "categories": ["cs.CL"], "comment": null, "summary": "The WuNeng architecture introduces a novel approach to enhancing the\nexpressivity and power of large language models by integrating recurrent neural\nnetwork (RNN)-based RWKV-7 with advanced attention mechanisms, prioritizing\nheightened contextual coherence over reducing KV cache size. Building upon the\nhybrid-head concept from Hymba, WuNeng augments standard multi-head attention\nwith additional RWKV-7 state-driven heads, rather than replacing existing\nheads, to enrich the model's representational capacity. A cross-head\ninteraction technique fosters dynamic synergy among standard, state-driven, and\nnewly introduced middle heads, leveraging concatenation, additive modulation,\nand gated fusion for robust information integration. Furthermore, a multi-token\nstate processing mechanism harnesses the continuous RWKV-7 state to capture\nintricate, sequence-wide dependencies, significantly boosting expressivity.\nRemarkably, these enhancements are achieved with minimal additional parameters,\nensuring efficiency while empowering the model to excel in complex reasoning\nand sequence generation tasks. WuNeng sets a new standard for balancing\nexpressivity and computational efficiency in modern neural architectures."}
{"id": "2504.19077", "pdf": "https://arxiv.org/pdf/2504.19077", "abs": "https://arxiv.org/abs/2504.19077", "authors": ["Mitchell Goff", "Greg Hogan", "George Hotz", "Armand du Parc Locmaria", "Kacper Raczy", "Harald Schäfer", "Adeeb Shihadeh", "Weixing Zhang", "Yassine Yousfi"], "title": "Learning to Drive from a World Model", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Most self-driving systems rely on hand-coded perception outputs and\nengineered driving rules. Learning directly from human driving data with an\nend-to-end method can allow for a training architecture that is simpler and\nscales well with compute and data.\n  In this work, we propose an end-to-end training architecture that uses real\ndriving data to train a driving policy in an on-policy simulator. We show two\ndifferent methods of simulation, one with reprojective simulation and one with\na learned world model. We show that both methods can be used to train a policy\nthat learns driving behavior without any hand-coded driving rules. We evaluate\nthe performance of these policies in a closed-loop simulation and when deployed\nin a real-world advanced driver-assistance system."}
{"id": "2504.20007", "pdf": "https://arxiv.org/pdf/2504.20007", "abs": "https://arxiv.org/abs/2504.20007", "authors": ["Anita Srbinovska", "Angela Srbinovska", "Vivek Senthil", "Adrian Martin", "John McCluskey", "Ernest Fokoué"], "title": "Towards AI-Driven Policing: Interdisciplinary Knowledge Discovery from Police Body-Worn Camera Footage", "categories": ["cs.AI", "cs.CV"], "comment": "6 pages, 2 figures, and 1 table", "summary": "This paper proposes a novel interdisciplinary framework for analyzing police\nbody-worn camera (BWC) footage from the Rochester Police Department (RPD) using\nadvanced artificial intelligence (AI) and statistical machine learning (ML)\ntechniques. Our goal is to detect, classify, and analyze patterns of\ninteraction between police officers and civilians to identify key behavioral\ndynamics, such as respect, disrespect, escalation, and de-escalation. We apply\nmultimodal data analysis by integrating video, audio, and natural language\nprocessing (NLP) techniques to extract meaningful insights from BWC footage. We\npresent our methodology, computational techniques, and findings, outlining a\npractical approach for law enforcement while advancing the frontiers of\nknowledge discovery from police BWC data."}
{"id": "2504.19209", "pdf": "https://arxiv.org/pdf/2504.19209", "abs": "https://arxiv.org/abs/2504.19209", "authors": ["Elisabeth Fittschen", "Bella Xia", "Leib Celnik", "Paul Dilley", "Tom Lippincott"], "title": "Dynamic Embedded Topic Models: properties and recommendations based on diverse corpora", "categories": ["cs.CL", "cs.LG"], "comment": "Under review", "summary": "We measure the effects of several implementation choices for the Dynamic\nEmbedded Topic Model, as applied to five distinct diachronic corpora, with the\ngoal of isolating important decisions for its use and further development. We\nidentify priorities that will maximize utility in applied scholarship,\nincluding the practical scalability of vocabulary size to best exploit the\nstrengths of embedded representations, and more flexible modeling of intervals\nto accommodate the uneven temporal distributions of historical writing. Of\nsimilar importance, we find performance is not significantly or consistently\naffected by several aspects that otherwise limit the model's application or\nmight consume the resources of a grid search."}
{"id": "2504.19080", "pdf": "https://arxiv.org/pdf/2504.19080", "abs": "https://arxiv.org/abs/2504.19080", "authors": ["Zhenkai Qin", "Jiaquan Liang", "Qiao Fang"], "title": "MIA-Mind: A Multidimensional Interactive Attention Mechanism Based on MindSpore", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Attention mechanisms have significantly advanced deep learning by enhancing\nfeature representation through selective focus. However, existing approaches\noften independently model channel importance and spatial saliency, overlooking\ntheir inherent interdependence and limiting their effectiveness. To address\nthis limitation, we propose MIA-Mind, a lightweight and modular\nMultidimensional Interactive Attention Mechanism, built upon the MindSpore\nframework. MIA-Mind jointly models spatial and channel features through a\nunified cross-attentive fusion strategy, enabling fine-grained feature\nrecalibration with minimal computational overhead. Extensive experiments are\nconducted on three representative datasets: on CIFAR-10, MIA-Mind achieves an\naccuracy of 82.9\\%; on ISBI2012, it achieves an accuracy of 78.7\\%; and on\nCIC-IDS2017, it achieves an accuracy of 91.9\\%. These results validate the\nversatility, lightweight design, and generalization ability of MIA-Mind across\nheterogeneous tasks. Future work will explore the extension of MIA-Mind to\nlarge-scale datasets, the development of ada,ptive attention fusion strategies,\nand distributed deployment to further enhance scalability and robustness."}
{"id": "2504.20010", "pdf": "https://arxiv.org/pdf/2504.20010", "abs": "https://arxiv.org/abs/2504.20010", "authors": ["Jacob Emmerson", "Rayid Ghani", "Zheyuan Ryan Shi"], "title": "Towards Automated Scoping of AI for Social Good Projects", "categories": ["cs.AI", "cs.CY"], "comment": null, "summary": "Artificial Intelligence for Social Good (AI4SG) is an emerging effort that\naims to address complex societal challenges with the powerful capabilities of\nAI systems. These challenges range from local issues with transit networks to\nglobal wildlife preservation. However, regardless of scale, a critical\nbottleneck for many AI4SG initiatives is the laborious process of problem\nscoping -- a complex and resource-intensive task -- due to a scarcity of\nprofessionals with both technical and domain expertise. Given the remarkable\napplications of large language models (LLM), we propose a Problem Scoping Agent\n(PSA) that uses an LLM to generate comprehensive project proposals grounded in\nscientific literature and real-world knowledge. We demonstrate that our PSA\nframework generates proposals comparable to those written by experts through a\nblind review and AI evaluations. Finally, we document the challenges of\nreal-world problem scoping and note several areas for future work."}
{"id": "2504.19254", "pdf": "https://arxiv.org/pdf/2504.19254", "abs": "https://arxiv.org/abs/2504.19254", "authors": ["Dylan Bouchard", "Mohit Singh Chauhan"], "title": "Uncertainty Quantification for Language Models: A Suite of Black-Box, White-Box, LLM Judge, and Ensemble Scorers", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "UQLM repository: https://github.com/cvs-health/uqlm", "summary": "Hallucinations are a persistent problem with Large Language Models (LLMs). As\nthese models become increasingly used in high-stakes domains, such as\nhealthcare and finance, the need for effective hallucination detection is\ncrucial. To this end, we propose a versatile framework for zero-resource\nhallucination detection that practitioners can apply to real-world use cases.\nTo achieve this, we adapt a variety of existing uncertainty quantification (UQ)\ntechniques, including black-box UQ, white-box UQ, and LLM-as-a-Judge,\ntransforming them as necessary into standardized response-level confidence\nscores ranging from 0 to 1. To enhance flexibility, we introduce a tunable\nensemble approach that incorporates any combination of the individual\nconfidence scores. This approach enables practitioners to optimize the ensemble\nfor a specific use case for improved performance. To streamline implementation,\nthe full suite of scorers is offered in this paper's companion Python toolkit,\nUQLM. To evaluate the performance of the various scorers, we conduct an\nextensive set of experiments using several LLM question-answering benchmarks.\nWe find that our tunable ensemble typically surpasses its individual components\nand outperforms existing hallucination detection methods. Our results\ndemonstrate the benefits of customized hallucination detection strategies for\nimproving the accuracy and reliability of LLMs."}
{"id": "2504.19086", "pdf": "https://arxiv.org/pdf/2504.19086", "abs": "https://arxiv.org/abs/2504.19086", "authors": ["Xiaoran Xu", "Jiangang Yang", "Wenyue Chong", "Wenhui Shi", "Shichu Sun", "Jing Xing", "Jian Liu"], "title": "Boosting Single-domain Generalized Object Detection via Vision-Language Knowledge Interaction", "categories": ["cs.CV"], "comment": null, "summary": "Single-Domain Generalized Object Detection~(S-DGOD) aims to train an object\ndetector on a single source domain while generalizing well to diverse unseen\ntarget domains, making it suitable for multimedia applications that involve\nvarious domain shifts, such as intelligent video surveillance and VR/AR\ntechnologies. With the success of large-scale Vision-Language Models, recent\nS-DGOD approaches exploit pre-trained vision-language knowledge to guide\ninvariant feature learning across visual domains. However, the utilized\nknowledge remains at a coarse-grained level~(e.g., the textual description of\nadverse weather paired with the image) and serves as an implicit regularization\nfor guidance, struggling to learn accurate region- and object-level features in\nvarying domains. In this work, we propose a new cross-modal feature learning\nmethod, which can capture generalized and discriminative regional features for\nS-DGOD tasks. The core of our method is the mechanism of Cross-modal and\nRegion-aware Feature Interaction, which simultaneously learns both inter-modal\nand intra-modal regional invariance through dynamic interactions between\nfine-grained textual and visual features. Moreover, we design a simple but\neffective strategy called Cross-domain Proposal Refining and Mixing, which\naligns the position of region proposals across multiple domains and diversifies\nthem, enhancing the localization ability of detectors in unseen scenarios. Our\nmethod achieves new state-of-the-art results on S-DGOD benchmark datasets, with\nimprovements of +8.8\\%~mPC on Cityscapes-C and +7.9\\%~mPC on DWD over\nbaselines, demonstrating its efficacy."}
{"id": "2504.18544", "pdf": "https://arxiv.org/pdf/2504.18544", "abs": "https://arxiv.org/abs/2504.18544", "authors": ["Nazia Nafis", "Inaki Esnaola", "Alvaro Martinez-Perez", "Maria-Cruz Villa-Uriol", "Venet Osmani"], "title": "Critical Challenges and Guidelines in Evaluating Synthetic Tabular Data: A Systematic Review", "categories": ["cs.LG", "cs.AI", "cs.CY"], "comment": null, "summary": "Generating synthetic tabular data can be challenging, however evaluation of\ntheir quality is just as challenging, if not more. This systematic review sheds\nlight on the critical importance of rigorous evaluation of synthetic health\ndata to ensure reliability, relevance, and their appropriate use. Based on\nscreening of 1766 papers and a detailed review of 101 papers we identified key\nchallenges, including lack of consensus on evaluation methods, improper use of\nevaluation metrics, limited input from domain experts, inadequate reporting of\ndataset characteristics, and limited reproducibility of results. In response,\nwe provide several guidelines on the generation and evaluation of synthetic\ndata, to allow the community to unlock and fully harness the transformative\npotential of synthetic data and accelerate innovation."}
{"id": "2504.19267", "pdf": "https://arxiv.org/pdf/2504.19267", "abs": "https://arxiv.org/abs/2504.19267", "authors": ["Mohamed Gado", "Towhid Taliee", "Muhammad Memon", "Dmitry Ignatov", "Radu Timofte"], "title": "VIST-GPT: Ushering in the Era of Visual Storytelling with LLMs?", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Visual storytelling is an interdisciplinary field combining computer vision\nand natural language processing to generate cohesive narratives from sequences\nof images. This paper presents a novel approach that leverages recent\nadvancements in multimodal models, specifically adapting transformer-based\narchitectures and large multimodal models, for the visual storytelling task.\nLeveraging the large-scale Visual Storytelling (VIST) dataset, our VIST-GPT\nmodel produces visually grounded, contextually appropriate narratives. We\naddress the limitations of traditional evaluation metrics, such as BLEU,\nMETEOR, ROUGE, and CIDEr, which are not suitable for this task. Instead, we\nutilize RoViST and GROOVIST, novel reference-free metrics designed to assess\nvisual storytelling, focusing on visual grounding, coherence, and\nnon-redundancy. These metrics provide a more nuanced evaluation of narrative\nquality, aligning closely with human judgment."}
{"id": "2504.19115", "pdf": "https://arxiv.org/pdf/2504.19115", "abs": "https://arxiv.org/abs/2504.19115", "authors": ["Jiaqi Peng", "Tai Wang", "Jiangmiao Pang", "Yuan Shen"], "title": "Towards Latency-Aware 3D Streaming Perception for Autonomous Driving", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Although existing 3D perception algorithms have demonstrated significant\nimprovements in performance, their deployment on edge devices continues to\nencounter critical challenges due to substantial runtime latency. We propose a\nnew benchmark tailored for online evaluation by considering runtime latency.\nBased on the benchmark, we build a Latency-Aware 3D Streaming Perception (LASP)\nframework that addresses the latency issue through two primary components: 1)\nlatency-aware history integration, which extends query propagation into a\ncontinuous process, ensuring the integration of historical feature regardless\nof varying latency; 2) latency-aware predictive detection, a module that\ncompensates the detection results with the predicted trajectory and the\nposterior accessed latency. By incorporating the latency-aware mechanism, our\nmethod shows generalization across various latency levels, achieving an online\nperformance that closely aligns with 80\\% of its offline evaluation on the\nJetson AGX Orin without any acceleration techniques."}
{"id": "2504.18556", "pdf": "https://arxiv.org/pdf/2504.18556", "abs": "https://arxiv.org/abs/2504.18556", "authors": ["Jialei Song", "Xingquan Zuo", "Feiyang Wang", "Hai Huang", "Tianle Zhang"], "title": "RDI: An adversarial robustness evaluation metric for deep neural networks based on sample clustering features", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Deep neural networks (DNNs) are highly susceptible to adversarial samples,\nraising concerns about their reliability in safety-critical tasks. Currently,\nmethods of evaluating adversarial robustness are primarily categorized into\nattack-based and certified robustness evaluation approaches. The former not\nonly relies on specific attack algorithms but also is highly time-consuming,\nwhile the latter due to its analytical nature, is typically difficult to\nimplement for large and complex models. A few studies evaluate model robustness\nbased on the model's decision boundary, but they suffer from low evaluation\naccuracy. To address the aforementioned issues, we propose a novel adversarial\nrobustness evaluation metric, Robustness Difference Index (RDI), which is based\non sample clustering features. RDI draws inspiration from clustering evaluation\nby analyzing the intra-class and inter-class distances of feature vectors\nseparated by the decision boundary to quantify model robustness. It is\nattack-independent and has high computational efficiency. Experiments show\nthat, RDI demonstrates a stronger correlation with the gold-standard\nadversarial robustness metric of attack success rate (ASR). The average\ncomputation time of RDI is only 1/30 of the evaluation method based on the PGD\nattack. Our open-source code is available at:\nhttps://anonymous.4open.science/r/RDI-B1DA."}
{"id": "2504.19298", "pdf": "https://arxiv.org/pdf/2504.19298", "abs": "https://arxiv.org/abs/2504.19298", "authors": ["Hanyu Lai", "Junjie Gao", "Xiao Liu", "Yifan Xu", "Shudan Zhang", "Yuxiao Dong", "Jie Tang"], "title": "AndroidGen: Building an Android Language Agent under Data Scarcity", "categories": ["cs.CL"], "comment": null, "summary": "Large language models have opened up a world of possibilities for various NLP\ntasks, sparking optimism for the future. Despite their potential, LLMs have yet\nto be widely used as agents on real mobile devices. The main challenge is the\nneed for high-quality data sources. Time constraints and labor intensity often\nhinder human annotation. On the other hand, existing LLMs exhibit inadequate\ncompletion rates and need a robust data filtration strategy. Given these\nchallenges, we develop a framework called AndroidGen to enhance the\ncapabilities of LLM-based agents under data scarcity. In addition, we leverage\nAndroidGen to collect trajectories given human tasks and train open-source LLMs\non these trajectories to develop an open-source mobile agent without manually\nlabeled trajectories. We extensively evaluate AndroidGen with AndroidWorld,\nAitW, and various popular applications, demonstrating its improvements and\nrevealing potential areas for future improvement. Code, model, and data are\navailable at https://github.com/THUDM/AndroidGen."}
{"id": "2504.19124", "pdf": "https://arxiv.org/pdf/2504.19124", "abs": "https://arxiv.org/abs/2504.19124", "authors": ["Zhongxuan Li"], "title": "Blind Source Separation Based on Sparsity", "categories": ["cs.CV"], "comment": null, "summary": "Blind source separation (BSS) is a key technique in array processing and data\nanalysis, aiming to recover unknown sources from observed mixtures without\nknowledge of the mixing matrix. Classical independent component analysis (ICA)\nmethods rely on the assumption that sources are mutually independent. To\naddress limitations of ICA, sparsity-based methods have been introduced, which\ndecompose source signals sparsely in a predefined dictionary. Morphological\nComponent Analysis (MCA), based on sparse representation theory, assumes that a\nsignal is a linear combination of components with distinct geometries, each\nsparsely representable in one dictionary and not in others. This approach has\nrecently been applied to BSS with promising results.\n  This report reviews key approaches derived from classical ICA and explores\nsparsity-based methods for BSS. It introduces the theory of sparse\nrepresentation and decomposition, followed by a block coordinate relaxation MCA\nalgorithm, whose variants are used in Multichannel MCA (MMCA) and Generalized\nMCA (GMCA). A local dictionary learning method using K-SVD is then presented.\nFinally, we propose an improved algorithm, SAC+BK-SVD, which enhances K-SVD by\nlearning a block-sparsifying dictionary that clusters and updates similar atoms\nin blocks.\n  The implementation includes experiments on image segmentation and blind image\nsource separation using the discussed techniques. We also compare the proposed\nblock-sparse dictionary learning algorithm with K-SVD. Simulation results\ndemonstrate that our method yields improved blind image separation quality."}
{"id": "2504.18560", "pdf": "https://arxiv.org/pdf/2504.18560", "abs": "https://arxiv.org/abs/2504.18560", "authors": ["Alessio Buscemi", "Cédric Lothritz", "Sergio Morales", "Marcos Gomez-Vazquez", "Robert Clarisó", "Jordi Cabot", "German Castignani"], "title": "Mind the Language Gap: Automated and Augmented Evaluation of Bias in LLMs for High- and Low-Resource Languages", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have exhibited impressive natural language\nprocessing capabilities but often perpetuate social biases inherent in their\ntraining data. To address this, we introduce MultiLingual Augmented Bias\nTesting (MLA-BiTe), a framework that improves prior bias evaluation methods by\nenabling systematic multilingual bias testing. MLA-BiTe leverages automated\ntranslation and paraphrasing techniques to support comprehensive assessments\nacross diverse linguistic settings. In this study, we evaluate the\neffectiveness of MLA-BiTe by testing four state-of-the-art LLMs in six\nlanguages -- including two low-resource languages -- focusing on seven\nsensitive categories of discrimination."}
{"id": "2504.19314", "pdf": "https://arxiv.org/pdf/2504.19314", "abs": "https://arxiv.org/abs/2504.19314", "authors": ["Peilin Zhou", "Bruce Leon", "Xiang Ying", "Can Zhang", "Yifan Shao", "Qichen Ye", "Dading Chong", "Zhiling Jin", "Chenxuan Xie", "Meng Cao", "Yuxin Gu", "Sixin Hong", "Jing Ren", "Jian Chen", "Chao Liu", "Yining Hua"], "title": "BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language Models in Chinese", "categories": ["cs.CL"], "comment": "Under Review", "summary": "As large language models (LLMs) evolve into tool-using agents, the ability to\nbrowse the web in real-time has become a critical yardstick for measuring their\nreasoning and retrieval competence. Existing benchmarks such as BrowseComp\nconcentrate on English and overlook the linguistic, infrastructural, and\ncensorship-related complexities of other major information ecosystems -- most\nnotably Chinese. To address this gap, we introduce BrowseComp-ZH, a\nhigh-difficulty benchmark purpose-built to comprehensively evaluate LLM agents\non the Chinese web. BrowseComp-ZH consists of 289 multi-hop questions spanning\n11 diverse domains. Each question is reverse-engineered from a short,\nobjective, and easily verifiable answer (e.g., a date, number, or proper noun).\nA two-stage quality control protocol is applied to strive for high question\ndifficulty and answer uniqueness. We benchmark over 20 state-of-the-art\nlanguage models and agentic search systems on our proposed BrowseComp-ZH.\nDespite their strong conversational and retrieval capabilities, most models\nstruggle severely: a large number achieve accuracy rates below 10%, and only a\nhandful exceed 20%. Even the best-performing system, OpenAI's DeepResearch,\nreaches just 42.9%. These results demonstrate the considerable difficulty of\nBrowseComp-ZH, where success demands not only effective retrieval strategies,\nbut also sophisticated reasoning and information reconciliation -- capabilities\nthat current models still struggle to master. Our dataset, construction\nguidelines, and benchmark results have been publicly released at\nhttps://github.com/PALIN2018/BrowseComp-ZH."}
{"id": "2504.19127", "pdf": "https://arxiv.org/pdf/2504.19127", "abs": "https://arxiv.org/abs/2504.19127", "authors": ["Jialang Lu", "Huayu Zhao", "Huiyu Zhai", "Xingxing Yang", "Shini Han"], "title": "DeepSPG: Exploring Deep Semantic Prior Guidance for Low-light Image Enhancement with Multimodal Learning", "categories": ["cs.CV", "cs.MM"], "comment": "Accepted by ICMR 2025 Main track. Code is available at\n  https://github.com/Wenyuzhy/DeepSPG", "summary": "There has long been a belief that high-level semantics learning can benefit\nvarious downstream computer vision tasks. However, in the low-light image\nenhancement (LLIE) community, existing methods learn a brutal mapping between\nlow-light and normal-light domains without considering the semantic information\nof different regions, especially in those extremely dark regions that suffer\nfrom severe information loss. To address this issue, we propose a new deep\nsemantic prior-guided framework (DeepSPG) based on Retinex image decomposition\nfor LLIE to explore informative semantic knowledge via a pre-trained semantic\nsegmentation model and multimodal learning. Notably, we incorporate both\nimage-level semantic prior and text-level semantic prior and thus formulate a\nmultimodal learning framework with combinatorial deep semantic prior guidance\nfor LLIE. Specifically, we incorporate semantic knowledge to guide the\nenhancement process via three designs: an image-level semantic prior guidance\nby leveraging hierarchical semantic features from a pre-trained semantic\nsegmentation model; a text-level semantic prior guidance by integrating natural\nlanguage semantic constraints via a pre-trained vision-language model; a\nmulti-scale semantic-aware structure that facilitates effective semantic\nfeature incorporation. Eventually, our proposed DeepSPG demonstrates superior\nperformance compared to state-of-the-art methods across five benchmark\ndatasets. The implementation details and code are publicly available at\nhttps://github.com/Wenyuzhy/DeepSPG."}
{"id": "2504.18562", "pdf": "https://arxiv.org/pdf/2504.18562", "abs": "https://arxiv.org/abs/2504.18562", "authors": ["Ayoub Jadouli", "Chaker El Amrani"], "title": "Deep Learning with Pretrained 'Internal World' Layers: A Gemma 3-Based Modular Architecture for Wildfire Prediction", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Deep learning models, especially large Transformers, carry substantial\n\"memory\" in their intermediate layers -- an \\emph{internal world} that encodes\na wealth of relational and contextual knowledge. This work harnesses that\ninternal world for wildfire occurrence prediction by introducing a modular\narchitecture built upon Gemma 3, a state-of-the-art multimodal model. Rather\nthan relying on Gemma 3's original embedding and positional encoding stacks, we\ndevelop a custom feed-forward module that transforms tabular wildfire features\ninto the hidden dimension required by Gemma 3's mid-layer Transformer blocks.\nWe freeze these Gemma 3 sub-layers -- thus preserving their pretrained\nrepresentation power -- while training only the smaller input and output\nnetworks. This approach minimizes the number of trainable parameters and\nreduces the risk of overfitting on limited wildfire data, yet retains the\nbenefits of Gemma 3's broad knowledge. Evaluations on a Moroccan wildfire\ndataset demonstrate improved predictive accuracy and robustness compared to\nstandard feed-forward and convolutional baselines. Ablation studies confirm\nthat the frozen Transformer layers consistently contribute to better\nrepresentations, underscoring the feasibility of reusing large-model mid-layers\nas a learned internal world. Our findings suggest that strategic modular reuse\nof pretrained Transformers can enable more data-efficient and interpretable\nsolutions for critical environmental applications such as wildfire risk\nmanagement."}
{"id": "2504.19333", "pdf": "https://arxiv.org/pdf/2504.19333", "abs": "https://arxiv.org/abs/2504.19333", "authors": ["James O' Neill", "Santhosh Subramanian", "Eric Lin", "Vaikkunth Mugunthan"], "title": "Unified Multi-Task Learning & Model Fusion for Efficient Language Model Guardrailing", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The trend towards large language models (LLMs) for guardrailing against\nundesired behaviors is increasing and has shown promise for censoring user\ninputs. However, increased latency, memory consumption, hosting expenses and\nnon-structured outputs can make their use prohibitive.\n  In this work, we show that task-specific data generation can lead to\nfine-tuned classifiers that significantly outperform current state of the art\n(SoTA) while being orders of magnitude smaller. Secondly, we show that using a\nsingle model, \\texttt{MultiTaskGuard}, that is pretrained on a large\nsynthetically generated dataset with unique task instructions further improves\ngeneralization. Thirdly, our most performant models, \\texttt{UniGuard}, are\nfound using our proposed search-based model merging approach that finds an\noptimal set of parameters to combine single-policy models and multi-policy\nguardrail models. % On 7 public datasets and 4 guardrail benchmarks we created,\nour efficient guardrail classifiers improve over the best performing SoTA\npublicly available LLMs and 3$^{\\text{rd}}$ party guardrail APIs in detecting\nunsafe and safe behaviors by an average F1 score improvement of \\textbf{29.92}\npoints over Aegis-LlamaGuard and \\textbf{21.62} over \\texttt{gpt-4o},\nrespectively. Lastly, our guardrail synthetic data generation process that uses\ncustom task-specific guardrail poli"}
{"id": "2504.19136", "pdf": "https://arxiv.org/pdf/2504.19136", "abs": "https://arxiv.org/abs/2504.19136", "authors": ["Huiling Zheng", "Xian Zhong", "Bin Liu", "Yi Xiao", "Bihan Wen", "Xiaofeng Li"], "title": "PAD: Phase-Amplitude Decoupling Fusion for Multi-Modal Land Cover Classification", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": "13 pages, 8 figures", "summary": "The fusion of Synthetic Aperture Radar (SAR) and RGB imagery for land cover\nclassification remains challenging due to modality heterogeneity and the\nunderutilization of spectral complementarity. Existing methods often fail to\ndecouple shared structural features from modality-specific radiometric\nattributes, leading to feature conflicts and information loss. To address this\nissue, we propose Phase-Amplitude Decoupling (PAD), a frequency-aware framework\nthat separates phase (modality-shared) and amplitude (modality-specific)\ncomponents in the Fourier domain. Specifically, PAD consists of two key\ncomponents: 1) Phase Spectrum Correction (PSC), which aligns cross-modal phase\nfeatures through convolution-guided scaling to enhance geometric consistency,\nand 2) Amplitude Spectrum Fusion (ASF), which dynamically integrates\nhigh-frequency details and low-frequency structures using frequency-adaptive\nmultilayer perceptrons. This approach leverages SAR's sensitivity to\nmorphological features and RGB's spectral richness. Extensive experiments on\nWHU-OPT-SAR and DDHR-SK datasets demonstrate state-of-the-art performance. Our\nwork establishes a new paradigm for physics-aware multi-modal fusion in remote\nsensing. The code will be available at https://github.com/RanFeng2/PAD."}
{"id": "2504.18563", "pdf": "https://arxiv.org/pdf/2504.18563", "abs": "https://arxiv.org/abs/2504.18563", "authors": ["Abha Jha", "Ashwath Vaithinathan Aravindan", "Matthew Salaway", "Atharva Sandeep Bhide", "Duygu Nur Yaldiz"], "title": "Backdoor Defense in Diffusion Models via Spatial Attention Unlearning", "categories": ["cs.CR", "cs.AI", "cs.CV"], "comment": null, "summary": "Text-to-image diffusion models are increasingly vulnerable to backdoor\nattacks, where malicious modifications to the training data cause the model to\ngenerate unintended outputs when specific triggers are present. While\nclassification models have seen extensive development of defense mechanisms,\ngenerative models remain largely unprotected due to their high-dimensional\noutput space, which complicates the detection and mitigation of subtle\nperturbations. Defense strategies for diffusion models, in particular, remain\nunder-explored. In this work, we propose Spatial Attention Unlearning (SAU), a\nnovel technique for mitigating backdoor attacks in diffusion models. SAU\nleverages latent space manipulation and spatial attention mechanisms to isolate\nand remove the latent representation of backdoor triggers, ensuring precise and\nefficient removal of malicious effects. We evaluate SAU across various types of\nbackdoor attacks, including pixel-based and style-based triggers, and\ndemonstrate its effectiveness in achieving 100% trigger removal accuracy.\nFurthermore, SAU achieves a CLIP score of 0.7023, outperforming existing\nmethods while preserving the model's ability to generate high-quality,\nsemantically aligned images. Our results show that SAU is a robust, scalable,\nand practical solution for securing text-to-image diffusion models against\nbackdoor attacks."}
{"id": "2504.19339", "pdf": "https://arxiv.org/pdf/2504.19339", "abs": "https://arxiv.org/abs/2504.19339", "authors": ["Dongqi Liu", "Xi Yu", "Vera Demberg", "Mirella Lapata"], "title": "Explanatory Summarization with Discourse-Driven Planning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by the Transactions of the Association for Computational\n  Linguistics (TACL)", "summary": "Lay summaries for scientific documents typically include explanations to help\nreaders grasp sophisticated concepts or arguments. However, current automatic\nsummarization methods do not explicitly model explanations, which makes it\ndifficult to align the proportion of explanatory content with human-written\nsummaries. In this paper, we present a plan-based approach that leverages\ndiscourse frameworks to organize summary generation and guide explanatory\nsentences by prompting responses to the plan. Specifically, we propose two\ndiscourse-driven planning strategies, where the plan is conditioned as part of\nthe input or part of the output prefix, respectively. Empirical experiments on\nthree lay summarization datasets show that our approach outperforms existing\nstate-of-the-art methods in terms of summary quality, and it enhances model\nrobustness, controllability, and mitigates hallucination."}
{"id": "2504.19161", "pdf": "https://arxiv.org/pdf/2504.19161", "abs": "https://arxiv.org/abs/2504.19161", "authors": ["Zheng Fang", "Kangjun Liu", "Ke Chen", "Qingyu Liu", "Jianguo Zhang", "Lingyang Song", "Yaowei Wang"], "title": "RadioFormer: A Multiple-Granularity Radio Map Estimation Transformer with 1\\textpertenthousand Spatial Sampling", "categories": ["cs.CV"], "comment": null, "summary": "The task of radio map estimation aims to generate a dense representation of\nelectromagnetic spectrum quantities, such as the received signal strength at\neach grid point within a geographic region, based on measurements from a subset\nof spatially distributed nodes (represented as pixels). Recently, deep vision\nmodels such as the U-Net have been adapted to radio map estimation, whose\neffectiveness can be guaranteed with sufficient spatial observations (typically\n0.01% to 1% of pixels) in each map, to model local dependency of observed\nsignal power. However, such a setting of sufficient measurements can be less\npractical in real-world scenarios, where extreme sparsity in spatial sampling\ncan be widely encountered. To address this challenge, we propose RadioFormer, a\nnovel multiple-granularity transformer designed to handle the constraints posed\nby spatial sparse observations. Our RadioFormer, through a dual-stream\nself-attention (DSA) module, can respectively discover the correlation of\npixel-wise observed signal power and also learn patch-wise buildings'\ngeometries in a style of multiple granularities, which are integrated into\nmulti-scale representations of radio maps by a cross stream cross-attention\n(CCA) module. Extensive experiments on the public RadioMapSeer dataset\ndemonstrate that RadioFormer outperforms state-of-the-art methods in radio map\nestimation while maintaining the lowest computational cost. Furthermore, the\nproposed approach exhibits exceptional generalization capabilities and robust\nzero-shot performance, underscoring its potential to advance radio map\nestimation in a more practical setting with very limited observation nodes."}
{"id": "2504.18564", "pdf": "https://arxiv.org/pdf/2504.18564", "abs": "https://arxiv.org/abs/2504.18564", "authors": ["Xinzhe Huang", "Kedong Xiu", "Tianhang Zheng", "Churui Zeng", "Wangze Ni", "Zhan Qiin", "Kui Ren", "Chun Chen"], "title": "DualBreach: Efficient Dual-Jailbreaking via Target-Driven Initialization and Multi-Target Optimization", "categories": ["cs.CR", "cs.AI"], "comment": "20 pages, 8 figures", "summary": "Recent research has focused on exploring the vulnerabilities of Large\nLanguage Models (LLMs), aiming to elicit harmful and/or sensitive content from\nLLMs. However, due to the insufficient research on dual-jailbreaking -- attacks\ntargeting both LLMs and Guardrails, the effectiveness of existing attacks is\nlimited when attempting to bypass safety-aligned LLMs shielded by guardrails.\nTherefore, in this paper, we propose DualBreach, a target-driven framework for\ndual-jailbreaking. DualBreach employs a Target-driven Initialization (TDI)\nstrategy to dynamically construct initial prompts, combined with a Multi-Target\nOptimization (MTO) method that utilizes approximate gradients to jointly adapt\nthe prompts across guardrails and LLMs, which can simultaneously save the\nnumber of queries and achieve a high dual-jailbreaking success rate. For\nblack-box guardrails, DualBreach either employs a powerful open-sourced\nguardrail or imitates the target black-box guardrail by training a proxy model,\nto incorporate guardrails into the MTO process.\n  We demonstrate the effectiveness of DualBreach in dual-jailbreaking scenarios\nthrough extensive evaluation on several widely-used datasets. Experimental\nresults indicate that DualBreach outperforms state-of-the-art methods with\nfewer queries, achieving significantly higher success rates across all\nsettings. More specifically, DualBreach achieves an average dual-jailbreaking\nsuccess rate of 93.67% against GPT-4 with Llama-Guard-3 protection, whereas the\nbest success rate achieved by other methods is 88.33%. Moreover, DualBreach\nonly uses an average of 1.77 queries per successful dual-jailbreak,\noutperforming other state-of-the-art methods. For the purpose of defense, we\npropose an XGBoost-based ensemble defensive mechanism named EGuard, which\nintegrates the strengths of multiple guardrails, demonstrating superior\nperformance compared with Llama-Guard-3."}
{"id": "2504.19395", "pdf": "https://arxiv.org/pdf/2504.19395", "abs": "https://arxiv.org/abs/2504.19395", "authors": ["Zhouxiang Fang", "Aayush Mishra", "Muhan Gao", "Anqi Liu", "Daniel Khashabi"], "title": "ICL CIPHERS: Quantifying \"Learning'' in In-Context Learning via Substitution Ciphers", "categories": ["cs.CL"], "comment": null, "summary": "Recent works have suggested that In-Context Learning (ICL) operates in dual\nmodes, i.e. task retrieval (remember learned patterns from pre-training) and\ntask learning (inference-time ``learning'' from demonstrations). However,\ndisentangling these the two modes remains a challenging goal. We introduce ICL\nCIPHERS, a class of task reformulations based on substitution ciphers borrowed\nfrom classic cryptography. In this approach, a subset of tokens in the\nin-context inputs are substituted with other (irrelevant) tokens, rendering\nEnglish sentences less comprehensible to human eye. However, by design, there\nis a latent, fixed pattern to this substitution, making it reversible. This\nbijective (reversible) cipher ensures that the task remains a well-defined task\nin some abstract sense, despite the transformations. It is a curious question\nif LLMs can solve ICL CIPHERS with a BIJECTIVE mapping, which requires\ndeciphering the latent cipher. We show that LLMs are better at solving ICL\nCIPHERS with BIJECTIVE mappings than the NON-BIJECTIVE (irreversible) baseline,\nproviding a novel approach to quantify ``learning'' in ICL. While this gap is\nsmall, it is consistent across the board on four datasets and six models.\nFinally, we examine LLMs' internal representations and identify evidence in\ntheir ability to decode the ciphered inputs."}
{"id": "2504.19165", "pdf": "https://arxiv.org/pdf/2504.19165", "abs": "https://arxiv.org/abs/2504.19165", "authors": ["Yuan Li", "Ziqian Bai", "Feitong Tan", "Zhaopeng Cui", "Sean Fanello", "Yinda Zhang"], "title": "IM-Portrait: Learning 3D-aware Video Diffusion for PhotorealisticTalking Heads from Monocular Videos", "categories": ["cs.CV"], "comment": "CVPR2025; project page:\n  https://y-u-a-n-l-i.github.io/projects/IM-Portrait/", "summary": "We propose a novel 3D-aware diffusion-based method for generating\nphotorealistic talking head videos directly from a single identity image and\nexplicit control signals (e.g., expressions). Our method generates Multiplane\nImages (MPIs) that ensure geometric consistency, making them ideal for\nimmersive viewing experiences like binocular videos for VR headsets. Unlike\nexisting methods that often require a separate stage or joint optimization to\nreconstruct a 3D representation (such as NeRF or 3D Gaussians), our approach\ndirectly generates the final output through a single denoising process,\neliminating the need for post-processing steps to render novel views\nefficiently. To effectively learn from monocular videos, we introduce a\ntraining mechanism that reconstructs the output MPI randomly in either the\ntarget or the reference camera space. This approach enables the model to\nsimultaneously learn sharp image details and underlying 3D information.\nExtensive experiments demonstrate the effectiveness of our method, which\nachieves competitive avatar quality and novel-view rendering capabilities, even\nwithout explicit 3D reconstruction or high-quality multi-view training data."}
{"id": "2504.18565", "pdf": "https://arxiv.org/pdf/2504.18565", "abs": "https://arxiv.org/abs/2504.18565", "authors": ["Sid Black", "Asa Cooper Stickland", "Jake Pencharz", "Oliver Sourbut", "Michael Schmatz", "Jay Bailey", "Ollie Matthews", "Ben Millwood", "Alex Remedios", "Alan Cooney"], "title": "RepliBench: Evaluating the autonomous replication capabilities of language model agents", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Uncontrollable autonomous replication of language model agents poses a\ncritical safety risk. To better understand this risk, we introduce RepliBench,\na suite of evaluations designed to measure autonomous replication capabilities.\nRepliBench is derived from a decomposition of these capabilities covering four\ncore domains: obtaining resources, exfiltrating model weights, replicating onto\ncompute, and persisting on this compute for long periods. We create 20 novel\ntask families consisting of 86 individual tasks. We benchmark 5 frontier\nmodels, and find they do not currently pose a credible threat of\nself-replication, but succeed on many components and are improving rapidly.\nModels can deploy instances from cloud compute providers, write\nself-propagating programs, and exfiltrate model weights under simple security\nsetups, but struggle to pass KYC checks or set up robust and persistent agent\ndeployments. Overall the best model we evaluated (Claude 3.7 Sonnet) has a >50%\npass@10 score on 15/20 task families, and a >50% pass@10 score for 9/20\nfamilies on the hardest variants. These findings suggest autonomous replication\ncapability could soon emerge with improvements in these remaining areas or with\nhuman assistance."}
{"id": "2504.19406", "pdf": "https://arxiv.org/pdf/2504.19406", "abs": "https://arxiv.org/abs/2504.19406", "authors": ["Mengxia Yu", "Bang Nguyen", "Olivia Zino", "Meng Jiang"], "title": "Context Selection and Rewriting for Video-based EducationalQuestion Generation", "categories": ["cs.CL"], "comment": null, "summary": "Educational question generation (EQG) is a crucial component of intelligent\neducational systems, significantly aiding self-assessment, active learning, and\npersonalized education. While EQG systems have emerged, existing datasets\ntypically rely on predefined, carefully edited texts, failing to represent\nreal-world classroom content, including lecture speech with a set of\ncomplementary slides. To bridge this gap, we collect a dataset of educational\nquestions based on lectures from real-world classrooms. On this realistic\ndataset, we find that current methods for EQG struggle with accurately\ngenerating questions from educational videos, particularly in aligning with\nspecific timestamps and target answers. Common challenges include selecting\ninformative contexts from extensive transcripts and ensuring generated\nquestions meaningfully incorporate the target answer. To address the\nchallenges, we introduce a novel framework utilizing large language models for\ndynamically selecting and rewriting contexts based on target timestamps and\nanswers. First, our framework selects contexts from both lecture transcripts\nand video keyframes based on answer relevance and temporal proximity. Then, we\nintegrate the contexts selected from both modalities and rewrite them into\nanswer-containing knowledge statements, to enhance the logical connection\nbetween the contexts and the desired answer. This approach significantly\nimproves the quality and relevance of the generated questions. Our dataset and\ncode are released in https://github.com/mengxiayu/COSER."}
{"id": "2504.19183", "pdf": "https://arxiv.org/pdf/2504.19183", "abs": "https://arxiv.org/abs/2504.19183", "authors": ["Mi Zheng", "Guanglei Yang", "Zitong Huang", "Zhenhua Guo", "Kevin Han", "Wangmeng Zuo"], "title": "Segmenting Objectiveness and Task-awareness Unknown Region for Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "With the emergence of transformer-based architectures and large language\nmodels (LLMs), the accuracy of road scene perception has substantially\nadvanced. Nonetheless, current road scene segmentation approaches are\npredominantly trained on closed-set data, resulting in insufficient detection\ncapabilities for out-of-distribution (OOD) objects. To overcome this\nlimitation, road anomaly detection methods have been proposed. However,\nexisting methods primarily depend on image inpainting and OOD distribution\ndetection techniques, facing two critical issues: (1) inadequate consideration\nof the objectiveness attributes of anomalous regions, causing incomplete\nsegmentation when anomalous objects share similarities with known classes, and\n(2) insufficient attention to environmental constraints, leading to the\ndetection of anomalies irrelevant to autonomous driving tasks. In this paper,\nwe propose a novel framework termed Segmenting Objectiveness and Task-Awareness\n(SOTA) for autonomous driving scenes. Specifically, SOTA enhances the\nsegmentation of objectiveness through a Semantic Fusion Block (SFB) and filters\nanomalies irrelevant to road navigation tasks using a Scene-understanding\nGuided Prompt-Context Adaptor (SG-PCA). Extensive empirical evaluations on\nmultiple benchmark datasets, including Fishyscapes Lost and Found,\nSegment-Me-If-You-Can, and RoadAnomaly, demonstrate that the proposed SOTA\nconsistently improves OOD detection performance across diverse detectors,\nachieving robust and accurate segmentation outcomes."}
{"id": "2504.18566", "pdf": "https://arxiv.org/pdf/2504.18566", "abs": "https://arxiv.org/abs/2504.18566", "authors": ["Harsh Patel"], "title": "Feature Selection via GANs (GANFS): Enhancing Machine Learning Models for DDoS Mitigation", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "Distributed Denial of Service (DDoS) attacks represent a persistent and\nevolving threat to modern networked systems, capable of causing large-scale\nservice disruptions. The complexity of such attacks, often hidden within\nhigh-dimensional and redundant network traffic data, necessitates robust and\nintelligent feature selection techniques for effective detection. Traditional\nmethods such as filter-based, wrapper-based, and embedded approaches, each\noffer strengths but struggle with scalability or adaptability in complex attack\nenvironments. In this study, we explore these existing techniques through a\ndetailed comparative analysis and highlight their limitations when applied to\nlarge-scale DDoS detection tasks. Building upon these insights, we introduce a\nnovel Generative Adversarial Network-based Feature Selection (GANFS) method\nthat leverages adversarial learning dynamics to identify the most informative\nfeatures. By training a GAN exclusively on attack traffic and employing a\nperturbation-based sensitivity analysis on the Discriminator, GANFS effectively\nranks feature importance without relying on full supervision. Experimental\nevaluations using the CIC-DDoS2019 dataset demonstrate that GANFS not only\nimproves the accuracy of downstream classifiers but also enhances computational\nefficiency by significantly reducing feature dimensionality. These results\npoint to the potential of integrating generative learning models into\ncybersecurity pipelines to build more adaptive and scalable detection systems."}
{"id": "2504.19413", "pdf": "https://arxiv.org/pdf/2504.19413", "abs": "https://arxiv.org/abs/2504.19413", "authors": ["Prateek Chhikara", "Dev Khant", "Saket Aryan", "Taranjeet Singh", "Deshraj Yadav"], "title": "Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable prowess in\ngenerating contextually coherent responses, yet their fixed context windows\npose fundamental challenges for maintaining consistency over prolonged\nmulti-session dialogues. We introduce Mem0, a scalable memory-centric\narchitecture that addresses this issue by dynamically extracting,\nconsolidating, and retrieving salient information from ongoing conversations.\nBuilding on this foundation, we further propose an enhanced variant that\nleverages graph-based memory representations to capture complex relational\nstructures among conversational elements. Through comprehensive evaluations on\nLOCOMO benchmark, we systematically compare our approaches against six baseline\ncategories: (i) established memory-augmented systems, (ii) retrieval-augmented\ngeneration (RAG) with varying chunk sizes and k-values, (iii) a full-context\napproach that processes the entire conversation history, (iv) an open-source\nmemory solution, (v) a proprietary model system, and (vi) a dedicated memory\nmanagement platform. Empirical results show that our methods consistently\noutperform all existing memory systems across four question categories:\nsingle-hop, temporal, multi-hop, and open-domain. Notably, Mem0 achieves 26%\nrelative improvements in the LLM-as-a-Judge metric over OpenAI, while Mem0 with\ngraph memory achieves around 2% higher overall score than the base\nconfiguration. Beyond accuracy gains, we also markedly reduce computational\noverhead compared to full-context method. In particular, Mem0 attains a 91%\nlower p95 latency and saves more than 90% token cost, offering a compelling\nbalance between advanced reasoning capabilities and practical deployment\nconstraints. Our findings highlight critical role of structured, persistent\nmemory mechanisms for long-term conversational coherence, paving the way for\nmore reliable and efficient LLM-driven AI agents."}
{"id": "2504.19186", "pdf": "https://arxiv.org/pdf/2504.19186", "abs": "https://arxiv.org/abs/2504.19186", "authors": ["Zhangshuo Qi", "Luqi Cheng", "Zijie Zhou", "Guangming Xiong"], "title": "LRFusionPR: A Polar BEV-Based LiDAR-Radar Fusion Network for Place Recognition", "categories": ["cs.CV", "cs.RO"], "comment": "8 pages, 6 figures", "summary": "In autonomous driving, place recognition is critical for global localization\nin GPS-denied environments. LiDAR and radar-based place recognition methods\nhave garnered increasing attention, as LiDAR provides precise ranging, whereas\nradar excels in adverse weather resilience. However, effectively leveraging\nLiDAR-radar fusion for place recognition remains challenging. The noisy and\nsparse nature of radar data limits its potential to further improve recognition\naccuracy. In addition, heterogeneous radar configurations complicate the\ndevelopment of unified cross-modality fusion frameworks. In this paper, we\npropose LRFusionPR, which improves recognition accuracy and robustness by\nfusing LiDAR with either single-chip or scanning radar. Technically, a\ndual-branch network is proposed to fuse different modalities within the unified\npolar coordinate bird's eye view (BEV) representation. In the fusion branch,\ncross-attention is utilized to perform cross-modality feature interactions. The\nknowledge from the fusion branch is simultaneously transferred to the\ndistillation branch, which takes radar as its only input to further improve the\nrobustness. Ultimately, the descriptors from both branches are concatenated,\nproducing the multimodal global descriptor for place retrieval. Extensive\nevaluations on multiple datasets demonstrate that our LRFusionPR achieves\naccurate place recognition, while maintaining robustness under varying weather\nconditions. Our open-source code will be released at\nhttps://github.com/QiZS-BIT/LRFusionPR."}
{"id": "2504.18569", "pdf": "https://arxiv.org/pdf/2504.18569", "abs": "https://arxiv.org/abs/2504.18569", "authors": ["Guanchen Wu", "Linzhi Zheng", "Han Xie", "Zhen Xiang", "Jiaying Lu", "Darren Liu", "Delgersuren Bold", "Bo Li", "Xiao Hu", "Carl Yang"], "title": "Large Language Model Empowered Privacy-Protected Framework for PHI Annotation in Clinical Notes", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": "Shorter version published in MedInfo 2025", "summary": "The de-identification of private information in medical data is a crucial\nprocess to mitigate the risk of confidentiality breaches, particularly when\npatient personal details are not adequately removed before the release of\nmedical records. Although rule-based and learning-based methods have been\nproposed, they often struggle with limited generalizability and require\nsubstantial amounts of annotated data for effective performance. Recent\nadvancements in large language models (LLMs) have shown significant promise in\naddressing these issues due to their superior language comprehension\ncapabilities. However, LLMs present challenges, including potential privacy\nrisks when using commercial LLM APIs and high computational costs for deploying\nopen-source LLMs locally. In this work, we introduce LPPA, an LLM-empowered\nPrivacy-Protected PHI Annotation framework for clinical notes, targeting the\nEnglish language. By fine-tuning LLMs locally with synthetic notes, LPPA\nensures strong privacy protection and high PHI annotation accuracy. Extensive\nexperiments demonstrate LPPA's effectiveness in accurately de-identifying\nprivate information, offering a scalable and efficient solution for enhancing\npatient privacy protection."}
{"id": "2504.19436", "pdf": "https://arxiv.org/pdf/2504.19436", "abs": "https://arxiv.org/abs/2504.19436", "authors": ["Jacky He", "Guiran Liu", "Binrong Zhu", "Hanlu Zhang", "Hongye Zheng", "Xiaokai Wang"], "title": "Context-Guided Dynamic Retrieval for Improving Generation Quality in RAG Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This paper focuses on the dynamic optimization of the Retrieval-Augmented\nGeneration (RAG) architecture. It proposes a state-aware dynamic knowledge\nretrieval mechanism to enhance semantic understanding and knowledge scheduling\nefficiency in large language models for open-domain question answering and\ncomplex generation tasks. The method introduces a multi-level perceptive\nretrieval vector construction strategy and a differentiable document matching\npath. These components enable end-to-end joint training and collaborative\noptimization of the retrieval and generation modules. This effectively\naddresses the limitations of static RAG structures in context adaptation and\nknowledge access. Experiments are conducted on the Natural Questions dataset.\nThe proposed structure is thoroughly evaluated across different large models,\nincluding GPT-4, GPT-4o, and DeepSeek. Comparative and ablation experiments\nfrom multiple perspectives confirm the significant improvements in BLEU and\nROUGE-L scores. The approach also demonstrates stronger robustness and\ngeneration consistency in tasks involving semantic ambiguity and multi-document\nfusion. These results highlight its broad application potential and practical\nvalue in building high-quality language generation systems."}
{"id": "2504.19198", "pdf": "https://arxiv.org/pdf/2504.19198", "abs": "https://arxiv.org/abs/2504.19198", "authors": ["Lingtao Peng", "Liheng Bian"], "title": "Adaptive Dual-domain Learning for Underwater Image Enhancement", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted by AAAI 2025", "summary": "Recently, learning-based Underwater Image Enhancement (UIE) methods have\ndemonstrated promising performance. However, existing learning-based methods\nstill face two challenges. 1) They rarely consider the inconsistent degradation\nlevels in different spatial regions and spectral bands simultaneously. 2) They\ntreat all regions equally, ignoring that the regions with high-frequency\ndetails are more difficult to reconstruct. To address these challenges, we\npropose a novel UIE method based on spatial-spectral dual-domain adaptive\nlearning, termed SS-UIE. Specifically, we first introduce a spatial-wise\nMulti-scale Cycle Selective Scan (MCSS) module and a Spectral-Wise\nSelf-Attention (SWSA) module, both with linear complexity, and combine them in\nparallel to form a basic Spatial-Spectral block (SS-block). Benefiting from the\nglobal receptive field of MCSS and SWSA, SS-block can effectively model the\ndegradation levels of different spatial regions and spectral bands, thereby\nenabling degradation level-based dual-domain adaptive UIE. By stacking multiple\nSS-blocks, we build our SS-UIE network. Additionally, a Frequency-Wise Loss\n(FWL) is introduced to narrow the frequency-wise discrepancy and reinforce the\nmodel's attention on the regions with high-frequency details. Extensive\nexperiments validate that the SS-UIE technique outperforms state-of-the-art UIE\nmethods while requiring cheaper computational and memory costs."}
{"id": "2504.18574", "pdf": "https://arxiv.org/pdf/2504.18574", "abs": "https://arxiv.org/abs/2504.18574", "authors": ["Aviv Bick", "Eric Xing", "Albert Gu"], "title": "Understanding the Skill Gap in Recurrent Language Models: The Role of the Gather-and-Aggregate Mechanism", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "SSMs offer efficient processing of long sequences with fixed state sizes, but\nstruggle with algorithmic tasks like retrieving past context. In this work, we\nexamine how such in-context retrieval operates within Transformer- and\nSSM-based language models. We find that both architectures develop the same\nfundamental Gather-and-Aggregate (G&A) mechanism. A Gather Head first\nidentifies and extracts relevant information from the context, which an\nAggregate Head then integrates into a final representation. Across both model\ntypes, G&A concentrates in just a few heads, making them critical bottlenecks\neven for benchmarks that require a basic form of retrieval. For example,\ndisabling a single Gather or Aggregate Head of a pruned Llama-3.1-8B degrades\nits ability to retrieve the correct answer letter in MMLU, reducing accuracy\nfrom 66% to 25%. This finding suggests that in-context retrieval can obscure\nthe limited knowledge demands of certain tasks. Despite strong MMLU performance\nwith retrieval intact, the pruned model fails on other knowledge tests. Similar\nG&A dependencies exist in GSM8K, BBH, and dialogue tasks. Given the\nsignificance of G&A in performance, we show that retrieval challenges in SSMs\nmanifest in how they implement G&A, leading to smoother attention patterns\nrather than the sharp token transitions that effective G&A relies on. Thus,\nwhile a gap exists between Transformers and SSMs in implementing in-context\nretrieval, it is confined to a few heads, not the entire model. This insight\nsuggests a unified explanation for performance differences between Transformers\nand SSMs while also highlighting ways to combine their strengths. For example,\nin pretrained hybrid models, attention components naturally take on the role of\nAggregate Heads. Similarly, in a pretrained pure SSM, replacing a single G&A\nhead with an attention-based variant significantly improves retrieval."}
{"id": "2504.19445", "pdf": "https://arxiv.org/pdf/2504.19445", "abs": "https://arxiv.org/abs/2504.19445", "authors": ["Yi-Long Lu", "Chunhui Zhang", "Wei Wang"], "title": "Systematic Bias in Large Language Models: Discrepant Response Patterns in Binary vs. Continuous Judgment Tasks", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly used in tasks such as\npsychological text analysis and decision-making in automated workflows.\nHowever, their reliability remains a concern due to potential biases inherited\nfrom their training process. In this study, we examine how different response\nformat: binary versus continuous, may systematically influence LLMs' judgments.\nIn a value statement judgments task and a text sentiment analysis task, we\nprompted LLMs to simulate human responses and tested both formats across\nseveral models, including both open-source and commercial models. Our findings\nrevealed a consistent negative bias: LLMs were more likely to deliver\n\"negative\" judgments in binary formats compared to continuous ones. Control\nexperiments further revealed that this pattern holds across both tasks. Our\nresults highlight the importance of considering response format when applying\nLLMs to decision tasks, as small changes in task design can introduce\nsystematic biases."}
{"id": "2504.19210", "pdf": "https://arxiv.org/pdf/2504.19210", "abs": "https://arxiv.org/abs/2504.19210", "authors": ["Yuming Zhao", "Qijian Zhang", "Junhui Hou", "Jiazhi Xia", "Wenping Wang", "Ying He"], "title": "FlexPara: Flexible Neural Surface Parameterization", "categories": ["cs.CV"], "comment": null, "summary": "Surface parameterization is a fundamental geometry processing task, laying\nthe foundations for the visual presentation of 3D assets and numerous\ndownstream shape analysis scenarios. Conventional parameterization approaches\ndemand high-quality mesh triangulation and are restricted to certain simple\ntopologies unless additional surface cutting and decomposition are provided. In\npractice, the optimal configurations (e.g., type of parameterization domains,\ndistribution of cutting seams, number of mapping charts) may vary drastically\nwith different surface structures and task characteristics, thus requiring more\nflexible and controllable processing pipelines. To this end, this paper\nintroduces FlexPara, an unsupervised neural optimization framework to achieve\nboth global and multi-chart surface parameterizations by establishing\npoint-wise mappings between 3D surface points and adaptively-deformed 2D UV\ncoordinates. We ingeniously design and combine a series of\ngeometrically-interpretable sub-networks, with specific functionalities of\ncutting, deforming, unwrapping, and wrapping, to construct a bi-directional\ncycle mapping framework for global parameterization without the need for\nmanually specified cutting seams. Furthermore, we construct a multi-chart\nparameterization framework with adaptively-learned chart assignment. Extensive\nexperiments demonstrate the universality, superiority, and inspiring potential\nof our neural surface parameterization paradigm. The code will be publicly\navailable at https://github.com/AidenZhao/FlexPara"}
{"id": "2504.18575", "pdf": "https://arxiv.org/pdf/2504.18575", "abs": "https://arxiv.org/abs/2504.18575", "authors": ["Ivan Evtimov", "Arman Zharmagambetov", "Aaron Grattafiori", "Chuan Guo", "Kamalika Chaudhuri"], "title": "WASP: Benchmarking Web Agent Security Against Prompt Injection Attacks", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Web navigation AI agents use language-and-vision foundation models to enhance\nproductivity but these models are known to be susceptible to indirect prompt\ninjections that get them to follow instructions different from the legitimate\nuser's. Existing explorations of this threat applied to web agents often focus\non a single isolated adversarial goal, test with injected instructions that are\neither too easy or not truly malicious, and often give the adversary\nunreasonable access. In order to better focus adversarial research, we\nconstruct a new benchmark called WASP (Web Agent Security against Prompt\ninjection attacks) that introduces realistic web agent hijacking objectives and\nan isolated environment to test them in that does not affect real users or the\nlive web. As part of WASP, we also develop baseline attacks against three\npopular web agentic systems (VisualWebArena, Claude Computer Use, and Operator)\ninstantiated with various state-of-the-art models. Our evaluation shows that\neven AI agents backed by models with advanced reasoning capabilities and by\nmodels with instruction hierarchy mitigations are susceptible to low-effort\nhuman-written prompt injections. However, the realistic objectives in WASP also\nallow us to observe that agents are currently not capable enough to complete\nthe goals of attackers end-to-end. Agents begin executing the adversarial\ninstruction between 16 and 86% of the time but only achieve the goal between 0\nand 17% of the time. Based on these findings, we argue that adversarial\nresearchers should demonstrate stronger attacks that more consistently maintain\ncontrol over the agent given realistic constraints on the adversary's power."}
{"id": "2504.19457", "pdf": "https://arxiv.org/pdf/2504.19457", "abs": "https://arxiv.org/abs/2504.19457", "authors": ["Siyi Liu", "Kishaloy Halder", "Zheng Qi", "Wei Xiao", "Nikolaos Pappas", "Phu Mon Htut", "Neha Anna John", "Yassine Benajiba", "Dan Roth"], "title": "Towards Long Context Hallucination Detection", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious tasks. However, they are prone to contextual hallucination, generating\ninformation that is either unsubstantiated or contradictory to the given\ncontext. Although many studies have investigated contextual hallucinations in\nLLMs, addressing them in long-context inputs remains an open problem. In this\nwork, we take an initial step toward solving this problem by constructing a\ndataset specifically designed for long-context hallucination detection.\nFurthermore, we propose a novel architecture that enables pre-trained encoder\nmodels, such as BERT, to process long contexts and effectively detect\ncontextual hallucinations through a decomposition and aggregation mechanism.\nOur experimental results show that the proposed architecture significantly\noutperforms previous models of similar size as well as LLM-based models across\nvarious metrics, while providing substantially faster inference."}
{"id": "2504.19212", "pdf": "https://arxiv.org/pdf/2504.19212", "abs": "https://arxiv.org/abs/2504.19212", "authors": ["Tuan Nguyen", "Naseem Khan", "Issa Khalil"], "title": "CapsFake: A Multimodal Capsule Network for Detecting Instruction-Guided Deepfakes", "categories": ["cs.CV", "cs.AI"], "comment": "20 pages", "summary": "The rapid evolution of deepfake technology, particularly in\ninstruction-guided image editing, threatens the integrity of digital images by\nenabling subtle, context-aware manipulations. Generated conditionally from real\nimages and textual prompts, these edits are often imperceptible to both humans\nand existing detection systems, revealing significant limitations in current\ndefenses. We propose a novel multimodal capsule network, CapsFake, designed to\ndetect such deepfake image edits by integrating low-level capsules from visual,\ntextual, and frequency-domain modalities. High-level capsules, predicted\nthrough a competitive routing mechanism, dynamically aggregate local features\nto identify manipulated regions with precision. Evaluated on diverse datasets,\nincluding MagicBrush, Unsplash Edits, Open Images Edits, and Multi-turn Edits,\nCapsFake outperforms state-of-the-art methods by up to 20% in detection\naccuracy. Ablation studies validate its robustness, achieving detection rates\nabove 94% under natural perturbations and 96% against adversarial attacks, with\nexcellent generalization to unseen editing scenarios. This approach establishes\na powerful framework for countering sophisticated image manipulations."}
{"id": "2504.18587", "pdf": "https://arxiv.org/pdf/2504.18587", "abs": "https://arxiv.org/abs/2504.18587", "authors": ["Tianbing Xu"], "title": "Training Large Language Models to Reason via EM Policy Gradient", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Recently, foundation models such as OpenAI's O1 and O3, along with DeepSeek's\nR1, have demonstrated strong reasoning capacities and problem-solving skills\nacquired through large-scale reinforcement learning (RL), with wide\napplications in mathematics, coding, science, intelligent agents, and virtual\nassistants. In this work, we introduce an off-policy reinforcement learning\nalgorithm, EM Policy Gradient, aimed at enhancing LLM reasoning by optimizing\nexpected return over reasoning trajectories. We frame the reasoning task as an\nExpectation-Maximization (EM) optimization problem, alternating between\nsampling diverse rationale trajectories and performing reward-guided\nfine-tuning. Unlike PPO and GRPO, which rely on complex importance weights and\nheuristic clipping, our method provides a simpler, more principled off-policy\npolicy gradient approach, eliminating these complexities while maintaining\nstrong performance. We evaluate the effectiveness of EM Policy Gradient on the\nGSM8K and MATH (HARD) datasets, where it achieves performance comparable to or\nslightly surpassing the state-of-the-art GRPO, while offering additional\nadvantages in scalability, simplicity, and reasoning conciseness. Moreover,\nmodels fine-tuned with our method exhibit cognitive behaviors, such as\nsub-problem decomposition, self-verification, and backtracking, highlighting\nits potential to enhance both the interpretability and robustness of LLM\nreasoning."}
{"id": "2504.19467", "pdf": "https://arxiv.org/pdf/2504.19467", "abs": "https://arxiv.org/abs/2504.19467", "authors": ["Jiageng Wu", "Bowen Gu", "Ren Zhou", "Kevin Xie", "Doug Snyder", "Yixing Jiang", "Valentina Carducci", "Richard Wyss", "Rishi J Desai", "Emily Alsentzer", "Leo Anthony Celi", "Adam Rodman", "Sebastian Schneeweiss", "Jonathan H. Chen", "Santiago Romero-Brufau", "Kueiyu Joshua Lin", "Jie Yang"], "title": "BRIDGE: Benchmarking Large Language Models for Understanding Real-world Clinical Practice Text", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) hold great promise for medical applications and\nare evolving rapidly, with new models being released at an accelerated pace.\nHowever, current evaluations of LLMs in clinical contexts remain limited. Most\nexisting benchmarks rely on medical exam-style questions or PubMed-derived\ntext, failing to capture the complexity of real-world electronic health record\n(EHR) data. Others focus narrowly on specific application scenarios, limiting\ntheir generalizability across broader clinical use. To address this gap, we\npresent BRIDGE, a comprehensive multilingual benchmark comprising 87 tasks\nsourced from real-world clinical data sources across nine languages. We\nsystematically evaluated 52 state-of-the-art LLMs (including DeepSeek-R1,\nGPT-4o, Gemini, and Llama 4) under various inference strategies. With a total\nof 13,572 experiments, our results reveal substantial performance variation\nacross model sizes, languages, natural language processing tasks, and clinical\nspecialties. Notably, we demonstrate that open-source LLMs can achieve\nperformance comparable to proprietary models, while medically fine-tuned LLMs\nbased on older architectures often underperform versus updated general-purpose\nmodels. The BRIDGE and its corresponding leaderboard serve as a foundational\nresource and a unique reference for the development and evaluation of new LLMs\nin real-world clinical text understanding."}
{"id": "2504.19223", "pdf": "https://arxiv.org/pdf/2504.19223", "abs": "https://arxiv.org/abs/2504.19223", "authors": ["Alexander Baumann", "Leonardo Ayala", "Silvia Seidlitz", "Jan Sellner", "Alexander Studier-Fischer", "Berkin Özdemir", "Lena Maier-Hein", "Slobodan Ilic"], "title": "CARL: Camera-Agnostic Representation Learning for Spectral Image Analysis", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Spectral imaging offers promising applications across diverse domains,\nincluding medicine and urban scene understanding, and is already established as\na critical modality in remote sensing. However, variability in channel\ndimensionality and captured wavelengths among spectral cameras impede the\ndevelopment of AI-driven methodologies, leading to camera-specific models with\nlimited generalizability and inadequate cross-camera applicability. To address\nthis bottleneck, we introduce $\\textbf{CARL}$, a model for\n$\\textbf{C}$amera-$\\textbf{A}$gnostic $\\textbf{R}$epresentation\n$\\textbf{L}$earning across RGB, multispectral, and hyperspectral imaging\nmodalities. To enable the conversion of a spectral image with any channel\ndimensionality to a camera-agnostic embedding, we introduce wavelength\npositional encoding and a self-attention-cross-attention mechanism to compress\nspectral information into learned query representations. Spectral-spatial\npre-training is achieved with a novel spectral self-supervised JEPA-inspired\nstrategy tailored to CARL. Large-scale experiments across the domains of\nmedical imaging, autonomous driving, and satellite imaging demonstrate our\nmodel's unique robustness to spectral heterogeneity, outperforming on datasets\nwith simulated and real-world cross-camera spectral variations. The scalability\nand versatility of the proposed approach position our model as a backbone for\nfuture spectral foundation models."}
{"id": "2504.18588", "pdf": "https://arxiv.org/pdf/2504.18588", "abs": "https://arxiv.org/abs/2504.18588", "authors": ["YongHui Xia", "Lan Wang", "Hao Wu"], "title": "Dynamic QoS Prediction via a Non-Negative Tensor Snowflake Factorization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Dynamic quality of service (QoS) data exhibit rich temporal patterns in\nuser-service interactions, which are crucial for a comprehensive understanding\nof user behavior and service conditions in Web service. As the number of users\nand services increases, there is a large amount of unobserved QoS data, which\nsignificantly affects users'choice of services. To predict unobserved QoS data,\nwe propose a Non-negative Snowflake Factorization of tensors model. This method\ndesigns a snowflake core tensor to enhance the model's learning capability.\nAdditionally, it employs a single latent factor-based, nonnegative\nmultiplication update on tensor (SLF-NMUT) for parameter learning. Empirical\nresults demonstrate that the proposed model more accurately learns dynamic\nuser-service interaction patterns, thereby yielding improved predictions for\nmissing QoS data."}
{"id": "2504.19472", "pdf": "https://arxiv.org/pdf/2504.19472", "abs": "https://arxiv.org/abs/2504.19472", "authors": ["Siyi Liu", "Dan Roth"], "title": "Conflicts in Texts: Data, Implications and Challenges", "categories": ["cs.CL"], "comment": null, "summary": "As NLP models become increasingly integrated into real-world applications, it\nbecomes clear that there is a need to address the fact that models often rely\non and generate conflicting information. Conflicts could reflect the complexity\nof situations, changes that need to be explained and dealt with, difficulties\nin data annotation, and mistakes in generated outputs. In all cases,\ndisregarding the conflicts in data could result in undesired behaviors of\nmodels and undermine NLP models' reliability and trustworthiness. This survey\ncategorizes these conflicts into three key areas: (1) natural texts on the web,\nwhere factual inconsistencies, subjective biases, and multiple perspectives\nintroduce contradictions; (2) human-annotated data, where annotator\ndisagreements, mistakes, and societal biases impact model training; and (3)\nmodel interactions, where hallucinations and knowledge conflicts emerge during\ndeployment. While prior work has addressed some of these conflicts in\nisolation, we unify them under the broader concept of conflicting information,\nanalyze their implications, and discuss mitigation strategies. We highlight key\nchallenges and future directions for developing conflict-aware NLP systems that\ncan reason over and reconcile conflicting information more effectively."}
{"id": "2504.19227", "pdf": "https://arxiv.org/pdf/2504.19227", "abs": "https://arxiv.org/abs/2504.19227", "authors": ["Shalini Maiti", "Lourdes Agapito", "Benjamin Graham"], "title": "Unsupervised 2D-3D lifting of non-rigid objects using local constraints", "categories": ["cs.CV"], "comment": null, "summary": "For non-rigid objects, predicting the 3D shape from 2D keypoint observations\nis ill-posed due to occlusions, and the need to disentangle changes in\nviewpoint and changes in shape. This challenge has often been addressed by\nembedding low-rank constraints into specialized models. These models can be\nhard to train, as they depend on finding a canonical way of aligning\nobservations, before they can learn detailed geometry. These constraints have\nlimited the reconstruction quality. We show that generic, high capacity models,\ntrained with an unsupervised loss, allow for more accurate predicted shapes. In\nparticular, applying low-rank constraints to localized subsets of the full\nshape allows the high capacity to be suitably constrained. We reduce the\nstate-of-the-art reconstruction error on the S-Up3D dataset by over 70%."}
{"id": "2504.18590", "pdf": "https://arxiv.org/pdf/2504.18590", "abs": "https://arxiv.org/abs/2504.18590", "authors": ["Guillaume Lauga", "Maël Chaumette", "Edgar Desainte-Maréville", "Étienne Lasalle", "Arthur Lebeurrier"], "title": "A multilevel approach to accelerate the training of Transformers", "categories": ["cs.LG", "cs.AI", "math.OC"], "comment": null, "summary": "In this article, we investigate the potential of multilevel approaches to\naccelerate the training of transformer architectures. Using an ordinary\ndifferential equation (ODE) interpretation of these architectures, we propose\nan appropriate way of varying the discretization of these ODE Transformers in\norder to accelerate the training. We validate our approach experimentally by a\ncomparison with the standard training procedure."}
{"id": "2504.19556", "pdf": "https://arxiv.org/pdf/2504.19556", "abs": "https://arxiv.org/abs/2504.19556", "authors": ["Kristen Sussman", "Daniel Carter"], "title": "Detecting Effects of AI-Mediated Communication on Language Complexity and Sentiment", "categories": ["cs.CL", "cs.HC", "J.4; K.4.0; I.2.7"], "comment": "5 pages, 3 figures, Companion Proceedings of the ACM Web Conference\n  2025", "summary": "Given the subtle human-like effects of large language models on linguistic\npatterns, this study examines shifts in language over time to detect the impact\nof AI-mediated communication (AI- MC) on social media. We compare a replicated\ndataset of 970,919 tweets from 2020 (pre-ChatGPT) with 20,000 tweets from the\nsame period in 2024, all of which mention Donald Trump during election periods.\nUsing a combination of Flesch-Kincaid readability and polarity scores, we\nanalyze changes in text complexity and sentiment. Our findings reveal a\nsignificant increase in mean sentiment polarity (0.12 vs. 0.04) and a shift\nfrom predominantly neutral content (54.8% in 2020 to 39.8% in 2024) to more\npositive expressions (28.6% to 45.9%). These findings suggest not only an\nincreasing presence of AI in social media communication but also its impact on\nlanguage and emotional expression patterns."}
{"id": "2504.19244", "pdf": "https://arxiv.org/pdf/2504.19244", "abs": "https://arxiv.org/abs/2504.19244", "authors": ["De Cheng", "Lingfeng He", "Nannan Wang", "Dingwen Zhang", "Xinbo Gao"], "title": "Semantic-Aligned Learning with Collaborative Refinement for Unsupervised VI-ReID", "categories": ["cs.CV"], "comment": "Accepted by IJCV 2025", "summary": "Unsupervised visible-infrared person re-identification (USL-VI-ReID) seeks to\nmatch pedestrian images of the same individual across different modalities\nwithout human annotations for model learning. Previous methods unify\npseudo-labels of cross-modality images through label association algorithms and\nthen design contrastive learning framework for global feature learning.\nHowever, these methods overlook the cross-modality variations in feature\nrepresentation and pseudo-label distributions brought by fine-grained patterns.\nThis insight results in insufficient modality-shared learning when only global\nfeatures are optimized. To address this issue, we propose a Semantic-Aligned\nLearning with Collaborative Refinement (SALCR) framework, which builds up\noptimization objective for specific fine-grained patterns emphasized by each\nmodality, thereby achieving complementary alignment between the label\ndistributions of different modalities. Specifically, we first introduce a Dual\nAssociation with Global Learning (DAGI) module to unify the pseudo-labels of\ncross-modality instances in a bi-directional manner. Afterward, a Fine-Grained\nSemantic-Aligned Learning (FGSAL) module is carried out to explore part-level\nsemantic-aligned patterns emphasized by each modality from cross-modality\ninstances. Optimization objective is then formulated based on the\nsemantic-aligned features and their corresponding label space. To alleviate the\nside-effects arising from noisy pseudo-labels, we propose a Global-Part\nCollaborative Refinement (GPCR) module to mine reliable positive sample sets\nfor the global and part features dynamically and optimize the inter-instance\nrelationships. Extensive experiments demonstrate the effectiveness of the\nproposed method, which achieves superior performances to state-of-the-art\nmethods. Our code is available at\n\\href{https://github.com/FranklinLingfeng/code-for-SALCR}."}
{"id": "2504.18591", "pdf": "https://arxiv.org/pdf/2504.18591", "abs": "https://arxiv.org/abs/2504.18591", "authors": ["Giovanni Catalani", "Michael Bauerheim", "Frédéric Tost", "Xavier Bertrand", "Joseph Morlier"], "title": "Geometry aware inference of steady state PDEs using Equivariant Neural Fields representations", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Recent advances in Neural Fields have enabled powerful,\ndiscretization-invariant methods for learning neural operators that approximate\nsolutions of Partial Differential Equations (PDEs) on general geometries.\nBuilding on these developments, we introduce enf2enf, an encoder--decoder\nmethodology for predicting steady-state Partial Differential Equations with\nnon-parameterized geometric variability, based on recently proposed Equivariant\nNeural Field architectures. In enf2enf, input geometries are encoded into\nlatent point cloud embeddings that inherently preserve geometric grounding and\ncapture local phenomena. The resulting representations are then combined with\nglobal parameters and directly decoded into continuous output fields, thus\nefficiently modeling the coupling between geometry and physics. By leveraging\nthe inductive biases of locality and translation invariance, our approach is\nable to capture fine-scale physical features as well as complex shape\nvariations, thereby enhancing generalization and physical compliance. Extensive\nexperiments on a high-fidelity aerodynamic dataset, a hyper-elastic material\nbenchmark, and multi-element airfoil geometries, demonstrate that the proposed\nmodel achieves superior or competitive performance compared to state-of-the-art\ngraph based, operator learning, and neural field methods. Notably, our method\nsupports real time inference and zero-shot super-resolution, enabling efficient\ntraining on low-resolution meshes while maintaining high accuracy on full-scale\ndiscretizations."}
{"id": "2504.19565", "pdf": "https://arxiv.org/pdf/2504.19565", "abs": "https://arxiv.org/abs/2504.19565", "authors": ["Meng Xiao", "Xunxin Cai", "Chengrui Wang", "Yuanchun Zhou"], "title": "m-KAILIN: Knowledge-Driven Agentic Scientific Corpus Distillation Framework for Biomedical Large Language Models Training", "categories": ["cs.CL", "cs.AI", "q-bio.QM"], "comment": "22 pages, Large Language Model, Agentic AI, Dataset Distillation,\n  Multi-agent Collaboration", "summary": "The rapid progress of large language models (LLMs) in biomedical research has\nunderscored the limitations of existing open-source annotated scientific\ncorpora, which are often insufficient in quantity and quality. Addressing the\nchallenge posed by the complex hierarchy of biomedical knowledge, we propose a\nknowledge-driven, multi-agent framework for scientific corpus distillation\ntailored for LLM training in the biomedical domain. Central to our approach is\na collaborative multi-agent architecture, where specialized agents, each guided\nby the Medical Subject Headings (MeSH) hierarchy, work in concert to\nautonomously extract, synthesize, and self-evaluate high-quality textual data\nfrom vast scientific literature. These agents collectively generate and refine\ndomain-specific question-answer pairs, ensuring comprehensive coverage and\nconsistency with biomedical ontologies while minimizing manual involvement.\nExtensive experimental results show that language models trained on our\nmulti-agent distilled datasets achieve notable improvements in biomedical\nquestion-answering tasks, outperforming both strong life sciences LLM baselines\nand advanced proprietary models. Notably, our AI-Ready dataset enables\nLlama3-70B to surpass GPT-4 with MedPrompt and Med-PaLM-2, despite their larger\nscale. Detailed ablation studies and case analyses further validate the\neffectiveness and synergy of each agent within the framework, highlighting the\npotential of multi-agent collaboration in biomedical LLM training."}
{"id": "2504.19249", "pdf": "https://arxiv.org/pdf/2504.19249", "abs": "https://arxiv.org/abs/2504.19249", "authors": ["Loc Phuc Truong Nguyen", "Hung Truong Thanh Nguyen", "Hung Cao"], "title": "ODExAI: A Comprehensive Object Detection Explainable AI Evaluation", "categories": ["cs.CV"], "comment": null, "summary": "Explainable Artificial Intelligence (XAI) techniques for interpreting object\ndetection models remain in an early stage, with no established standards for\nsystematic evaluation. This absence of consensus hinders both the comparative\nanalysis of methods and the informed selection of suitable approaches. To\naddress this gap, we introduce the Object Detection Explainable AI Evaluation\n(ODExAI), a comprehensive framework designed to assess XAI methods in object\ndetection based on three core dimensions: localization accuracy, faithfulness\nto model behavior, and computational complexity. We benchmark a set of XAI\nmethods across two widely used object detectors (YOLOX and Faster R-CNN) and\nstandard datasets (MS-COCO and PASCAL VOC). Empirical results demonstrate that\nregion-based methods (e.g., D-CLOSE) achieve strong localization (PG = 88.49%)\nand high model faithfulness (OA = 0.863), though with substantial computational\noverhead (Time = 71.42s). On the other hand, CAM-based methods (e.g., G-CAME)\nachieve superior localization (PG = 96.13%) and significantly lower runtime\n(Time = 0.54s), but at the expense of reduced faithfulness (OA = 0.549). These\nfindings demonstrate critical trade-offs among existing XAI approaches and\nreinforce the need for task-specific evaluation when deploying them in object\ndetection pipelines. Our implementation and evaluation benchmarks are publicly\navailable at: https://github.com/Analytics-Everywhere-Lab/odexai."}
{"id": "2504.18593", "pdf": "https://arxiv.org/pdf/2504.18593", "abs": "https://arxiv.org/abs/2504.18593", "authors": ["Akram Shojaei", "Mehdi Delrobaei"], "title": "Severity Classification of Chronic Obstructive Pulmonary Disease in Intensive Care Units: A Semi-Supervised Approach Using MIMIC-III Dataset", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Chronic obstructive pulmonary disease (COPD) represents a significant global\nhealth burden, where precise severity assessment is particularly critical for\neffective clinical management in intensive care unit (ICU) settings. This study\nintroduces an innovative machine learning framework for COPD severity\nclassification utilizing the MIMIC-III critical care database, thereby\nexpanding the applications of artificial intelligence in critical care\nmedicine. Our research developed a robust classification model incorporating\nkey ICU parameters such as blood gas measurements and vital signs, while\nimplementing semi-supervised learning techniques to effectively utilize\nunlabeled data and enhance model performance. The random forest classifier\nemerged as particularly effective, demonstrating exceptional discriminative\ncapability with 92.51% accuracy and 0.98 ROC AUC in differentiating between\nmild-to-moderate and severe COPD cases. This machine learning approach provides\nclinicians with a practical, accurate, and efficient tool for rapid COPD\nseverity evaluation in ICU environments, with significant potential to improve\nboth clinical decision-making processes and patient outcomes. Future research\ndirections should prioritize external validation across diverse patient\npopulations and integration with clinical decision support systems to optimize\nCOPD management in critical care settings."}
{"id": "2504.19590", "pdf": "https://arxiv.org/pdf/2504.19590", "abs": "https://arxiv.org/abs/2504.19590", "authors": ["Israa Alsiyat"], "title": "Arabic Metaphor Sentiment Classification Using Semantic Information", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this paper, I discuss the testing of the Arabic Metaphor Corpus (AMC) [1]\nusing newly designed automatic tools for sentiment classification for AMC based\non semantic tags. The tool incorporates semantic emotional tags for sentiment\nclassification. I evaluate the tool using standard methods, which are F-score,\nrecall, and precision. The method is to show the impact of Arabic online\nmetaphors on sentiment through the newly designed tools. To the best of our\nknowledge, this is the first approach to conduct sentiment classification for\nArabic metaphors using semantic tags to find the impact of the metaphor."}
{"id": "2504.19256", "pdf": "https://arxiv.org/pdf/2504.19256", "abs": "https://arxiv.org/abs/2504.19256", "authors": ["Songsong Xiong", "Hamidreza Kasaei"], "title": "LM-MCVT: A Lightweight Multi-modal Multi-view Convolutional-Vision Transformer Approach for 3D Object Recognition", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "In human-centered environments such as restaurants, homes, and warehouses,\nrobots often face challenges in accurately recognizing 3D objects. These\nchallenges stem from the complexity and variability of these environments,\nincluding diverse object shapes. In this paper, we propose a novel Lightweight\nMulti-modal Multi-view Convolutional-Vision Transformer network (LM-MCVT) to\nenhance 3D object recognition in robotic applications. Our approach leverages\nthe Globally Entropy-based Embeddings Fusion (GEEF) method to integrate\nmulti-views efficiently. The LM-MCVT architecture incorporates pre- and\nmid-level convolutional encoders and local and global transformers to enhance\nfeature extraction and recognition accuracy. We evaluate our method on the\nsynthetic ModelNet40 dataset and achieve a recognition accuracy of 95.6% using\na four-view setup, surpassing existing state-of-the-art methods. To further\nvalidate its effectiveness, we conduct 5-fold cross-validation on the\nreal-world OmniObject3D dataset using the same configuration. Results\nconsistently show superior performance, demonstrating the method's robustness\nin 3D object recognition across synthetic and real-world 3D data."}
{"id": "2504.18594", "pdf": "https://arxiv.org/pdf/2504.18594", "abs": "https://arxiv.org/abs/2504.18594", "authors": ["Tongrui Su", "Qingbin Li", "Shengyu Zhu", "Wei Chen", "Xueqi Cheng"], "title": "A Simple DropConnect Approach to Transfer-based Targeted Attack", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We study the problem of transfer-based black-box attack, where adversarial\nsamples generated using a single surrogate model are directly applied to target\nmodels. Compared with untargeted attacks, existing methods still have lower\nAttack Success Rates (ASRs) in the targeted setting, i.e., the obtained\nadversarial examples often overfit the surrogate model but fail to mislead\nother models. In this paper, we hypothesize that the pixels or features in\nthese adversarial examples collaborate in a highly dependent manner to maximize\nthe success of an adversarial attack on the surrogate model, which we refer to\nas perturbation co-adaptation. Then, we propose to Mitigate perturbation\nCo-adaptation by DropConnect (MCD) to enhance transferability, by creating\ndiverse variants of surrogate model at each optimization iteration. We conduct\nextensive experiments across various CNN- and Transformer-based models to\ndemonstrate the effectiveness of MCD. In the challenging scenario of\ntransferring from a CNN-based model to Transformer-based models, MCD achieves\n13% higher average ASRs compared with state-of-the-art baselines. MCD boosts\nthe performance of self-ensemble methods by bringing in more diversification\nacross the variants while reserving sufficient semantic information for each\nvariant. In addition, MCD attains the highest performance gain when scaling the\ncompute of crafting adversarial examples."}
{"id": "2504.19606", "pdf": "https://arxiv.org/pdf/2504.19606", "abs": "https://arxiv.org/abs/2504.19606", "authors": ["Hieu-Dai Tran", "Duc-Vu Nguyen", "Ngan Luu-Thuy Nguyen"], "title": "Coreference Resolution for Vietnamese Narrative Texts", "categories": ["cs.CL"], "comment": "Accepted at PACLIC 2024", "summary": "Coreference resolution is a vital task in natural language processing (NLP)\nthat involves identifying and linking different expressions in a text that\nrefer to the same entity. This task is particularly challenging for Vietnamese,\na low-resource language with limited annotated datasets. To address these\nchallenges, we developed a comprehensive annotated dataset using narrative\ntexts from VnExpress, a widely-read Vietnamese online news platform. We\nestablished detailed guidelines for annotating entities, focusing on ensuring\nconsistency and accuracy. Additionally, we evaluated the performance of large\nlanguage models (LLMs), specifically GPT-3.5-Turbo and GPT-4, on this dataset.\nOur results demonstrate that GPT-4 significantly outperforms GPT-3.5-Turbo in\nterms of both accuracy and response consistency, making it a more reliable tool\nfor coreference resolution in Vietnamese."}
{"id": "2504.19258", "pdf": "https://arxiv.org/pdf/2504.19258", "abs": "https://arxiv.org/abs/2504.19258", "authors": ["Shuhao Kang", "Martin Y. Liao", "Yan Xia", "Olaf Wysocki", "Boris Jutzi", "Daniel Cremers"], "title": "OPAL: Visibility-aware LiDAR-to-OpenStreetMap Place Recognition via Adaptive Radial Fusion", "categories": ["cs.CV", "cs.RO"], "comment": "Technical report. 15 pages, 9 figures", "summary": "LiDAR place recognition is a critical capability for autonomous navigation\nand cross-modal localization in large-scale outdoor environments. Existing\napproaches predominantly depend on pre-built 3D dense maps or aerial imagery,\nwhich impose significant storage overhead and lack real-time adaptability. In\nthis paper, we propose OPAL, a novel network for LiDAR place recognition that\nleverages OpenStreetMap as a lightweight and up-to-date prior. Our key\ninnovation lies in bridging the domain disparity between sparse LiDAR scans and\nstructured OSM data through two carefully designed components: a cross-modal\nvisibility mask that identifies maximal observable regions from both modalities\nto guide feature learning, and an adaptive radial fusion module that\ndynamically consolidates multiscale radial features into discriminative global\ndescriptors. Extensive experiments on the augmented KITTI and KITTI-360\ndatasets demonstrate OPAL's superiority, achieving 15.98% higher recall at @1m\nthreshold for top-1 retrieved matches while operating at 12x faster inference\nspeeds compared to state-of-the-art approaches. Code and datasets are publicly\navailable at: https://github.com/WHU-USI3DV/OPAL ."}
{"id": "2504.18595", "pdf": "https://arxiv.org/pdf/2504.18595", "abs": "https://arxiv.org/abs/2504.18595", "authors": ["Uzma", "Fabien Cholet", "Domenic Quinn", "Cindy Smith", "Siming You", "William Sloan"], "title": "EnviroPiNet: A Physics-Guided AI Model for Predicting Biofilter Performance", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Environmental biotechnologies, such as drinking water biofilters, rely on\ncomplex interactions between microbial communities and their surrounding\nphysical-chemical environments. Predicting the performance of these systems is\nchallenging due to high-dimensional, sparse datasets that lack diversity and\nfail to fully capture system behaviour. Accurate predictive models require\ninnovative, science-guided approaches. In this study, we present the first\napplication of Buckingham Pi theory to modelling biofilter performance. This\ndimensionality reduction technique identifies meaningful, dimensionless\nvariables that enhance predictive accuracy and improve model interpretability.\nUsing these variables, we developed the Environmental Buckingham Pi Neural\nNetwork (EnviroPiNet), a physics-guided model benchmarked against traditional\ndata-driven methods, including Principal Component Analysis (PCA) and\nautoencoder neural networks. Our findings demonstrate that the EnviroPiNet\nmodel achieves an R^2 value of 0.9236 on the testing dataset, significantly\noutperforming PCA and autoencoder methods. The Buckingham Pi variables also\nprovide insights into the physical and chemical relationships governing\nbiofilter behaviour, with implications for system design and optimization. This\nstudy highlights the potential of combining physical principles with AI\napproaches to model complex environmental systems characterized by sparse,\nhigh-dimensional datasets."}
{"id": "2504.19627", "pdf": "https://arxiv.org/pdf/2504.19627", "abs": "https://arxiv.org/abs/2504.19627", "authors": ["Run Luo", "Renke Shan", "Longze Chen", "Ziqiang Liu", "Lu Wang", "Min Yang", "Xiaobo Xia"], "title": "VCM: Vision Concept Modeling Based on Implicit Contrastive Learning with Vision-Language Instruction Fine-Tuning", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "VCM", "summary": "Large Vision-Language Models (LVLMs) are pivotal for real-world AI tasks like\nembodied intelligence due to their strong vision-language reasoning abilities.\nHowever, current LVLMs process entire images at the token level, which is\ninefficient compared to humans who analyze information and generate content at\nthe conceptual level, extracting relevant visual concepts with minimal effort.\nThis inefficiency, stemming from the lack of a visual concept model, limits\nLVLMs' usability in real-world applications. To address this, we propose VCM,\nan end-to-end self-supervised visual concept modeling framework. VCM leverages\nimplicit contrastive learning across multiple sampled instances and\nvision-language fine-tuning to construct a visual concept model without\nrequiring costly concept-level annotations. Our results show that VCM\nsignificantly reduces computational costs (e.g., 85\\% fewer FLOPs for\nLLaVA-1.5-7B) while maintaining strong performance across diverse image\nunderstanding tasks. Moreover, VCM enhances visual encoders' capabilities in\nclassic visual concept perception tasks. Extensive quantitative and qualitative\nexperiments validate the effectiveness and efficiency of VCM."}
{"id": "2504.19261", "pdf": "https://arxiv.org/pdf/2504.19261", "abs": "https://arxiv.org/abs/2504.19261", "authors": ["Xiaofeng Jin", "Yan Fang", "Matteo Frosi", "Jianfei Ge", "Jiangjian Xiao", "Matteo Matteucci"], "title": "Rendering Anywhere You See: Renderability Field-guided Gaussian Splatting", "categories": ["cs.CV", "65D18, 68U05", "I.3.7; I.4.8"], "comment": "8 pages,8 figures", "summary": "Scene view synthesis, which generates novel views from limited perspectives,\nis increasingly vital for applications like virtual reality, augmented reality,\nand robotics. Unlike object-based tasks, such as generating 360{\\deg} views of\na car, scene view synthesis handles entire environments where non-uniform\nobservations pose unique challenges for stable rendering quality. To address\nthis issue, we propose a novel approach: renderability field-guided gaussian\nsplatting (RF-GS). This method quantifies input inhomogeneity through a\nrenderability field, guiding pseudo-view sampling to enhanced visual\nconsistency. To ensure the quality of wide-baseline pseudo-views, we train an\nimage restoration model to map point projections to visible-light styles.\nAdditionally, our validated hybrid data optimization strategy effectively fuses\ninformation of pseudo-view angles and source view textures. Comparative\nexperiments on simulated and real-world data show that our method outperforms\nexisting approaches in rendering stability."}
{"id": "2504.18596", "pdf": "https://arxiv.org/pdf/2504.18596", "abs": "https://arxiv.org/abs/2504.18596", "authors": ["Anantha Sharma", "Swetha Devabhaktuni", "Eklove Mohan"], "title": "Optimizing the Privacy-Utility Balance using Synthetic Data and Configurable Perturbation Pipelines", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG", "math.PR"], "comment": "18 pages, 8 figures, 5 tables", "summary": "This paper explores the strategic use of modern synthetic data generation and\nadvanced data perturbation techniques to enhance security, maintain analytical\nutility, and improve operational efficiency when managing large datasets, with\na particular focus on the Banking, Financial Services, and Insurance (BFSI)\nsector. We contrast these advanced methods encompassing generative models like\nGANs, sophisticated context-aware PII transformation, configurable statistical\nperturbation, and differential privacy with traditional anonymization\napproaches.\n  The goal is to create realistic, privacy-preserving datasets that retain high\nutility for complex machine learning tasks and analytics, a critical need in\nthe data-sensitive industries like BFSI, Healthcare, Retail, and\nTelecommunications. We discuss how these modern techniques potentially offer\nsignificant improvements in balancing privacy preservation while maintaining\ndata utility compared to older methods. Furthermore, we examine the potential\nfor operational gains, such as reduced overhead and accelerated analytics, by\nusing these privacy-enhanced datasets. We also explore key use cases where\nthese methods can mitigate regulatory risks and enable scalable, data-driven\ninnovation without compromising sensitive customer information."}
{"id": "2504.19645", "pdf": "https://arxiv.org/pdf/2504.19645", "abs": "https://arxiv.org/abs/2504.19645", "authors": ["Shadan Shukr Sabr", "Nazira Sabr Mustafa", "Talar Sabah Omar", "Salah Hwayyiz Rasool", "Nawzad Anwer Omer", "Darya Sabir Hamad", "Hemin Abdulhameed Shams", "Omer Mahmood Kareem", "Rozhan Noori Abdullah", "Khabat Atar Abdullah", "Mahabad Azad Mohammad", "Haneen Al-Raghefy", "Safar M. Asaad", "Sara Jamal Mohammed", "Twana Saeed Ali", "Fazil Shawrow", "Halgurd S. Maghdid"], "title": "A Comprehensive Part-of-Speech Tagging to Standardize Central-Kurdish Language: A Research Guide for Kurdish Natural Language Processing Tasks", "categories": ["cs.CL", "cs.AI", "K.5; K.7; J.7"], "comment": "25 pages, 4 figures, 2 tables", "summary": "- The field of natural language processing (NLP) has dramatically expanded\nwithin the last decade. Many human-being applications are conducted daily via\nNLP tasks, starting from machine translation, speech recognition, text\ngeneration and recommendations, Part-of-Speech tagging (POS), and Named-Entity\nRecognition (NER). However, low-resourced languages, such as the\nCentral-Kurdish language (CKL), mainly remain unexamined due to shortage of\nnecessary resources to support their development. The POS tagging task is the\nbase of other NLP tasks; for example, the POS tag set has been used to\nstandardized languages to provide the relationship between words among the\nsentences, followed by machine translation and text recommendation.\nSpecifically, for the CKL, most of the utilized or provided POS tagsets are\nneither standardized nor comprehensive. To this end, this study presented an\naccurate and comprehensive POS tagset for the CKL to provide better performance\nof the Kurdish NLP tasks. The article also collected most of the POS tags from\ndifferent studies as well as from Kurdish linguistic experts to standardized\npart-of-speech tags. The proposed POS tagset is designed to annotate a large\nCKL corpus and support Kurdish NLP tasks. The initial investigations of this\nstudy via comparison with the Universal Dependencies framework for standard\nlanguages, show that the proposed POS tagset can streamline or correct\nsentences more accurately for Kurdish NLP tasks."}
{"id": "2504.19266", "pdf": "https://arxiv.org/pdf/2504.19266", "abs": "https://arxiv.org/abs/2504.19266", "authors": ["Xiaofeng Jin", "Matteo Frosi", "Matteo Matteucci"], "title": "OpenFusion++: An Open-vocabulary Real-time Scene Understanding System", "categories": ["cs.CV", "68T45, 68U05", "I.2.10; I.4.8"], "comment": "8 pages, 9 figures", "summary": "Real-time open-vocabulary scene understanding is essential for efficient 3D\nperception in applications such as vision-language navigation, embodied\nintelligence, and augmented reality. However, existing methods suffer from\nimprecise instance segmentation, static semantic updates, and limited handling\nof complex queries. To address these issues, we present OpenFusion++, a\nTSDF-based real-time 3D semantic-geometric reconstruction system. Our approach\nrefines 3D point clouds by fusing confidence maps from foundational models,\ndynamically updates global semantic labels via an adaptive cache based on\ninstance area, and employs a dual-path encoding framework that integrates\nobject attributes with environmental context for precise query responses.\nExperiments on the ICL, Replica, ScanNet, and ScanNet++ datasets demonstrate\nthat OpenFusion++ significantly outperforms the baseline in both semantic\naccuracy and query responsiveness."}
{"id": "2504.18598", "pdf": "https://arxiv.org/pdf/2504.18598", "abs": "https://arxiv.org/abs/2504.18598", "authors": ["Qingyue Wang", "Qi Pang", "Xixun Lin", "Shuai Wang", "Daoyuan Wu"], "title": "BadMoE: Backdooring Mixture-of-Experts LLMs via Optimizing Routing Triggers and Infecting Dormant Experts", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Mixture-of-Experts (MoE) have emerged as a powerful architecture for\n  large language models (LLMs), enabling efficient scaling of model capacity\n  while maintaining manageable computational costs. The key advantage lies in\n  their ability to route different tokens to different ``expert'' networks\n  within the model, enabling specialization and efficient handling of diverse\n  input. However, the vulnerabilities of MoE-based LLMs still have barely been\n  studied, and the potential for backdoor attacks in this context remains\n  largely unexplored. This paper presents the first backdoor attack against\n  MoE-based LLMs where the attackers poison ``dormant experts'' (i.e.,\nunderutilized\n  experts) and activate them by optimizing routing triggers, thereby gaining\n  control over the model's output. We first rigorously prove the existence of a\nfew ``dominating\n  experts'' in MoE models, whose outputs can determine the overall MoE's\n  output. We also show that dormant experts can serve as dominating experts to\nmanipulate model predictions.\n  Accordingly, our attack, namely \\textsc{BadMoE}, exploits the unique\n  architecture of MoE models by 1) identifying dormant experts unrelated to the\ntarget task, 2)\n  constructing a routing-aware loss to optimize the activation triggers of\nthese experts, and 3) promoting dormant experts to dominating roles via\npoisoned training data."}
{"id": "2504.19669", "pdf": "https://arxiv.org/pdf/2504.19669", "abs": "https://arxiv.org/abs/2504.19669", "authors": ["Chen Su", "Yuanhe Tian", "Yan Song"], "title": "Multimodal Conditioned Diffusive Time Series Forecasting", "categories": ["cs.CL"], "comment": null, "summary": "Diffusion models achieve remarkable success in processing images and text,\nand have been extended to special domains such as time series forecasting\n(TSF). Existing diffusion-based approaches for TSF primarily focus on modeling\nsingle-modality numerical sequences, overlooking the rich multimodal\ninformation in time series data. To effectively leverage such information for\nprediction, we propose a multimodal conditioned diffusion model for TSF,\nnamely, MCD-TSF, to jointly utilize timestamps and texts as extra guidance for\ntime series modeling, especially for forecasting. Specifically, Timestamps are\ncombined with time series to establish temporal and semantic correlations among\ndifferent data points when aggregating information along the temporal\ndimension. Texts serve as supplementary descriptions of time series' history,\nand adaptively aligned with data points as well as dynamically controlled in a\nclassifier-free manner. Extensive experiments on real-world benchmark datasets\nacross eight domains demonstrate that the proposed MCD-TSF model achieves\nstate-of-the-art performance."}
{"id": "2504.19270", "pdf": "https://arxiv.org/pdf/2504.19270", "abs": "https://arxiv.org/abs/2504.19270", "authors": ["Chamin Hewa Koneputugodage", "Yizhak Ben-Shabat", "Sameera Ramasinghe", "Stephen Gould"], "title": "VI3NR: Variance Informed Initialization for Implicit Neural Representations", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Implicit Neural Representations (INRs) are a versatile and powerful tool for\nencoding various forms of data, including images, videos, sound, and 3D shapes.\nA critical factor in the success of INRs is the initialization of the network,\nwhich can significantly impact the convergence and accuracy of the learned\nmodel. Unfortunately, commonly used neural network initializations are not\nwidely applicable for many activation functions, especially those used by INRs.\nIn this paper, we improve upon previous initialization methods by deriving an\ninitialization that has stable variance across layers, and applies to any\nactivation function. We show that this generalizes many previous initialization\nmethods, and has even better stability for well studied activations. We also\nshow that our initialization leads to improved results with INR activation\nfunctions in multiple signal modalities. Our approach is particularly effective\nfor Gaussian INRs, where we demonstrate that the theory of our initialization\nmatches with task performance in multiple experiments, allowing us to achieve\nimprovements in image, audio, and 3D surface reconstruction."}
{"id": "2504.18601", "pdf": "https://arxiv.org/pdf/2504.18601", "abs": "https://arxiv.org/abs/2504.18601", "authors": ["Philipp Koralus"], "title": "The Philosophic Turn for AI Agents: Replacing centralized digital rhetoric with decentralized truth-seeking", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "In the face of rapidly advancing AI technology, individuals will increasingly\nrely on AI agents to navigate life's growing complexities, raising critical\nconcerns about maintaining both human agency and autonomy. This paper addresses\na fundamental dilemma posed by AI decision-support systems: the risk of either\nbecoming overwhelmed by complex decisions, thus losing agency, or having\nautonomy compromised by externally controlled choice architectures reminiscent\nof ``nudging'' practices. While the ``nudge'' framework, based on the use of\nchoice-framing to guide individuals toward presumed beneficial outcomes,\ninitially appeared to preserve liberty, at AI-driven scale, it threatens to\nerode autonomy. To counteract this risk, the paper proposes a philosophic turn\nin AI design. AI should be constructed to facilitate decentralized\ntruth-seeking and open-ended inquiry, mirroring the Socratic method of\nphilosophical dialogue. By promoting individual and collective adaptive\nlearning, such AI systems would empower users to maintain control over their\njudgments, augmenting their agency without undermining autonomy. The paper\nconcludes by outlining essential features for autonomy-preserving AI systems,\nsketching a path toward AI systems that enhance human judgment rather than\nundermine it."}
{"id": "2504.19675", "pdf": "https://arxiv.org/pdf/2504.19675", "abs": "https://arxiv.org/abs/2504.19675", "authors": ["Osma Suominen", "Juho Inkinen", "Mona Lehtinen"], "title": "Annif at SemEval-2025 Task 5: Traditional XMTC augmented by LLMs", "categories": ["cs.CL", "cs.AI", "cs.DL", "cs.IR", "cs.LG", "I.2.7"], "comment": "6 pages, 4 figures, submitted to SemEval-2025 workshop Task 5:\n  LLMs4Subjects", "summary": "This paper presents the Annif system in SemEval-2025 Task 5 (LLMs4Subjects),\nwhich focussed on subject indexing using large language models (LLMs). The task\nrequired creating subject predictions for bibliographic records from the\nbilingual TIBKAT database using the GND subject vocabulary. Our approach\ncombines traditional natural language processing and machine learning\ntechniques implemented in the Annif toolkit with innovative LLM-based methods\nfor translation and synthetic data generation, and merging predictions from\nmonolingual models. The system ranked first in the all-subjects category and\nsecond in the tib-core-subjects category in the quantitative evaluation, and\nfourth in qualitative evaluations. These findings demonstrate the potential of\ncombining traditional XMTC algorithms with modern LLM techniques to improve the\naccuracy and efficiency of subject indexing in multilingual contexts."}
{"id": "2504.19271", "pdf": "https://arxiv.org/pdf/2504.19271", "abs": "https://arxiv.org/abs/2504.19271", "authors": ["Athul M. Mathew", "Arshad Ali Khan", "Thariq Khalid", "Faroq AL-Tam", "Riad Souissi"], "title": "Leveraging Multi-Modal Saliency and Fusion for Gaze Target Detection", "categories": ["cs.CV"], "comment": "accepted at NeurIPS 2023 Gaze Meets ML Workshop", "summary": "Gaze target detection (GTD) is the task of predicting where a person in an\nimage is looking. This is a challenging task, as it requires the ability to\nunderstand the relationship between the person's head, body, and eyes, as well\nas the surrounding environment. In this paper, we propose a novel method for\nGTD that fuses multiple pieces of information extracted from an image. First,\nwe project the 2D image into a 3D representation using monocular depth\nestimation. We then extract a depth-infused saliency module map, which\nhighlights the most salient (\\textit{attention-grabbing}) regions in image for\nthe subject in consideration. We also extract face and depth modalities from\nthe image, and finally fuse all the extracted modalities to identify the gaze\ntarget. We quantitatively evaluated our method, including the ablation analysis\non three publicly available datasets, namely VideoAttentionTarget, GazeFollow\nand GOO-Real, and showed that it outperforms other state-of-the-art methods.\nThis suggests that our method is a promising new approach for GTD."}
{"id": "2504.18603", "pdf": "https://arxiv.org/pdf/2504.18603", "abs": "https://arxiv.org/abs/2504.18603", "authors": ["Iizalaarab Elhaimeur", "Nikos Chrisochoides"], "title": "Toward Personalizing Quantum Computing Education: An Evolutionary LLM-Powered Approach", "categories": ["cs.CY", "cs.AI", "cs.MA"], "comment": null, "summary": "Quantum computing education faces significant challenges due to its\ncomplexity and the limitations of current tools; this paper introduces a novel\nIntelligent Teaching Assistant for quantum computing education and details its\nevolutionary design process. The system combines a knowledge-graph-augmented\narchitecture with two specialized Large Language Model (LLM) agents: a Teaching\nAgent for dynamic interaction, and a Lesson Planning Agent for lesson plan\ngeneration. The system is designed to adapt to individual student needs, with\ninteractions meticulously tracked and stored in a knowledge graph. This graph\nrepresents student actions, learning resources, and relationships, aiming to\nenable reasoning about effective learning pathways. We describe the\nimplementation of the system, highlighting the challenges encountered and the\nsolutions implemented, including introducing a dual-agent architecture where\ntasks are separated, all coordinated through a central knowledge graph that\nmaintains system awareness, and a user-facing tag system intended to mitigate\nLLM hallucination and improve user control. Preliminary results illustrate the\nsystem's potential to capture rich interaction data, dynamically adapt lesson\nplans based on student feedback via a tag system in simulation, and facilitate\ncontext-aware tutoring through the integrated knowledge graph, though\nsystematic evaluation is required."}
{"id": "2504.19720", "pdf": "https://arxiv.org/pdf/2504.19720", "abs": "https://arxiv.org/abs/2504.19720", "authors": ["Ranran Zhen", "Juntao Li", "Yixin Ji", "Zhenlin Yang", "Tong Liu", "Qingrong Xia", "Xinyu Duan", "Zhefeng Wang", "Baoxing Huai", "Min Zhang"], "title": "Taming the Titans: A Survey of Efficient LLM Inference Serving", "categories": ["cs.CL", "cs.AI", "cs.DC", "cs.LG"], "comment": "work in progress;11 pages of main paper with 7 main figures, overall\n  20 pages", "summary": "Large Language Models (LLMs) for Generative AI have achieved remarkable\nprogress, evolving into sophisticated and versatile tools widely adopted across\nvarious domains and applications. However, the substantial memory overhead\ncaused by their vast number of parameters, combined with the high computational\ndemands of the attention mechanism, poses significant challenges in achieving\nlow latency and high throughput for LLM inference services. Recent\nadvancements, driven by groundbreaking research, have significantly accelerated\nprogress in this field. This paper provides a comprehensive survey of these\nmethods, covering fundamental instance-level approaches, in-depth cluster-level\nstrategies, emerging scenario directions, and other miscellaneous but important\nareas. At the instance level, we review model placement, request scheduling,\ndecoding length prediction, storage management, and the disaggregation\nparadigm. At the cluster level, we explore GPU cluster deployment,\nmulti-instance load balancing, and cloud service solutions. For emerging\nscenarios, we organize the discussion around specific tasks, modules, and\nauxiliary methods. To ensure a holistic overview, we also highlight several\nniche yet critical areas. Finally, we outline potential research directions to\nfurther advance the field of LLM inference serving."}
{"id": "2504.19279", "pdf": "https://arxiv.org/pdf/2504.19279", "abs": "https://arxiv.org/abs/2504.19279", "authors": ["Vita V. Vlasova", "Vladimir G. Kuzmin", "Maria S. Varetsa", "Natalia A. Ibragimova", "Oleg Y. Rogov", "Elena V. Lyapuntsova"], "title": "Optimal Hyperspectral Undersampling Strategy for Satellite Imaging", "categories": ["cs.CV"], "comment": "16 pages", "summary": "Hyperspectral image (HSI) classification presents significant challenges due\nto the high dimensionality, spectral redundancy, and limited labeled data\ntypically available in real-world applications. To address these issues and\noptimize classification performance, we propose a novel band selection strategy\nknown as Iterative Wavelet-based Gradient Sampling (IWGS). This method\nincrementally selects the most informative spectral bands by analyzing\ngradients within the wavelet-transformed domain, enabling efficient and\ntargeted dimensionality reduction. Unlike traditional selection methods, IWGS\nleverages the multi-resolution properties of wavelets to better capture subtle\nspectral variations relevant for classification. The iterative nature of the\napproach ensures that redundant or noisy bands are systematically excluded\nwhile maximizing the retention of discriminative features. We conduct\ncomprehensive experiments on two widely-used benchmark HSI datasets: Houston\n2013 and Indian Pines. Results demonstrate that IWGS consistently outperforms\nstate-of-the-art band selection and classification techniques in terms of both\naccuracy and computational efficiency. These improvements make our method\nespecially suitable for deployment in edge devices or other\nresource-constrained environments, where memory and processing power are\nlimited. In particular, IWGS achieved an overall accuracy up to 97.8% on Indian\nPines for selected classes, confirming its effectiveness and generalizability\nacross different HSI scenarios."}
{"id": "2504.18636", "pdf": "https://arxiv.org/pdf/2504.18636", "abs": "https://arxiv.org/abs/2504.18636", "authors": ["Lohith Srikanth Pentapalli", "Jon Salisbury", "Josette Riep", "Kelly Cohen"], "title": "A Gradient-Optimized TSK Fuzzy Framework for Explainable Phishing Detection", "categories": ["cs.CR", "cs.AI", "cs.LO"], "comment": "14 pages, 5 figures", "summary": "Phishing attacks represent an increasingly sophisticated and pervasive threat\nto individuals and organizations, causing significant financial losses,\nidentity theft, and severe damage to institutional reputations. Existing\nphishing detection methods often struggle to simultaneously achieve high\naccuracy and explainability, either failing to detect novel attacks or\noperating as opaque black-box models. To address this critical gap, we propose\na novel phishing URL detection system based on a first-order Takagi-Sugeno-Kang\n(TSK) fuzzy inference model optimized through gradient-based techniques. Our\napproach intelligently combines the interpretability and human-like reasoning\ncapabilities of fuzzy logic with the precision and adaptability provided by\ngradient optimization methods, specifically leveraging the Adam optimizer for\nefficient parameter tuning. Experiments conducted using a comprehensive dataset\nof over 235,000 URLs demonstrate rapid convergence, exceptional predictive\nperformance (accuracy averaging 99.95% across 5 cross-validation folds, with a\nperfect AUC i.e. 1.00). Furthermore, optimized fuzzy rules and membership\nfunctions improve interoperability, clearly indicating how the model makes\ndecisions - an essential feature for cybersecurity applications. This\nhigh-performance, transparent, and interpretable phishing detection framework\nsignificantly advances current cybersecurity defenses, providing practitioners\nwith accurate and explainable decision-making tools."}
{"id": "2504.19734", "pdf": "https://arxiv.org/pdf/2504.19734", "abs": "https://arxiv.org/abs/2504.19734", "authors": ["Ying Na", "Shihui Feng"], "title": "LLM-Assisted Automated Deductive Coding of Dialogue Data: Leveraging Dialogue-Specific Characteristics to Enhance Contextual Understanding", "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "Dialogue data has been a key source for understanding learning processes,\noffering critical insights into how students engage in collaborative\ndiscussions and how these interactions shape their knowledge construction. The\nadvent of Large Language Models (LLMs) has introduced promising opportunities\nfor advancing qualitative research, particularly in the automated coding of\ndialogue data. However, the inherent contextual complexity of dialogue presents\nunique challenges for these models, especially in understanding and\ninterpreting complex contextual information. This study addresses these\nchallenges by developing a novel LLM-assisted automated coding approach for\ndialogue data. The novelty of our proposed framework is threefold: 1) We\npredict the code for an utterance based on dialogue-specific characteristics --\ncommunicative acts and communicative events -- using separate prompts following\nthe role prompts and chain-of-thoughts methods; 2) We engaged multiple LLMs\nincluding GPT-4-turbo, GPT-4o, DeepSeek in collaborative code prediction; 3) We\nleveraged the interrelation between events and acts to implement consistency\nchecking using GPT-4o. In particular, our contextual consistency checking\nprovided a substantial accuracy improvement. We also found the accuracy of act\npredictions was consistently higher than that of event predictions. This study\ncontributes a new methodological framework for enhancing the precision of\nautomated coding of dialogue data as well as offers a scalable solution for\naddressing the contextual challenges inherent in dialogue analysis."}
{"id": "2504.19289", "pdf": "https://arxiv.org/pdf/2504.19289", "abs": "https://arxiv.org/abs/2504.19289", "authors": ["Alexandra Malyugina", "Guoxi Huang", "Eduardo Ruiz", "Benjamin Leslie", "Nantheera Anantrasirichai"], "title": "Marine Snow Removal Using Internally Generated Pseudo Ground Truth", "categories": ["cs.CV"], "comment": null, "summary": "Underwater videos often suffer from degraded quality due to light absorption,\nscattering, and various noise sources. Among these, marine snow, which is\nsuspended organic particles appearing as bright spots or noise, significantly\nimpacts machine vision tasks, particularly those involving feature matching.\nExisting methods for removing marine snow are ineffective due to the lack of\npaired training data. To address this challenge, this paper proposes a novel\nenhancement framework that introduces a new approach for generating paired\ndatasets from raw underwater videos. The resulting dataset consists of paired\nimages of generated snowy and snow, free underwater videos, enabling supervised\ntraining for video enhancement. We describe the dataset creation process,\nhighlight its key characteristics, and demonstrate its effectiveness in\nenhancing underwater image restoration in the absence of ground truth."}
{"id": "2504.18658", "pdf": "https://arxiv.org/pdf/2504.18658", "abs": "https://arxiv.org/abs/2504.18658", "authors": ["Siddharth Singh", "Mahua Singh", "Abhinav Bhatele"], "title": "The Big Send-off: High Performance Collectives on GPU-based Supercomputers", "categories": ["cs.DC", "cs.AI", "cs.LG"], "comment": null, "summary": "We evaluate the current state of collective communication on GPU-based\nsupercomputers for large language model (LLM) training at scale. Existing\nlibraries such as RCCL and Cray-MPICH exhibit critical limitations on systems\nsuch as Frontier -- Cray-MPICH underutilizes network and compute resources,\nwhile RCCL suffers from severe scalability issues. To address these challenges,\nwe introduce PCCL, a communication library with highly optimized\nimplementations of all-gather and reduce-scatter operations tailored for\ndistributed deep learning workloads. PCCL is designed to maximally utilize all\navailable network and compute resources and to scale efficiently to thousands\nof GPUs. It achieves substantial performance improvements, delivering 6-33x\nspeedups over RCCL and 28-70x over Cray-MPICH for all-gather on 2048 GCDs of\nFrontier. These gains translate directly to end-to-end performance: in\nlarge-scale GPT-3-style training, PCCL provides up to 60% and 40% speedups over\nRCCL for 7B and 13B parameter models, respectively."}
{"id": "2504.19759", "pdf": "https://arxiv.org/pdf/2504.19759", "abs": "https://arxiv.org/abs/2504.19759", "authors": ["Huichi Zhou", "Zehao Xu", "Munan Zhao", "Kaihong Li", "Yiqiang Li", "Hongtao Wang"], "title": "Moral Reasoning Across Languages: The Critical Role of Low-Resource Languages in LLMs", "categories": ["cs.CL"], "comment": "5 pages, 2 figures", "summary": "In this paper, we introduce the Multilingual Moral Reasoning Benchmark (MMRB)\nto evaluate the moral reasoning abilities of large language models (LLMs)\nacross five typologically diverse languages and three levels of contextual\ncomplexity: sentence, paragraph, and document. Our results show moral reasoning\nperformance degrades with increasing context complexity, particularly for\nlow-resource languages such as Vietnamese. We further fine-tune the open-source\nLLaMA-3-8B model using curated monolingual data for alignment and poisoning.\nSurprisingly, low-resource languages have a stronger impact on multilingual\nreasoning than high-resource ones, highlighting their critical role in\nmultilingual NLP."}
{"id": "2504.19295", "pdf": "https://arxiv.org/pdf/2504.19295", "abs": "https://arxiv.org/abs/2504.19295", "authors": ["Kangbiao Shi", "Yixu Feng", "Tao Hu", "Yu Cao", "Peng Wu", "Yijin Liang", "Yanning Zhang", "Qingsen Yan"], "title": "FusionNet: Multi-model Linear Fusion Framework for Low-light Image Enhancement", "categories": ["cs.CV"], "comment": null, "summary": "The advent of Deep Neural Networks (DNNs) has driven remarkable progress in\nlow-light image enhancement (LLIE), with diverse architectures (e.g., CNNs and\nTransformers) and color spaces (e.g., sRGB, HSV, HVI) yielding impressive\nresults. Recent efforts have sought to leverage the complementary strengths of\nthese paradigms, offering promising solutions to enhance performance across\nvarying degradation scenarios. However, existing fusion strategies are hindered\nby challenges such as parameter explosion, optimization instability, and\nfeature misalignment, limiting further improvements. To overcome these issues,\nwe introduce FusionNet, a novel multi-model linear fusion framework that\noperates in parallel to effectively capture global and local features across\ndiverse color spaces. By incorporating a linear fusion strategy underpinned by\nHilbert space theoretical guarantees, FusionNet mitigates network collapse and\nreduces excessive training costs. Our method achieved 1st place in the CVPR2025\nNTIRE Low Light Enhancement Challenge. Extensive experiments conducted on\nsynthetic and real-world benchmark datasets demonstrate that the proposed\nmethod significantly outperforms state-of-the-art methods in terms of both\nquantitative and qualitative results, delivering robust enhancement under\ndiverse low-light conditions."}
{"id": "2504.18662", "pdf": "https://arxiv.org/pdf/2504.18662", "abs": "https://arxiv.org/abs/2504.18662", "authors": ["Daniel Sliwowski", "Dongheui Lee"], "title": "M2R2: MulitModal Robotic Representation for Temporal Action Segmentation", "categories": ["cs.RO", "cs.AI"], "comment": "8 pages, 6 figures, 2 tables", "summary": "Temporal action segmentation (TAS) has long been a key area of research in\nboth robotics and computer vision. In robotics, algorithms have primarily\nfocused on leveraging proprioceptive information to determine skill boundaries,\nwith recent approaches in surgical robotics incorporating vision. In contrast,\ncomputer vision typically relies on exteroceptive sensors, such as cameras.\nExisting multimodal TAS models in robotics integrate feature fusion within the\nmodel, making it difficult to reuse learned features across different models.\nMeanwhile, pretrained vision-only feature extractors commonly used in computer\nvision struggle in scenarios with limited object visibility. In this work, we\naddress these challenges by proposing M2R2, a multimodal feature extractor\ntailored for TAS, which combines information from both proprioceptive and\nexteroceptive sensors. We introduce a novel pretraining strategy that enables\nthe reuse of learned features across multiple TAS models. Our method achieves\nstate-of-the-art performance on the REASSEMBLE dataset, a challenging\nmultimodal robotic assembly dataset, outperforming existing robotic action\nsegmentation models by 46.6%. Additionally, we conduct an extensive ablation\nstudy to evaluate the contribution of different modalities in robotic TAS\ntasks."}
{"id": "2504.19811", "pdf": "https://arxiv.org/pdf/2504.19811", "abs": "https://arxiv.org/abs/2504.19811", "authors": ["Takuya Tamura", "Taro Yano", "Masafumi Enomoto", "Masafumi Oyamada"], "title": "Can a Crow Hatch a Falcon? Lineage Matters in Predicting Large Language Model Performance", "categories": ["cs.CL"], "comment": null, "summary": "Accurately forecasting the performance of Large Language Models (LLMs) before\nextensive fine-tuning or merging can substantially reduce both computational\nexpense and development time. Although prior approaches like scaling laws\naccount for global factors such as parameter size or training tokens, they\noften overlook explicit lineage relationships - i.e., which models are derived\nor merged from which parents. In this work, we propose a novel\nLineage-Regularized Matrix Factorization (LRMF) framework that encodes\nancestral ties among LLMs via a graph Laplacian regularizer. By leveraging\nmulti-hop parent-child connections, LRMF consistently outperforms conventional\nmatrix factorization and collaborative filtering methods in both instance-level\nand benchmark-level performance prediction. Our large-scale study includes\n2,934 publicly available Hugging Face models and 21,000+ instances across 6\nmajor benchmarks, showing that lineage constraints yield up to 7-10 percentage\npoints higher correlation with actual performance compared to baselines.\nMoreover, LRMF effectively addresses the cold-start problem, providing accurate\nestimates for newly derived or merged models even with minimal data. This\nlineage-guided strategy thus offers a resource-efficient way to inform\nhyperparameter tuning, data selection, and model combination in modern LLM\ndevelopment."}
{"id": "2504.19300", "pdf": "https://arxiv.org/pdf/2504.19300", "abs": "https://arxiv.org/abs/2504.19300", "authors": ["Ni Yao", "Xiangyu Liu", "Danyang Sun", "Chuang Han", "Yanting Li", "Jiaofen Nan", "Chengyang Li", "Fubao Zhu", "Weihua Zhou", "Chen Zhao"], "title": "Myocardial Region-guided Feature Aggregation Net for Automatic Coronary artery Segmentation and Stenosis Assessment using Coronary Computed Tomography Angiography", "categories": ["cs.CV"], "comment": "31 pages, 12 figures", "summary": "Coronary artery disease (CAD) remains a leading cause of mortality worldwide,\nrequiring accurate segmentation and stenosis detection using Coronary Computed\nTomography angiography (CCTA). Existing methods struggle with challenges such\nas low contrast, morphological variability and small vessel segmentation. To\naddress these limitations, we propose the Myocardial Region-guided Feature\nAggregation Net, a novel U-shaped dual-encoder architecture that integrates\nanatomical prior knowledge to enhance robustness in coronary artery\nsegmentation. Our framework incorporates three key innovations: (1) a\nMyocardial Region-guided Module that directs attention to coronary regions via\nmyocardial contour expansion and multi-scale feature fusion, (2) a Residual\nFeature Extraction Encoding Module that combines parallel spatial channel\nattention with residual blocks to enhance local-global feature discrimination,\nand (3) a Multi-scale Feature Fusion Module for adaptive aggregation of\nhierarchical vascular features. Additionally, Monte Carlo dropout f quantifies\nprediction uncertainty, supporting clinical interpretability. For stenosis\ndetection, a morphology-based centerline extraction algorithm separates the\nvascular tree into anatomical branches, enabling cross-sectional area\nquantification and stenosis grading. The superiority of MGFA-Net was\ndemonstrated by achieving an Dice score of 85.04%, an accuracy of 84.24%, an\nHD95 of 6.1294 mm, and an improvement of 5.46% in true positive rate for\nstenosis detection compared to3D U-Net. The integrated segmentation-to-stenosis\npipeline provides automated, clinically interpretable CAD assessment, bridging\ndeep learning with anatomical prior knowledge for precision medicine. Our code\nis publicly available at http://github.com/chenzhao2023/MGFA_CCTA"}
{"id": "2504.18684", "pdf": "https://arxiv.org/pdf/2504.18684", "abs": "https://arxiv.org/abs/2504.18684", "authors": ["Nader Zantout", "Haochen Zhang", "Pujith Kachana", "Jinkai Qiu", "Ji Zhang", "Wenshan Wang"], "title": "SORT3D: Spatial Object-centric Reasoning Toolbox for Zero-Shot 3D Grounding Using Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "7 pages, 6 figures, submitted to IROS 2025", "summary": "Interpreting object-referential language and grounding objects in 3D with\nspatial relations and attributes is essential for robots operating alongside\nhumans. However, this task is often challenging due to the diversity of scenes,\nlarge number of fine-grained objects, and complex free-form nature of language\nreferences. Furthermore, in the 3D domain, obtaining large amounts of natural\nlanguage training data is difficult. Thus, it is important for methods to learn\nfrom little data and zero-shot generalize to new environments. To address these\nchallenges, we propose SORT3D, an approach that utilizes rich object attributes\nfrom 2D data and merges a heuristics-based spatial reasoning toolbox with the\nability of large language models (LLMs) to perform sequential reasoning.\nImportantly, our method does not require text-to-3D data for training and can\nbe applied zero-shot to unseen environments. We show that SORT3D achieves\nstate-of-the-art performance on complex view-dependent grounding tasks on two\nbenchmarks. We also implement the pipeline to run real-time on an autonomous\nvehicle and demonstrate that our approach can be used for object-goal\nnavigation on previously unseen real-world environments. All source code for\nthe system pipeline is publicly released at https://github.com/nzantout/SORT3D ."}
{"id": "2504.19850", "pdf": "https://arxiv.org/pdf/2504.19850", "abs": "https://arxiv.org/abs/2504.19850", "authors": ["Kyo Gerrits", "Ana Guerberof-Arenas"], "title": "To MT or not to MT: An eye-tracking study on the reception by Dutch readers of different translation and creativity levels", "categories": ["cs.CL"], "comment": "This paper has been accepted to the MT Summit 2025 to be held in\n  Geneva on June 23-27 2025", "summary": "This article presents the results of a pilot study involving the reception of\na fictional short story translated from English into Dutch under four\nconditions: machine translation (MT), post-editing (PE), human translation (HT)\nand original source text (ST). The aim is to understand how creativity and\nerrors in different translation modalities affect readers, specifically\nregarding cognitive load. Eight participants filled in a questionnaire, read a\nstory using an eye-tracker, and conducted a retrospective think-aloud (RTA)\ninterview. The results show that units of creative potential (UCP) increase\ncognitive load and that this effect is highest for HT and lowest for MT; no\neffect of error was observed. Triangulating the data with RTAs leads us to\nhypothesize that the higher cognitive load in UCPs is linked to increases in\nreader enjoyment and immersion. The effect of translation creativity on\ncognitive load in different translation modalities at word-level is novel and\nopens up new avenues for further research. All the code and data are available\nat https://github.com/INCREC/Pilot_to_MT_or_not_to_MT"}
{"id": "2504.19327", "pdf": "https://arxiv.org/pdf/2504.19327", "abs": "https://arxiv.org/abs/2504.19327", "authors": ["Moulik Choraria", "Xinbo Wu", "Akhil Bhimaraju", "Nitesh Sekhar", "Yue Wu", "Xu Zhang", "Prateek Singhal", "Lav R. Varshney"], "title": "Platonic Grounding for Efficient Multimodal Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The hyperscaling of data and parameter count in Transformer-based models is\nyielding diminishing performance improvement, especially when weighed against\ntraining costs. Such plateauing indicates the importance of methods for more\nefficient finetuning and inference, while retaining similar performance. This\nis especially relevant for multimodal learning paradigms, where inference costs\nof processing multimodal tokens can determine the model's practical viability.\nAt the same time, research on representations and mechanistic interpretability\nhas improved our understanding of the inner workings of Transformer-based\nmodels; one such line of work reveals an implicit alignment in the deeper\nlayers of pretrained models, across modalities. Taking inspiration from this,\nwe motivate and propose a simple modification to existing multimodal frameworks\nthat rely on aligning pretrained models. We demonstrate that our approach\nmaintains and, in some cases, even improves performance of baseline methods\nwhile achieving significant gains in both training and inference-time compute.\nOur work also has implications for combining pretrained models into larger\nsystems efficiently."}
{"id": "2504.18689", "pdf": "https://arxiv.org/pdf/2504.18689", "abs": "https://arxiv.org/abs/2504.18689", "authors": ["Apoorva Beedu", "Irfan Essa"], "title": "HierSum: A Global and Local Attention Mechanism for Video Summarization", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Video summarization creates an abridged version (i.e., a summary) that\nprovides a quick overview of the video while retaining pertinent information.\nIn this work, we focus on summarizing instructional videos and propose a method\nfor breaking down a video into meaningful segments, each corresponding to\nessential steps in the video. We propose \\textbf{HierSum}, a hierarchical\napproach that integrates fine-grained local cues from subtitles with global\ncontextual information provided by video-level instructions. Our approach\nutilizes the ``most replayed\" statistic as a supervisory signal to identify\ncritical segments, thereby improving the effectiveness of the summary. We\nevaluate on benchmark datasets such as TVSum, BLiSS, Mr.HiSum, and the WikiHow\ntest set, and show that HierSum consistently outperforms existing methods in\nkey metrics such as F1-score and rank correlation. We also curate a new\nmulti-modal dataset using WikiHow and EHow videos and associated articles\ncontaining step-by-step instructions. Through extensive ablation studies, we\ndemonstrate that training on this dataset significantly enhances summarization\non the target datasets."}
{"id": "2504.19856", "pdf": "https://arxiv.org/pdf/2504.19856", "abs": "https://arxiv.org/abs/2504.19856", "authors": ["Anastasia Zhukova", "Christian E. Matt", "Terry Ruas", "Bela Gipp"], "title": "Efficient Domain-adaptive Continual Pretraining for the Process Industry in the German Language", "categories": ["cs.CL"], "comment": null, "summary": "Domain-adaptive continual pretraining (DAPT) is a state-of-the-art technique\nthat further trains a language model (LM) on its pretraining task, e.g.,\nlanguage masking. Although popular, it requires a significant corpus of\ndomain-related data, which is difficult to obtain for specific domains in\nlanguages other than English, such as the process industry in the German\nlanguage. This paper introduces an efficient approach called ICL-augmented\npretraining or ICL-APT that leverages in-context learning (ICL) and k-nearest\nneighbors (kNN) to augment target data with domain-related and in-domain texts,\nsignificantly reducing GPU time while maintaining strong model performance. Our\nresults show that this approach performs better than traditional DAPT by 3.5 of\nthe average IR metrics (e.g., mAP, MRR, and nDCG) and requires almost 4 times\nless computing time, providing a cost-effective solution for industries with\nlimited computational capacity. The findings highlight the broader\napplicability of this framework to other low-resource industries, making\nNLP-based solutions more accessible and feasible in production environments."}
{"id": "2504.19334", "pdf": "https://arxiv.org/pdf/2504.19334", "abs": "https://arxiv.org/abs/2504.19334", "authors": ["Sidharth Rai", "Aryan Dalal", "Riley Slichter", "Ajay Sharda"], "title": "Enhancing seeding efficiency using a computer vision system to monitor furrow quality in real-time", "categories": ["cs.CV"], "comment": null, "summary": "Effective seed sowing in precision agriculture is hindered by challenges such\nas residue accumulation, low soil temperatures, and hair pinning (crop residue\npushed in the trench by furrow opener), which obstruct optimal trench\nformation. Row cleaners are employed to mitigate these issues, but there is a\nlack of quantitative methods to assess trench cleanliness. In this study, a\nnovel computer vision-based method was developed to evaluate row cleaner\nperformance. Multiple air seeders were equipped with a video acquisition system\nto capture trench conditions after row cleaner operation, enabling an effective\ncomparison of the performance of each row cleaner. The captured data were used\nto develop a segmentation model that analyzed key elements such as soil, straw,\nand machinery. Using the results from the segmentation model, an objective\nmethod was developed to quantify row cleaner performance. The results\ndemonstrated the potential of this method to improve row cleaner selection and\nenhance seeding efficiency in precision agriculture."}
{"id": "2504.18691", "pdf": "https://arxiv.org/pdf/2504.18691", "abs": "https://arxiv.org/abs/2504.18691", "authors": ["Ali Alfageeh", "Sadegh AlMahdi Kazemi Zarkouei", "Daye Nam", "Daniel Prol", "Matin Amoozadeh", "Souti Chattopadhyay", "James Prather", "Paul Denny", "Juho Leinonen", "Michael Hilton", "Sruti Srinivasa Ragavan", "Mohammad Amin Alipour"], "title": "From Prompts to Propositions: A Logic-Based Lens on Student-LLM Interactions", "categories": ["cs.HC", "cs.AI", "cs.SE"], "comment": null, "summary": "Background and Context. The increasing integration of large language models\n(LLMs) in computing education presents an emerging challenge in understanding\nhow students use LLMs and craft prompts to solve computational tasks. Prior\nresearch has used both qualitative and quantitative methods to analyze\nprompting behavior, but these approaches lack scalability or fail to\neffectively capture the semantic evolution of prompts. Objective. In this\npaper, we investigate whether students prompts can be systematically analyzed\nusing propositional logic constraints. We examine whether this approach can\nidentify patterns in prompt evolution, detect struggling students, and provide\ninsights into effective and ineffective strategies. Method. We introduce\nPrompt2Constraints, a novel method that translates students prompts into\nlogical constraints. The constraints are able to represent the intent of the\nprompts in succinct and quantifiable ways. We used this approach to analyze a\ndataset of 1,872 prompts from 203 students solving introductory programming\ntasks. Findings. We find that while successful and unsuccessful attempts tend\nto use a similar number of constraints overall, when students fail, they often\nmodify their prompts more significantly, shifting problem-solving strategies\nmidway. We also identify points where specific interventions could be most\nhelpful to students for refining their prompts. Implications. This work offers\na new and scalable way to detect students who struggle in solving natural\nlanguage programming tasks. This work could be extended to investigate more\ncomplex tasks and integrated into programming tools to provide real-time\nsupport."}
{"id": "2504.19867", "pdf": "https://arxiv.org/pdf/2504.19867", "abs": "https://arxiv.org/abs/2504.19867", "authors": ["Ke Hong", "Lufang Chen", "Zhong Wang", "Xiuhong Li", "Qiuli Mao", "Jianping Ma", "Chao Xiong", "Guanyu Wu", "Buhe Han", "Guohao Dai", "Yun Liang", "Yu Wang"], "title": "semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated Computation and Unified Storage", "categories": ["cs.CL", "cs.DC", "cs.LG"], "comment": "18 pages, 16 figures", "summary": "Existing large language model (LLM) serving systems fall into two categories:\n1) a unified system where prefill phase and decode phase are co-located on the\nsame GPU, sharing the unified computational resource and storage, and 2) a\ndisaggregated system where the two phases are disaggregated to different GPUs.\nThe design of the disaggregated system addresses the latency interference and\nsophisticated scheduling issues in the unified system but leads to storage\nchallenges including 1) replicated weights for both phases that prevent\nflexible deployment, 2) KV cache transfer overhead between the two phases, 3)\nstorage imbalance that causes substantial wasted space of the GPU capacity, and\n4) suboptimal resource adjustment arising from the difficulties in migrating KV\ncache. Such storage inefficiency delivers poor serving performance under high\nrequest rates.\n  In this paper, we identify that the advantage of the disaggregated system\nlies in the disaggregated computation, i.e., partitioning the computational\nresource to enable the asynchronous computation of two phases. Thus, we propose\na novel LLM serving system, semi-PD, characterized by disaggregated computation\nand unified storage. In semi-PD, we introduce a computation resource controller\nto achieve disaggregated computation at the streaming multi-processor (SM)\nlevel, and a unified memory manager to manage the asynchronous memory access\nfrom both phases. semi-PD has a low-overhead resource adjustment mechanism\nbetween the two phases, and a service-level objective (SLO) aware dynamic\npartitioning algorithm to optimize the SLO attainment. Compared to\nstate-of-the-art systems, semi-PD maintains lower latency at higher request\nrates, reducing the average end-to-end latency per request by 1.27-2.58x on\nDeepSeek series models, and serves 1.55-1.72x more requests adhering to latency\nconstraints on Llama series models."}
{"id": "2504.19347", "pdf": "https://arxiv.org/pdf/2504.19347", "abs": "https://arxiv.org/abs/2504.19347", "authors": ["Rayson Laroca", "Marcelo dos Santos", "David Menotti"], "title": "Improving Small Drone Detection Through Multi-Scale Processing and Data Augmentation", "categories": ["cs.CV"], "comment": "Accepted for presentation at the International Joint Conference on\n  Neural Networks (IJCNN) 2025", "summary": "Detecting small drones, often indistinguishable from birds, is crucial for\nmodern surveillance. This work introduces a drone detection methodology built\nupon the medium-sized YOLOv11 object detection model. To enhance its\nperformance on small targets, we implemented a multi-scale approach in which\nthe input image is processed both as a whole and in segmented parts, with\nsubsequent prediction aggregation. We also utilized a copy-paste data\naugmentation technique to enrich the training dataset with diverse drone and\nbird examples. Finally, we implemented a post-processing technique that\nleverages frame-to-frame consistency to mitigate missed detections. The\nproposed approach attained a top-3 ranking in the 8th WOSDETC Drone-vsBird\nDetection Grand Challenge, held at the 2025 International Joint Conference on\nNeural Networks (IJCNN), showcasing its capability to detect drones in complex\nenvironments effectively."}
{"id": "2504.18693", "pdf": "https://arxiv.org/pdf/2504.18693", "abs": "https://arxiv.org/abs/2504.18693", "authors": ["Sina Gogani-Khiabani", "Varsha Dewangan", "Nina Olson", "Ashutosh Trivedi", "Saeid Tizpaz-Niari"], "title": "Technical Challenges in Maintaining Tax Prep Software with Large Language Models", "categories": ["cs.SE", "cs.AI", "https://www.irs.gov/statistics/fourteenth-annual-irs-tpc-joint-research-conference-on-tax-administration"], "comment": "Accepted to 14th Annual IRS/TPC Joint Research Conference on Tax\n  Administration (IRS-TPC 2024)", "summary": "As the US tax law evolves to adapt to ever-changing politico-economic\nrealities, tax preparation software plays a significant role in helping\ntaxpayers navigate these complexities. The dynamic nature of tax regulations\nposes a significant challenge to accurately and timely maintaining tax software\nartifacts. The state-of-the-art in maintaining tax prep software is\ntime-consuming and error-prone as it involves manual code analysis combined\nwith an expert interpretation of tax law amendments. We posit that the rigor\nand formality of tax amendment language, as expressed in IRS publications,\nmakes it amenable to automatic translation to executable specifications (code).\nOur research efforts focus on identifying, understanding, and tackling\ntechnical challenges in leveraging Large Language Models (LLMs), such as\nChatGPT and Llama, to faithfully extract code differentials from IRS\npublications and automatically integrate them with the prior version of the\ncode to automate tax prep software maintenance."}
{"id": "2504.19898", "pdf": "https://arxiv.org/pdf/2504.19898", "abs": "https://arxiv.org/abs/2504.19898", "authors": ["Mingqian He", "Fei Zhao", "Chonggang Lu", "Ziyan Liu", "Yue Wang", "Haofu Qian"], "title": "GenCLS++: Pushing the Boundaries of Generative Classification in LLMs Through Comprehensive SFT and RL Studies Across Diverse Datasets", "categories": ["cs.CL"], "comment": null, "summary": "As a fundamental task in machine learning, text classification plays a\ncrucial role in many areas. With the rapid scaling of Large Language Models\n(LLMs), particularly through reinforcement learning (RL), there is a growing\nneed for more capable discriminators. Consequently, advances in classification\nare becoming increasingly vital for enhancing the overall capabilities of LLMs.\nTraditional discriminative methods map text to labels but overlook LLMs'\nintrinsic generative strengths. Generative classification addresses this by\nprompting the model to directly output labels. However, existing studies still\nrely on simple SFT alone, seldom probing the interplay between training and\ninference prompts, and no work has systematically leveraged RL for generative\ntext classifiers and unified SFT, RL, and inference-time prompting in one\nframework. We bridge this gap with GenCLS++, a framework that jointly optimizes\nSFT and RL while systematically exploring five high-level strategy\ndimensions-in-context learning variants, category definitions, explicit\nuncertainty labels, semantically irrelevant numeric labels, and\nperplexity-based decoding-during both training and inference. After an SFT\n\"policy warm-up,\" we apply RL with a simple rule-based reward, yielding sizable\nextra gains. Across seven datasets, GenCLS++ achieves an average accuracy\nimprovement of 3.46% relative to the naive SFT baseline; on public datasets,\nthis improvement rises to 4.00%. Notably, unlike reasoning-intensive tasks that\nbenefit from explicit thinking processes, we find that classification tasks\nperform better without such reasoning steps. These insights into the role of\nexplicit reasoning provide valuable guidance for future LLM applications."}
{"id": "2504.19357", "pdf": "https://arxiv.org/pdf/2504.19357", "abs": "https://arxiv.org/abs/2504.19357", "authors": ["Jiahao Lu", "Chong Yin", "Silvia Ingala", "Kenny Erleben", "Michael Bachmann Nielsen", "Sune Darkner"], "title": "MERA: Multimodal and Multiscale Self-Explanatory Model with Considerably Reduced Annotation for Lung Nodule Diagnosis", "categories": ["cs.CV"], "comment": null, "summary": "Lung cancer, a leading cause of cancer-related deaths globally, emphasises\nthe importance of early detection for better patient outcomes. Pulmonary\nnodules, often early indicators of lung cancer, necessitate accurate, timely\ndiagnosis. Despite Explainable Artificial Intelligence (XAI) advances, many\nexisting systems struggle providing clear, comprehensive explanations,\nespecially with limited labelled data. This study introduces MERA, a Multimodal\nand Multiscale self-Explanatory model designed for lung nodule diagnosis with\nconsiderably Reduced Annotation requirements. MERA integrates unsupervised and\nweakly supervised learning strategies (self-supervised learning techniques and\nVision Transformer architecture for unsupervised feature extraction) and a\nhierarchical prediction mechanism leveraging sparse annotations via\nsemi-supervised active learning in the learned latent space. MERA explains its\ndecisions on multiple levels: model-level global explanations via semantic\nlatent space clustering, instance-level case-based explanations showing similar\ninstances, local visual explanations via attention maps, and concept\nexplanations using critical nodule attributes. Evaluations on the public LIDC\ndataset show MERA's superior diagnostic accuracy and self-explainability. With\nonly 1% annotated samples, MERA achieves diagnostic accuracy comparable to or\nexceeding state-of-the-art methods requiring full annotation. The model's\ninherent design delivers comprehensive, robust, multilevel explanations aligned\nclosely with clinical practice, enhancing trustworthiness and transparency.\nDemonstrated viability of unsupervised and weakly supervised learning lowers\nthe barrier to deploying diagnostic AI in broader medical domains. Our complete\ncode is open-source available: https://github.com/diku-dk/credanno."}
{"id": "2504.18710", "pdf": "https://arxiv.org/pdf/2504.18710", "abs": "https://arxiv.org/abs/2504.18710", "authors": ["Patrícia Muñoz Ewald"], "title": "Explicit neural network classifiers for non-separable data", "categories": ["cs.LG", "cs.AI", "math.OC", "stat.ML", "57R70, 62M45"], "comment": "10 pages", "summary": "We fully characterize a large class of feedforward neural networks in terms\nof truncation maps. As an application, we show how a ReLU neural network can\nimplement a feature map which separates concentric data."}
{"id": "2504.19940", "pdf": "https://arxiv.org/pdf/2504.19940", "abs": "https://arxiv.org/abs/2504.19940", "authors": ["Luigia Costabile", "Gian Marco Orlando", "Valerio La Gatta", "Vincenzo Moscato"], "title": "Assessing the Potential of Generative Agents in Crowdsourced Fact-Checking", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": null, "summary": "The growing spread of online misinformation has created an urgent need for\nscalable, reliable fact-checking solutions. Crowdsourced fact-checking - where\nnon-experts evaluate claim veracity - offers a cost-effective alternative to\nexpert verification, despite concerns about variability in quality and bias.\nEncouraged by promising results in certain contexts, major platforms such as X\n(formerly Twitter), Facebook, and Instagram have begun shifting from\ncentralized moderation to decentralized, crowd-based approaches.\n  In parallel, advances in Large Language Models (LLMs) have shown strong\nperformance across core fact-checking tasks, including claim detection and\nevidence evaluation. However, their potential role in crowdsourced workflows\nremains unexplored. This paper investigates whether LLM-powered generative\nagents - autonomous entities that emulate human behavior and decision-making -\ncan meaningfully contribute to fact-checking tasks traditionally reserved for\nhuman crowds. Using the protocol of La Barbera et al. (2024), we simulate\ncrowds of generative agents with diverse demographic and ideological profiles.\nAgents retrieve evidence, assess claims along multiple quality dimensions, and\nissue final veracity judgments.\n  Our results show that agent crowds outperform human crowds in truthfulness\nclassification, exhibit higher internal consistency, and show reduced\nsusceptibility to social and cognitive biases. Compared to humans, agents rely\nmore systematically on informative criteria such as Accuracy, Precision, and\nInformativeness, suggesting a more structured decision-making process. Overall,\nour findings highlight the potential of generative agents as scalable,\nconsistent, and less biased contributors to crowd-based fact-checking systems."}
{"id": "2504.19370", "pdf": "https://arxiv.org/pdf/2504.19370", "abs": "https://arxiv.org/abs/2504.19370", "authors": ["Jean-Rémy Conti", "Stéphan Clémençon"], "title": "Mitigating Bias in Facial Recognition Systems: Centroid Fairness Loss Optimization", "categories": ["cs.CV", "cs.AI", "cs.LG", "stat.ML"], "comment": "Accepted at both the AFME and RegML Workshops at NeurIPS 2024. A\n  preliminary version has been accepted for publication by Springer Nature, in\n  the context of the ICPR 2024 conference", "summary": "The urging societal demand for fair AI systems has put pressure on the\nresearch community to develop predictive models that are not only globally\naccurate but also meet new fairness criteria, reflecting the lack of disparate\nmistreatment with respect to sensitive attributes ($\\textit{e.g.}$ gender,\nethnicity, age). In particular, the variability of the errors made by certain\nFacial Recognition (FR) systems across specific segments of the population\ncompromises the deployment of the latter, and was judged unacceptable by\nregulatory authorities. Designing fair FR systems is a very challenging\nproblem, mainly due to the complex and functional nature of the performance\nmeasure used in this domain ($\\textit{i.e.}$ ROC curves) and because of the\nhuge heterogeneity of the face image datasets usually available for training.\nIn this paper, we propose a novel post-processing approach to improve the\nfairness of pre-trained FR models by optimizing a regression loss which acts on\ncentroid-based scores. Beyond the computational advantages of the method, we\npresent numerical experiments providing strong empirical evidence of the gain\nin fairness and of the ability to preserve global accuracy."}
{"id": "2504.18722", "pdf": "https://arxiv.org/pdf/2504.18722", "abs": "https://arxiv.org/abs/2504.18722", "authors": ["Aashutosh Nema", "Samaksh Gulati", "Evangelos Giakoumakis", "Bipana Thapaliya"], "title": "MODP: Multi Objective Directional Prompting", "categories": ["cs.CC", "cs.AI", "I.2.0; I.2.6; I.2.7; H.3.3"], "comment": "10 pages, 5 figures, submission to KDD 2025", "summary": "Recent advances in large language models (LLMs) have led to their popularity\nacross multiple use-cases. However, prompt engineering, the process for\noptimally utilizing such models, remains approximation-driven and subjective.\nMost of the current research on prompt engineering focuses on task-specific\noptimization, while neglecting the behavior of the LLM under consideration\nduring prompt development. This paper introduces MODP -- Multi Objective\nDirectional Prompting, a framework based on two key concepts: 1)\nmulti-objectivity: the importance of considering an LLM's intrinsic behavior as\nan additional objective in prompt development, and 2) directional prompting: a\nmetrics-driven method for prompt engineering to ensure development of robust\nand high-precision prompts. We demonstrate the effectiveness of our proposed\nideas on a summarization task, using a synthetically created dataset, achieving\na 26% performance gain over initial prompts. Finally, we apply MODP to develop\nprompts for Dell's Next Best Action support tool, which is now in production\nand is used by more than 10,000 internal support agents and serving millions of\ncustomers worldwide."}
{"id": "2504.19982", "pdf": "https://arxiv.org/pdf/2504.19982", "abs": "https://arxiv.org/abs/2504.19982", "authors": ["Emre Can Acikgoz", "Carl Guo", "Suvodip Dey", "Akul Datta", "Takyoung Kim", "Gokhan Tur", "Dilek Hakkani-Tür"], "title": "TD-EVAL: Revisiting Task-Oriented Dialogue Evaluation by Combining Turn-Level Precision with Dialogue-Level Comparisons", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Task-oriented dialogue (TOD) systems are experiencing a revolution driven by\nLarge Language Models (LLMs), yet the evaluation methodologies for these\nsystems remain insufficient for their growing sophistication. While traditional\nautomatic metrics effectively assessed earlier modular systems, they focus\nsolely on the dialogue level and cannot detect critical intermediate errors\nthat can arise during user-agent interactions. In this paper, we introduce\nTD-EVAL (Turn and Dialogue-level Evaluation), a two-step evaluation framework\nthat unifies fine-grained turn-level analysis with holistic dialogue-level\ncomparisons. At turn level, we evaluate each response along three TOD-specific\ndimensions: conversation cohesion, backend knowledge consistency, and policy\ncompliance. Meanwhile, we design TOD Agent Arena that uses pairwise comparisons\nto provide a measure of dialogue-level quality. Through experiments on MultiWOZ\n2.4 and {\\tau}-Bench, we demonstrate that TD-EVAL effectively identifies the\nconversational errors that conventional metrics miss. Furthermore, TD-EVAL\nexhibits better alignment with human judgments than traditional and LLM-based\nmetrics. These findings demonstrate that TD-EVAL introduces a new paradigm for\nTOD system evaluation, efficiently assessing both turn and system levels with a\nplug-and-play framework for future research."}
{"id": "2504.19390", "pdf": "https://arxiv.org/pdf/2504.19390", "abs": "https://arxiv.org/abs/2504.19390", "authors": ["Jakub Zadrożny", "Hakan Bilen"], "title": "HumMorph: Generalized Dynamic Human Neural Fields from Few Views", "categories": ["cs.CV"], "comment": "Project page: https://jakubzadrozny.github.io/hummorph", "summary": "We introduce HumMorph, a novel generalized approach to free-viewpoint\nrendering of dynamic human bodies with explicit pose control. HumMorph renders\na human actor in any specified pose given a few observed views (starting from\njust one) in arbitrary poses. Our method enables fast inference as it relies\nonly on feed-forward passes through the model. We first construct a coarse\nrepresentation of the actor in the canonical T-pose, which combines visual\nfeatures from individual partial observations and fills missing information\nusing learned prior knowledge. The coarse representation is complemented by\nfine-grained pixel-aligned features extracted directly from the observed views,\nwhich provide high-resolution appearance information. We show that HumMorph is\ncompetitive with the state-of-the-art when only a single input view is\navailable, however, we achieve results with significantly better visual quality\ngiven just 2 monocular observations. Moreover, previous generalized methods\nassume access to accurate body shape and pose parameters obtained using\nsynchronized multi-camera setups. In contrast, we consider a more practical\nscenario where these body parameters are noisily estimated directly from the\nobserved views. Our experimental results demonstrate that our architecture is\nmore robust to errors in the noisy parameters and clearly outperforms the state\nof the art in this setting."}
{"id": "2504.18727", "pdf": "https://arxiv.org/pdf/2504.18727", "abs": "https://arxiv.org/abs/2504.18727", "authors": ["Ali Rostami", "Z Xie", "A Ishino", "Y Yamakata", "K Aizawa", "Ramesh Jain"], "title": "World Food Atlas Project", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "A coronavirus pandemic is forcing people to be \"at home\" all over the world.\nIn a life of hardly ever going out, we would have realized how the food we eat\naffects our bodies. What can we do to know our food more and control it better?\nTo give us a clue, we are trying to build a World Food Atlas (WFA) that\ncollects all the knowledge about food in the world. In this paper, we present\ntwo of our trials. The first is the Food Knowledge Graph (FKG), which is a\ngraphical representation of knowledge about food and ingredient relationships\nderived from recipes and food nutrition data. The second is the FoodLog Athl\nand the RecipeLog that are applications for collecting people's detailed\nrecords about food habit. We also discuss several problems that we try to solve\nto build the WFA by integrating these two ideas."}
{"id": "2504.20000", "pdf": "https://arxiv.org/pdf/2504.20000", "abs": "https://arxiv.org/abs/2504.20000", "authors": ["Rishika Sen", "Sujoy Roychowdhury", "Sumit Soman", "H. G. Ranjani", "Srikhetra Mohanty"], "title": "Knowledge Distillation of Domain-adapted LLMs for Question-Answering in Telecom", "categories": ["cs.CL", "cs.IR", "cs.LG", "68T50", "I.2.7"], "comment": "10 pages, 4 figures, 3 tables", "summary": "Knowledge Distillation (KD) is one of the approaches to reduce the size of\nLarge Language Models (LLMs). A LLM with smaller number of model parameters\n(student) is trained to mimic the performance of a LLM of a larger size\n(teacher model) on a specific task. For domain-specific tasks, it is not clear\nif teacher or student model, or both, must be considered for domain adaptation.\nIn this work, we study this problem from perspective of telecom domain\nQuestion-Answering (QA) task. We systematically experiment with Supervised\nFine-tuning (SFT) of teacher only, SFT of student only and SFT of both prior to\nKD. We design experiments to study the impact of vocabulary (same and\ndifferent) and KD algorithms (vanilla KD and Dual Space KD, DSKD) on the\ndistilled model. Multi-faceted evaluation of the distillation using 14\ndifferent metrics (N-gram, embedding and LLM-based metrics) is considered.\nExperimental results show that SFT of teacher improves performance of distilled\nmodel when both models have same vocabulary, irrespective of algorithm and\nmetrics. Overall, SFT of both teacher and student results in better performance\nacross all metrics, although the statistical significance of the same depends\non the vocabulary of the teacher models."}
{"id": "2504.19398", "pdf": "https://arxiv.org/pdf/2504.19398", "abs": "https://arxiv.org/abs/2504.19398", "authors": ["Shuo Wang", "Weili Shi", "Shuai Yang", "Jiahao Cui", "Qinwei Guo"], "title": "Dynamic Arthroscopic Navigation System for Anterior Cruciate Ligament Reconstruction Based on Multi-level Memory Architecture", "categories": ["cs.CV", "I.4.9; I.2.10; J.3; I.4.8; I.5.4"], "comment": "28 pages, 13 figures", "summary": "This paper presents a dynamic arthroscopic navigation system based on\nmulti-level memory architecture for anterior cruciate ligament (ACL)\nreconstruction surgery. The system extends our previously proposed markerless\nnavigation method from static image matching to dynamic video sequence\ntracking. By integrating the Atkinson-Shiffrin memory model's three-level\narchitecture (sensory memory, working memory, and long-term memory), our system\nmaintains continuous tracking of the femoral condyle throughout the surgical\nprocedure, providing stable navigation support even in complex situations\ninvolving viewpoint changes, instrument occlusion, and tissue deformation.\nUnlike existing methods, our system operates in real-time on standard\narthroscopic equipment without requiring additional tracking hardware,\nachieving 25.3 FPS with a latency of only 39.5 ms, representing a 3.5-fold\nimprovement over our previous static system. For extended sequences (1000\nframes), the dynamic system maintained an error of 5.3 plus-minus 1.5 pixels,\ncompared to the static system's 12.6 plus-minus 3.7 pixels - an improvement of\napproximately 45 percent. For medium-length sequences (500 frames) and short\nsequences (100 frames), the system achieved approximately 35 percent and 19\npercent accuracy improvements, respectively. Experimental results demonstrate\nthe system overcomes limitations of traditional static matching methods,\nproviding new technical support for improving surgical precision in ACL\nreconstruction."}
{"id": "2504.18735", "pdf": "https://arxiv.org/pdf/2504.18735", "abs": "https://arxiv.org/abs/2504.18735", "authors": ["Tanvir Islam"], "title": "TLoRA: Tri-Matrix Low-Rank Adaptation of Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We propose TLoRA, a novel tri-matrix low-rank adaptation method that\ndecomposes weight updates into three matrices: two fixed random matrices and\none trainable matrix, combined with a learnable, layer-wise scaling factor.\nThis tri-matrix design enables TLoRA to achieve highly efficient parameter\nadaptation while introducing minimal additional computational overhead. Through\nextensive experiments on the GLUE benchmark, we demonstrate that TLoRA achieves\ncomparable performance to existing low-rank methods such as LoRA and\nAdapter-based techniques, while requiring significantly fewer trainable\nparameters. Analyzing the adaptation dynamics, we observe that TLoRA exhibits\nGaussian-like weight distributions, stable parameter norms, and scaling factor\nvariability across layers, further highlighting its expressive power and\nadaptability. Additionally, we show that TLoRA closely resembles LoRA in its\neigenvalue distributions, parameter norms, and cosine similarity of updates,\nunderscoring its ability to effectively approximate LoRA's adaptation behavior.\nOur results establish TLoRA as a highly efficient and effective fine-tuning\nmethod for LLMs, offering a significant step forward in resource-efficient\nmodel adaptation."}
{"id": "2504.20013", "pdf": "https://arxiv.org/pdf/2504.20013", "abs": "https://arxiv.org/abs/2504.20013", "authors": ["Beizhe Hu", "Qiang Sheng", "Juan Cao", "Yang Li", "Danding Wang"], "title": "LLM-Generated Fake News Induces Truth Decay in News Ecosystem: A Case Study on Neural News Recommendation", "categories": ["cs.CL", "cs.CY", "cs.IR"], "comment": "ACM SIGIR 2025 Full Paper", "summary": "Online fake news moderation now faces a new challenge brought by the\nmalicious use of large language models (LLMs) in fake news production. Though\nexisting works have shown LLM-generated fake news is hard to detect from an\nindividual aspect, it remains underexplored how its large-scale release will\nimpact the news ecosystem. In this study, we develop a simulation pipeline and\na dataset with ~56k generated news of diverse types to investigate the effects\nof LLM-generated fake news within neural news recommendation systems. Our\nfindings expose a truth decay phenomenon, where real news is gradually losing\nits advantageous position in news ranking against fake news as LLM-generated\nnews is involved in news recommendation. We further provide an explanation\nabout why truth decay occurs from a familiarity perspective and show the\npositive correlation between perplexity and news ranking. Finally, we discuss\nthe threats of LLM-generated fake news and provide possible countermeasures. We\nurge stakeholders to address this emerging challenge to preserve the integrity\nof news ecosystems."}
{"id": "2504.19402", "pdf": "https://arxiv.org/pdf/2504.19402", "abs": "https://arxiv.org/abs/2504.19402", "authors": ["Khoa Tuan Nguyen", "Francesca Tozzi", "Wouter Willaert", "Joris Vankerschaver", "Nikdokht Rashidian", "Wesley De Neve"], "title": "Boosting 3D Liver Shape Datasets with Diffusion Models and Implicit Neural Representations", "categories": ["cs.CV"], "comment": null, "summary": "While the availability of open 3D medical shape datasets is increasing,\noffering substantial benefits to the research community, we have found that\nmany of these datasets are, unfortunately, disorganized and contain artifacts.\nThese issues limit the development and training of robust models, particularly\nfor accurate 3D reconstruction tasks. In this paper, we examine the current\nstate of available 3D liver shape datasets and propose a solution using\ndiffusion models combined with implicit neural representations (INRs) to\naugment and expand existing datasets. Our approach utilizes the generative\ncapabilities of diffusion models to create realistic, diverse 3D liver shapes,\ncapturing a wide range of anatomical variations and addressing the problem of\ndata scarcity. Experimental results indicate that our method enhances dataset\ndiversity, providing a scalable solution to improve the accuracy and\nreliability of 3D liver reconstruction and generation in medical applications.\nFinally, we suggest that diffusion models can also be applied to other\ndownstream tasks in 3D medical imaging."}
{"id": "2504.18766", "pdf": "https://arxiv.org/pdf/2504.18766", "abs": "https://arxiv.org/abs/2504.18766", "authors": ["Wenjun Cao"], "title": "Dynamic Action Interpolation: A Universal Approach for Accelerating Reinforcement Learning with Expert Guidance", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement learning (RL) suffers from severe sample inefficiency,\nespecially during early training, requiring extensive environmental\ninteractions to perform competently. Existing methods tend to solve this by\nincorporating prior knowledge, but introduce significant architectural and\nimplementation complexity. We propose Dynamic Action Interpolation (DAI), a\nuniversal yet straightforward framework that interpolates expert and RL actions\nvia a time-varying weight $\\alpha(t)$, integrating into any Actor-Critic\nalgorithm with just a few lines of code and without auxiliary networks or\nadditional losses. Our theoretical analysis shows that DAI reshapes state\nvisitation distributions to accelerate value function learning while preserving\nconvergence guarantees. Empirical evaluations across MuJoCo continuous control\ntasks demonstrate that DAI improves early-stage performance by over 160\\% on\naverage and final performance by more than 50\\%, with the Humanoid task showing\na 4$\\times$ improvement early on and a 2$\\times$ gain at convergence. These\nresults challenge the assumption that complex architectural modifications are\nnecessary for sample-efficient reinforcement learning."}
{"id": "2504.20022", "pdf": "https://arxiv.org/pdf/2504.20022", "abs": "https://arxiv.org/abs/2504.20022", "authors": ["Pritika Rohera", "Chaitrali Ginimav", "Gayatri Sawant", "Raviraj Joshi"], "title": "Better To Ask in English? Evaluating Factual Accuracy of Multilingual LLMs in English and Low-Resource Languages", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Multilingual Large Language Models (LLMs) have demonstrated significant\neffectiveness across various languages, particularly in high-resource languages\nsuch as English. However, their performance in terms of factual accuracy across\nother low-resource languages, especially Indic languages, remains an area of\ninvestigation. In this study, we assess the factual accuracy of LLMs - GPT-4o,\nGemma-2-9B, Gemma-2-2B, and Llama-3.1-8B - by comparing their performance in\nEnglish and Indic languages using the IndicQuest dataset, which contains\nquestion-answer pairs in English and 19 Indic languages. By asking the same\nquestions in English and their respective Indic translations, we analyze\nwhether the models are more reliable for regional context questions in Indic\nlanguages or when operating in English. Our findings reveal that LLMs often\nperform better in English, even for questions rooted in Indic contexts.\nNotably, we observe a higher tendency for hallucination in responses generated\nin low-resource Indic languages, highlighting challenges in the multilingual\nunderstanding capabilities of current LLMs."}
{"id": "2504.19414", "pdf": "https://arxiv.org/pdf/2504.19414", "abs": "https://arxiv.org/abs/2504.19414", "authors": ["Sehyeong Jo", "Gangjae Jang", "Haesol Park"], "title": "GMAR: Gradient-Driven Multi-Head Attention Rollout for Vision Transformer Interpretability", "categories": ["cs.CV"], "comment": null, "summary": "The Vision Transformer (ViT) has made significant advancements in computer\nvision, utilizing self-attention mechanisms to achieve state-of-the-art\nperformance across various tasks, including image classification, object\ndetection, and segmentation. Its architectural flexibility and capabilities\nhave made it a preferred choice among researchers and practitioners. However,\nthe intricate multi-head attention mechanism of ViT presents significant\nchallenges to interpretability, as the underlying prediction process remains\nopaque. A critical limitation arises from an observation commonly noted in\ntransformer architectures: \"Not all attention heads are equally meaningful.\"\nOverlooking the relative importance of specific heads highlights the\nlimitations of existing interpretability methods. To address these challenges,\nwe introduce Gradient-Driven Multi-Head Attention Rollout (GMAR), a novel\nmethod that quantifies the importance of each attention head using\ngradient-based scores. These scores are normalized to derive a weighted\naggregate attention score, effectively capturing the relative contributions of\nindividual heads. GMAR clarifies the role of each head in the prediction\nprocess, enabling more precise interpretability at the head level. Experimental\nresults demonstrate that GMAR consistently outperforms traditional attention\nrollout techniques. This work provides a practical contribution to\ntransformer-based architectures, establishing a robust framework for enhancing\nthe interpretability of Vision Transformer models."}
{"id": "2504.18770", "pdf": "https://arxiv.org/pdf/2504.18770", "abs": "https://arxiv.org/abs/2504.18770", "authors": ["Manuel Weber", "Carly Beneke"], "title": "PyViT-FUSE: A Foundation Model for Multi-Sensor Earth Observation Data", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "11 pages, 13 figures, Published at ICLR 2025 - Machine Learning for\n  Remote Sensing (ML4RS) Workshop", "summary": "We propose PyViT-FUSE, a foundation model for earth observation data\nexplicitly designed to handle multi-modal imagery by learning to fuse an\narbitrary number of mixed-resolution input bands into a single representation\nthrough an attention mechanism. The learned patch tokens are further processed\nby a stack of vision transformers with a novel pyramidal structure. We train\nthe model on a globally sampled dataset in a self-supervised manner, leveraging\ncore concepts of the SwAV algorithm. We show the interpretability of the fusion\nmechanism by visualization of the attention scores and the models applicability\nto downstream tasks."}
{"id": "2504.20039", "pdf": "https://arxiv.org/pdf/2504.20039", "abs": "https://arxiv.org/abs/2504.20039", "authors": ["Roman Garipov", "Fedor Velikonivtsev", "Ruslan Svirschevski", "Vage Egiazarian", "Max Ryabinin"], "title": "AutoJudge: Judge Decoding Without Manual Annotation", "categories": ["cs.CL", "cs.LG"], "comment": "Preprint, Work in progress", "summary": "We introduce AutoJudge, a framework that accelerates large language model\n(LLM) inference with task-specific lossy speculative decoding. Instead of\nmatching the original model output distribution token-by-token, we identify\nwhich of the generated tokens affect the downstream quality of the generated\nresponse, relaxing the guarantee so that the \"unimportant\" tokens can be\ngenerated faster. Our approach relies on a semi-greedy search algorithm to test\nwhich of the mismatches between target and draft model should be corrected to\npreserve quality, and which ones may be skipped. We then train a lightweight\nclassifier based on existing LLM embeddings to predict, at inference time,\nwhich mismatching tokens can be safely accepted without compromising the final\nanswer quality. We test our approach with Llama 3.2 1B (draft) and Llama 3.1 8B\n(target) models on zero-shot GSM8K reasoning, where it achieves up to 1.5x more\naccepted tokens per verification cycle with under 1% degradation in answer\naccuracy compared to standard speculative decoding and over 2x with small loss\nin accuracy. When applied to the LiveCodeBench benchmark, our approach\nautomatically detects other, programming-specific important tokens and shows\nsimilar speedups, demonstrating its ability to generalize across tasks."}
{"id": "2504.19417", "pdf": "https://arxiv.org/pdf/2504.19417", "abs": "https://arxiv.org/abs/2504.19417", "authors": ["Dehao Yuan", "Cornelia Fermüller"], "title": "A Real-Time Event-Based Normal Flow Estimator", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents a real-time, asynchronous, event-based normal flow\nestimator. It follows the same algorithm as Learning Normal Flow Directly From\nEvent Neighborhoods, but with a more optimized implementation. The original\nmethod treats event slices as 3D point clouds, encodes each event's local\ngeometry into a fixed-length vector, and uses a multi-layer perceptron to\npredict normal flow. It constructs representations by multiplying an adjacency\nmatrix with a feature matrix, resulting in quadratic time complexity with\nrespect to the number of events. In contrast, we leverage the fact that event\ncoordinates are integers and reformulate the representation step as a pooling\noperation. This achieves the same effect as the adjacency matrix but with much\nlower computational cost. As a result, our method supports real-time normal\nflow prediction on event cameras. Our estimator uses 1 GB of CUDA memory and\nruns at 4 million normal flows per second on an RTX 3070, or 6 million per\nsecond on an RTX A5000. We release the CUDA implementation along with a Python\ninterface at https://github.com/dhyuan99/VecKM_flow_cpp."}
{"id": "2504.18781", "pdf": "https://arxiv.org/pdf/2504.18781", "abs": "https://arxiv.org/abs/2504.18781", "authors": ["Hassan Wasswa", "Timothy Lynar", "Aziida Nanyonga", "Hussein Abbass"], "title": "IoT Botnet Detection: Application of Vision Transformer to Classification of Network Flow Traffic", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Despite the demonstrated effectiveness of transformer models in NLP, and\nimage and video classification, the available tools for extracting features\nfrom captured IoT network flow packets fail to capture sequential patterns in\naddition to the absence of spatial patterns consequently limiting transformer\nmodel application. This work introduces a novel preprocessing method to adapt\ntransformer models, the vision transformer (ViT) in particular, for IoT botnet\nattack detection using network flow packets. The approach involves feature\nextraction from .pcap files and transforming each instance into a 1-channel 2D\nimage shape, enabling ViT-based classification. Also, the ViT model was\nenhanced to allow use any classifier besides Multilayer Perceptron (MLP) that\nwas deployed in the initial ViT paper. Models including the conventional feed\nforward Deep Neural Network (DNN), LSTM and Bidirectional-LSTM (BLSTM)\ndemonstrated competitive performance in terms of precision, recall, and\nF1-score for multiclass-based attack detection when evaluated on two IoT attack\ndatasets."}
{"id": "2504.18596", "pdf": "https://arxiv.org/pdf/2504.18596", "abs": "https://arxiv.org/abs/2504.18596", "authors": ["Anantha Sharma", "Swetha Devabhaktuni", "Eklove Mohan"], "title": "Optimizing the Privacy-Utility Balance using Synthetic Data and Configurable Perturbation Pipelines", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG", "math.PR"], "comment": "18 pages, 8 figures, 5 tables", "summary": "This paper explores the strategic use of modern synthetic data generation and\nadvanced data perturbation techniques to enhance security, maintain analytical\nutility, and improve operational efficiency when managing large datasets, with\na particular focus on the Banking, Financial Services, and Insurance (BFSI)\nsector. We contrast these advanced methods encompassing generative models like\nGANs, sophisticated context-aware PII transformation, configurable statistical\nperturbation, and differential privacy with traditional anonymization\napproaches.\n  The goal is to create realistic, privacy-preserving datasets that retain high\nutility for complex machine learning tasks and analytics, a critical need in\nthe data-sensitive industries like BFSI, Healthcare, Retail, and\nTelecommunications. We discuss how these modern techniques potentially offer\nsignificant improvements in balancing privacy preservation while maintaining\ndata utility compared to older methods. Furthermore, we examine the potential\nfor operational gains, such as reduced overhead and accelerated analytics, by\nusing these privacy-enhanced datasets. We also explore key use cases where\nthese methods can mitigate regulatory risks and enable scalable, data-driven\ninnovation without compromising sensitive customer information."}
{"id": "2504.19432", "pdf": "https://arxiv.org/pdf/2504.19432", "abs": "https://arxiv.org/abs/2504.19432", "authors": ["Zhe Dong", "Yuzhe Sun", "Tianzhu Liu", "Wangmeng Zuo", "Yanfeng Gu"], "title": "EarthMapper: Visual Autoregressive Models for Controllable Bidirectional Satellite-Map Translation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Satellite imagery and maps, as two fundamental data modalities in remote\nsensing, offer direct observations of the Earth's surface and\nhuman-interpretable geographic abstractions, respectively. The task of\nbidirectional translation between satellite images and maps (BSMT) holds\nsignificant potential for applications in urban planning and disaster response.\nHowever, this task presents two major challenges: first, the absence of precise\npixel-wise alignment between the two modalities substantially complicates the\ntranslation process; second, it requires achieving both high-level abstraction\nof geographic features and high-quality visual synthesis, which further\nelevates the technical complexity. To address these limitations, we introduce\nEarthMapper, a novel autoregressive framework for controllable bidirectional\nsatellite-map translation. EarthMapper employs geographic coordinate embeddings\nto anchor generation, ensuring region-specific adaptability, and leverages\nmulti-scale feature alignment within a geo-conditioned joint scale\nautoregression (GJSA) process to unify bidirectional translation in a single\ntraining cycle. A semantic infusion (SI) mechanism is introduced to enhance\nfeature-level consistency, while a key point adaptive guidance (KPAG) mechanism\nis proposed to dynamically balance diversity and precision during inference. We\nfurther contribute CNSatMap, a large-scale dataset comprising 302,132 precisely\naligned satellite-map pairs across 38 Chinese cities, enabling robust\nbenchmarking. Extensive experiments on CNSatMap and the New York dataset\ndemonstrate EarthMapper's superior performance, achieving significant\nimprovements in visual realism, semantic consistency, and structural fidelity\nover state-of-the-art methods. Additionally, EarthMapper excels in zero-shot\ntasks like in-painting, out-painting and coordinate-conditional generation,\nunderscoring its versatility."}
{"id": "2504.18800", "pdf": "https://arxiv.org/pdf/2504.18800", "abs": "https://arxiv.org/abs/2504.18800", "authors": ["Ryo Takizawa", "Satoshi Kodera", "Tempei Kabayama", "Ryo Matsuoka", "Yuta Ando", "Yuto Nakamura", "Haruki Settai", "Norihiko Takeda"], "title": "Video CLIP Model for Multi-View Echocardiography Interpretation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Echocardiography involves recording videos of the heart using ultrasound,\nenabling clinicians to evaluate its condition. Recent advances in large-scale\nvision-language models (VLMs) have garnered attention for automating the\ninterpretation of echocardiographic videos. However, most existing VLMs\nproposed for medical interpretation thus far rely on single-frame (i.e., image)\ninputs. Consequently, these image-based models often exhibit lower diagnostic\naccuracy for conditions identifiable through cardiac motion. Moreover,\nechocardiographic videos are recorded from various views that depend on the\ndirection of ultrasound emission, and certain views are more suitable than\nothers for interpreting specific conditions. Incorporating multiple views could\npotentially yield further improvements in accuracy. In this study, we developed\na video-language model that takes five different views and full video sequences\nas input, training it on pairs of echocardiographic videos and clinical reports\nfrom 60,747 cases. Our experiments demonstrate that this expanded approach\nachieves higher interpretation accuracy than models trained with only\nsingle-view videos or with still images."}
{"id": "2504.18748", "pdf": "https://arxiv.org/pdf/2504.18748", "abs": "https://arxiv.org/abs/2504.18748", "authors": ["Kaustubh D. Dhole", "Nikhita Vedula", "Saar Kuzi", "Giuseppe Castellucci", "Eugene Agichtein", "Shervin Malmasi"], "title": "Generative Product Recommendations for Implicit Superlative Queries", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "In Recommender Systems, users often seek the best products through indirect,\nvague, or under-specified queries, such as \"best shoes for trail running\". Such\nqueries, also referred to as implicit superlative queries, pose a significant\nchallenge for standard retrieval and ranking systems as they lack an explicit\nmention of attributes and require identifying and reasoning over complex\nfactors. We investigate how Large Language Models (LLMs) can generate implicit\nattributes for ranking as well as reason over them to improve product\nrecommendations for such queries. As a first step, we propose a novel\nfour-point schema for annotating the best product candidates for superlative\nqueries called SUPERB, paired with LLM-based product annotations. We then\nempirically evaluate several existing retrieval and ranking approaches on our\nnew dataset, providing insights and discussing their integration into\nreal-world e-commerce production systems."}
{"id": "2504.19443", "pdf": "https://arxiv.org/pdf/2504.19443", "abs": "https://arxiv.org/abs/2504.19443", "authors": ["Yejin Jeong", "Donghun Lee"], "title": "CLIP-KOA: Enhancing Knee Osteoarthritis Diagnosis with Multi-Modal Learning and Symmetry-Aware Loss Functions", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 2 figures", "summary": "Knee osteoarthritis (KOA) is a universal chronic musculoskeletal disorders\nworldwide, making early diagnosis crucial. Currently, the Kellgren and Lawrence\n(KL) grading system is widely used to assess KOA severity. However, its high\ninter-observer variability and subjectivity hinder diagnostic consistency. To\naddress these limitations, automated diagnostic techniques using deep learning\nhave been actively explored in recent years. In this study, we propose a\nCLIP-based framework (CLIP-KOA) to enhance the consistency and reliability of\nKOA grade prediction. To achieve this, we introduce a learning approach that\nintegrates image and text information and incorporate Symmetry Loss and\nConsistency Loss to ensure prediction consistency between the original and\nflipped images. CLIP-KOA achieves state-of-the-art accuracy of 71.86\\% on KOA\nseverity prediction task, and ablation studies show that CLIP-KOA has 2.36\\%\nimprovement in accuracy over the standard CLIP model due to our contribution.\nThis study shows a novel direction for data-driven medical prediction not only\nto improve reliability of fine-grained diagnosis and but also to explore\nmultimodal methods for medical image analysis. Our code is available at\nhttps://github.com/anonymized-link."}
{"id": "2504.18804", "pdf": "https://arxiv.org/pdf/2504.18804", "abs": "https://arxiv.org/abs/2504.18804", "authors": ["Jagrit Acharya", "Gouri Ginde"], "title": "Can We Enhance Bug Report Quality Using LLMs?: An Empirical Study of LLM-Based Bug Report Generation", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Bug reports contain the information developers need to triage and fix\nsoftware bugs. However, unclear, incomplete, or ambiguous information may lead\nto delays and excessive manual effort spent on bug triage and resolution. In\nthis paper, we explore whether Instruction fine-tuned Large Language Models\n(LLMs) can automatically transform casual, unstructured bug reports into\nhigh-quality, structured bug reports adhering to a standard template. We\nevaluate three open-source instruction-tuned LLMs (\\emph{Qwen 2.5, Mistral, and\nLlama 3.2}) against ChatGPT-4o, measuring performance on established metrics\nsuch as CTQRS, ROUGE, METEOR, and SBERT. Our experiments show that fine-tuned\nQwen 2.5 achieves a CTQRS score of \\textbf{77%}, outperforming both fine-tuned\nMistral (\\textbf{71%}), Llama 3.2 (\\textbf{63%}) and ChatGPT in 3-shot learning\n(\\textbf{75%}). Further analysis reveals that Llama 3.2 shows higher accuracy\nof detecting missing fields particularly Expected Behavior and Actual Behavior,\nwhile Qwen 2.5 demonstrates superior performance in capturing\nSteps-to-Reproduce, with an F1 score of 76%. Additional testing of the models\non other popular projects (e.g., Eclipse, GCC) demonstrates that our approach\ngeneralizes well, achieving up to \\textbf{70%} CTQRS in unseen projects' bug\nreports. These findings highlight the potential of instruction fine-tuning in\nautomating structured bug report generation, reducing manual effort for\ndevelopers and streamlining the software maintenance process."}
{"id": "2504.18919", "pdf": "https://arxiv.org/pdf/2504.18919", "abs": "https://arxiv.org/abs/2504.18919", "authors": ["Andrew M. Bean", "Rebecca Payne", "Guy Parsons", "Hannah Rose Kirk", "Juan Ciro", "Rafael Mosquera", "Sara Hincapié Monsalve", "Aruna S. Ekanayaka", "Lionel Tarassenko", "Luc Rocher", "Adam Mahdi"], "title": "Clinical knowledge in LLMs does not translate to human interactions", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "52 pages, 4 figures", "summary": "Global healthcare providers are exploring use of large language models (LLMs)\nto provide medical advice to the public. LLMs now achieve nearly perfect scores\non medical licensing exams, but this does not necessarily translate to accurate\nperformance in real-world settings. We tested if LLMs can assist members of the\npublic in identifying underlying conditions and choosing a course of action\n(disposition) in ten medical scenarios in a controlled study with 1,298\nparticipants. Participants were randomly assigned to receive assistance from an\nLLM (GPT-4o, Llama 3, Command R+) or a source of their choice (control). Tested\nalone, LLMs complete the scenarios accurately, correctly identifying conditions\nin 94.9% of cases and disposition in 56.3% on average. However, participants\nusing the same LLMs identified relevant conditions in less than 34.5% of cases\nand disposition in less than 44.2%, both no better than the control group. We\nidentify user interactions as a challenge to the deployment of LLMs for medical\nadvice. Standard benchmarks for medical knowledge and simulated patient\ninteractions do not predict the failures we find with human participants.\nMoving forward, we recommend systematic human user testing to evaluate\ninteractive capabilities prior to public deployments in healthcare."}
{"id": "2504.19455", "pdf": "https://arxiv.org/pdf/2504.19455", "abs": "https://arxiv.org/abs/2504.19455", "authors": ["Yuki Hirakawa", "Ryotaro Shimizu"], "title": "Masked Language Prompting for Generative Data Augmentation in Few-shot Fashion Style Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Constructing dataset for fashion style recognition is challenging due to the\ninherent subjectivity and ambiguity of style concepts. Recent advances in\ntext-to-image models have facilitated generative data augmentation by\nsynthesizing images from labeled data, yet existing methods based solely on\nclass names or reference captions often fail to balance visual diversity and\nstyle consistency. In this work, we propose \\textbf{Masked Language Prompting\n(MLP)}, a novel prompting strategy that masks selected words in a reference\ncaption and leverages large language models to generate diverse yet\nsemantically coherent completions. This approach preserves the structural\nsemantics of the original caption while introducing attribute-level variations\naligned with the intended style, enabling style-consistent and diverse image\ngeneration without fine-tuning. Experimental results on the FashionStyle14\ndataset demonstrate that our MLP-based augmentation consistently outperforms\nclass-name and caption-based baselines, validating its effectiveness for\nfashion style recognition under limited supervision."}
{"id": "2504.18805", "pdf": "https://arxiv.org/pdf/2504.18805", "abs": "https://arxiv.org/abs/2504.18805", "authors": ["Jong Inn Park", "Maanas Taneja", "Qianwen Wang", "Dongyeop Kang"], "title": "Stealing Creator's Workflow: A Creator-Inspired Agentic Framework with Iterative Feedback Loop for Improved Scientific Short-form Generation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Project page: https://minnesotanlp.github.io/scitalk-project-page/", "summary": "Generating engaging, accurate short-form videos from scientific papers is\nchallenging due to content complexity and the gap between expert authors and\nreaders. Existing end-to-end methods often suffer from factual inaccuracies and\nvisual artifacts, limiting their utility for scientific dissemination. To\naddress these issues, we propose SciTalk, a novel multi-LLM agentic framework,\ngrounding videos in various sources, such as text, figures, visual styles, and\navatars. Inspired by content creators' workflows, SciTalk uses specialized\nagents for content summarization, visual scene planning, and text and layout\nediting, and incorporates an iterative feedback mechanism where video agents\nsimulate user roles to give feedback on generated videos from previous\niterations and refine generation prompts. Experimental evaluations show that\nSciTalk outperforms simple prompting methods in generating scientifically\naccurate and engaging content over the refined loop of video generation.\nAlthough preliminary results are still not yet matching human creators'\nquality, our framework provides valuable insights into the challenges and\nbenefits of feedback-driven video generation. Our code, data, and generated\nvideos will be publicly available."}
{"id": "2504.18988", "pdf": "https://arxiv.org/pdf/2504.18988", "abs": "https://arxiv.org/abs/2504.18988", "authors": ["Saramsh Gautam", "Mahmood Jasim"], "title": "LINC: Supporting Language Independent Communication and Comprehension to Enhance Contribution in Multilingual Collaborative Meetings", "categories": ["cs.HC", "cs.CL", "H.5.3"], "comment": "19 pages, 4 figures. Multimodal system design and evaluation study", "summary": "Collaborative research often includes contributors with varied perspectives\nfrom diverse linguistic backgrounds. However, English as a Second Language\n(ESL) researchers often struggle to communicate during meetings in English and\ncomprehend discussions, leading to limited contribution. To investigate these\nchallenges, we surveyed 64 ESL researchers who frequently collaborate in\nmultilingual teams and identified four key design goals around participation,\ncomprehension, documentation, and feedback. Guided by these design goals, we\ndeveloped LINC, a multimodal Language INdependent Collaboration system with two\ncomponents: a real-time module for multilingual communication during meetings\nand a post-meeting dashboard for discussion analysis. We evaluated the system\nthrough a two-phased study with six triads of multilingual teams. We found that\nusing LINC, participants benefited from communicating in their preferred\nlanguage, recalled and reviewed actionable insights, and prepared for upcoming\nmeetings effectively. We discuss external factors that impact multilingual\nmeeting participation beyond language preferences and the implications of\nmultimodal systems in facilitating meetings in hybrid multilingual\ncollaborative settings beyond research."}
{"id": "2504.19475", "pdf": "https://arxiv.org/pdf/2504.19475", "abs": "https://arxiv.org/abs/2504.19475", "authors": ["Sonia Joseph", "Praneet Suresh", "Lorenz Hufe", "Edward Stevinson", "Robert Graham", "Yash Vadi", "Danilo Bzdok", "Sebastian Lapuschkin", "Lee Sharkey", "Blake Aaron Richards"], "title": "Prisma: An Open Source Toolkit for Mechanistic Interpretability in Vision and Video", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "4 pages, 3 figures, 9 tables. Oral and Tutorial at the CVPR\n  Mechanistic Interpretability for Vision (MIV) Workshop", "summary": "Robust tooling and publicly available pre-trained models have helped drive\nrecent advances in mechanistic interpretability for language models. However,\nsimilar progress in vision mechanistic interpretability has been hindered by\nthe lack of accessible frameworks and pre-trained weights. We present Prisma\n(Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an\nopen-source framework designed to accelerate vision mechanistic\ninterpretability research, providing a unified toolkit for accessing 75+ vision\nand video transformers; support for sparse autoencoder (SAE), transcoder, and\ncrosscoder training; a suite of 80+ pre-trained SAE weights; activation\ncaching, circuit analysis tools, and visualization tools; and educational\nresources. Our analysis reveals surprising findings, including that effective\nvision SAEs can exhibit substantially lower sparsity patterns than language\nSAEs, and that in some instances, SAE reconstructions can decrease model loss.\nPrisma enables new research directions for understanding vision model internals\nwhile lowering barriers to entry in this emerging field."}
{"id": "2504.18807", "pdf": "https://arxiv.org/pdf/2504.18807", "abs": "https://arxiv.org/abs/2504.18807", "authors": ["Siân Brooke"], "title": "Clones in the Machine: A Feminist Critique of Agency in Digital Cloning", "categories": ["cs.HC", "cs.AI"], "comment": "ACM CHI Conference on Human Factors in Computing Systems 2025", "summary": "This paper critiques digital cloning in academic research, highlighting how\nit exemplifies AI solutionism. Digital clones, which replicate user data to\nsimulate behavior, are often seen as scalable tools for behavioral insights.\nHowever, this framing obscures ethical concerns around consent, agency, and\nrepresentation. Drawing on feminist theories of agency, the paper argues that\ndigital cloning oversimplifies human complexity and risks perpetuating systemic\nbiases. To address these issues, it proposes decentralized data repositories\nand dynamic consent models, promoting ethical, context-aware AI practices that\nchallenge the reductionist logic of AI solutionism"}
{"id": "2504.19056", "pdf": "https://arxiv.org/pdf/2504.19056", "abs": "https://arxiv.org/abs/2504.19056", "authors": ["Mohammad Mahdi Abootorabi", "Omid Ghahroodi", "Pardis Sadat Zahraei", "Hossein Behzadasl", "Alireza Mirrokni", "Mobina Salimipanah", "Arash Rasouli", "Bahar Behzadipour", "Sara Azarnoush", "Benyamin Maleki", "Erfan Sadraiye", "Kiarash Kiani Feriz", "Mahdi Teymouri Nahad", "Ali Moghadasi", "Abolfazl Eshagh Abianeh", "Nizi Nazar", "Hamid R. Rabiee", "Mahdieh Soleymani Baghshah", "Meisam Ahmadi", "Ehsaneddin Asgari"], "title": "Generative AI for Character Animation: A Comprehensive Survey of Techniques, Applications, and Future Directions", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "50 main pages, 30 pages appendix, 21 figures, 8 tables, GitHub\n  Repository:\n  https://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey", "summary": "Generative AI is reshaping art, gaming, and most notably animation. Recent\nbreakthroughs in foundation and diffusion models have reduced the time and cost\nof producing animated content. Characters are central animation components,\ninvolving motion, emotions, gestures, and facial expressions. The pace and\nbreadth of advances in recent months make it difficult to maintain a coherent\nview of the field, motivating the need for an integrative review. Unlike\nearlier overviews that treat avatars, gestures, or facial animation in\nisolation, this survey offers a single, comprehensive perspective on all the\nmain generative AI applications for character animation. We begin by examining\nthe state-of-the-art in facial animation, expression rendering, image\nsynthesis, avatar creation, gesture modeling, motion synthesis, object\ngeneration, and texture synthesis. We highlight leading research, practical\ndeployments, commonly used datasets, and emerging trends for each area. To\nsupport newcomers, we also provide a comprehensive background section that\nintroduces foundational models and evaluation metrics, equipping readers with\nthe knowledge needed to enter the field. We discuss open challenges and map\nfuture research directions, providing a roadmap to advance AI-driven\ncharacter-animation technologies. This survey is intended as a resource for\nresearchers and developers entering the field of generative AI animation or\nadjacent fields. Resources are available at:\nhttps://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey."}
{"id": "2504.19478", "pdf": "https://arxiv.org/pdf/2504.19478", "abs": "https://arxiv.org/abs/2504.19478", "authors": ["Weitao Feng", "Hang Zhou", "Jing Liao", "Li Cheng", "Wenbo Zhou"], "title": "CasaGPT: Cuboid Arrangement and Scene Assembly for Interior Design", "categories": ["cs.CV"], "comment": null, "summary": "We present a novel approach for indoor scene synthesis, which learns to\narrange decomposed cuboid primitives to represent 3D objects within a scene.\nUnlike conventional methods that use bounding boxes to determine the placement\nand scale of 3D objects, our approach leverages cuboids as a straightforward\nyet highly effective alternative for modeling objects. This allows for compact\nscene generation while minimizing object intersections. Our approach, coined\nCasaGPT for Cuboid Arrangement and Scene Assembly, employs an autoregressive\nmodel to sequentially arrange cuboids, producing physically plausible scenes.\nBy applying rejection sampling during the fine-tuning stage to filter out\nscenes with object collisions, our model further reduces intersections and\nenhances scene quality. Additionally, we introduce a refined dataset,\n3DFRONT-NC, which eliminates significant noise presented in the original\ndataset, 3D-FRONT. Extensive experiments on the 3D-FRONT dataset as well as our\ndataset demonstrate that our approach consistently outperforms the\nstate-of-the-art methods, enhancing the realism of generated scenes, and\nproviding a promising direction for 3D scene synthesis."}
{"id": "2504.18810", "pdf": "https://arxiv.org/pdf/2504.18810", "abs": "https://arxiv.org/abs/2504.18810", "authors": ["Yifan Xie", "Fei Ma", "Yi Bin", "Ying He", "Fei Yu"], "title": "Audio-Driven Talking Face Video Generation with Joint Uncertainty Learning", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 7 figures", "summary": "Talking face video generation with arbitrary speech audio is a significant\nchallenge within the realm of digital human technology. The previous studies\nhave emphasized the significance of audio-lip synchronization and visual\nquality. Currently, limited attention has been given to the learning of visual\nuncertainty, which creates several issues in existing systems, including\ninconsistent visual quality and unreliable performance across different input\nconditions. To address the problem, we propose a Joint Uncertainty Learning\nNetwork (JULNet) for high-quality talking face video generation, which\nincorporates a representation of uncertainty that is directly related to visual\nerror. Specifically, we first design an uncertainty module to individually\npredict the error map and uncertainty map after obtaining the generated image.\nThe error map represents the difference between the generated image and the\nground truth image, while the uncertainty map is used to predict the\nprobability of incorrect estimates. Furthermore, to match the uncertainty\ndistribution with the error distribution through a KL divergence term, we\nintroduce a histogram technique to approximate the distributions. By jointly\noptimizing error and uncertainty, the performance and robustness of our model\ncan be enhanced. Extensive experiments demonstrate that our method achieves\nsuperior high-fidelity and audio-lip synchronization in talking face video\ngeneration compared to previous methods."}
{"id": "2504.19062", "pdf": "https://arxiv.org/pdf/2504.19062", "abs": "https://arxiv.org/abs/2504.19062", "authors": ["Yu Zhang", "Wenxiang Guo", "Changhao Pan", "Zhiyuan Zhu", "Ruiqi Li", "Jingyu Lu", "Rongjie Huang", "Ruiyuan Zhang", "Zhiqing Hong", "Ziyue Jiang", "Zhou Zhao"], "title": "Versatile Framework for Song Generation with Prompt-based Control", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": null, "summary": "Song generation focuses on producing controllable high-quality songs based on\nvarious prompts. However, existing methods struggle to generate vocals and\naccompaniments with prompt-based control and proper alignment. Additionally,\nthey fall short in supporting various tasks. To address these challenges, we\nintroduce VersBand, a multi-task song generation framework for synthesizing\nhigh-quality, aligned songs with prompt-based control. VersBand comprises these\nprimary models: 1) VocalBand, a decoupled model, leverages the flow-matching\nmethod for generating singing styles, pitches, and mel-spectrograms, allowing\nfast, high-quality vocal generation with style control. 2) AccompBand, a\nflow-based transformer model, incorporates the Band-MOE, selecting suitable\nexperts for enhanced quality, alignment, and control. This model allows for\ngenerating controllable, high-quality accompaniments aligned with vocals. 3)\nTwo generation models, LyricBand for lyrics and MelodyBand for melodies,\ncontribute to the comprehensive multi-task song generation system, allowing for\nextensive control based on multiple prompts. Experimental results demonstrate\nthat VersBand performs better over baseline models across multiple song\ngeneration tasks using objective and subjective metrics. Audio samples are\navailable at https://VersBand.github.io."}
{"id": "2504.19500", "pdf": "https://arxiv.org/pdf/2504.19500", "abs": "https://arxiv.org/abs/2504.19500", "authors": ["Yan Wang", "Baoxiong Jia", "Ziyu Zhu", "Siyuan Huang"], "title": "Masked Point-Entity Contrast for Open-Vocabulary 3D Scene Understanding", "categories": ["cs.CV", "cs.CL"], "comment": "CVPR 2025", "summary": "Open-vocabulary 3D scene understanding is pivotal for enhancing physical\nintelligence, as it enables embodied agents to interpret and interact\ndynamically within real-world environments. This paper introduces MPEC, a novel\nMasked Point-Entity Contrastive learning method for open-vocabulary 3D semantic\nsegmentation that leverages both 3D entity-language alignment and point-entity\nconsistency across different point cloud views to foster entity-specific\nfeature representations. Our method improves semantic discrimination and\nenhances the differentiation of unique instances, achieving state-of-the-art\nresults on ScanNet for open-vocabulary 3D semantic segmentation and\ndemonstrating superior zero-shot scene understanding capabilities. Extensive\nfine-tuning experiments on 8 datasets, spanning from low-level perception to\nhigh-level reasoning tasks, showcase the potential of learned 3D features,\ndriving consistent performance gains across varied 3D scene understanding\ntasks. Project website: https://mpec-3d.github.io/"}
{"id": "2504.18814", "pdf": "https://arxiv.org/pdf/2504.18814", "abs": "https://arxiv.org/abs/2504.18814", "authors": ["Abdelaziz Amara korba", "Nour Elislem Karabadji", "Yacine Ghamri-Doudane"], "title": "Zero-Day Botnet Attack Detection in IoV: A Modular Approach Using Isolation Forests and Particle Swarm Optimization", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "The Internet of Vehicles (IoV) is transforming transportation by enhancing\nconnectivity and enabling autonomous driving. However, this increased\ninterconnectivity introduces new security vulnerabilities. Bot malware and\ncyberattacks pose significant risks to Connected and Autonomous Vehicles\n(CAVs), as demonstrated by real-world incidents involving remote vehicle system\ncompromise. To address these challenges, we propose an edge-based Intrusion\nDetection System (IDS) that monitors network traffic to and from CAVs. Our\ndetection model is based on a meta-ensemble classifier capable of recognizing\nknown (Nday) attacks and detecting previously unseen (zero-day) attacks. The\napproach involves training multiple Isolation Forest (IF) models on\nMulti-access Edge Computing (MEC) servers, with each IF specialized in\nidentifying a specific type of botnet attack. These IFs, either trained locally\nor shared by other MEC nodes, are then aggregated using a Particle Swarm\nOptimization (PSO) based stacking strategy to construct a robust\nmeta-classifier. The proposed IDS has been evaluated on a vehicular botnet\ndataset, achieving an average detection rate of 92.80% for N-day attacks and\n77.32% for zero-day attacks. These results highlight the effectiveness of our\nsolution in detecting both known and emerging threats, providing a scalable and\nadaptive defense mechanism for CAVs within the IoV ecosystem."}
{"id": "2504.19188", "pdf": "https://arxiv.org/pdf/2504.19188", "abs": "https://arxiv.org/abs/2504.19188", "authors": ["Jianlong Chen", "Chao Li", "Yang Yuan", "Andrew C Yao"], "title": "Hierarchical Attention Generates Better Proofs", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.LO"], "comment": "15 pages with 3 figures", "summary": "Large language models (LLMs) have shown promise in formal theorem proving,\nbut their token-level processing often fails to capture the inherent\nhierarchical nature of mathematical proofs. We introduce \\textbf{Hierarchical\nAttention}, a regularization method that aligns LLMs' attention mechanisms with\nmathematical reasoning structures. Our approach establishes a five-level\nhierarchy from foundational elements to high-level concepts, ensuring\nstructured information flow in proof generation. Experiments demonstrate that\nour method improves proof success rates by 2.05\\% on miniF2F and 1.69\\% on\nProofNet while reducing proof complexity by 23.81\\% and 16.50\\% respectively.\nThe code is available at https://github.com/Car-pe/HAGBP."}
{"id": "2504.19506", "pdf": "https://arxiv.org/pdf/2504.19506", "abs": "https://arxiv.org/abs/2504.19506", "authors": ["Xinyang Li", "Chengjie Yi", "Jiawei Lai", "Mingbao Lin", "Yansong Qu", "Shengchuan Zhang", "Liujuan Cao"], "title": "SynergyAmodal: Deocclude Anything with Text Control", "categories": ["cs.CV"], "comment": "17 pages", "summary": "Image deocclusion (or amodal completion) aims to recover the invisible\nregions (\\ie, shape and appearance) of occluded instances in images. Despite\nrecent advances, the scarcity of high-quality data that balances diversity,\nplausibility, and fidelity remains a major obstacle. To address this challenge,\nwe identify three critical elements: leveraging in-the-wild image data for\ndiversity, incorporating human expertise for plausibility, and utilizing\ngenerative priors for fidelity. We propose SynergyAmodal, a novel framework for\nco-synthesizing in-the-wild amodal datasets with comprehensive shape and\nappearance annotations, which integrates these elements through a tripartite\ndata-human-model collaboration. First, we design an occlusion-grounded\nself-supervised learning algorithm to harness the diversity of in-the-wild\nimage data, fine-tuning an inpainting diffusion model into a partial completion\ndiffusion model. Second, we establish a co-synthesis pipeline to iteratively\nfilter, refine, select, and annotate the initial deocclusion results of the\npartial completion diffusion model, ensuring plausibility and fidelity through\nhuman expert guidance and prior model constraints. This pipeline generates a\nhigh-quality paired amodal dataset with extensive category and scale diversity,\ncomprising approximately 16K pairs. Finally, we train a full completion\ndiffusion model on the synthesized dataset, incorporating text prompts as\nconditioning signals. Extensive experiments demonstrate the effectiveness of\nour framework in achieving zero-shot generalization and textual\ncontrollability. Our code, dataset, and models will be made publicly available\nat https://github.com/imlixinyang/SynergyAmodal."}
{"id": "2504.18819", "pdf": "https://arxiv.org/pdf/2504.18819", "abs": "https://arxiv.org/abs/2504.18819", "authors": ["Hassan Wasswa", "Aziida Nanyonga", "Timothy Lynar"], "title": "Preserving Seasonal and Trend Information: A Variational Autoencoder-Latent Space Arithmetic Based Approach for Non-stationary Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "AI models have garnered significant research attention towards predictive\ntask automation. However, a stationary training environment is an underlying\nassumption for most models and such models simply do not work on non-stationary\ndata since a stationary relationship is learned. The existing solutions propose\nmaking data stationary prior to model training and evaluation. This leads to\nloss of trend and seasonal patterns which are vital components for learning\ntemporal dependencies of the system under study. This research aims to address\nthis limitation by proposing a method for enforcing stationary behaviour within\nthe latent space while preserving trend and seasonal information. The method\ndeploys techniques including Differencing, Time-series decomposition, and\nLatent Space Arithmetic (LSA), to learn information vital for efficient\napproximation of trend and seasonal information which is then stored as\nembeddings within the latent space of a Variational Autoencoder (VAE). The\napproach's ability to preserve trend and seasonal information was evaluated on\ntwo time-series non-stationary datasets. For predictive performance evaluation,\nfour deep learning models were trained on the latent vector representations of\nthe datasets after application of the proposed method and all models produced\ncompetitive results in comparison with state-of-the-art techniques using RMSE\nas the performance metric."}
{"id": "2504.19276", "pdf": "https://arxiv.org/pdf/2504.19276", "abs": "https://arxiv.org/abs/2504.19276", "authors": ["Yiyang Zhou", "Zhaoyang Wang", "Tianle Wang", "Shangyu Xing", "Peng Xia", "Bo Li", "Kaiyuan Zheng", "Zijian Zhang", "Zhaorun Chen", "Wenhao Zheng", "Xuchao Zhang", "Chetan Bansal", "Weitong Zhang", "Ying Wei", "Mohit Bansal", "Huaxiu Yao"], "title": "Anyprefer: An Agentic Framework for Preference Data Synthesis", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "High-quality preference data is essential for aligning foundation models with\nhuman values through preference learning. However, manual annotation of such\ndata is often time-consuming and costly. Recent methods often adopt a\nself-rewarding approach, where the target model generates and annotates its own\npreference data, but this can lead to inaccuracies since the reward model\nshares weights with the target model, thereby amplifying inherent biases. To\naddress these issues, we propose Anyprefer, a framework designed to synthesize\nhigh-quality preference data for aligning the target model. Anyprefer frames\nthe data synthesis process as a cooperative two-player Markov Game, where the\ntarget model and the judge model collaborate together. Here, a series of\nexternal tools are introduced to assist the judge model in accurately rewarding\nthe target model's responses, mitigating biases in the rewarding process. In\naddition, a feedback mechanism is introduced to optimize prompts for both\nmodels, enhancing collaboration and improving data quality. The synthesized\ndata is compiled into a new preference dataset, Anyprefer-V1, consisting of 58K\nhigh-quality preference pairs. Extensive experiments show that Anyprefer\nsignificantly improves model alignment performance across four main\napplications, covering 21 datasets, achieving average improvements of 18.55% in\nfive natural language generation datasets, 3.66% in nine vision-language\nunderstanding datasets, 30.05% in three medical image analysis datasets, and\n16.00% in four visuo-motor control tasks."}
{"id": "2504.19514", "pdf": "https://arxiv.org/pdf/2504.19514", "abs": "https://arxiv.org/abs/2504.19514", "authors": ["Rong Gao", "Xin Liu", "Zhuozhao Hu", "Bohao Xing", "Baiqiang Xia", "Zitong Yu", "Heikki Kälviäinen"], "title": "FSBench: A Figure Skating Benchmark for Advancing Artistic Sports Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Figure skating, known as the \"Art on Ice,\" is among the most artistic sports,\nchallenging to understand due to its blend of technical elements (like jumps\nand spins) and overall artistic expression. Existing figure skating datasets\nmainly focus on single tasks, such as action recognition or scoring, lacking\ncomprehensive annotations for both technical and artistic evaluation. Current\nsports research is largely centered on ball games, with limited relevance to\nartistic sports like figure skating. To address this, we introduce FSAnno, a\nlarge-scale dataset advancing artistic sports understanding through figure\nskating. FSAnno includes an open-access training and test dataset, alongside a\nbenchmark dataset, FSBench, for fair model evaluation. FSBench consists of\nFSBench-Text, with multiple-choice questions and explanations, and\nFSBench-Motion, containing multimodal data and Question and Answer (QA) pairs,\nsupporting tasks from technical analysis to performance commentary. Initial\ntests on FSBench reveal significant limitations in existing models'\nunderstanding of artistic sports. We hope FSBench will become a key tool for\nevaluating and enhancing model comprehension of figure skating."}
{"id": "2504.18827", "pdf": "https://arxiv.org/pdf/2504.18827", "abs": "https://arxiv.org/abs/2504.18827", "authors": ["Teeradaj Racharak", "Chaiyong Ragkhitwetsagul", "Chommakorn Sontesadisai", "Thanwadee Sunetnanta"], "title": "Test It Before You Trust It: Applying Software Testing for Trustworthy In-context Learning", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "In-context learning (ICL) has emerged as a powerful capability of large\nlanguage models (LLMs), enabling them to perform new tasks based on a few\nprovided examples without explicit fine-tuning. Despite their impressive\nadaptability, these models remain vulnerable to subtle adversarial\nperturbations and exhibit unpredictable behavior when faced with linguistic\nvariations. Inspired by software testing principles, we introduce a software\ntesting-inspired framework, called MMT4NL, for evaluating the trustworthiness\nof in-context learning by utilizing adversarial perturbations and software\ntesting techniques. It includes diverse evaluation aspects of linguistic\ncapabilities for testing the ICL capabilities of LLMs. MMT4NL is built around\nthe idea of crafting metamorphic adversarial examples from a test set in order\nto quantify and pinpoint bugs in the designed prompts of ICL. Our philosophy is\nto treat any LLM as software and validate its functionalities just like testing\nthe software. Finally, we demonstrate applications of MMT4NL on the sentiment\nanalysis and question-answering tasks. Our experiments could reveal various\nlinguistic bugs in state-of-the-art LLMs."}
{"id": "2504.19444", "pdf": "https://arxiv.org/pdf/2504.19444", "abs": "https://arxiv.org/abs/2504.19444", "authors": ["Kang Yang", "Xinjun Mao", "Shangwen Wang", "Yanlin Wang", "Tanghaoran Zhang", "Bo Lin", "Yihao Qin", "Zhang Zhang", "Yao Lu", "Kamal Al-Sabahi"], "title": "Large Language Models are Qualified Benchmark Builders: Rebuilding Pre-Training Datasets for Advancing Code Intelligence Tasks", "categories": ["cs.SE", "cs.CL"], "comment": "Awarded the ACM SIGSOFT Distinguished Paper Award in ICPC 2025", "summary": "Pre-trained code models rely heavily on high-quality pre-training data,\nparticularly human-written reference comments that bridge code and natural\nlanguage. However, these comments often become outdated as software evolves,\ndegrading model performance. Large language models (LLMs) excel at generating\nhigh-quality code comments. We investigate whether replacing human-written\ncomments with LLM-generated ones improves pre-training datasets. Since standard\nmetrics cannot assess reference comment quality, we propose two novel\nreference-free evaluation tasks: code-comment inconsistency detection and\nsemantic code search. Results show that LLM-generated comments are more\nsemantically consistent with code than human-written ones, as confirmed by\nmanual evaluation. Leveraging this finding, we rebuild the CodeSearchNet\ndataset with LLM-generated comments and re-pre-train CodeT5. Evaluations\ndemonstrate that models trained on LLM-enhanced data outperform those using\noriginal human comments in code summarization, generation, and translation\ntasks. This work validates rebuilding pre-training datasets with LLMs to\nadvance code intelligence, challenging the traditional reliance on human\nreference comments."}
{"id": "2504.19524", "pdf": "https://arxiv.org/pdf/2504.19524", "abs": "https://arxiv.org/abs/2504.19524", "authors": ["Peijian Zeng", "Feiyan Pang", "Zhanbo Wang", "Aimin Yang"], "title": "LR-IAD:Mask-Free Industrial Anomaly Detection with Logical Reasoning", "categories": ["cs.CV"], "comment": "10 pages", "summary": "Industrial Anomaly Detection (IAD) is critical for ensuring product quality\nby identifying defects. Traditional methods such as feature embedding and\nreconstruction-based approaches require large datasets and struggle with\nscalability. Existing vision-language models (VLMs) and Multimodal Large\nLanguage Models (MLLMs) address some limitations but rely on mask annotations,\nleading to high implementation costs and false positives. Additionally,\nindustrial datasets like MVTec-AD and VisA suffer from severe class imbalance,\nwith defect samples constituting only 23.8% and 11.1% of total data\nrespectively. To address these challenges, we propose a reward function that\ndynamically prioritizes rare defect patterns during training to handle class\nimbalance. We also introduce a mask-free reasoning framework using Chain of\nThought (CoT) and Group Relative Policy Optimization (GRPO) mechanisms,\nenabling anomaly detection directly from raw images without annotated masks.\nThis approach generates interpretable step-by-step explanations for defect\nlocalization. Our method achieves state-of-the-art performance, outperforming\nprior approaches by 36% in accuracy on MVTec-AD and 16% on VisA. By eliminating\nmask dependency and reducing costs while providing explainable outputs, this\nwork advances industrial anomaly detection and supports scalable quality\ncontrol in manufacturing. Code to reproduce the experiment is available at\nhttps://github.com/LilaKen/LR-IAD."}
{"id": "2504.18845", "pdf": "https://arxiv.org/pdf/2504.18845", "abs": "https://arxiv.org/abs/2504.18845", "authors": ["Mehmet Ali Ferah", "Tufan Kumbasar"], "title": "Introducing Interval Neural Networks for Uncertainty-Aware System Identification", "categories": ["cs.LG", "cs.AI"], "comment": "In International Congress on Human-Computer Interaction, Optimization\n  and Robotic Applications, 2025", "summary": "System Identification (SysID) is crucial for modeling and understanding\ndynamical systems using experimental data. While traditional SysID methods\nemphasize linear models, their inability to fully capture nonlinear dynamics\nhas driven the adoption of Deep Learning (DL) as a more powerful alternative.\nHowever, the lack of uncertainty quantification (UQ) in DL-based models poses\nchallenges for reliability and safety, highlighting the necessity of\nincorporating UQ. This paper introduces a systematic framework for constructing\nand learning Interval Neural Networks (INNs) to perform UQ in SysID tasks. INNs\nare derived by transforming the learnable parameters (LPs) of pre-trained\nneural networks into interval-valued LPs without relying on probabilistic\nassumptions. By employing interval arithmetic throughout the network, INNs can\ngenerate Prediction Intervals (PIs) that capture target coverage effectively.\nWe extend Long Short-Term Memory (LSTM) and Neural Ordinary Differential\nEquations (Neural ODEs) into Interval LSTM (ILSTM) and Interval NODE (INODE)\narchitectures, providing the mathematical foundations for their application in\nSysID. To train INNs, we propose a DL framework that integrates a UQ loss\nfunction and parameterization tricks to handle constraints arising from\ninterval LPs. We introduce novel concept \"elasticity\" for underlying\nuncertainty causes and validate ILSTM and INODE in SysID experiments,\ndemonstrating their effectiveness."}
{"id": "2504.19458", "pdf": "https://arxiv.org/pdf/2504.19458", "abs": "https://arxiv.org/abs/2504.19458", "authors": ["Taoyu Su", "Jiawei Sheng", "Duohe Ma", "Xiaodong Li", "Juwei Yue", "Mengxiao Song", "Yingkai Tang", "Tingwen Liu"], "title": "Mitigating Modality Bias in Multi-modal Entity Alignment from a Causal Perspective", "categories": ["cs.MM", "cs.CL", "cs.IR"], "comment": "Accepted by SIGIR 2025, 11 pages, 10 figures, 4 tables,", "summary": "Multi-Modal Entity Alignment (MMEA) aims to retrieve equivalent entities from\ndifferent Multi-Modal Knowledge Graphs (MMKGs), a critical information\nretrieval task. Existing studies have explored various fusion paradigms and\nconsistency constraints to improve the alignment of equivalent entities, while\noverlooking that the visual modality may not always contribute positively.\nEmpirically, entities with low-similarity images usually generate\nunsatisfactory performance, highlighting the limitation of overly relying on\nvisual features. We believe the model can be biased toward the visual modality,\nleading to a shortcut image-matching task. To address this, we propose a\ncounterfactual debiasing framework for MMEA, termed CDMEA, which investigates\nvisual modality bias from a causal perspective. Our approach aims to leverage\nboth visual and graph modalities to enhance MMEA while suppressing the direct\ncausal effect of the visual modality on model predictions. By estimating the\nTotal Effect (TE) of both modalities and excluding the Natural Direct Effect\n(NDE) of the visual modality, we ensure that the model predicts based on the\nTotal Indirect Effect (TIE), effectively utilizing both modalities and reducing\nvisual modality bias. Extensive experiments on 9 benchmark datasets show that\nCDMEA outperforms 14 state-of-the-art methods, especially in low-similarity,\nhigh-noise, and low-resource data scenarios."}
{"id": "2504.19529", "pdf": "https://arxiv.org/pdf/2504.19529", "abs": "https://arxiv.org/abs/2504.19529", "authors": ["Guobiao Li", "Lei Tan", "Yuliang Xue", "Gaozhi Liu", "Zhenxing Qian", "Sheng Li", "Xinpeng Zhang"], "title": "Adversarial Shallow Watermarking", "categories": ["cs.CV", "cs.MM"], "comment": "10 pages, 12 figures", "summary": "Recent advances in digital watermarking make use of deep neural networks for\nmessage embedding and extraction. They typically follow the ``encoder-noise\nlayer-decoder''-based architecture. By deliberately establishing a\ndifferentiable noise layer to simulate the distortion of the watermarked\nsignal, they jointly train the deep encoder and decoder to fit the noise layer\nto guarantee robustness. As a result, they are usually weak against unknown\ndistortions that are not used in their training pipeline. In this paper, we\npropose a novel watermarking framework to resist unknown distortions, namely\nAdversarial Shallow Watermarking (ASW). ASW utilizes only a shallow decoder\nthat is randomly parameterized and designed to be insensitive to distortions\nfor watermarking extraction. During the watermark embedding, ASW freezes the\nshallow decoder and adversarially optimizes a host image until its updated\nversion (i.e., the watermarked image) stably triggers the shallow decoder to\noutput the watermark message. During the watermark extraction, it accurately\nrecovers the message from the watermarked image by leveraging the insensitive\nnature of the shallow decoder against arbitrary distortions. Our ASW is\ntraining-free, encoder-free, and noise layer-free. Experiments indicate that\nthe watermarked images created by ASW have strong robustness against various\nunknown distortions. Compared to the existing ``encoder-noise layer-decoder''\napproaches, ASW achieves comparable results on known distortions and better\nrobustness on unknown distortions."}
{"id": "2504.18847", "pdf": "https://arxiv.org/pdf/2504.18847", "abs": "https://arxiv.org/abs/2504.18847", "authors": ["Hidayet Ersin Dursun", "Yusuf Güven", "Tufan Kumbasar"], "title": "Imitation Learning for Autonomous Driving: Insights from Real-World Testing", "categories": ["cs.RO", "cs.AI"], "comment": "In International Congress on Human-Computer Interaction, Optimization\n  and Robotic Applications, 2025", "summary": "This work focuses on the design of a deep learning-based autonomous driving\nsystem deployed and tested on the real-world MIT Racecar to assess its\neffectiveness in driving scenarios. The Deep Neural Network (DNN) translates\nraw image inputs into real-time steering commands in an end-to-end learning\nfashion, following the imitation learning framework. The key design challenge\nis to ensure that DNN predictions are accurate and fast enough, at a high\nsampling frequency, and result in smooth vehicle operation under different\noperating conditions. In this study, we design and compare various DNNs, to\nidentify the most effective approach for real-time autonomous driving. In\ndesigning the DNNs, we adopted an incremental design approach that involved\nenhancing the model capacity and dataset to address the challenges of\nreal-world driving scenarios. We designed a PD system, CNN, CNN-LSTM, and\nCNN-NODE, and evaluated their performance on the real-world MIT Racecar. While\nthe PD system handled basic lane following, it struggled with sharp turns and\nlighting variations. The CNN improved steering but lacked temporal awareness,\nwhich the CNN-LSTM addressed as it resulted in smooth driving performance. The\nCNN-NODE performed similarly to the CNN-LSTM in handling driving dynamics, yet\nwith slightly better driving performance. The findings of this research\nhighlight the importance of iterative design processes in developing robust\nDNNs for autonomous driving applications. The experimental video is available\nat https://www.youtube.com/watch?v=FNNYgU--iaY."}
{"id": "2504.19483", "pdf": "https://arxiv.org/pdf/2504.19483", "abs": "https://arxiv.org/abs/2504.19483", "authors": ["Bertram Højer", "Oliver Jarvis", "Stefan Heinrich"], "title": "Improving Reasoning Performance in Large Language Models via Representation Engineering", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Has been accepted at \"The Thirteenth International Conference on\n  Learning Representations (ICLR 2025)\" Link to publication:\n  https://openreview.net/forum?id=IssPhpUsKt", "summary": "Recent advancements in large language models (LLMs) have resulted in\nincreasingly anthropomorphic language concerning the ability of LLMs to reason.\nWhether reasoning in LLMs should be understood to be inherently different is,\nhowever, widely debated. We propose utilizing a representation engineering\napproach wherein model activations are read from the residual stream of an LLM\nwhen processing a reasoning task. The activations are used to derive a control\nvector that is applied to the model as an inference-time intervention,\nmodulating the representational space of the model, to improve performance on\nthe specified task. We publish the code for deriving control vectors and\nanalyzing model representations. The method allows us to improve performance on\nreasoning benchmarks and assess how control vectors influence the final logit\ndistribution of a model via metrics such as KL divergence and entropy. We apply\ncontrol vectors to Mistral-7B-Instruct and a range of Pythia models on an\ninductive, a deductive and mathematical reasoning task. We show that an LLM\ncan, to a certain degree, be controlled to improve its perceived reasoning\nability by modulating activations. The intervention is dependent upon the\nability to reliably extract the model's typical state when correctly solving a\ntask. Our results suggest that reasoning performance can be modulated in the\nsame manner as other information-processing tasks performed by LLMs and\ndemonstrate that we are capable of improving performance on specific tasks via\na simple intervention on the residual stream with no additional training."}
{"id": "2504.19545", "pdf": "https://arxiv.org/pdf/2504.19545", "abs": "https://arxiv.org/abs/2504.19545", "authors": ["Zezeng Li", "Zhihui Qi", "Weimin Wang", "Ziliang Wang", "Junyi Duan", "Na Lei"], "title": "Point2Quad: Generating Quad Meshes from Point Clouds via Face Prediction", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Quad meshes are essential in geometric modeling and computational mechanics.\nAlthough learning-based methods for triangle mesh demonstrate considerable\nadvancements, quad mesh generation remains less explored due to the challenge\nof ensuring coplanarity, convexity, and quad-only meshes. In this paper, we\npresent Point2Quad, the first learning-based method for quad-only mesh\ngeneration from point clouds. The key idea is learning to identify quad mesh\nwith fused pointwise and facewise features. Specifically, Point2Quad begins\nwith a k-NN-based candidate generation considering the coplanarity and\nsquareness. Then, two encoders are followed to extract geometric and\ntopological features that address the challenge of quad-related constraints,\nespecially by combining in-depth quadrilaterals-specific characteristics.\nSubsequently, the extracted features are fused to train the classifier with a\ndesigned compound loss. The final results are derived after the refinement by a\nquad-specific post-processing. Extensive experiments on both clear and noise\ndata demonstrate the effectiveness and superiority of Point2Quad, compared to\nbaseline methods under comprehensive metrics."}
{"id": "2504.18854", "pdf": "https://arxiv.org/pdf/2504.18854", "abs": "https://arxiv.org/abs/2504.18854", "authors": ["Tengfei Xing", "Xiaodan Ren", "Jie Li"], "title": "Predicting Stress in Two-phase Random Materials and Super-Resolution Method for Stress Images by Embedding Physical Information", "categories": ["cond-mat.mtrl-sci", "cs.AI", "cs.LG"], "comment": null, "summary": "Stress analysis is an important part of material design. For materials with\ncomplex microstructures, such as two-phase random materials (TRMs), material\nfailure is often accompanied by stress concentration. Phase interfaces in\ntwo-phase materials are critical for stress concentration. Therefore, the\nprediction error of stress at phase boundaries is crucial. In practical\nengineering, the pixels of the obtained material microstructure images are\nlimited, which limits the resolution of stress images generated by deep\nlearning methods, making it difficult to observe stress concentration regions.\nExisting Image Super-Resolution (ISR) technologies are all based on data-driven\nsupervised learning. However, stress images have natural physical constraints,\nwhich provide new ideas for new ISR technologies. In this study, we constructed\na stress prediction framework for TRMs. First, the framework uses a proposed\nMultiple Compositions U-net (MC U-net) to predict stress in low-resolution\nmaterial microstructures. By considering the phase interface information of the\nmicrostructure, the MC U-net effectively reduces the problem of excessive\nprediction errors at phase boundaries. Secondly, a Mixed Physics-Informed\nNeural Network (MPINN) based method for stress ISR (SRPINN) was proposed. By\nintroducing the constraints of physical information, the new method does not\nrequire paired stress images for training and can increase the resolution of\nstress images to any multiple. This enables a multiscale analysis of the stress\nconcentration regions at phase boundaries. Finally, we performed stress\nanalysis on TRMs with different phase volume fractions and loading states\nthrough transfer learning. The results show the proposed stress prediction\nframework has satisfactory accuracy and generalization ability."}
{"id": "2504.19500", "pdf": "https://arxiv.org/pdf/2504.19500", "abs": "https://arxiv.org/abs/2504.19500", "authors": ["Yan Wang", "Baoxiong Jia", "Ziyu Zhu", "Siyuan Huang"], "title": "Masked Point-Entity Contrast for Open-Vocabulary 3D Scene Understanding", "categories": ["cs.CV", "cs.CL"], "comment": "CVPR 2025", "summary": "Open-vocabulary 3D scene understanding is pivotal for enhancing physical\nintelligence, as it enables embodied agents to interpret and interact\ndynamically within real-world environments. This paper introduces MPEC, a novel\nMasked Point-Entity Contrastive learning method for open-vocabulary 3D semantic\nsegmentation that leverages both 3D entity-language alignment and point-entity\nconsistency across different point cloud views to foster entity-specific\nfeature representations. Our method improves semantic discrimination and\nenhances the differentiation of unique instances, achieving state-of-the-art\nresults on ScanNet for open-vocabulary 3D semantic segmentation and\ndemonstrating superior zero-shot scene understanding capabilities. Extensive\nfine-tuning experiments on 8 datasets, spanning from low-level perception to\nhigh-level reasoning tasks, showcase the potential of learned 3D features,\ndriving consistent performance gains across varied 3D scene understanding\ntasks. Project website: https://mpec-3d.github.io/"}
{"id": "2504.19546", "pdf": "https://arxiv.org/pdf/2504.19546", "abs": "https://arxiv.org/abs/2504.19546", "authors": ["Tong Xiao", "Qunming Wang", "Ping Lu", "Tenghai Huang", "Xiaohua Tong", "Peter M. Atkinson"], "title": "Crowd Detection Using Very-Fine-Resolution Satellite Imagery", "categories": ["cs.CV"], "comment": "17 pages, 12 figures, 5 tables", "summary": "Accurate crowd detection (CD) is critical for public safety and historical\npattern analysis, yet existing methods relying on ground and aerial imagery\nsuffer from limited spatio-temporal coverage. The development of\nvery-fine-resolution (VFR) satellite sensor imagery (e.g., ~0.3 m spatial\nresolution) provides unprecedented opportunities for large-scale crowd activity\nanalysis, but it has never been considered for this task. To address this gap,\nwe proposed CrowdSat-Net, a novel point-based convolutional neural network,\nwhich features two innovative components: Dual-Context Progressive Attention\nNetwork (DCPAN) to improve feature representation of individuals by aggregating\nscene context and local individual characteristics, and High-Frequency Guided\nDeformable Upsampler (HFGDU) that recovers high-frequency information during\nupsampling through frequency-domain guided deformable convolutions. To validate\nthe effectiveness of CrowdSat-Net, we developed CrowdSat, the first VFR\nsatellite imagery dataset designed specifically for CD tasks, comprising over\n120k manually labeled individuals from multi-source satellite platforms\n(Beijing-3N, Jilin-1 Gaofen-04A and Google Earth) across China. In the\nexperiments, CrowdSat-Net was compared with five state-of-the-art point-based\nCD methods (originally designed for ground or aerial imagery) using CrowdSat\nand achieved the largest F1-score of 66.12% and Precision of 73.23%, surpassing\nthe second-best method by 1.71% and 2.42%, respectively. Moreover, extensive\nablation experiments validated the importance of the DCPAN and HFGDU modules.\nFurthermore, cross-regional evaluation further demonstrated the spatial\ngeneralizability of CrowdSat-Net. This research advances CD capability by\nproviding both a newly developed network architecture for CD and a pioneering\nbenchmark dataset to facilitate future CD development."}
{"id": "2504.18857", "pdf": "https://arxiv.org/pdf/2504.18857", "abs": "https://arxiv.org/abs/2504.18857", "authors": ["Yi Lu", "Wanxu Zhao", "Xin Zhou", "Chenxin An", "Chenglong Wang", "Shuo Li", "Yuming Yang", "Jun Zhao", "Tao Ji", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "Effective Length Extrapolation via Dimension-Wise Positional Embeddings Manipulation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) often struggle to process and generate coherent\ncontext when the number of input tokens exceeds the pre-trained length. Recent\nadvancements in long-context extension have significantly expanded the context\nwindow of LLMs but require expensive overhead to train the large-scale models\nwith longer context. In this work, we propose Dimension-Wise Positional\nEmbeddings Manipulation (DPE), a training-free framework to extrapolate the\ncontext window of LLMs by diving into RoPE's different hidden dimensions.\nInstead of manipulating all dimensions equally, DPE detects the effective\nlength for every dimension and finds the key dimensions for context extension.\nWe reuse the original position indices with their embeddings from the\npre-trained model and manipulate the key dimensions' position indices to their\nmost effective lengths. In this way, DPE adjusts the pre-trained models with\nminimal modifications while ensuring that each dimension reaches its optimal\nstate for extrapolation. DPE significantly surpasses well-known baselines such\nas YaRN and Self-Extend. DPE enables Llama3-8k 8B to support context windows of\n128k tokens without continual training and integrates seamlessly with Flash\nAttention 2. In addition to its impressive extrapolation capability, DPE also\ndramatically improves the models' performance within training length, such as\nLlama3.1 70B, by over 18 points on popular long-context benchmarks RULER. When\ncompared with commercial models, Llama 3.1 70B with DPE even achieves better\nperformance than GPT-4-128K."}
{"id": "2504.19519", "pdf": "https://arxiv.org/pdf/2504.19519", "abs": "https://arxiv.org/abs/2504.19519", "authors": ["Ke Hong", "Xiuhong Li", "Minxu Liu", "Qiuli Mao", "Tianqi Wu", "Zixiao Huang", "Lufang Chen", "Zhong Wang", "Yichong Zhang", "Zhenhua Zhu", "Guohao Dai", "Yu Wang"], "title": "FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation", "categories": ["cs.DC", "cs.CL", "cs.LG"], "comment": "17 pages, 11 figures, 4 tables", "summary": "Generative models have achieved remarkable success across various\napplications, driving the demand for multi-GPU computing. Inter-GPU\ncommunication becomes a bottleneck in multi-GPU computing systems, particularly\non consumer-grade GPUs. By exploiting concurrent hardware execution,\noverlapping computation and communication latency is an effective technique for\nmitigating the communication overhead. We identify that an efficient and\nadaptable overlapping design should satisfy (1) tile-wise overlapping to\nmaximize the overlapping opportunity, (2) interference-free computation to\nmaintain the original computational performance, and (3) communication\nagnosticism to reduce the development burden against varying communication\nprimitives. Nevertheless, current designs fail to simultaneously optimize for\nall of those features.\n  To address the issue, we propose FlashOverlap, a lightweight design\ncharacterized by tile-wise overlapping, interference-free computation, and\ncommunication agnosticism. FlashOverlap utilizes a novel signaling mechanism to\nidentify tile-wise data dependency without interrupting the computation\nprocess, and reorders data to contiguous addresses, enabling communication by\nsimply calling NCCL APIs. Experiments show that such a lightweight design\nachieves up to 1.65x speedup, outperforming existing works in most cases."}
{"id": "2504.19549", "pdf": "https://arxiv.org/pdf/2504.19549", "abs": "https://arxiv.org/abs/2504.19549", "authors": ["Deng Li", "Bohao Xing", "Xin Liu", "Baiqiang Xia", "Bihan Wen", "Heikki Kälviäinen"], "title": "DEEMO: De-identity Multimodal Emotion Recognition and Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Emotion understanding is a critical yet challenging task. Most existing\napproaches rely heavily on identity-sensitive information, such as facial\nexpressions and speech, which raises concerns about personal privacy. To\naddress this, we introduce the De-identity Multimodal Emotion Recognition and\nReasoning (DEEMO), a novel task designed to enable emotion understanding using\nde-identified video and audio inputs. The DEEMO dataset consists of two\nsubsets: DEEMO-NFBL, which includes rich annotations of Non-Facial Body\nLanguage (NFBL), and DEEMO-MER, an instruction dataset for Multimodal Emotion\nRecognition and Reasoning using identity-free cues. This design supports\nemotion understanding without compromising identity privacy. In addition, we\npropose DEEMO-LLaMA, a Multimodal Large Language Model (MLLM) that integrates\nde-identified audio, video, and textual information to enhance both emotion\nrecognition and reasoning. Extensive experiments show that DEEMO-LLaMA achieves\nstate-of-the-art performance on both tasks, outperforming existing MLLMs by a\nsignificant margin, achieving 74.49% accuracy and 74.45% F1-score in\nde-identity emotion recognition, and 6.20 clue overlap and 7.66 label overlap\nin de-identity emotion reasoning. Our work contributes to ethical AI by\nadvancing privacy-preserving emotion understanding and promoting responsible\naffective computing."}
{"id": "2504.18858", "pdf": "https://arxiv.org/pdf/2504.18858", "abs": "https://arxiv.org/abs/2504.18858", "authors": ["Vahid Garousi"], "title": "Why you shouldn't fully trust ChatGPT: A synthesis of this AI tool's error rates across disciplines and the software engineering lifecycle", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Context: ChatGPT and other large language models (LLMs) are widely used\nacross healthcare, business, economics, engineering, and software engineering\n(SE). Despite their popularity, concerns persist about their reliability,\nespecially their error rates across domains and the software development\nlifecycle (SDLC).\n  Objective: This study synthesizes and quantifies ChatGPT's reported error\nrates across major domains and SE tasks aligned with SDLC phases. It provides\nan evidence-based view of where ChatGPT excels, where it fails, and how\nreliability varies by task, domain, and model version (GPT-3.5, GPT-4,\nGPT-4-turbo, GPT-4o).\n  Method: A Multivocal Literature Review (MLR) was conducted, gathering data\nfrom academic studies, reports, benchmarks, and grey literature up to 2025.\nFactual, reasoning, coding, and interpretive errors were considered. Data were\ngrouped by domain and SE phase and visualized using boxplots to show error\ndistributions.\n  Results: Error rates vary across domains and versions. In healthcare, rates\nranged from 8% to 83%. Business and economics saw error rates drop from ~50%\nwith GPT-3.5 to 15-20% with GPT-4. Engineering tasks averaged 20-30%.\nProgramming success reached 87.5%, though complex debugging still showed over\n50% errors. In SE, requirements and design phases showed lower error rates\n(~5-20%), while coding, testing, and maintenance phases had higher variability\n(10-50%). Upgrades from GPT-3.5 to GPT-4 improved reliability.\n  Conclusion: Despite improvements, ChatGPT still exhibits non-negligible error\nrates varying by domain, task, and SDLC phase. Full reliance without human\noversight remains risky, especially in critical settings. Continuous evaluation\nand critical validation are essential to ensure reliability and\ntrustworthiness."}
{"id": "2504.19583", "pdf": "https://arxiv.org/pdf/2504.19583", "abs": "https://arxiv.org/abs/2504.19583", "authors": ["Hanlu Zhang", "Yumeng Ma", "Shuo Wang", "Guiran Liu", "Binrong Zhu"], "title": "Graph-Based Spectral Decomposition for Parameter Coordination in Language Model Fine-Tuning", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "This paper proposes a parameter collaborative optimization algorithm for\nlarge language models, enhanced with graph spectral analysis. The goal is to\nimprove both fine-tuning efficiency and structural awareness during training.\nIn the proposed method, the parameters of a pre-trained language model are\ntreated as nodes in a graph. A weighted graph is constructed, and Laplacian\nspectral decomposition is applied to enable frequency-domain modeling and\nstructural representation of the parameter space. Based on this structure, a\njoint loss function is designed. It combines the task loss with a spectral\nregularization term to facilitate collaborative updates among parameters. In\naddition, a spectral filtering mechanism is introduced during the optimization\nphase. This mechanism adjusts gradients in a structure-aware manner, enhancing\nthe model's training stability and convergence behavior. The method is\nevaluated on multiple tasks, including traditional fine-tuning comparisons,\nfew-shot generalization tests, and convergence speed analysis. In all settings,\nthe proposed approach demonstrates superior performance. The experimental\nresults confirm that the spectral collaborative optimization framework\neffectively reduces parameter perturbations and improves fine-tuning quality\nwhile preserving overall model performance. This work contributes significantly\nto the field of artificial intelligence by advancing parameter-efficient\ntraining methodologies for large-scale models, reinforcing the importance of\nstructural signal processing in deep learning optimization, and offering a\nrobust, generalizable framework for enhancing language model adaptability and\nperformance."}
{"id": "2504.19557", "pdf": "https://arxiv.org/pdf/2504.19557", "abs": "https://arxiv.org/abs/2504.19557", "authors": ["Mohammad Altillawi", "Fengyi Shen", "Liudi Yang", "Sai Manoj Prakhya", "Ziyuan Liu"], "title": "CE-NPBG: Connectivity Enhanced Neural Point-Based Graphics for Novel View Synthesis in Autonomous Driving Scenes", "categories": ["cs.CV"], "comment": "Accepted in 2025 IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition Workshops (CVPRW)", "summary": "Current point-based approaches encounter limitations in scalability and\nrendering quality when using large 3D point cloud maps because using them\ndirectly for novel view synthesis (NVS) leads to degraded visualizations. We\nidentify the primary issue behind these low-quality renderings as a visibility\nmismatch between geometry and appearance, stemming from using these two\nmodalities together. To address this problem, we present CE-NPBG, a new\napproach for novel view synthesis (NVS) in large-scale autonomous driving\nscenes. Our method is a neural point-based technique that leverages two\nmodalities: posed images (cameras) and synchronized raw 3D point clouds\n(LiDAR). We first employ a connectivity relationship graph between appearance\nand geometry, which retrieves points from a large 3D point cloud map observed\nfrom the current camera perspective and uses them for rendering. By leveraging\nthis connectivity, our method significantly improves rendering quality and\nenhances run-time and scalability by using only a small subset of points from\nthe large 3D point cloud map. Our approach associates neural descriptors with\nthe points and uses them to synthesize views. To enhance the encoding of these\ndescriptors and elevate rendering quality, we propose a joint adversarial and\npoint rasterization training. During training, we pair an image-synthesizer\nnetwork with a multi-resolution discriminator. At inference, we decouple them\nand use the image-synthesizer to generate novel views. We also integrate our\nproposal into the recent 3D Gaussian Splatting work to highlight its benefits\nfor improved rendering and scalability."}
{"id": "2504.18878", "pdf": "https://arxiv.org/pdf/2504.18878", "abs": "https://arxiv.org/abs/2504.18878", "authors": ["Robert Leppich", "Michael Stenger", "Daniel Grillmeyer", "Vanessa Borst", "Samuel Kounev"], "title": "TSRM: A Lightweight Temporal Feature Encoding Architecture for Time Series Forecasting and Imputation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We introduce a temporal feature encoding architecture called Time Series\nRepresentation Model (TSRM) for multivariate time series forecasting and\nimputation. The architecture is structured around CNN-based representation\nlayers, each dedicated to an independent representation learning task and\ndesigned to capture diverse temporal patterns, followed by an attention-based\nfeature extraction layer and a merge layer, designed to aggregate extracted\nfeatures. The architecture is fundamentally based on a configuration that is\ninspired by a Transformer encoder, with self-attention mechanisms at its core.\nThe TSRM architecture outperforms state-of-the-art approaches on most of the\nseven established benchmark datasets considered in our empirical evaluation for\nboth forecasting and imputation tasks. At the same time, it significantly\nreduces complexity in the form of learnable parameters. The source code is\navailable at https://github.com/RobertLeppich/TSRM."}
{"id": "2504.19730", "pdf": "https://arxiv.org/pdf/2504.19730", "abs": "https://arxiv.org/abs/2504.19730", "authors": ["Wenhan Mu", "Ling Xu", "Shuren Pei", "Le Mi", "Huichi Zhou"], "title": "Evaluate-and-Purify: Fortifying Code Language Models Against Adversarial Attacks Using LLM-as-a-Judge", "categories": ["cs.SE", "cs.CL"], "comment": "25 pages, 6 figures", "summary": "The widespread adoption of code language models in software engineering tasks\nhas exposed vulnerabilities to adversarial attacks, especially the identifier\nsubstitution attacks. Although existing identifier substitution attackers\ndemonstrate high success rates, they often produce adversarial examples with\nunnatural code patterns. In this paper, we systematically assess the quality of\nadversarial examples using LLM-as-a-Judge. Our analysis reveals that over 80%\nof adversarial examples generated by state-of-the-art identifier substitution\nattackers (e.g., ALERT) are actually detectable. Based on this insight, we\npropose EP-Shield, a unified framework for evaluating and purifying identifier\nsubstitution attacks via naturalness-aware reasoning. Specifically, we first\nevaluate the naturalness of code and identify the perturbed adversarial code,\nthen purify it so that the victim model can restore correct prediction.\nExtensive experiments demonstrate the superiority of EP-Shield over adversarial\nfine-tuning (up to 83.36% improvement) and its lightweight design 7B\nparameters) with GPT-4-level performance."}
{"id": "2504.19572", "pdf": "https://arxiv.org/pdf/2504.19572", "abs": "https://arxiv.org/abs/2504.19572", "authors": ["Peter Hönig", "Matthias Hirschmanner", "Markus Vincze"], "title": "Category-Level and Open-Set Object Pose Estimation for Robotics", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted at Austrian Robotics Workshop 2025", "summary": "Object pose estimation enables a variety of tasks in computer vision and\nrobotics, including scene understanding and robotic grasping. The complexity of\na pose estimation task depends on the unknown variables related to the target\nobject. While instance-level methods already excel for opaque and Lambertian\nobjects, category-level and open-set methods, where texture, shape, and size\nare partially or entirely unknown, still struggle with these basic material\nproperties. Since texture is unknown in these scenarios, it cannot be used for\ndisambiguating object symmetries, another core challenge of 6D object pose\nestimation. The complexity of estimating 6D poses with such a manifold of\nunknowns led to various datasets, accuracy metrics, and algorithmic solutions.\nThis paper compares datasets, accuracy metrics, and algorithms for solving 6D\npose estimation on the category-level. Based on this comparison, we analyze how\nto bridge category-level and open-set object pose estimation to reach\ngeneralization and provide actionable recommendations."}
{"id": "2504.18882", "pdf": "https://arxiv.org/pdf/2504.18882", "abs": "https://arxiv.org/abs/2504.18882", "authors": ["Ce Ju", "Reinmar J. Kobler", "Antoine Collas", "Motoaki Kawanabe", "Cuntai Guan", "Bertrand Thirion"], "title": "SPD Learning for Covariance-Based Neuroimaging Analysis: Perspectives, Methods, and Challenges", "categories": ["cs.LG", "cs.AI", "eess.IV", "q-bio.NC", "I.2.0"], "comment": "20 pages, 3 figures, 2 tables; This paper has been submitted for\n  possible publication, and currently under review", "summary": "Neuroimaging provides a critical framework for characterizing brain activity\nby quantifying connectivity patterns and functional architecture across\nmodalities. While modern machine learning has significantly advanced our\nunderstanding of neural processing mechanisms through these datasets, decoding\ntask-specific signatures must contend with inherent neuroimaging constraints,\nfor example, low signal-to-noise ratios in raw electrophysiological recordings,\ncross-session non-stationarity, and limited sample sizes. This review focuses\non machine learning approaches for covariance-based neuroimaging data, where\noften symmetric positive definite (SPD) matrices under full-rank conditions\nencode inter-channel relationships. By equipping the space of SPD matrices with\nRiemannian metrics (e.g., affine-invariant or log-Euclidean), their space forms\na Riemannian manifold enabling geometric analysis. We unify methodologies\noperating on this manifold under the SPD learning framework, which\nsystematically leverages the SPD manifold's geometry to process covariance\nfeatures, thereby advancing brain imaging analytics."}
{"id": "2504.19754", "pdf": "https://arxiv.org/pdf/2504.19754", "abs": "https://arxiv.org/abs/2504.19754", "authors": ["Carlo Merola", "Jaspinder Singh"], "title": "Reconstructing Context: Evaluating Advanced Chunking Strategies for Retrieval-Augmented Generation", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "13 pages, 2 figures, Second Workshop on Knowledge-Enhanced\n  Information Retrieval, ECIR 2025", "summary": "Retrieval-augmented generation (RAG) has become a transformative approach for\nenhancing large language models (LLMs) by grounding their outputs in external\nknowledge sources. Yet, a critical question persists: how can vast volumes of\nexternal knowledge be managed effectively within the input constraints of LLMs?\nTraditional methods address this by chunking external documents into smaller,\nfixed-size segments. While this approach alleviates input limitations, it often\nfragments context, resulting in incomplete retrieval and diminished coherence\nin generation. To overcome these shortcomings, two advanced techniques, late\nchunking and contextual retrieval, have been introduced, both aiming to\npreserve global context. Despite their potential, their comparative strengths\nand limitations remain unclear. This study presents a rigorous analysis of late\nchunking and contextual retrieval, evaluating their effectiveness and\nefficiency in optimizing RAG systems. Our results indicate that contextual\nretrieval preserves semantic coherence more effectively but requires greater\ncomputational resources. In contrast, late chunking offers higher efficiency\nbut tends to sacrifice relevance and completeness."}
{"id": "2504.19574", "pdf": "https://arxiv.org/pdf/2504.19574", "abs": "https://arxiv.org/abs/2504.19574", "authors": ["Seongmin Hwang", "Daeyoung Han", "Moongu Jeon"], "title": "DG-DETR: Toward Domain Generalized Detection Transformer", "categories": ["cs.CV"], "comment": "Under Review", "summary": "End-to-end Transformer-based detectors (DETRs) have demonstrated strong\ndetection performance. However, domain generalization (DG) research has\nprimarily focused on convolutional neural network (CNN)-based detectors, while\npaying little attention to enhancing the robustness of DETRs. In this letter,\nwe introduce a Domain Generalized DEtection TRansformer (DG-DETR), a simple,\neffective, and plug-and-play method that improves out-of-distribution (OOD)\nrobustness for DETRs. Specifically, we propose a novel domain-agnostic query\nselection strategy that removes domain-induced biases from object queries via\northogonal projection onto the instance-specific style space. Additionally, we\nleverage a wavelet decomposition to disentangle features into domain-invariant\nand domain-specific components, enabling synthesis of diverse latent styles\nwhile preserving the semantic features of objects. Experimental results\nvalidate the effectiveness of DG-DETR. Our code is available at\nhttps://github.com/sminhwang/DG-DETR."}
{"id": "2504.18884", "pdf": "https://arxiv.org/pdf/2504.18884", "abs": "https://arxiv.org/abs/2504.18884", "authors": ["Junichiro Niimi"], "title": "A Simple Ensemble Strategy for LLM Inference: Towards More Stable Text Classification", "categories": ["cs.CL", "cs.AI"], "comment": "This manuscript has been accepted for the 30th International\n  Conference on Natural Language & Information Systems (NLDB 2025). The final\n  version will appear in the Springer LNCS proceedings. arXiv admin note: text\n  overlap with arXiv:2407.13069", "summary": "With the advance of large language models (LLMs), LLMs have been utilized for\nthe various tasks. However, the issues of variability and reproducibility of\nresults from each trial of LLMs have been largely overlooked in existing\nliterature while actual human annotation uses majority voting to resolve\ndisagreements among annotators. Therefore, this study introduces the\nstraightforward ensemble strategy to a sentiment analysis using LLMs. As the\nresults, we demonstrate that the ensemble of multiple inference using\nmedium-sized LLMs produces more robust and accurate results than using a large\nmodel with a single attempt with reducing RMSE by 18.6%."}
{"id": "2504.19581", "pdf": "https://arxiv.org/pdf/2504.19581", "abs": "https://arxiv.org/abs/2504.19581", "authors": ["Chengzhi Wu", "Yuxin Wan", "Hao Fu", "Julius Pfrommer", "Zeyun Zhong", "Junwei Zheng", "Jiaming Zhang", "Jürgen Beyerer"], "title": "SAMBLE: Shape-Specific Point Cloud Sampling for an Optimal Trade-Off Between Local Detail and Global Uniformity", "categories": ["cs.CV"], "comment": null, "summary": "Driven by the increasing demand for accurate and efficient representation of\n3D data in various domains, point cloud sampling has emerged as a pivotal\nresearch topic in 3D computer vision. Recently, learning-to-sample methods have\ngarnered growing interest from the community, particularly for their ability to\nbe jointly trained with downstream tasks. However, previous learning-based\nsampling methods either lead to unrecognizable sampling patterns by generating\na new point cloud or biased sampled results by focusing excessively on sharp\nedge details. Moreover, they all overlook the natural variations in point\ndistribution across different shapes, applying a similar sampling strategy to\nall point clouds. In this paper, we propose a Sparse Attention Map and\nBin-based Learning method (termed SAMBLE) to learn shape-specific sampling\nstrategies for point cloud shapes. SAMBLE effectively achieves an improved\nbalance between sampling edge points for local details and preserving\nuniformity in the global shape, resulting in superior performance across\nmultiple common point cloud downstream tasks, even in scenarios with few-point\nsampling."}
{"id": "2504.18886", "pdf": "https://arxiv.org/pdf/2504.18886", "abs": "https://arxiv.org/abs/2504.18886", "authors": ["Simone Maurizio La Cava", "Roberto Casula", "Sara Concas", "Giulia Orrù", "Ruben Tolosana", "Martin Drahansky", "Julian Fierrez", "Gian Luca Marcialis"], "title": "Exploiting Multiple Representations: 3D Face Biometrics Fusion with Application to Surveillance", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "3D face reconstruction (3DFR) algorithms are based on specific assumptions\ntailored to the limits and characteristics of the different application\nscenarios. In this study, we investigate how multiple state-of-the-art 3DFR\nalgorithms can be used to generate a better representation of subjects, with\nthe final goal of improving the performance of face recognition systems in\nchallenging uncontrolled scenarios. We also explore how different parametric\nand non-parametric score-level fusion methods can exploit the unique strengths\nof multiple 3DFR algorithms to enhance biometric recognition robustness. With\nthis goal, we propose a comprehensive analysis of several face recognition\nsystems across diverse conditions, such as varying distances and camera setups,\nintra-dataset and cross-dataset, to assess the robustness of the proposed\nensemble method. The results demonstrate that the distinct information provided\nby different 3DFR algorithms can alleviate the problem of generalizing over\nmultiple application scenarios. In addition, the present study highlights the\npotential of advanced fusion strategies to enhance the reliability of\n3DFR-based face recognition systems, providing the research community with key\ninsights to exploit them in real-world applications effectively. Although the\nexperiments are carried out in a specific face verification setup, our proposed\nfusion-based 3DFR methods may be applied to other tasks around face biometrics\nthat are not strictly related to identity recognition."}
{"id": "2504.19584", "pdf": "https://arxiv.org/pdf/2504.19584", "abs": "https://arxiv.org/abs/2504.19584", "authors": ["Sangmin Kim", "Seunguk Do", "Jaesik Park"], "title": "ShowMak3r: Compositional TV Show Reconstruction", "categories": ["cs.CV"], "comment": "Project page : https://nstar1125.github.io/showmak3r", "summary": "Reconstructing dynamic radiance fields from video clips is challenging,\nespecially when entertainment videos like TV shows are given. Many challenges\nmake the reconstruction difficult due to (1) actors occluding with each other\nand having diverse facial expressions, (2) cluttered stages, and (3) small\nbaseline views or sudden shot changes. To address these issues, we present\nShowMak3r, a comprehensive reconstruction pipeline that allows the editing of\nscenes like how video clips are made in a production control room. In\nShowMak3r, a 3DLocator module locates recovered actors on the stage using depth\nprior and estimates unseen human poses via interpolation. The proposed\nShotMatcher module then tracks the actors under shot changes. Furthermore,\nShowMak3r introduces a face-fitting network that dynamically recovers the\nactors' expressions. Experiments on Sitcoms3D dataset show that our pipeline\ncan reassemble TV show scenes with new cameras at different timestamps. We also\ndemonstrate that ShowMak3r enables interesting applications such as synthetic\nshot-making, actor relocation, insertion, deletion, and pose manipulation.\nProject page : https://nstar1125.github.io/showmak3r"}
{"id": "2504.18902", "pdf": "https://arxiv.org/pdf/2504.18902", "abs": "https://arxiv.org/abs/2504.18902", "authors": ["Cyril Shih-Huan Hsu", "Anestis Dalgkitsis", "Chrysa Papagianni", "Paola Grosso"], "title": "Transformer-Empowered Actor-Critic Reinforcement Learning for Sequence-Aware Service Function Chain Partitioning", "categories": ["cs.NI", "cs.AI", "cs.LG", "cs.NE"], "comment": null, "summary": "In the forthcoming era of 6G networks, characterized by unprecedented data\nrates, ultra-low latency, and extensive connectivity, effective management of\nVirtualized Network Functions (VNFs) is essential. VNFs are software-based\ncounterparts of traditional hardware devices that facilitate flexible and\nscalable service provisioning. Service Function Chains (SFCs), structured as\nordered sequences of VNFs, are pivotal in orchestrating complex network\nservices. Nevertheless, partitioning SFCs across multi-domain network\ninfrastructures presents substantial challenges due to stringent latency\nconstraints and limited resource availability. Conventional optimization-based\nmethods typically exhibit low scalability, whereas existing data-driven\napproaches often fail to adequately balance computational efficiency with the\ncapability to effectively account for dependencies inherent in SFCs. To\novercome these limitations, we introduce a Transformer-empowered actor-critic\nframework specifically designed for sequence-aware SFC partitioning. By\nutilizing the self-attention mechanism, our approach effectively models complex\ninter-dependencies among VNFs, facilitating coordinated and parallelized\ndecision-making processes. Additionally, we enhance training stability and\nconvergence using $\\epsilon$-LoPe exploration strategy as well as Asymptotic\nReturn Normalization. Comprehensive simulation results demonstrate that the\nproposed methodology outperforms existing state-of-the-art solutions in terms\nof long-term acceptance rates, resource utilization efficiency, and\nscalability, while achieving rapid inference. This study not only advances\nintelligent network orchestration by delivering a scalable and robust solution\nfor SFC partitioning within emerging 6G environments, but also bridging recent\nadvancements in Large Language Models (LLMs) with the optimization of\nnext-generation networks."}
{"id": "2504.19589", "pdf": "https://arxiv.org/pdf/2504.19589", "abs": "https://arxiv.org/abs/2504.19589", "authors": ["Daniele Rege Cambrin", "Luca Colomba", "Paolo Garza"], "title": "Magnifier: A Multi-grained Neural Network-based Architecture for Burned Area Delineation", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted in IEEE Journal of Selected Topics in Applied Earth\n  Observations and Remote Sensing", "summary": "In crisis management and remote sensing, image segmentation plays a crucial\nrole, enabling tasks like disaster response and emergency planning by analyzing\nvisual data. Neural networks are able to analyze satellite acquisitions and\ndetermine which areas were affected by a catastrophic event. The problem in\ntheir development in this context is the data scarcity and the lack of\nextensive benchmark datasets, limiting the capabilities of training large\nneural network models. In this paper, we propose a novel methodology, namely\nMagnifier, to improve segmentation performance with limited data availability.\nThe Magnifier methodology is applicable to any existing encoder-decoder\narchitecture, as it extends a model by merging information at different\ncontextual levels through a dual-encoder approach: a local and global encoder.\nMagnifier analyzes the input data twice using the dual-encoder approach. In\nparticular, the local and global encoders extract information from the same\ninput at different granularities. This allows Magnifier to extract more\ninformation than the other approaches given the same set of input images.\nMagnifier improves the quality of the results of +2.65% on average IoU while\nleading to a restrained increase in terms of the number of trainable parameters\ncompared to the original model. We evaluated our proposed approach with\nstate-of-the-art burned area segmentation models, demonstrating, on average,\ncomparable or better performances in less than half of the GFLOPs."}
{"id": "2504.18910", "pdf": "https://arxiv.org/pdf/2504.18910", "abs": "https://arxiv.org/abs/2504.18910", "authors": ["Ali Nazari", "Mohsen Ebrahimi Moghaddam", "Omidreza Borzoei"], "title": "Kinship Verification through a Forest Neural Network", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Early methods used face representations in kinship verification, which are\nless accurate than joint representations of parents' and children's facial\nimages learned from scratch. We propose an approach featuring graph neural\nnetwork concepts to utilize face representations and have comparable results to\njoint representation algorithms. Moreover, we designed the structure of the\nclassification module and introduced a new combination of losses to engage the\ncenter loss gradually in training our network. Additionally, we conducted\nexperiments on KinFaceW-I and II, demonstrating the effectiveness of our\napproach. We achieved the best result on KinFaceW-II, an average improvement of\nnearly 1.6 for all kinship types, and we were near the best on KinFaceW-I. The\ncode is available at https://github.com/ali-nazari/Kinship-Verification"}
{"id": "2504.19592", "pdf": "https://arxiv.org/pdf/2504.19592", "abs": "https://arxiv.org/abs/2504.19592", "authors": ["Roman Malashin", "Daniil Ilyukhin"], "title": "Neural network task specialization via domain constraining", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper introduces a concept of neural network specialization via\ntask-specific domain constraining, aimed at enhancing network performance on\ndata subspace in which the network operates. The study presents experiments on\ntraining specialists for image classification and object detection tasks. The\nresults demonstrate that specialization can enhance a generalist's accuracy\neven without additional data or changing training regimes: solely by\nconstraining class label space in which the network performs. Theoretical and\nexperimental analyses indicate that effective specialization requires modifying\ntraditional fine-tuning methods and constraining data space to semantically\ncoherent subsets. The specialist extraction phase before tuning the network is\nproposed for maximal performance gains. We also provide analysis of the\nevolution of the feature space during specialization. This study paves way to\nfuture research for developing more advanced dynamically configurable image\nanalysis systems, where computations depend on the specific input.\nAdditionally, the proposed methods can help improve system performance in\nscenarios where certain data domains should be excluded from consideration of\nthe generalist network."}
{"id": "2504.18916", "pdf": "https://arxiv.org/pdf/2504.18916", "abs": "https://arxiv.org/abs/2504.18916", "authors": ["Sarang S", "Druva Dhakshinamoorthy", "Aditya Shiva Sharma", "Yuvraj Singh Bhadauria", "Siddharth Chaitra Vivek", "Arihant Bansal", "Arnab K. Paul"], "title": "UnifyFL: Enabling Decentralized Cross-Silo Federated Learning", "categories": ["cs.DC", "cs.AI"], "comment": "12 pages, 7 figures, 7 tables. Accepted at the 26th ACM/IFIP\n  International Middleware Conference (MIDDLEWARE 2025)", "summary": "Federated Learning (FL) is a decentralized machine learning (ML) paradigm in\nwhich models are trained on private data across several devices called clients\nand combined at a single node called an aggregator rather than aggregating the\ndata itself. Many organizations employ FL to have better privacy-aware\nML-driven decision-making capabilities. However, organizations often operate\nindependently rather than collaborate to enhance their FL capabilities due to\nthe lack of an effective mechanism for collaboration. The challenge lies in\nbalancing trust and resource efficiency. One approach relies on trusting a\nthird-party aggregator to consolidate models from all organizations (multilevel\nFL), but this requires trusting an entity that may be biased or unreliable.\nAlternatively, organizations can bypass a third party by sharing their local\nmodels directly, which requires significant computational resources for\nvalidation. Both approaches reflect a fundamental trade-off between trust and\nresource constraints, with neither offering an ideal solution. In this work, we\ndevelop a trust-based cross-silo FL framework called \\proj, which uses\ndecentralized orchestration and distributed storage. \\proj provides flexibility\nto the participating organizations and presents synchronous and asynchronous\nmodes to handle stragglers. Our evaluation on a diverse testbed shows that\n\\proj achieves a performance comparable to the ideal multilevel centralized FL\nwhile allowing trust and optimal use of resources."}
{"id": "2504.19598", "pdf": "https://arxiv.org/pdf/2504.19598", "abs": "https://arxiv.org/abs/2504.19598", "authors": ["Dou Quan", "Rufan Zhou", "Shuang Wang", "Ning Huyan", "Dong Zhao", "Yunan Li", "Licheng Jiao"], "title": "Lightweight Adapter Learning for More Generalized Remote Sensing Change Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Deep learning methods have shown promising performances in remote sensing\nimage change detection (CD). However, existing methods usually train a\ndataset-specific deep network for each dataset. Due to the significant\ndifferences in the data distribution and labeling between various datasets, the\ntrained dataset-specific deep network has poor generalization performances on\nother datasets. To solve this problem, this paper proposes a change adapter\nnetwork (CANet) for a more universal and generalized CD. CANet contains\ndataset-shared and dataset-specific learning modules. The former explores the\ndiscriminative features of images, and the latter designs a lightweight adapter\nmodel, to deal with the characteristics of different datasets in data\ndistribution and labeling. The lightweight adapter can quickly generalize the\ndeep network for new CD tasks with a small computation cost. Specifically, this\npaper proposes an interesting change region mask (ICM) in the adapter, which\ncan adaptively focus on interested change objects and decrease the influence of\nlabeling differences in various datasets. Moreover, CANet adopts a unique batch\nnormalization layer for each dataset to deal with data distribution\ndifferences. Compared with existing deep learning methods, CANet can achieve\nsatisfactory CD performances on various datasets simultaneously. Experimental\nresults on several public datasets have verified the effectiveness and\nadvantages of the proposed CANet on CD. CANet has a stronger generalization\nability, smaller training costs (merely updating 4.1%-7.7% parameters), and\nbetter performances under limited training datasets than other deep learning\nmethods, which also can be flexibly inserted with existing deep models."}
{"id": "2504.18919", "pdf": "https://arxiv.org/pdf/2504.18919", "abs": "https://arxiv.org/abs/2504.18919", "authors": ["Andrew M. Bean", "Rebecca Payne", "Guy Parsons", "Hannah Rose Kirk", "Juan Ciro", "Rafael Mosquera", "Sara Hincapié Monsalve", "Aruna S. Ekanayaka", "Lionel Tarassenko", "Luc Rocher", "Adam Mahdi"], "title": "Clinical knowledge in LLMs does not translate to human interactions", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "52 pages, 4 figures", "summary": "Global healthcare providers are exploring use of large language models (LLMs)\nto provide medical advice to the public. LLMs now achieve nearly perfect scores\non medical licensing exams, but this does not necessarily translate to accurate\nperformance in real-world settings. We tested if LLMs can assist members of the\npublic in identifying underlying conditions and choosing a course of action\n(disposition) in ten medical scenarios in a controlled study with 1,298\nparticipants. Participants were randomly assigned to receive assistance from an\nLLM (GPT-4o, Llama 3, Command R+) or a source of their choice (control). Tested\nalone, LLMs complete the scenarios accurately, correctly identifying conditions\nin 94.9% of cases and disposition in 56.3% on average. However, participants\nusing the same LLMs identified relevant conditions in less than 34.5% of cases\nand disposition in less than 44.2%, both no better than the control group. We\nidentify user interactions as a challenge to the deployment of LLMs for medical\nadvice. Standard benchmarks for medical knowledge and simulated patient\ninteractions do not predict the failures we find with human participants.\nMoving forward, we recommend systematic human user testing to evaluate\ninteractive capabilities prior to public deployments in healthcare."}
{"id": "2504.19600", "pdf": "https://arxiv.org/pdf/2504.19600", "abs": "https://arxiv.org/abs/2504.19600", "authors": ["Pengfei Zhang", "Shouqing Jia"], "title": "Image Generation Method Based on Heat Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Denoising Diffusion Probabilistic Models (DDPMs) achieve high-quality image\ngeneration without adversarial training, but they process images as a whole.\nSince adjacent pixels are highly likely to belong to the same object, we\npropose the Heat Diffusion Model (HDM) to further preserve image details and\ngenerate more realistic images. HDM is a model that incorporates pixel-level\noperations while maintaining the same training process as DDPM. In HDM, the\ndiscrete form of the two-dimensional heat equation is integrated into the\ndiffusion and generation formulas of DDPM, enabling the model to compute\nrelationships between neighboring pixels during image processing. Our\nexperiments demonstrate that HDM can generate higher-quality samples compared\nto models such as DDPM, Consistency Diffusion Models (CDM), Latent Diffusion\nModels (LDM), and Vector Quantized Generative Adversarial Networks (VQGAN)."}
{"id": "2504.18929", "pdf": "https://arxiv.org/pdf/2504.18929", "abs": "https://arxiv.org/abs/2504.18929", "authors": ["Ruifeng Ren", "Yong Liu"], "title": "Revisiting Transformers through the Lens of Low Entropy and Dynamic Sparsity", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Compression has been a critical lens to understand the success of\nTransformers. In the past, we have typically taken the target distribution as a\ncriterion to evaluate a model's compression performance. Nevertheless,it often\nremains challenging to precisely assess how well the model achieves compression\nand to compare the information content of the learned distribution with that of\nthe target distribution during compression,as the target distribution is\ntypically unknown and entropy computation often incurs exponential cost. In\nthis work, we explore these issues under a controlled experimental setup. We\nfind that Transformers exhibit a unique inductive bias in data compression:\nbeyond approaching the target distribution, they tend to favor learning\nlower-entropy distributions, with this tendency becoming more pronounced as the\nmodel size increases. This preference prevents Transformers from perfectly\naligning with the target distribution, instead further compressing its\ninformation content. Furthermore, we show that the FFN module plays a critical\nrole in driving this bias. In addition, while models remove informational\nredundancy from data during compression, they also exhibit redundancy within\ntheir parameters, which enables compression and can be characterized through\ndynamic sparsity. However, the dynamic sparsity patterns in Transformers,\nparticularly in attention and FFN modules, demand further exploration. As for\nthis, we show that larger Transformers show stronger preferences for bypassing\nattention computations via residual connections and have lower proportion of\nactive neurons. Interestingly, we also find that training instability in larger\nmodels strongly correlates with sudden increases in dead neurons. Our work\ncontributes to a deeper understanding of Transformers from the lens of entropy\nand dynamic sparsity."}
{"id": "2504.19614", "pdf": "https://arxiv.org/pdf/2504.19614", "abs": "https://arxiv.org/abs/2504.19614", "authors": ["Junpeng Jiang", "Gangyi Hong", "Miao Zhang", "Hengtong Hu", "Kun Zhan", "Rui Shao", "Liqiang Nie"], "title": "DiVE: Efficient Multi-View Driving Scenes Generation Based on Video Diffusion Transformer", "categories": ["cs.CV"], "comment": null, "summary": "Collecting multi-view driving scenario videos to enhance the performance of\n3D visual perception tasks presents significant challenges and incurs\nsubstantial costs, making generative models for realistic data an appealing\nalternative. Yet, the videos generated by recent works suffer from poor quality\nand spatiotemporal consistency, undermining their utility in advancing\nperception tasks under driving scenarios. To address this gap, we propose DiVE,\na diffusion transformer-based generative framework meticulously engineered to\nproduce high-fidelity, temporally coherent, and cross-view consistent\nmulti-view videos, aligning seamlessly with bird's-eye view layouts and textual\ndescriptions. DiVE leverages a unified cross-attention and a SketchFormer to\nexert precise control over multimodal data, while incorporating a view-inflated\nattention mechanism that adds no extra parameters, thereby guaranteeing\nconsistency across views. Despite these advancements, synthesizing\nhigh-resolution videos under multimodal constraints introduces dual challenges:\ninvestigating the optimal classifier-free guidance coniguration under intricate\nmulti-condition inputs and mitigating excessive computational latency in\nhigh-resolution rendering--both of which remain underexplored in prior\nresearches. To resolve these limitations, we introduce two innovations:\nMulti-Control Auxiliary Branch Distillation, which streamlines multi-condition\nCFG selection while circumventing high computational overhead, and Resolution\nProgressive Sampling, a training-free acceleration strategy that staggers\nresolution scaling to reduce high latency due to high resolution. These\ninnovations collectively achieve a 2.62x speedup with minimal quality\ndegradation. Evaluated on the nuScenes dataset, DiVE achieves SOTA performance\nin multi-view video generation, yielding photorealistic outputs with\nexceptional temporal and cross-view coherence."}
{"id": "2504.18931", "pdf": "https://arxiv.org/pdf/2504.18931", "abs": "https://arxiv.org/abs/2504.18931", "authors": ["Dianwei Chen", "Yaobang Gong", "Xianfeng Yang"], "title": "Advanced Longitudinal Control and Collision Avoidance for High-Risk Edge Cases in Autonomous Driving", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Advanced Driver Assistance Systems (ADAS) and Advanced Driving Systems (ADS)\nare key to improving road safety, yet most existing implementations focus\nprimarily on the vehicle ahead, neglecting the behavior of following vehicles.\nThis shortfall often leads to chain reaction collisions in high speed, densely\nspaced traffic particularly when a middle vehicle suddenly brakes and trailing\nvehicles cannot respond in time. To address this critical gap, we propose a\nnovel longitudinal control and collision avoidance algorithm that integrates\nadaptive cruising with emergency braking. Leveraging deep reinforcement\nlearning, our method simultaneously accounts for both leading and following\nvehicles. Through a data preprocessing framework that calibrates real-world\nsensor data, we enhance the robustness and reliability of the training process,\nensuring the learned policy can handle diverse driving conditions. In simulated\nhigh risk scenarios (e.g., emergency braking in dense traffic), the algorithm\neffectively prevents potential pile up collisions, even in situations involving\nheavy duty vehicles. Furthermore, in typical highway scenarios where three\nvehicles decelerate, the proposed DRL approach achieves a 99% success rate far\nsurpassing the standard Federal Highway Administration speed concepts guide,\nwhich reaches only 36.77% success under the same conditions."}
{"id": "2504.19634", "pdf": "https://arxiv.org/pdf/2504.19634", "abs": "https://arxiv.org/abs/2504.19634", "authors": ["Yechan Kim", "DongHo Yoon", "SooYeon Kim", "Moongu Jeon"], "title": "NSegment : Noisy Segment Improves Remote Sensing Image Segmentation", "categories": ["cs.CV"], "comment": "Preprint", "summary": "Labeling errors in remote sensing (RS) image segmentation datasets often\nremain implicit and subtle due to ambiguous class boundaries, mixed pixels,\nshadows, complex terrain features, and subjective annotator bias. Furthermore,\nthe scarcity of annotated RS data due to high image acquisition and labeling\ncosts complicates training noise-robust models. While sophisticated mechanisms\nsuch as label selection or noise correction might address this issue, they tend\nto increase training time and add implementation complexity. In this letter, we\npropose NSegment-a simple yet effective data augmentation solution to mitigate\nthis issue. Unlike traditional methods, it applies elastic transformations only\nto segmentation labels, varying deformation intensity per sample in each\ntraining epoch to address annotation inconsistencies. Experimental results\ndemonstrate that our approach improves the performance of RS image segmentation\non various state-of-the-art models."}
{"id": "2504.18932", "pdf": "https://arxiv.org/pdf/2504.18932", "abs": "https://arxiv.org/abs/2504.18932", "authors": ["Dong Whi Yoo", "Jiayue Melissa Shi", "Violeta J. Rodriguez", "Koustuv Saha"], "title": "AI Chatbots for Mental Health: Values and Harms from Lived Experiences of Depression", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Recent advancements in LLMs enable chatbots to interact with individuals on a\nrange of queries, including sensitive mental health contexts. Despite\nuncertainties about their effectiveness and reliability, the development of\nLLMs in these areas is growing, potentially leading to harms. To better\nidentify and mitigate these harms, it is critical to understand how the values\nof people with lived experiences relate to the harms. In this study, we\ndeveloped a technology probe, a GPT-4o based chatbot called Zenny, enabling\nparticipants to engage with depression self-management scenarios informed by\nprevious research. We used Zenny to interview 17 individuals with lived\nexperiences of depression. Our thematic analysis revealed key values:\ninformational support, emotional support, personalization, privacy, and crisis\nmanagement. This work explores the relationship between lived experience\nvalues, potential harms, and design recommendations for mental health AI\nchatbots, aiming to enhance self-management support while minimizing risks."}
{"id": "2504.19637", "pdf": "https://arxiv.org/pdf/2504.19637", "abs": "https://arxiv.org/abs/2504.19637", "authors": ["Junlong Ren", "Gangjian Zhang", "Yu Hu", "Jian Shu", "Hao Wang"], "title": "Exploiting Inter-Sample Correlation and Intra-Sample Redundancy for Partially Relevant Video Retrieval", "categories": ["cs.CV"], "comment": null, "summary": "Partially Relevant Video Retrieval (PRVR) aims to retrieve the target video\nthat is partially relevant to the text query. The primary challenge in PRVR\narises from the semantic asymmetry between textual and visual modalities, as\nvideos often contain substantial content irrelevant to the query. Existing\nmethods coarsely align paired videos and text queries to construct the semantic\nspace, neglecting the critical cross-modal dual nature inherent in this task:\ninter-sample correlation and intra-sample redundancy. To this end, we propose a\nnovel PRVR framework to systematically exploit these two characteristics. Our\nframework consists of three core modules. First, the Inter Correlation\nEnhancement (ICE) module captures inter-sample correlation by identifying\nsemantically similar yet unpaired text queries and video moments, combining\nthem to form pseudo-positive pairs for more robust semantic space construction.\nSecond, the Intra Redundancy Mining (IRM) module mitigates intra-sample\nredundancy by mining redundant video moment features and treating them as hard\nnegative samples, thereby encouraging the model to learn more discriminative\nrepresentations. Finally, to reinforce these modules, we introduce the Temporal\nCoherence Prediction (TCP) module, which enhances feature discrimination by\ntraining the model to predict the original temporal order of randomly shuffled\nvideo frames and moments. Extensive experiments on three datasets demonstrate\nthe superiority of our approach compared to previous methods, achieving\nstate-of-the-art results."}
{"id": "2504.18942", "pdf": "https://arxiv.org/pdf/2504.18942", "abs": "https://arxiv.org/abs/2504.18942", "authors": ["Debarati Das", "Khanh Chi Le", "Ritik Sachin Parkar", "Karin De Langis", "Brendan Madson", "Chad M. Berryman", "Robin M. Willis", "Daniel H. Moses", "Brett McDonnell", "Daniel Schwarcz", "Dongyeop Kang"], "title": "LawFlow : Collecting and Simulating Lawyers' Thought Processes", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "submitted to COLM 2025", "summary": "Legal practitioners, particularly those early in their careers, face complex,\nhigh-stakes tasks that require adaptive, context-sensitive reasoning. While AI\nholds promise in supporting legal work, current datasets and models are\nnarrowly focused on isolated subtasks and fail to capture the end-to-end\ndecision-making required in real-world practice. To address this gap, we\nintroduce LawFlow, a dataset of complete end-to-end legal workflows collected\nfrom trained law students, grounded in real-world business entity formation\nscenarios. Unlike prior datasets focused on input-output pairs or linear chains\nof thought, LawFlow captures dynamic, modular, and iterative reasoning\nprocesses that reflect the ambiguity, revision, and client-adaptive strategies\nof legal practice. Using LawFlow, we compare human and LLM-generated workflows,\nrevealing systematic differences in structure, reasoning flexibility, and plan\nexecution. Human workflows tend to be modular and adaptive, while LLM workflows\nare more sequential, exhaustive, and less sensitive to downstream implications.\nOur findings also suggest that legal professionals prefer AI to carry out\nsupportive roles, such as brainstorming, identifying blind spots, and surfacing\nalternatives, rather than executing complex workflows end-to-end. Building on\nthese findings, we propose a set of design suggestions, rooted in empirical\nobservations, that align AI assistance with human goals of clarity,\ncompleteness, creativity, and efficiency, through hybrid planning, adaptive\nexecution, and decision-point support. Our results highlight both the current\nlimitations of LLMs in supporting complex legal workflows and opportunities for\ndeveloping more collaborative, reasoning-aware legal AI systems. All data and\ncode are available on our project page\n(https://minnesotanlp.github.io/LawFlow-website/)."}
{"id": "2504.19643", "pdf": "https://arxiv.org/pdf/2504.19643", "abs": "https://arxiv.org/abs/2504.19643", "authors": ["Pin-Chi Pan", "Soo-Chang Pei"], "title": "BARIS: Boundary-Aware Refinement with Environmental Degradation Priors for Robust Underwater Instance Segmentation", "categories": ["cs.CV"], "comment": "15 pages, 9 figures, and 11 tables", "summary": "Underwater instance segmentation is challenging due to adverse visual\nconditions such as light attenuation, scattering, and color distortion, which\ndegrade model performance. In this work, we propose BARIS-Decoder\n(Boundary-Aware Refinement Decoder for Instance Segmentation), a framework that\nenhances segmentation accuracy through feature refinement. To address\nunderwater degradations, we introduce the Environmental Robust Adapter (ERA),\nwhich efficiently models underwater degradation patterns while reducing\ntrainable parameters by over 90\\% compared to full fine-tuning. The integration\nof BARIS-Decoder with ERA-tuning, referred to as BARIS-ERA, achieves\nstate-of-the-art performance, surpassing Mask R-CNN by 3.4 mAP with a Swin-B\nbackbone and 3.8 mAP with ConvNeXt V2. Our findings demonstrate the\neffectiveness of BARIS-ERA in advancing underwater instance segmentation,\nproviding a robust and efficient solution."}
{"id": "2504.18943", "pdf": "https://arxiv.org/pdf/2504.18943", "abs": "https://arxiv.org/abs/2504.18943", "authors": ["Martin Berger", "Nathanaël Fijalkow", "Mojtaba Valizadeh"], "title": "GPU accelerated program synthesis: Enumerate semantics, not syntax!", "categories": ["cs.PL", "cs.AI", "cs.LO", "68", "D.3"], "comment": "10 pages", "summary": "Program synthesis is an umbrella term for generating programs and logical\nformulae from specifications. With the remarkable performance improvements that\nGPUs enable for deep learning, a natural question arose: can we also implement\na search-based program synthesiser on GPUs to achieve similar performance\nimprovements? In this article we discuss our insights on this question, based\non recent works~. The goal is to build a synthesiser running on GPUs which\ntakes as input positive and negative example traces and returns a logical\nformula accepting the positive and rejecting the negative traces. With\nGPU-friendly programming techniques -- using the semantics of formulae to\nminimise data movement and reduce data-dependent branching -- our synthesiser\nscales to significantly larger synthesis problems, and operates much faster\nthan the previous CPU-based state-of-the-art. We believe the insights that make\nour approach GPU-friendly have wide potential for enhancing the performance of\nother formal methods (FM) workloads."}
{"id": "2504.19646", "pdf": "https://arxiv.org/pdf/2504.19646", "abs": "https://arxiv.org/abs/2504.19646", "authors": ["Anjith George", "Sebastien Marcel"], "title": "xEdgeFace: Efficient Cross-Spectral Face Recognition for Edge Devices", "categories": ["cs.CV"], "comment": "11 pages", "summary": "Heterogeneous Face Recognition (HFR) addresses the challenge of matching face\nimages across different sensing modalities, such as thermal to visible or\nnear-infrared to visible, expanding the applicability of face recognition\nsystems in real-world, unconstrained environments. While recent HFR methods\nhave shown promising results, many rely on computation-intensive architectures,\nlimiting their practicality for deployment on resource-constrained edge\ndevices. In this work, we present a lightweight yet effective HFR framework by\nadapting a hybrid CNN-Transformer architecture originally designed for face\nrecognition. Our approach enables efficient end-to-end training with minimal\npaired heterogeneous data while preserving strong performance on standard RGB\nface recognition tasks. This makes it a compelling solution for both\nhomogeneous and heterogeneous scenarios. Extensive experiments across multiple\nchallenging HFR and face recognition benchmarks demonstrate that our method\nconsistently outperforms state-of-the-art approaches while maintaining a low\ncomputational overhead."}
{"id": "2504.18953", "pdf": "https://arxiv.org/pdf/2504.18953", "abs": "https://arxiv.org/abs/2504.18953", "authors": ["Sahar Ramezani Jolfaei", "Sepehr Khodadadi Hossein Abadi"], "title": "Application of the Brain Drain Optimization Algorithm to the N-Queens Problem", "categories": ["cs.NE", "cs.AI"], "comment": null, "summary": "This paper introduces the application of the Brain Drain Optimization\nalgorithm -- a swarm-based metaheuristic inspired by the emigration of\nintellectual elites -- to the N-Queens problem. The N-Queens problem, a classic\ncombinatorial optimization problem, serves as a challenge for applying the\nBRADO. A designed cost function guides the search, and the configurations are\ntuned using a TOPSIS-based multicriteria decision making process. BRADO\nconsistently outperforms alternatives in terms of solution quality, achieving\nfewer threats and better objective function values. To assess BRADO's efficacy,\nit is benchmarked against several established metaheuristic algorithms,\nincluding Particle Swarm Optimization (PSO), Genetic Algorithm (GA),\nImperialist Competitive Algorithm (ICA), Iterated Local Search (ILS), and basic\nLocal Search (LS). The study highlights BRADO's potential as a general-purpose\nsolver for combinatorial problems, opening pathways for future applications in\nother domains of artificial intelligence."}
{"id": "2504.19682", "pdf": "https://arxiv.org/pdf/2504.19682", "abs": "https://arxiv.org/abs/2504.19682", "authors": ["Nikolaos Chaidos", "Angeliki Dimitriou", "Nikolaos Spanos", "Athanasios Voulodimos", "Giorgos Stamou"], "title": "Explaining Vision GNNs: A Semantic and Visual Analysis of Graph-based Image Classification", "categories": ["cs.CV", "cs.LG"], "comment": "13 pages, 3 figures, accepted for presentation at\n  xAI-World-Conference 2025, code is available at\n  https://github.com/nickhaidos/Vision-GNNs-Explainer", "summary": "Graph Neural Networks (GNNs) have emerged as an efficient alternative to\nconvolutional approaches for vision tasks such as image classification,\nleveraging patch-based representations instead of raw pixels. These methods\nconstruct graphs where image patches serve as nodes, and edges are established\nbased on patch similarity or classification relevance. Despite their\nefficiency, the explainability of GNN-based vision models remains\nunderexplored, even though graphs are naturally interpretable. In this work, we\nanalyze the semantic consistency of the graphs formed at different layers of\nGNN-based image classifiers, focusing on how well they preserve object\nstructures and meaningful relationships. A comprehensive analysis is presented\nby quantifying the extent to which inter-layer graph connections reflect\nsemantic similarity and spatial coherence. Explanations from standard and\nadversarial settings are also compared to assess whether they reflect the\nclassifiers' robustness. Additionally, we visualize the flow of information\nacross layers through heatmap-based visualization techniques, thereby\nhighlighting the models' explainability. Our findings demonstrate that the\ndecision-making processes of these models can be effectively explained, while\nalso revealing that their reasoning does not necessarily align with human\nperception, especially in deeper layers."}
{"id": "2504.18954", "pdf": "https://arxiv.org/pdf/2504.18954", "abs": "https://arxiv.org/abs/2504.18954", "authors": ["Marco Mezzina", "Pieter De Backer", "Tom Vercauteren", "Matthew Blaschko", "Alexandre Mottrie", "Tinne Tuytelaars"], "title": "Surgeons vs. Computer Vision: A comparative analysis on surgical phase recognition capabilities", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Purpose: Automated Surgical Phase Recognition (SPR) uses Artificial\nIntelligence (AI) to segment the surgical workflow into its key events,\nfunctioning as a building block for efficient video review, surgical education\nas well as skill assessment. Previous research has focused on short and linear\nsurgical procedures and has not explored if temporal context influences\nexperts' ability to better classify surgical phases. This research addresses\nthese gaps, focusing on Robot-Assisted Partial Nephrectomy (RAPN) as a highly\nnon-linear procedure. Methods: Urologists of varying expertise were grouped and\ntasked to indicate the surgical phase for RAPN on both single frames and video\nsnippets using a custom-made web platform. Participants reported their\nconfidence levels and the visual landmarks used in their decision-making. AI\narchitectures without and with temporal context as trained and benchmarked on\nthe Cholec80 dataset were subsequently trained on this RAPN dataset. Results:\nVideo snippets and presence of specific visual landmarks improved phase\nclassification accuracy across all groups. Surgeons displayed high confidence\nin their classifications and outperformed novices, who struggled discriminating\nphases. The performance of the AI models is comparable to the surgeons in the\nsurvey, with improvements when temporal context was incorporated in both cases.\nConclusion: SPR is an inherently complex task for expert surgeons and computer\nvision, where both perform equally well when given the same context.\nPerformance increases when temporal information is provided. Surgical tools and\norgans form the key landmarks for human interpretation and are expected to\nshape the future of automated SPR."}
{"id": "2504.19684", "pdf": "https://arxiv.org/pdf/2504.19684", "abs": "https://arxiv.org/abs/2504.19684", "authors": ["Anush Lakshman Sivaraman", "Kojo Adu-Gyamfi", "Ibne Farabi Shihab", "Anuj Sharma"], "title": "ClearVision: Leveraging CycleGAN and SigLIP-2 for Robust All-Weather Classification in Traffic Camera Imagery", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Accurate weather classification from low-quality traffic camera imagery\nremains a challenging task, particularly under adverse nighttime conditions. In\nthis study, we propose a scalable framework that combines generative domain\nadaptation with efficient contrastive learning to enhance classification\nperformance. Using CycleGAN-based domain translation, we improve the quality of\nnighttime images, enabling better feature extraction by downstream models.\nWhile the baseline EVA-02 model employing CLIP-based contrastive loss achieves\nan overall accuracy of 96.55\\%, it exhibits a significant performance gap\nbetween daytime (97.21\\%) and nighttime conditions (63.40\\%). Replacing CLIP\nwith the lightweight SigLIP-2 (Sigmoid contrastive loss) achieves a competitive\noverall accuracy of 94.00\\%, with substantial improvements in nighttime\nperformance (85.90\\% accuracy). The combination of Vision-SigLIP-2,\nText-SigLIP-2, CycleGAN, and contrastive training achieves the best nighttime\naccuracy (85.90\\%) among all models tested, while EVA-02 with CycleGAN\nmaintains the highest overall accuracy (97.01\\%) and per-class accuracies.\nThese findings demonstrate the potential of combining domain adaptation and\nefficient contrastive learning to build practical, resource-efficient weather\nclassification systems for intelligent transportation infrastructure."}
{"id": "2504.18961", "pdf": "https://arxiv.org/pdf/2504.18961", "abs": "https://arxiv.org/abs/2504.18961", "authors": ["Junjie Zhou"], "title": "Feature Fusion Revisited: Multimodal CTR Prediction for MMCTR Challenge", "categories": ["cs.IR", "cs.AI"], "comment": "A technical report for the MMCTR Challenge held by EReL@MIR Workshop\n  at WWW 2025", "summary": "With the rapid advancement of Multimodal Large Language Models (MLLMs), an\nincreasing number of researchers are exploring their application in\nrecommendation systems. However, the high latency associated with large models\npresents a significant challenge for such use cases. The EReL@MIR workshop\nprovided a valuable opportunity to experiment with various approaches aimed at\nimproving the efficiency of multimodal representation learning for information\nretrieval tasks. As part of the competition's requirements, participants were\nmandated to submit a technical report detailing their methodologies and\nfindings. Our team was honored to receive the award for Task 2 - Winner\n(Multimodal CTR Prediction). In this technical report, we present our methods\nand key findings. Additionally, we propose several directions for future work,\nparticularly focusing on how to effectively integrate recommendation signals\ninto multimodal representations. The codebase for our implementation is\npublicly available at: https://github.com/Lattice-zjj/MMCTR_Code, and the\ntrained model weights can be accessed at:\nhttps://huggingface.co/FireFlyCourageous/MMCTR_DIN_MicroLens_1M_x1."}
{"id": "2504.19687", "pdf": "https://arxiv.org/pdf/2504.19687", "abs": "https://arxiv.org/abs/2504.19687", "authors": ["Baoshun Shi", "Bing Chen", "Shaolei Zhang", "Huazhu Fu", "Zhanli Hu"], "title": "Prompt Guiding Multi-Scale Adaptive Sparse Representation-driven Network for Low-Dose CT MAR", "categories": ["cs.CV"], "comment": null, "summary": "Low-dose CT (LDCT) is capable of reducing X-ray radiation exposure, but it\nwill potentially degrade image quality, even yields metal artifacts at the case\nof metallic implants. For simultaneous LDCT reconstruction and metal artifact\nreduction (LDMAR), existing deep learning-based efforts face two main\nlimitations: i) the network design neglects multi-scale and within-scale\ninformation; ii) training a distinct model for each dose necessitates\nsignificant storage space for multiple doses. To fill these gaps, we propose a\nprompt guiding multi-scale adaptive sparse representation-driven network,\nabbreviated as PMSRNet, for LDMAR task. Specifically, we construct PMSRNet\ninspired from multi-scale sparsifying frames, and it can simultaneously employ\nwithin-scale characteristics and cross-scale complementarity owing to an\nelaborated prompt guiding scale-adaptive threshold generator (PSATG) and a\nbuilt multi-scale coefficient fusion module (MSFuM). The PSATG can adaptively\ncapture multiple contextual information to generate more faithful thresholds,\nachieved by fusing features from local, regional, and global levels.\nFurthermore, we elaborate a model interpretable dual domain LDMAR framework\ncalled PDuMSRNet, and train single model with a prompt guiding strategy for\nmultiple dose levels. We build a prompt guiding module, whose input contains\ndose level, metal mask and input instance, to provide various guiding\ninformation, allowing a single model to accommodate various CT dose settings.\nExtensive experiments at various dose levels demonstrate that the proposed\nmethods outperform the state-of-the-art LDMAR methods."}
{"id": "2504.19013", "pdf": "https://arxiv.org/pdf/2504.19013", "abs": "https://arxiv.org/abs/2504.19013", "authors": ["Júlia Vicens Figueres", "Juliette Vanderhaeghen", "Federica Bragone", "Kateryna Morozovska", "Khemraj Shukla"], "title": "\\$PINN -- a Domain Decomposition Method for Bayesian Physics-Informed Neural Networks", "categories": ["cs.LG", "cs.AI", "math.AP"], "comment": "37 pages, 22 figures", "summary": "Physics-Informed Neural Networks (PINNs) are a novel computational approach\nfor solving partial differential equations (PDEs) with noisy and sparse initial\nand boundary data. Although, efficient quantification of epistemic and\naleatoric uncertainties in big multi-scale problems remains challenging. We\npropose \\$PINN a novel method of computing global uncertainty in PDEs using a\nBayesian framework, by combining local Bayesian Physics-Informed Neural\nNetworks (BPINN) with domain decomposition. The solution continuity across\nsubdomains is obtained by imposing the flux continuity across the interface of\nneighboring subdomains. To demonstrate the effectiveness of \\$PINN, we conduct\na series of computational experiments on PDEs in 1D and 2D spatial domains.\nAlthough we have adopted conservative PINNs (cPINNs), the method can be\nseamlessly extended to other domain decomposition techniques. The results infer\nthat the proposed method recovers the global uncertainty by computing the local\nuncertainty exactly more efficiently as the uncertainty in each subdomain can\nbe computed concurrently. The robustness of \\$PINN is verified by adding\nuncorrelated random noise to the training data up to 15% and testing for\ndifferent domain sizes."}
{"id": "2504.19695", "pdf": "https://arxiv.org/pdf/2504.19695", "abs": "https://arxiv.org/abs/2504.19695", "authors": ["Lucas Morin", "Gerhard Ingmar Meijer", "Valéry Weber", "Luc Van Gool", "Peter W. J. Staar"], "title": "SubGrapher: Visual Fingerprinting of Chemical Structures", "categories": ["cs.CV"], "comment": null, "summary": "Automatic extraction of chemical structures from scientific literature plays\na crucial role in accelerating research across fields ranging from drug\ndiscovery to materials science. Patent documents, in particular, contain\nmolecular information in visual form, which is often inaccessible through\ntraditional text-based searches. In this work, we introduce SubGrapher, a\nmethod for the visual fingerprinting of chemical structure images. Unlike\nconventional Optical Chemical Structure Recognition (OCSR) models that attempt\nto reconstruct full molecular graphs, SubGrapher focuses on extracting\nmolecular fingerprints directly from chemical structure images. Using\nlearning-based instance segmentation, SubGrapher identifies functional groups\nand carbon backbones, constructing a substructure-based fingerprint that\nenables chemical structure retrieval. Our approach is evaluated against\nstate-of-the-art OCSR and fingerprinting methods, demonstrating superior\nretrieval performance and robustness across diverse molecular depictions. The\ndataset, models, and code will be made publicly available."}
{"id": "2504.19019", "pdf": "https://arxiv.org/pdf/2504.19019", "abs": "https://arxiv.org/abs/2504.19019", "authors": ["Mohammad Akbar-Tajari", "Mohammad Taher Pilehvar", "Mohammad Mahmoody"], "title": "Graph of Attacks: Improved Black-Box and Interpretable Jailbreaks for LLMs", "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": "19 pages, 1 figure, 6 tables", "summary": "The challenge of ensuring Large Language Models (LLMs) align with societal\nstandards is of increasing interest, as these models are still prone to\nadversarial jailbreaks that bypass their safety mechanisms. Identifying these\nvulnerabilities is crucial for enhancing the robustness of LLMs against such\nexploits. We propose Graph of ATtacks (GoAT), a method for generating\nadversarial prompts to test the robustness of LLM alignment using the Graph of\nThoughts framework [Besta et al., 2024]. GoAT excels at generating highly\neffective jailbreak prompts with fewer queries to the victim model than\nstate-of-the-art attacks, achieving up to five times better jailbreak success\nrate against robust models like Llama. Notably, GoAT creates high-quality,\nhuman-readable prompts without requiring access to the targeted model's\nparameters, making it a black-box attack. Unlike approaches constrained by\ntree-based reasoning, GoAT's reasoning is based on a more intricate graph\nstructure. By making simultaneous attack paths aware of each other's progress,\nthis dynamic framework allows a deeper integration and refinement of reasoning\npaths, significantly enhancing the collaborative exploration of adversarial\nvulnerabilities in LLMs. At a technical level, GoAT starts with a graph\nstructure and iteratively refines it by combining and improving thoughts,\nenabling synergy between different thought paths. The code for our\nimplementation can be found at: https://github.com/GoAT-pydev/Graph_of_Attacks."}
{"id": "2504.19706", "pdf": "https://arxiv.org/pdf/2504.19706", "abs": "https://arxiv.org/abs/2504.19706", "authors": ["Song Xia", "Yi Yu", "Henghui Ding", "Wenhan Yang", "Shifei Liu", "Alex C. Kot", "Xudong Jiang"], "title": "Open-set Anomaly Segmentation in Complex Scenarios", "categories": ["cs.CV"], "comment": null, "summary": "Precise segmentation of out-of-distribution (OoD) objects, herein referred to\nas anomalies, is crucial for the reliable deployment of semantic segmentation\nmodels in open-set, safety-critical applications, such as autonomous driving.\nCurrent anomalous segmentation benchmarks predominantly focus on favorable\nweather conditions, resulting in untrustworthy evaluations that overlook the\nrisks posed by diverse meteorological conditions in open-set environments, such\nas low illumination, dense fog, and heavy rain. To bridge this gap, this paper\nintroduces the ComsAmy, a challenging benchmark specifically designed for\nopen-set anomaly segmentation in complex scenarios. ComsAmy encompasses a wide\nspectrum of adverse weather conditions, dynamic driving environments, and\ndiverse anomaly types to comprehensively evaluate the model performance in\nrealistic open-world scenarios. Our extensive evaluation of several\nstate-of-the-art anomalous segmentation models reveals that existing methods\ndemonstrate significant deficiencies in such challenging scenarios,\nhighlighting their serious safety risks for real-world deployment. To solve\nthat, we propose a novel energy-entropy learning (EEL) strategy that integrates\nthe complementary information from energy and entropy to bolster the robustness\nof anomaly segmentation under complex open-world environments. Additionally, a\ndiffusion-based anomalous training data synthesizer is proposed to generate\ndiverse and high-quality anomalous images to enhance the existing copy-paste\ntraining data synthesizer. Extensive experimental results on both public and\nComsAmy benchmarks demonstrate that our proposed diffusion-based synthesizer\nwith energy and entropy learning (DiffEEL) serves as an effective and\ngeneralizable plug-and-play method to enhance existing models, yielding an\naverage improvement of around 4.96% in $\\rm{AUPRC}$ and 9.87% in\n$\\rm{FPR}_{95}$."}
{"id": "2504.19021", "pdf": "https://arxiv.org/pdf/2504.19021", "abs": "https://arxiv.org/abs/2504.19021", "authors": ["Zhyar Rzgar K Rostam", "Gábor Kertész"], "title": "Advancing Scientific Text Classification: Fine-Tuned Models with Dataset Expansion and Hard-Voting", "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 1 figure, 8 tables", "summary": "Efficient text classification is essential for handling the increasing volume\nof academic publications. This study explores the use of pre-trained language\nmodels (PLMs), including BERT, SciBERT, BioBERT, and BlueBERT, fine-tuned on\nthe Web of Science (WoS-46985) dataset for scientific text classification. To\nenhance performance, we augment the dataset by executing seven targeted queries\nin the WoS database, retrieving 1,000 articles per category aligned with\nWoS-46985's main classes. PLMs predict labels for this unlabeled data, and a\nhard-voting strategy combines predictions for improved accuracy and confidence.\nFine-tuning on the expanded dataset with dynamic learning rates and early\nstopping significantly boosts classification accuracy, especially in\nspecialized domains. Domain-specific models like SciBERT and BioBERT\nconsistently outperform general-purpose models such as BERT. These findings\nunderscore the efficacy of dataset augmentation, inference-driven label\nprediction, hard-voting, and fine-tuning techniques in creating robust and\nscalable solutions for automated academic text classification."}
{"id": "2504.19719", "pdf": "https://arxiv.org/pdf/2504.19719", "abs": "https://arxiv.org/abs/2504.19719", "authors": ["Lukas Folkman", "Quynh LK Vo", "Colin Johnston", "Bela Stantic", "Kylie A Pitt"], "title": "A computer vision method to estimate ventilation rate of Atlantic salmon in sea fish farms", "categories": ["cs.CV"], "comment": null, "summary": "The increasing demand for aquaculture production necessitates the development\nof innovative, intelligent tools to effectively monitor and manage fish health\nand welfare. While non-invasive video monitoring has become a common practice\nin finfish aquaculture, existing intelligent monitoring methods predominantly\nfocus on assessing body condition or fish swimming patterns and are often\ndeveloped and evaluated in controlled tank environments, without demonstrating\ntheir applicability to real-world aquaculture settings in open sea farms. This\nunderscores the necessity for methods that can monitor physiological traits\ndirectly within the production environment of sea fish farms. To this end, we\nhave developed a computer vision method for monitoring ventilation rates of\nAtlantic salmon (Salmo salar), which was specifically designed for videos\nrecorded in the production environment of commercial sea fish farms using the\nexisting infrastructure. Our approach uses a fish head detection model, which\nclassifies the mouth state as either open or closed using a convolutional\nneural network. This is followed with multiple object tracking to create\ntemporal sequences of fish swimming across the field of view of the underwater\nvideo camera to estimate ventilation rates. The method demonstrated high\nefficiency, achieving a Pearson correlation coefficient of 0.82 between ground\ntruth and predicted ventilation rates in a test set of 100 fish collected\nindependently of the training data. By accurately identifying pens where fish\nexhibit signs of respiratory distress, our method offers broad applicability\nand the potential to transform fish health and welfare monitoring in finfish\naquaculture."}
{"id": "2504.19030", "pdf": "https://arxiv.org/pdf/2504.19030", "abs": "https://arxiv.org/abs/2504.19030", "authors": ["Sidahmed Lachenani", "Hamza Kheddar", "Mohamed Ouldzmirli"], "title": "Improving Pretrained YAMNet for Enhanced Speech Command Detection via Transfer Learning", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "This work addresses the need for enhanced accuracy and efficiency in speech\ncommand recognition systems, a critical component for improving user\ninteraction in various smart applications. Leveraging the robust pretrained\nYAMNet model and transfer learning, this study develops a method that\nsignificantly improves speech command recognition. We adapt and train a YAMNet\ndeep learning model to effectively detect and interpret speech commands from\naudio signals. Using the extensively annotated Speech Commands dataset\n(speech_commands_v0.01), our approach demonstrates the practical application of\ntransfer learning to accurately recognize a predefined set of speech commands.\nThe dataset is meticulously augmented, and features are strategically extracted\nto boost model performance. As a result, the final model achieved a recognition\naccuracy of 95.28%, underscoring the impact of advanced machine learning\ntechniques on speech command recognition. This achievement marks substantial\nprogress in audio processing technologies and establishes a new benchmark for\nfuture research in the field."}
{"id": "2504.19722", "pdf": "https://arxiv.org/pdf/2504.19722", "abs": "https://arxiv.org/abs/2504.19722", "authors": ["Rupert Polley", "Nikolai Polley", "Dominik Heid", "Marc Heinrich", "Sven Ochs", "J. Marius Zöllner"], "title": "The ATLAS of Traffic Lights: A Reliable Perception Framework for Autonomous Driving", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at IEEE Intelligent Vehicles Symposium (IV 2025). Dataset\n  link: https://url.fzi.de/ATLAS", "summary": "Traffic light perception is an essential component of the camera-based\nperception system for autonomous vehicles, enabling accurate detection and\ninterpretation of traffic lights to ensure safe navigation through complex\nurban environments. In this work, we propose a modularized perception framework\nthat integrates state-of-the-art detection models with a novel real-time\nassociation and decision framework, enabling seamless deployment into an\nautonomous driving stack. To address the limitations of existing public\ndatasets, we introduce the ATLAS dataset, which provides comprehensive\nannotations of traffic light states and pictograms across diverse environmental\nconditions and camera setups. This dataset is publicly available at\nhttps://url.fzi.de/ATLAS. We train and evaluate several state-of-the-art\ntraffic light detection architectures on ATLAS, demonstrating significant\nperformance improvements in both accuracy and robustness. Finally, we evaluate\nthe framework in real-world scenarios by deploying it in an autonomous vehicle\nto make decisions at traffic light-controlled intersections, highlighting its\nreliability and effectiveness for real-time operation."}
{"id": "2504.19032", "pdf": "https://arxiv.org/pdf/2504.19032", "abs": "https://arxiv.org/abs/2504.19032", "authors": ["Niaz Ahmad", "Youngmoon Lee", "Guanghui Wang"], "title": "VISUALCENT: Visual Human Analysis using Dynamic Centroid Representation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We introduce VISUALCENT, a unified human pose and instance segmentation\nframework to address generalizability and scalability limitations to multi\nperson visual human analysis. VISUALCENT leverages centroid based bottom up\nkeypoint detection paradigm and uses Keypoint Heatmap incorporating Disk\nRepresentation and KeyCentroid to identify the optimal keypoint coordinates.\nFor the unified segmentation task, an explicit keypoint is defined as a dynamic\ncentroid called MaskCentroid to swiftly cluster pixels to specific human\ninstance during rapid changes in human body movement or significantly occluded\nenvironment. Experimental results on COCO and OCHuman datasets demonstrate\nVISUALCENTs accuracy and real time performance advantages, outperforming\nexisting methods in mAP scores and execution frame rate per second. The\nimplementation is available on the project page."}
{"id": "2504.19724", "pdf": "https://arxiv.org/pdf/2504.19724", "abs": "https://arxiv.org/abs/2504.19724", "authors": ["Haofan Wang", "Yujia Xu", "Yimeng Li", "Junchen Li", "Chaowei Zhang", "Jing Wang", "Kejia Yang", "Zhibo Chen"], "title": "RepText: Rendering Visual Text via Replicating", "categories": ["cs.CV"], "comment": "Technical Report. https://reptext.github.io/", "summary": "Although contemporary text-to-image generation models have achieved\nremarkable breakthroughs in producing visually appealing images, their capacity\nto generate precise and flexible typographic elements, especially non-Latin\nalphabets, remains constrained. To address these limitations, we start from an\nnaive assumption that text understanding is only a sufficient condition for\ntext rendering, but not a necessary condition. Based on this, we present\nRepText, which aims to empower pre-trained monolingual text-to-image generation\nmodels with the ability to accurately render, or more precisely, replicate,\nmultilingual visual text in user-specified fonts, without the need to really\nunderstand them. Specifically, we adopt the setting from ControlNet and\nadditionally integrate language agnostic glyph and position of rendered text to\nenable generating harmonized visual text, allowing users to customize text\ncontent, font and position on their needs. To improve accuracy, a text\nperceptual loss is employed along with the diffusion loss. Furthermore, to\nstabilize rendering process, at the inference phase, we directly initialize\nwith noisy glyph latent instead of random initialization, and adopt region\nmasks to restrict the feature injection to only the text region to avoid\ndistortion of the background. We conducted extensive experiments to verify the\neffectiveness of our RepText relative to existing works, our approach\noutperforms existing open-source methods and achieves comparable results to\nnative multi-language closed-source models. To be more fair, we also\nexhaustively discuss its limitations in the end."}
{"id": "2504.19040", "pdf": "https://arxiv.org/pdf/2504.19040", "abs": "https://arxiv.org/abs/2504.19040", "authors": ["Nandan Joshi", "Erhan Guven"], "title": "Improved Molecular Generation through Attribute-Driven Integrative Embeddings and GAN Selectivity", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The growing demand for molecules with tailored properties in fields such as\ndrug discovery and chemical engineering has driven advancements in\ncomputational methods for molecular design. Machine learning-based approaches\nfor de-novo molecular generation have recently garnered significant attention.\nThis paper introduces a transformer-based vector embedding generator combined\nwith a modified Generative Adversarial Network (GAN) to generate molecules with\ndesired properties. The embedding generator utilizes a novel molecular\ndescriptor, integrating Morgan fingerprints with global molecular attributes,\nenabling the transformer to capture local functional groups and broader\nmolecular characteristics. Modifying the GAN generator loss function ensures\nthe generation of molecules with specific desired properties. The transformer\nachieves a reconversion accuracy of 94% while translating molecular descriptors\nback to SMILES strings, validating the utility of the proposed embeddings for\ngenerative tasks. The approach is validated by generating novel odorant\nmolecules using a labeled dataset of odorant and non-odorant compounds. With\nthe modified range-loss function, the GAN exclusively generates odorant\nmolecules. This work underscores the potential of combining novel vector\nembeddings with transformers and modified GAN architectures to accelerate the\ndiscovery of tailored molecules, offering a robust tool for diverse molecular\ndesign applications."}
{"id": "2504.19735", "pdf": "https://arxiv.org/pdf/2504.19735", "abs": "https://arxiv.org/abs/2504.19735", "authors": ["Rustam Tagiew", "Prasannavenkatesh Balaji"], "title": "Measuring Train Driver Performance as Key to Approval of Driverless Trains", "categories": ["cs.CV"], "comment": "6 pages, 3 figures, abstract accepted by IAVVC 2025, full paper to be\n  submitted to IAVVC 2025", "summary": "Points 2.1.4(b), 2.4.2(b) and 2.4.3(b) in Annex I of Implementing Regulation\n(EU) No. 402/2013 allow a simplified approach for the safety approval of\ncomputer vision systems for driverless trains, if they have 'similar' functions\nand interfaces as the replaced human driver. The human driver is not replaced\none-to-one by a technical system - only a limited set of cognitive functions\nare replaced. However, performance in the most challenging function, obstacle\ndetection, is difficult to quantify due to the deficiency of published\nmeasurement results. This article summarizes the data published so far. This\narticle also goes a long way to remedy this situation by providing a new public\nand anonymized dataset of 711 train driver performance measurements from\ncontrolled experiments. The measurements are made for different speeds,\nobstacle sizes, train protection systems and obstacle color contrasts\nrespectively. The measured values are reaction time and distance to the\nobstacle. The goal of this paper is an unbiased and exhaustive description of\nthe presented dataset for research, standardization and regulation. Further\nproject related information including the dataset and source code is available\nat https://atosense-02371c.usercontent.opencode.de/"}
{"id": "2504.19042", "pdf": "https://arxiv.org/pdf/2504.19042", "abs": "https://arxiv.org/abs/2504.19042", "authors": ["James Giroux", "Michael Martinez", "Cristiano Fanelli"], "title": "Generative Models for Fast Simulation of Cherenkov Detectors at the Electron-Ion Collider", "categories": ["physics.ins-det", "cs.AI", "cs.LG", "hep-ex", "nucl-ex"], "comment": "45 pages, 27 figures", "summary": "The integration of Deep Learning (DL) into experimental nuclear and particle\nphysics has driven significant progress in simulation and reconstruction\nworkflows. However, traditional simulation frameworks such as Geant4 remain\ncomputationally intensive, especially for Cherenkov detectors, where simulating\noptical photon transport through complex geometries and reflective surfaces\nintroduces a major bottleneck. To address this, we present an open, standalone\nfast simulation tool for Detection of Internally Reflected Cherenkov Light\n(DIRC) detectors, with a focus on the High-Performance DIRC (hpDIRC) at the\nfuture Electron-Ion Collider (EIC). Our framework incorporates a suite of\ngenerative models tailored to accelerate particle identification (PID) tasks by\noffering a scalable, GPU-accelerated alternative to full Geant4-based\nsimulations. Designed with accessibility in mind, our simulation package\nenables both DL researchers and physicists to efficiently generate\nhigh-fidelity large-scale datasets on demand, without relying on complex\ntraditional simulation stacks. This flexibility supports the development and\nbenchmarking of novel DL-driven PID methods. Moreover, this fast simulation\npipeline represents a critical step toward enabling EIC-wide PID strategies\nthat depend on virtually unlimited simulated samples, spanning the full\nacceptance of the hpDIRC."}
{"id": "2504.19737", "pdf": "https://arxiv.org/pdf/2504.19737", "abs": "https://arxiv.org/abs/2504.19737", "authors": ["Abhishek Kuriyal", "Elliot Vincent", "Mathieu Aubry", "Loic Landrieu"], "title": "CoDEx: Combining Domain Expertise for Spatial Generalization in Satellite Image Analysis", "categories": ["cs.CV"], "comment": "CVPR 2025 EarthVision Workshop", "summary": "Global variations in terrain appearance raise a major challenge for satellite\nimage analysis, leading to poor model performance when training on locations\nthat differ from those encountered at test time. This remains true even with\nrecent large global datasets. To address this challenge, we propose a novel\ndomain-generalization framework for satellite images. Instead of trying to\nlearn a single generalizable model, we train one expert model per training\ndomain, while learning experts' similarity and encouraging similar experts to\nbe consistent. A model selection module then identifies the most suitable\nexperts for a given test sample and aggregates their predictions. Experiments\non four datasets (DynamicEarthNet, MUDS, OSCD, and FMoW) demonstrate consistent\ngains over existing domain generalization and adaptation methods. Our code is\npublicly available at https://github.com/Abhishek19009/CoDEx."}
{"id": "2504.19046", "pdf": "https://arxiv.org/pdf/2504.19046", "abs": "https://arxiv.org/abs/2504.19046", "authors": ["Billel Essaid", "Hamza Kheddar", "Noureddine Batel"], "title": "Enhancing Cochlear Implant Signal Coding with Scaled Dot-Product Attention", "categories": ["eess.AS", "cs.AI", "cs.SD"], "comment": null, "summary": "Cochlear implants (CIs) play a vital role in restoring hearing for\nindividuals with severe to profound sensorineural hearing loss by directly\nstimulating the auditory nerve with electrical signals. While traditional\ncoding strategies, such as the advanced combination encoder (ACE), have proven\neffective, they are constrained by their adaptability and precision. This paper\ninvestigates the use of deep learning (DL) techniques to generate\nelectrodograms for CIs, presenting our model as an advanced alternative. We\ncompared the performance of our model with the ACE strategy by evaluating the\nintelligibility of reconstructed audio signals using the short-time objective\nintelligibility (STOI) metric. The results indicate that our model achieves a\nSTOI score of 0.6031, closely approximating the 0.6126 score of the ACE\nstrategy, and offers potential advantages in flexibility and adaptability. This\nstudy underscores the benefits of incorporating artificial intelligent (AI)\ninto CI technology, such as enhanced personalization and efficiency."}
{"id": "2504.19739", "pdf": "https://arxiv.org/pdf/2504.19739", "abs": "https://arxiv.org/abs/2504.19739", "authors": ["Muzammil Behzad", "Guoying Zhao"], "title": "Contrastive Language-Image Learning with Augmented Textual Prompts for 3D/4D FER Using Vision-Language Model", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we introduce AffectVLM, a vision-language model designed to\nintegrate multiviews for a semantically rich and visually comprehensive\nunderstanding of facial emotions from 3D/4D data. To effectively capture visual\nfeatures, we propose a joint representation learning framework paired with a\nnovel gradient-friendly loss function that accelerates model convergence\ntowards optimal feature representation. Additionally, we introduce augmented\ntextual prompts to enhance the model's linguistic capabilities and employ mixed\nview augmentation to expand the visual dataset. We also develop a Streamlit app\nfor a real-time interactive inference and enable the model for distributed\nlearning. Extensive experiments validate the superior performance of AffectVLM\nacross multiple benchmarks."}
{"id": "2504.19047", "pdf": "https://arxiv.org/pdf/2504.19047", "abs": "https://arxiv.org/abs/2504.19047", "authors": ["David Almog"], "title": "AI Recommendations and Non-instrumental Image Concerns", "categories": ["econ.GN", "cs.AI", "cs.HC", "q-fin.EC"], "comment": null, "summary": "There is growing enthusiasm about the potential for humans and AI to\ncollaborate by leveraging their respective strengths. Yet in practice, this\npromise often falls short. This paper uses an online experiment to identify\nnon-instrumental image concerns as a key reason individuals underutilize AI\nrecommendations. I show that concerns about how one is perceived, even when\nthose perceptions carry no monetary consequences, lead participants to\ndisregard AI advice and reduce task performance."}
{"id": "2504.19742", "pdf": "https://arxiv.org/pdf/2504.19742", "abs": "https://arxiv.org/abs/2504.19742", "authors": ["Valerie Zermatten", "Javiera Castillo-Navarro", "Pallavi Jain", "Devis Tuia", "Diego Marcos"], "title": "EcoWikiRS: Learning Ecological Representation of Satellite Images from Weak Supervision with Species Observations and Wikipedia", "categories": ["cs.CV"], "comment": "Accepted at EarthVision 2025 (CVPRW 2025)", "summary": "The presence of species provides key insights into the ecological properties\nof a location such as land cover, climatic conditions or even soil properties.\nWe propose a method to predict such ecological properties directly from remote\nsensing (RS) images by aligning them with species habitat descriptions. We\nintroduce the EcoWikiRS dataset, consisting of high-resolution aerial images,\nthe corresponding geolocated species observations, and, for each species, the\ntextual descriptions of their habitat from Wikipedia. EcoWikiRS offers a\nscalable way of supervision for RS vision language models (RS-VLMs) for\necology. This is a setting with weak and noisy supervision, where, for\ninstance, some text may describe properties that are specific only to part of\nthe species' niche or is irrelevant to a specific image. We tackle this by\nproposing WINCEL, a weighted version of the InfoNCE loss. We evaluate our model\non the task of ecosystem zero-shot classification by following the habitat\ndefinitions from the European Nature Information System (EUNIS). Our results\nshow that our approach helps in understanding RS images in a more ecologically\nmeaningful manner. The code and the dataset are available at\nhttps://github.com/eceo-epfl/EcoWikiRS."}
{"id": "2504.19056", "pdf": "https://arxiv.org/pdf/2504.19056", "abs": "https://arxiv.org/abs/2504.19056", "authors": ["Mohammad Mahdi Abootorabi", "Omid Ghahroodi", "Pardis Sadat Zahraei", "Hossein Behzadasl", "Alireza Mirrokni", "Mobina Salimipanah", "Arash Rasouli", "Bahar Behzadipour", "Sara Azarnoush", "Benyamin Maleki", "Erfan Sadraiye", "Kiarash Kiani Feriz", "Mahdi Teymouri Nahad", "Ali Moghadasi", "Abolfazl Eshagh Abianeh", "Nizi Nazar", "Hamid R. Rabiee", "Mahdieh Soleymani Baghshah", "Meisam Ahmadi", "Ehsaneddin Asgari"], "title": "Generative AI for Character Animation: A Comprehensive Survey of Techniques, Applications, and Future Directions", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "50 main pages, 30 pages appendix, 21 figures, 8 tables, GitHub\n  Repository:\n  https://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey", "summary": "Generative AI is reshaping art, gaming, and most notably animation. Recent\nbreakthroughs in foundation and diffusion models have reduced the time and cost\nof producing animated content. Characters are central animation components,\ninvolving motion, emotions, gestures, and facial expressions. The pace and\nbreadth of advances in recent months make it difficult to maintain a coherent\nview of the field, motivating the need for an integrative review. Unlike\nearlier overviews that treat avatars, gestures, or facial animation in\nisolation, this survey offers a single, comprehensive perspective on all the\nmain generative AI applications for character animation. We begin by examining\nthe state-of-the-art in facial animation, expression rendering, image\nsynthesis, avatar creation, gesture modeling, motion synthesis, object\ngeneration, and texture synthesis. We highlight leading research, practical\ndeployments, commonly used datasets, and emerging trends for each area. To\nsupport newcomers, we also provide a comprehensive background section that\nintroduces foundational models and evaluation metrics, equipping readers with\nthe knowledge needed to enter the field. We discuss open challenges and map\nfuture research directions, providing a roadmap to advance AI-driven\ncharacter-animation technologies. This survey is intended as a resource for\nresearchers and developers entering the field of generative AI animation or\nadjacent fields. Resources are available at:\nhttps://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey."}
{"id": "2504.19749", "pdf": "https://arxiv.org/pdf/2504.19749", "abs": "https://arxiv.org/abs/2504.19749", "authors": ["Zhimin Liao", "Ping Wei", "Shuaijia Chen", "Haoxuan Wang", "Ziyang Ren"], "title": "STCOcc: Sparse Spatial-Temporal Cascade Renovation for 3D Occupancy and Scene Flow Prediction", "categories": ["cs.CV"], "comment": null, "summary": "3D occupancy and scene flow offer a detailed and dynamic representation of 3D\nscene. Recognizing the sparsity and complexity of 3D space, previous\nvision-centric methods have employed implicit learning-based approaches to\nmodel spatial and temporal information. However, these approaches struggle to\ncapture local details and diminish the model's spatial discriminative ability.\nTo address these challenges, we propose a novel explicit state-based modeling\nmethod designed to leverage the occupied state to renovate the 3D features.\nSpecifically, we propose a sparse occlusion-aware attention mechanism,\nintegrated with a cascade refinement strategy, which accurately renovates 3D\nfeatures with the guidance of occupied state information. Additionally, we\nintroduce a novel method for modeling long-term dynamic interactions, which\nreduces computational costs and preserves spatial information. Compared to the\nprevious state-of-the-art methods, our efficient explicit renovation strategy\nnot only delivers superior performance in terms of RayIoU and mAVE for\noccupancy and scene flow prediction but also markedly reduces GPU memory usage\nduring training, bringing it down to 8.7GB. Our code is available on\nhttps://github.com/lzzzzzm/STCOcc"}
{"id": "2504.19061", "pdf": "https://arxiv.org/pdf/2504.19061", "abs": "https://arxiv.org/abs/2504.19061", "authors": ["Anindya Bijoy Das", "Shibbir Ahmed", "Shahnewaz Karim Sakib"], "title": "Hallucinations and Key Information Extraction in Medical Texts: A Comprehensive Assessment of Open-Source Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Clinical summarization is crucial in healthcare as it distills complex\nmedical data into digestible information, enhancing patient understanding and\ncare management. Large language models (LLMs) have shown significant potential\nin automating and improving the accuracy of such summarizations due to their\nadvanced natural language understanding capabilities. These models are\nparticularly applicable in the context of summarizing medical/clinical texts,\nwhere precise and concise information transfer is essential. In this paper, we\ninvestigate the effectiveness of open-source LLMs in extracting key events from\ndischarge reports, such as reasons for hospital admission, significant\nin-hospital events, and critical follow-up actions. In addition, we also assess\nthe prevalence of various types of hallucinations in the summaries produced by\nthese models. Detecting hallucinations is vital as it directly influences the\nreliability of the information, potentially affecting patient care and\ntreatment outcomes. We conduct comprehensive numerical simulations to\nrigorously evaluate the performance of these models, further probing the\naccuracy and fidelity of the extracted content in clinical summarization."}
{"id": "2504.19755", "pdf": "https://arxiv.org/pdf/2504.19755", "abs": "https://arxiv.org/abs/2504.19755", "authors": ["Kapil Kashyap", "Sean Fargose", "Chrisil Dabre", "Fatema Dolaria", "Nilesh Patil", "Aniket Kore"], "title": "Hybrid Approach Combining Ultrasound and Blood Test Analysis with a Voting Classifier for Accurate Liver Fibrosis and Cirrhosis Assessment", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Liver cirrhosis is an insidious condition involving the substitution of\nnormal liver tissue with fibrous scar tissue and causing major health\ncomplications. The conventional method of diagnosis using liver biopsy is\ninvasive and, therefore, inconvenient for use in regular screening. In this\npaper,we present a hybrid model that combines machine learning techniques with\nclinical data and ultrasoundscans to improve liver fibrosis and cirrhosis\ndetection accuracy is presented. The model integrates fixed blood test\nprobabilities with deep learning model predictions (DenseNet-201) for\nultrasonic images. The combined hybrid model achieved an accuracy of 92.5%. The\nfindings establish the viability of the combined model in enhancing diagnosis\naccuracy and supporting early intervention in liver disease care."}
{"id": "2504.19066", "pdf": "https://arxiv.org/pdf/2504.19066", "abs": "https://arxiv.org/abs/2504.19066", "authors": ["Deeksha Varshney", "Keane Ong", "Rui Mao", "Erik Cambria", "Gianmarco Mengaldo"], "title": "ClimaEmpact: Domain-Aligned Small Language Models and Datasets for Extreme Weather Analytics", "categories": ["cs.CL", "cs.AI", "cs.LG", "physics.ao-ph"], "comment": null, "summary": "Accurate assessments of extreme weather events are vital for research and\npolicy, yet localized and granular data remain scarce in many parts of the\nworld. This data gap limits our ability to analyze potential outcomes and\nimplications of extreme weather events, hindering effective decision-making.\nLarge Language Models (LLMs) can process vast amounts of unstructured text\ndata, extract meaningful insights, and generate detailed assessments by\nsynthesizing information from multiple sources. Furthermore, LLMs can\nseamlessly transfer their general language understanding to smaller models,\nenabling these models to retain key knowledge while being fine-tuned for\nspecific tasks. In this paper, we propose Extreme Weather Reasoning-Aware\nAlignment (EWRA), a method that enhances small language models (SLMs) by\nincorporating structured reasoning paths derived from LLMs, and\nExtremeWeatherNews, a large dataset of extreme weather event-related news\narticles. EWRA and ExtremeWeatherNews together form the overall framework,\nClimaEmpact, that focuses on addressing three critical extreme-weather tasks:\ncategorization of tangible vulnerabilities/impacts, topic labeling, and emotion\nanalysis. By aligning SLMs with advanced reasoning strategies on\nExtremeWeatherNews (and its derived dataset ExtremeAlign used specifically for\nSLM alignment), EWRA improves the SLMs' ability to generate well-grounded and\ndomain-specific responses for extreme weather analytics. Our results show that\nthe approach proposed guides SLMs to output domain-aligned responses,\nsurpassing the performance of task-specific models and offering enhanced\nreal-world applicability for extreme weather analytics."}
{"id": "2504.19819", "pdf": "https://arxiv.org/pdf/2504.19819", "abs": "https://arxiv.org/abs/2504.19819", "authors": ["Hoang Chuong Nguyen", "Wei Mao", "Jose M. Alvarez", "Miaomiao Liu"], "title": "Joint Optimization of Neural Radiance Fields and Continuous Camera Motion from a Monocular Video", "categories": ["cs.CV"], "comment": null, "summary": "Neural Radiance Fields (NeRF) has demonstrated its superior capability to\nrepresent 3D geometry but require accurately precomputed camera poses during\ntraining. To mitigate this requirement, existing methods jointly optimize\ncamera poses and NeRF often relying on good pose initialisation or depth\npriors. However, these approaches struggle in challenging scenarios, such as\nlarge rotations, as they map each camera to a world coordinate system. We\npropose a novel method that eliminates prior dependencies by modeling\ncontinuous camera motions as time-dependent angular velocity and velocity.\nRelative motions between cameras are learned first via velocity integration,\nwhile camera poses can be obtained by aggregating such relative motions up to a\nworld coordinate system defined at a single time step within the video.\nSpecifically, accurate continuous camera movements are learned through a\ntime-dependent NeRF, which captures local scene geometry and motion by training\nfrom neighboring frames for each time step. The learned motions enable\nfine-tuning the NeRF to represent the full scene geometry. Experiments on Co3D\nand Scannet show our approach achieves superior camera pose and depth\nestimation and comparable novel-view synthesis performance compared to\nstate-of-the-art methods. Our code is available at\nhttps://github.com/HoangChuongNguyen/cope-nerf."}
{"id": "2504.19080", "pdf": "https://arxiv.org/pdf/2504.19080", "abs": "https://arxiv.org/abs/2504.19080", "authors": ["Zhenkai Qin", "Jiaquan Liang", "Qiao Fang"], "title": "MIA-Mind: A Multidimensional Interactive Attention Mechanism Based on MindSpore", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Attention mechanisms have significantly advanced deep learning by enhancing\nfeature representation through selective focus. However, existing approaches\noften independently model channel importance and spatial saliency, overlooking\ntheir inherent interdependence and limiting their effectiveness. To address\nthis limitation, we propose MIA-Mind, a lightweight and modular\nMultidimensional Interactive Attention Mechanism, built upon the MindSpore\nframework. MIA-Mind jointly models spatial and channel features through a\nunified cross-attentive fusion strategy, enabling fine-grained feature\nrecalibration with minimal computational overhead. Extensive experiments are\nconducted on three representative datasets: on CIFAR-10, MIA-Mind achieves an\naccuracy of 82.9\\%; on ISBI2012, it achieves an accuracy of 78.7\\%; and on\nCIC-IDS2017, it achieves an accuracy of 91.9\\%. These results validate the\nversatility, lightweight design, and generalization ability of MIA-Mind across\nheterogeneous tasks. Future work will explore the extension of MIA-Mind to\nlarge-scale datasets, the development of ada,ptive attention fusion strategies,\nand distributed deployment to further enhance scalability and robustness."}
{"id": "2504.19824", "pdf": "https://arxiv.org/pdf/2504.19824", "abs": "https://arxiv.org/abs/2504.19824", "authors": ["Mohamed Hassan", "Mohammad Wasil", "Sebastian Houben"], "title": "Taming the Randomness: Towards Label-Preserving Cropping in Contrastive Learning", "categories": ["cs.CV"], "comment": null, "summary": "Contrastive learning (CL) approaches have gained great recognition as a very\nsuccessful subset of self-supervised learning (SSL) methods. SSL enables\nlearning from unlabeled data, a crucial step in the advancement of deep\nlearning, particularly in computer vision (CV), given the plethora of unlabeled\nimage data. CL works by comparing different random augmentations (e.g.,\ndifferent crops) of the same image, thus achieving self-labeling. Nevertheless,\nrandomly augmenting images and especially random cropping can result in an\nimage that is semantically very distant from the original and therefore leads\nto false labeling, hence undermining the efficacy of the methods. In this\nresearch, two novel parameterized cropping methods are introduced that increase\nthe robustness of self-labeling and consequently increase the efficacy. The\nresults show that the use of these methods significantly improves the accuracy\nof the model by between 2.7\\% and 12.4\\% on the downstream task of classifying\nCIFAR-10, depending on the crop size compared to that of the non-parameterized\nrandom cropping method."}
{"id": "2504.19093", "pdf": "https://arxiv.org/pdf/2504.19093", "abs": "https://arxiv.org/abs/2504.19093", "authors": ["Yu Li", "Qizhi Pei", "Mengyuan Sun", "Honglin Lin", "Chenlin Ming", "Xin Gao", "Jiang Wu", "Conghui He", "Lijun Wu"], "title": "CipherBank: Exploring the Boundary of LLM Reasoning Capabilities through Cryptography Challenges", "categories": ["cs.CR", "cs.AI", "cs.PF"], "comment": "Work in progress", "summary": "Large language models (LLMs) have demonstrated remarkable capabilities,\nespecially the recent advancements in reasoning, such as o1 and o3, pushing the\nboundaries of AI. Despite these impressive achievements in mathematics and\ncoding, the reasoning abilities of LLMs in domains requiring cryptographic\nexpertise remain underexplored. In this paper, we introduce CipherBank, a\ncomprehensive benchmark designed to evaluate the reasoning capabilities of LLMs\nin cryptographic decryption tasks. CipherBank comprises 2,358 meticulously\ncrafted problems, covering 262 unique plaintexts across 5 domains and 14\nsubdomains, with a focus on privacy-sensitive and real-world scenarios that\nnecessitate encryption. From a cryptographic perspective, CipherBank\nincorporates 3 major categories of encryption methods, spanning 9 distinct\nalgorithms, ranging from classical ciphers to custom cryptographic techniques.\nWe evaluate state-of-the-art LLMs on CipherBank, e.g., GPT-4o, DeepSeek-V3, and\ncutting-edge reasoning-focused models such as o1 and DeepSeek-R1. Our results\nreveal significant gaps in reasoning abilities not only between general-purpose\nchat LLMs and reasoning-focused LLMs but also in the performance of current\nreasoning-focused models when applied to classical cryptographic decryption\ntasks, highlighting the challenges these models face in understanding and\nmanipulating encrypted data. Through detailed analysis and error\ninvestigations, we provide several key observations that shed light on the\nlimitations and potential improvement areas for LLMs in cryptographic\nreasoning. These findings underscore the need for continuous advancements in\nLLM reasoning capabilities."}
{"id": "2504.19828", "pdf": "https://arxiv.org/pdf/2504.19828", "abs": "https://arxiv.org/abs/2504.19828", "authors": ["Zhiming Hu", "Daniel Haeufle", "Syn Schmitt", "Andreas Bulling"], "title": "HOIGaze: Gaze Estimation During Hand-Object Interactions in Extended Reality Exploiting Eye-Hand-Head Coordination", "categories": ["cs.CV"], "comment": "Accepted at SIGGRAPH 2025, link:\n  https://zhiminghu.net/hu25_hoigaze.html", "summary": "We present HOIGaze - a novel learning-based approach for gaze estimation\nduring hand-object interactions (HOI) in extended reality (XR). HOIGaze\naddresses the challenging HOI setting by building on one key insight: The eye,\nhand, and head movements are closely coordinated during HOIs and this\ncoordination can be exploited to identify samples that are most useful for gaze\nestimator training - as such, effectively denoising the training data. This\ndenoising approach is in stark contrast to previous gaze estimation methods\nthat treated all training samples as equal. Specifically, we propose: 1) a\nnovel hierarchical framework that first recognises the hand currently visually\nattended to and then estimates gaze direction based on the attended hand; 2) a\nnew gaze estimator that uses cross-modal Transformers to fuse head and\nhand-object features extracted using a convolutional neural network and a\nspatio-temporal graph convolutional network; and 3) a novel eye-head\ncoordination loss that upgrades training samples belonging to the coordinated\neye-head movements. We evaluate HOIGaze on the HOT3D and Aria digital twin\n(ADT) datasets and show that it significantly outperforms state-of-the-art\nmethods, achieving an average improvement of 15.6% on HOT3D and 6.0% on ADT in\nmean angular error. To demonstrate the potential of our method, we further\nreport significant performance improvements for the sample downstream task of\neye-based activity recognition on ADT. Taken together, our results underline\nthe significant information content available in eye-hand-head coordination\nand, as such, open up an exciting new direction for learning-based gaze\nestimation."}
{"id": "2504.19099", "pdf": "https://arxiv.org/pdf/2504.19099", "abs": "https://arxiv.org/abs/2504.19099", "authors": ["Ning Wang", "Bingkun Yao", "Jie Zhou", "Yuchen Hu", "Xi Wang", "Nan Guan", "Zhe Jiang"], "title": "VeriDebug: A Unified LLM for Verilog Debugging via Contrastive Embedding and Guided Correction", "categories": ["cs.SE", "cs.AI", "cs.AR"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable potential in\ndebugging for various programming languages. However, the application of LLMs\nto Verilog debugging remains insufficiently explored. Here, we present\nVeriDebug, an approach that integrates contrastive representation and guided\ncorrection capabilities for automated Verilog debugging. Unlike existing\nmethods, VeriDebug employs an embedding-based technique to accurately retrieve\ninternal information, followed by bug-fixing. VeriDebug unifies Verilog bug\ndetection and correction through a shared parameter space. By simultaneously\nlearning bug patterns and fixes, it streamlines debugging via contrastive\nembedding and guided correction. Empirical results show the efficacy of\nVeriDebug in enhancing Verilog debugging. Our VeriDebugLoc, Type model achieves\n64.7 accuracy in bug fixing (Acc1), a significant improvement from the existing\nopen-source SOTAs 11.3. This performance not only outperforms open-source\nalternatives but also exceeds larger closed-source models like GPT-3.5-turbo\n(36.6), offering a more accurate alternative to conventional debugging methods."}
{"id": "2504.19834", "pdf": "https://arxiv.org/pdf/2504.19834", "abs": "https://arxiv.org/abs/2504.19834", "authors": ["Xiaoyu Liu", "Mingshuai Yao", "Yabo Zhang", "Xianhui Lin", "Peiran Ren", "Xiaoming Li", "Ming Liu", "Wangmeng Zuo"], "title": "AnimateAnywhere: Rouse the Background in Human Image Animation", "categories": ["cs.CV"], "comment": null, "summary": "Human image animation aims to generate human videos of given characters and\nbackgrounds that adhere to the desired pose sequence. However, existing methods\nfocus more on human actions while neglecting the generation of background,\nwhich typically leads to static results or inharmonious movements. The\ncommunity has explored camera pose-guided animation tasks, yet preparing the\ncamera trajectory is impractical for most entertainment applications and\nordinary users. As a remedy, we present an AnimateAnywhere framework, rousing\nthe background in human image animation without requirements on camera\ntrajectories. In particular, based on our key insight that the movement of the\nhuman body often reflects the motion of the background, we introduce a\nbackground motion learner (BML) to learn background motions from human pose\nsequences. To encourage the model to learn more accurate cross-frame\ncorrespondences, we further deploy an epipolar constraint on the 3D attention\nmap. Specifically, the mask used to suppress geometrically unreasonable\nattention is carefully constructed by combining an epipolar mask and the\ncurrent 3D attention map. Extensive experiments demonstrate that our\nAnimateAnywhere effectively learns the background motion from human pose\nsequences, achieving state-of-the-art performance in generating human animation\nresults with vivid and realistic backgrounds. The source code and model will be\navailable at https://github.com/liuxiaoyu1104/AnimateAnywhere."}
{"id": "2504.19120", "pdf": "https://arxiv.org/pdf/2504.19120", "abs": "https://arxiv.org/abs/2504.19120", "authors": ["Gaojian Huang", "Yantong Jin", "Wei-Hsiang Lo"], "title": "Beyond Levels of Driving Automation: A Triadic Framework of Human-AI Collaboration in On-Road Mobility", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "The goal of the current study is to introduce a triadic human-AI\ncollaboration framework for the automated vehicle domain. Previous\nclassifications (e.g., SAE Levels of Automation) focus on defining automation\nlevels based on who controls the vehicle. However, it remains unclear how human\nusers and AI should collaborate in real-time, especially in dynamic driving\ncontexts, where roles can shift frequently. To fill the gap, this study\nproposes a triadic human-AI collaboration framework with three AI roles (i.e.,\nAdvisor, Co-Pilot, and Guardian) that dynamically adapt to human needs.\nOverall, the study lays a foundation for developing adaptive, role-based\nhuman-AI collaboration strategies in automated vehicles."}
{"id": "2504.19839", "pdf": "https://arxiv.org/pdf/2504.19839", "abs": "https://arxiv.org/abs/2504.19839", "authors": ["Yulong Guo", "Zilun Zhang", "Yongheng Shang", "Tiancheng Zhao", "Shuiguang Deng", "Yingchun Yang", "Jianwei Yin"], "title": "SRMF: A Data Augmentation and Multimodal Fusion Approach for Long-Tail UHR Satellite Image Segmentation", "categories": ["cs.CV"], "comment": "None", "summary": "The long-tail problem presents a significant challenge to the advancement of\nsemantic segmentation in ultra-high-resolution (UHR) satellite imagery. While\nprevious efforts in UHR semantic segmentation have largely focused on\nmulti-branch network architectures that emphasize multi-scale feature\nextraction and fusion, they have often overlooked the importance of addressing\nthe long-tail issue. In contrast to prior UHR methods that focused on\nindependent feature extraction, we emphasize data augmentation and multimodal\nfeature fusion to alleviate the long-tail problem. In this paper, we introduce\nSRMF, a novel framework for semantic segmentation in UHR satellite imagery. Our\napproach addresses the long-tail class distribution by incorporating a\nmulti-scale cropping technique alongside a data augmentation strategy based on\nsemantic reordering and resampling. To further enhance model performance, we\npropose a multimodal fusion-based general representation knowledge injection\nmethod, which, for the first time, fuses text and visual features without the\nneed for individual region text descriptions, extracting more robust features.\nExtensive experiments on the URUR, GID, and FBP datasets demonstrate that our\nmethod improves mIoU by 3.33\\%, 0.66\\%, and 0.98\\%, respectively, achieving\nstate-of-the-art performance. Code is available at:\nhttps://github.com/BinSpa/SRMF.git."}
{"id": "2504.19136", "pdf": "https://arxiv.org/pdf/2504.19136", "abs": "https://arxiv.org/abs/2504.19136", "authors": ["Huiling Zheng", "Xian Zhong", "Bin Liu", "Yi Xiao", "Bihan Wen", "Xiaofeng Li"], "title": "PAD: Phase-Amplitude Decoupling Fusion for Multi-Modal Land Cover Classification", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": "13 pages, 8 figures", "summary": "The fusion of Synthetic Aperture Radar (SAR) and RGB imagery for land cover\nclassification remains challenging due to modality heterogeneity and the\nunderutilization of spectral complementarity. Existing methods often fail to\ndecouple shared structural features from modality-specific radiometric\nattributes, leading to feature conflicts and information loss. To address this\nissue, we propose Phase-Amplitude Decoupling (PAD), a frequency-aware framework\nthat separates phase (modality-shared) and amplitude (modality-specific)\ncomponents in the Fourier domain. Specifically, PAD consists of two key\ncomponents: 1) Phase Spectrum Correction (PSC), which aligns cross-modal phase\nfeatures through convolution-guided scaling to enhance geometric consistency,\nand 2) Amplitude Spectrum Fusion (ASF), which dynamically integrates\nhigh-frequency details and low-frequency structures using frequency-adaptive\nmultilayer perceptrons. This approach leverages SAR's sensitivity to\nmorphological features and RGB's spectral richness. Extensive experiments on\nWHU-OPT-SAR and DDHR-SK datasets demonstrate state-of-the-art performance. Our\nwork establishes a new paradigm for physics-aware multi-modal fusion in remote\nsensing. The code will be available at https://github.com/RanFeng2/PAD."}
{"id": "2504.19847", "pdf": "https://arxiv.org/pdf/2504.19847", "abs": "https://arxiv.org/abs/2504.19847", "authors": ["Juhan Park", "Kyungjae Lee", "Hyung Jin Chang", "Jungchan Cho"], "title": "Foundation Model-Driven Framework for Human-Object Interaction Prediction with Segmentation Mask Integration", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In this work, we introduce Segmentation to Human-Object Interaction\n(\\textit{\\textbf{Seg2HOI}}) approach, a novel framework that integrates\nsegmentation-based vision foundation models with the human-object interaction\ntask, distinguished from traditional detection-based Human-Object Interaction\n(HOI) methods. Our approach enhances HOI detection by not only predicting the\nstandard triplets but also introducing quadruplets, which extend HOI triplets\nby including segmentation masks for human-object pairs. More specifically,\nSeg2HOI inherits the properties of the vision foundation model (e.g.,\npromptable and interactive mechanisms) and incorporates a decoder that applies\nthese attributes to HOI task. Despite training only for HOI, without additional\ntraining mechanisms for these properties, the framework demonstrates that such\nfeatures still operate efficiently. Extensive experiments on two public\nbenchmark datasets demonstrate that Seg2HOI achieves performance comparable to\nstate-of-the-art methods, even in zero-shot scenarios. Lastly, we propose that\nSeg2HOI can generate HOI quadruplets and interactive HOI segmentation from\nnovel text and visual prompts that were not used during training, making it\nversatile for a wide range of applications by leveraging this flexibility."}
{"id": "2504.19139", "pdf": "https://arxiv.org/pdf/2504.19139", "abs": "https://arxiv.org/abs/2504.19139", "authors": ["Yun Qu", "Qi", "Wang", "Yixiu Mao", "Yiqin Lv", "Xiangyang Ji"], "title": "Fast and Robust: Task Sampling with Posterior and Diversity Synergies for Adaptive Decision-Makers in Randomized Environments", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Task robust adaptation is a long-standing pursuit in sequential\ndecision-making. Some risk-averse strategies, e.g., the conditional\nvalue-at-risk principle, are incorporated in domain randomization or meta\nreinforcement learning to prioritize difficult tasks in optimization, which\ndemand costly intensive evaluations. The efficiency issue prompts the\ndevelopment of robust active task sampling to train adaptive policies, where\nrisk-predictive models are used to surrogate policy evaluation. This work\ncharacterizes the optimization pipeline of robust active task sampling as a\nMarkov decision process, posits theoretical and practical insights, and\nconstitutes robustness concepts in risk-averse scenarios. Importantly, we\npropose an easy-to-implement method, referred to as Posterior and Diversity\nSynergized Task Sampling (PDTS), to accommodate fast and robust sequential\ndecision-making. Extensive experiments show that PDTS unlocks the potential of\nrobust active task sampling, significantly improves the zero-shot and few-shot\nadaptation robustness in challenging tasks, and even accelerates the learning\nprocess under certain scenarios. Our project website is at\nhttps://thu-rllab.github.io/PDTS_project_page."}
{"id": "2504.19860", "pdf": "https://arxiv.org/pdf/2504.19860", "abs": "https://arxiv.org/abs/2504.19860", "authors": ["Chenhan Jiang", "Yihan Zeng", "Hang Xu", "Dit-Yan Yeung"], "title": "CoherenDream: Boosting Holistic Text Coherence in 3D Generation via Multimodal Large Language Models Feedback", "categories": ["cs.CV"], "comment": null, "summary": "Score Distillation Sampling (SDS) has achieved remarkable success in\ntext-to-3D content generation. However, SDS-based methods struggle to maintain\nsemantic fidelity for user prompts, particularly when involving multiple\nobjects with intricate interactions. While existing approaches often address 3D\nconsistency through multiview diffusion model fine-tuning on 3D datasets, this\nstrategy inadvertently exacerbates text-3D alignment degradation. The\nlimitation stems from SDS's inherent accumulation of view-independent biases\nduring optimization, which progressively diverges from the ideal text alignment\ndirection. To alleviate this limitation, we propose a novel SDS objective,\ndubbed as Textual Coherent Score Distillation (TCSD), which integrates\nalignment feedback from multimodal large language models (MLLMs). Our TCSD\nleverages cross-modal understanding capabilities of MLLMs to assess and guide\nthe text-3D correspondence during the optimization. We further develop\n3DLLaVA-CRITIC - a fine-tuned MLLM specialized for evaluating multiview text\nalignment in 3D generations. Additionally, we introduce an LLM-layout\ninitialization that significantly accelerates optimization convergence through\nsemantic-aware spatial configuration. Comprehensive evaluations demonstrate\nthat our framework, CoherenDream, establishes state-of-the-art performance in\ntext-aligned 3D generation across multiple benchmarks, including T$^3$Bench and\nTIFA subset. Qualitative results showcase the superior performance of\nCoherenDream in preserving textual consistency and semantic interactions. As\nthe first study to incorporate MLLMs into SDS optimization, we also conduct\nextensive ablation studies to explore optimal MLLM adaptations for 3D\ngeneration tasks."}
{"id": "2504.19142", "pdf": "https://arxiv.org/pdf/2504.19142", "abs": "https://arxiv.org/abs/2504.19142", "authors": ["Chenhao Xu", "Chunyu Chen", "Jinglin Peng", "Jiannan Wang", "Jun Gao"], "title": "BQSched: A Non-intrusive Scheduler for Batch Concurrent Queries via Reinforcement Learning", "categories": ["cs.DB", "cs.AI"], "comment": "Accepted by ICDE '25", "summary": "Most large enterprises build predefined data pipelines and execute them\nperiodically to process operational data using SQL queries for various tasks. A\nkey issue in minimizing the overall makespan of these pipelines is the\nefficient scheduling of concurrent queries within the pipelines. Existing tools\nmainly rely on simple heuristic rules due to the difficulty of expressing the\ncomplex features and mutual influences of queries. The latest reinforcement\nlearning (RL) based methods have the potential to capture these patterns from\nfeedback, but it is non-trivial to apply them directly due to the large\nscheduling space, high sampling cost, and poor sample utilization.\n  Motivated by these challenges, we propose BQSched, a non-intrusive Scheduler\nfor Batch concurrent Queries via reinforcement learning. Specifically, BQSched\ndesigns an attention-based state representation to capture the complex query\npatterns, and proposes IQ-PPO, an auxiliary task-enhanced proximal policy\noptimization (PPO) algorithm, to fully exploit the rich signals of Individual\nQuery completion in logs. Based on the RL framework above, BQSched further\nintroduces three optimization strategies, including adaptive masking to prune\nthe action space, scheduling gain-based query clustering to deal with large\nquery sets, and an incremental simulator to reduce sampling cost. To our\nknowledge, BQSched is the first non-intrusive batch query scheduler via RL.\nExtensive experiments show that BQSched can significantly improve the\nefficiency and stability of batch query scheduling, while also achieving\nremarkable scalability and adaptability in both data and queries. For example,\nacross all DBMSs and scales tested, BQSched reduces the overall makespan of\nbatch queries on TPC-DS benchmark by an average of 34% and 13%, compared with\nthe commonly used heuristic strategy and the adapted RL-based scheduler,\nrespectively."}
{"id": "2504.19863", "pdf": "https://arxiv.org/pdf/2504.19863", "abs": "https://arxiv.org/abs/2504.19863", "authors": ["Daniel Kienzle", "Robin Schön", "Rainer Lienhart", "Shin'Ichi Satoh"], "title": "Towards Ball Spin and Trajectory Analysis in Table Tennis Broadcast Videos via Physically Grounded Synthetic-to-Real Transfer", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "To be published in 2025 IEEE/CVF International Conference on Computer\n  Vision and Pattern Recognition Workshops (CVPRW)", "summary": "Analyzing a player's technique in table tennis requires knowledge of the\nball's 3D trajectory and spin. While, the spin is not directly observable in\nstandard broadcasting videos, we show that it can be inferred from the ball's\ntrajectory in the video. We present a novel method to infer the initial spin\nand 3D trajectory from the corresponding 2D trajectory in a video. Without\nground truth labels for broadcast videos, we train a neural network solely on\nsynthetic data. Due to the choice of our input data representation, physically\ncorrect synthetic training data, and using targeted augmentations, the network\nnaturally generalizes to real data. Notably, these simple techniques are\nsufficient to achieve generalization. No real data at all is required for\ntraining. To the best of our knowledge, we are the first to present a method\nfor spin and trajectory prediction in simple monocular broadcast videos,\nachieving an accuracy of 92.0% in spin classification and a 2D reprojection\nerror of 0.19% of the image diagonal."}
{"id": "2504.19155", "pdf": "https://arxiv.org/pdf/2504.19155", "abs": "https://arxiv.org/abs/2504.19155", "authors": ["Hussein Harb", "Didier Benoit", "Axel Rannou", "Chi-Hieu Pham", "Valentin Tissot", "Bahaa Nasr", "Julien Bert"], "title": "Machine Learning-Based Modeling of the Anode Heel Effect in X-ray Beam Monte Carlo Simulations", "categories": ["physics.med-ph", "cs.AI"], "comment": "15 pages, 8 figures", "summary": "This study enhances Monte Carlo simulation accuracy in X-ray imaging by\ndeveloping an AI-driven model for the anode heel effect, achieving improved\nbeam intensity distribution and dosimetric precision. Through dynamic\nadjustments to beam weights on the anode and cathode sides of the X-ray tube,\nour machine learning model effectively replicates the asymmetry characteristic\nof clinical X-ray beams. Experimental results reveal dose rate increases of up\nto 9.6% on the cathode side and reductions of up to 12.5% on the anode side,\nfor energy levels between 50 and 120 kVp. These experimentally optimized beam\nweights were integrated into the OpenGATE and GGEMS Monte Carlo toolkits,\nsignificantly advancing dosimetric simulation accuracy and the image quality\nwhich closely resembles the clinical imaging. Validation with fluence and dose\nactors demonstrated that the AI-based model closely mirrors clinical beam\nbehavior, providing substantial improvements in dose consistency and accuracy\nover conventional X-ray models. This approach provides a robust framework for\nimproving X-ray dosimetry, with potential applications in dose optimization,\nimaging quality enhancement, and radiation safety in both clinical and research\nsettings."}
{"id": "2504.19876", "pdf": "https://arxiv.org/pdf/2504.19876", "abs": "https://arxiv.org/abs/2504.19876", "authors": ["Mamadou Keita", "Wassim Hamidouche", "Hessen Bougueffa Eutamene", "Abdelmalik Taleb-Ahmed", "Abdenour Hadid"], "title": "DeeCLIP: A Robust and Generalizable Transformer-Based Framework for Detecting AI-Generated Images", "categories": ["cs.CV", "cs.CR"], "comment": null, "summary": "This paper introduces DeeCLIP, a novel framework for detecting AI-generated\nimages using CLIP-ViT and fusion learning. Despite significant advancements in\ngenerative models capable of creating highly photorealistic images, existing\ndetection methods often struggle to generalize across different models and are\nhighly sensitive to minor perturbations. To address these challenges, DeeCLIP\nincorporates DeeFuser, a fusion module that combines high-level and low-level\nfeatures, improving robustness against degradations such as compression and\nblurring. Additionally, we apply triplet loss to refine the embedding space,\nenhancing the model's ability to distinguish between real and synthetic\ncontent. To further enable lightweight adaptation while preserving pre-trained\nknowledge, we adopt parameter-efficient fine-tuning using low-rank adaptation\n(LoRA) within the CLIP-ViT backbone. This approach supports effective zero-shot\nlearning without sacrificing generalization. Trained exclusively on 4-class\nProGAN data, DeeCLIP achieves an average accuracy of 89.00% on 19 test subsets\ncomposed of generative adversarial network (GAN) and diffusion models. Despite\nhaving fewer trainable parameters, DeeCLIP outperforms existing methods,\ndemonstrating superior robustness against various generative models and\nreal-world distortions. The code is publicly available at\nhttps://github.com/Mamadou-Keita/DeeCLIP for research purposes."}
{"id": "2504.19162", "pdf": "https://arxiv.org/pdf/2504.19162", "abs": "https://arxiv.org/abs/2504.19162", "authors": ["Jiaqi Chen", "Bang Zhang", "Ruotian Ma", "Peisong Wang", "Xiaodan Liang", "Zhaopeng Tu", "Xiaolong Li", "Kwan-Yee K. Wong"], "title": "SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Project: https://chen-judge.github.io/SPC/", "summary": "Evaluating the step-by-step reliability of large language model (LLM)\nreasoning, such as Chain-of-Thought, remains challenging due to the difficulty\nand cost of obtaining high-quality step-level supervision. In this paper, we\nintroduce Self-Play Critic (SPC), a novel approach where a critic model evolves\nits ability to assess reasoning steps through adversarial self-play games,\neliminating the need for manual step-level annotation. SPC involves fine-tuning\ntwo copies of a base model to play two roles, namely a \"sneaky generator\" that\ndeliberately produces erroneous steps designed to be difficult to detect, and a\n\"critic\" that analyzes the correctness of reasoning steps. These two models\nengage in an adversarial game in which the generator aims to fool the critic,\nwhile the critic model seeks to identify the generator's errors. Using\nreinforcement learning based on the game outcomes, the models iteratively\nimprove; the winner of each confrontation receives a positive reward and the\nloser receives a negative reward, driving continuous self-evolution.\nExperiments on three reasoning process benchmarks (ProcessBench, PRM800K,\nDeltaBench) demonstrate that our SPC progressively enhances its error detection\ncapabilities (e.g., accuracy increases from 70.8% to 77.7% on ProcessBench) and\nsurpasses strong baselines, including distilled R1 model. Furthermore, applying\nSPC to guide the test-time search of diverse LLMs significantly improves their\nmathematical reasoning performance on MATH500 and AIME2024, outperforming\nstate-of-the-art process reward models."}
{"id": "2504.19881", "pdf": "https://arxiv.org/pdf/2504.19881", "abs": "https://arxiv.org/abs/2504.19881", "authors": ["Claire Warwick", "Andrew Beresford", "Soazig Casteau", "Hubert P. H. Shum", "Dan Smith", "Francis Xiatian Zhang"], "title": "Using Fixed and Mobile Eye Tracking to Understand How Visitors View Art in a Museum: A Study at the Bowes Museum, County Durham, UK", "categories": ["cs.CV"], "comment": null, "summary": "The following paper describes a collaborative project involving researchers\nat Durham University, and professionals at the Bowes Museum, Barnard Castle,\nCounty Durham, UK, during which we used fixed and mobile eye tracking to\nunderstand how visitors view art. Our study took place during summer 2024 and\nbuilds on work presented at DH2017 (Bailey-Ross et al., 2017). Our\ninterdisciplinary team included researchers from digital humanities,\npsychology, art history and computer science, working in collaboration with\nprofessionals from the museum. We used fixed and mobile eye tracking to\nunderstand how museum visitors view art in a physical gallery setting. This\nresearch will enable us to make recommendations about how the Museum's\ncollections could be more effectively displayed, encouraging visitors to engage\nwith them more fully."}
{"id": "2504.19188", "pdf": "https://arxiv.org/pdf/2504.19188", "abs": "https://arxiv.org/abs/2504.19188", "authors": ["Jianlong Chen", "Chao Li", "Yang Yuan", "Andrew C Yao"], "title": "Hierarchical Attention Generates Better Proofs", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.LO"], "comment": "15 pages with 3 figures", "summary": "Large language models (LLMs) have shown promise in formal theorem proving,\nbut their token-level processing often fails to capture the inherent\nhierarchical nature of mathematical proofs. We introduce \\textbf{Hierarchical\nAttention}, a regularization method that aligns LLMs' attention mechanisms with\nmathematical reasoning structures. Our approach establishes a five-level\nhierarchy from foundational elements to high-level concepts, ensuring\nstructured information flow in proof generation. Experiments demonstrate that\nour method improves proof success rates by 2.05\\% on miniF2F and 1.69\\% on\nProofNet while reducing proof complexity by 23.81\\% and 16.50\\% respectively.\nThe code is available at https://github.com/Car-pe/HAGBP."}
{"id": "2504.19882", "pdf": "https://arxiv.org/pdf/2504.19882", "abs": "https://arxiv.org/abs/2504.19882", "authors": ["Runhui Zhang", "Sijin Zhou", "Zhuang Qi"], "title": "Federated Out-of-Distribution Generalization: A Causal Augmentation View", "categories": ["cs.CV"], "comment": "IJCNN 2025 Accepted", "summary": "Federated learning aims to collaboratively model by integrating multi-source\ninformation to obtain a model that can generalize across all client data.\nExisting methods often leverage knowledge distillation or data augmentation to\nmitigate the negative impact of data bias across clients. However, the limited\nperformance of teacher models on out-of-distribution samples and the inherent\nquality gap between augmented and original data hinder their effectiveness and\nthey typically fail to leverage the advantages of incorporating rich contextual\ninformation. To address these limitations, this paper proposes a Federated\nCausal Augmentation method, termed FedCAug, which employs causality-inspired\ndata augmentation to break the spurious correlation between attributes and\ncategories. Specifically, it designs a causal region localization module to\naccurately identify and decouple the background and objects in the image,\nproviding rich contextual information for causal data augmentation.\nAdditionally, it designs a causality-inspired data augmentation module that\nintegrates causal features and within-client context to generate counterfactual\nsamples. This significantly enhances data diversity, and the entire process\ndoes not require any information sharing between clients, thereby contributing\nto the protection of data privacy. Extensive experiments conducted on three\ndatasets reveal that FedCAug markedly reduces the model's reliance on\nbackground to predict sample labels, achieving superior performance compared to\nstate-of-the-art methods."}
{"id": "2504.19197", "pdf": "https://arxiv.org/pdf/2504.19197", "abs": "https://arxiv.org/abs/2504.19197", "authors": ["Sandipan Dhar", "Nanda Dulal Jana", "Swagatam Das"], "title": "Generative Adversarial Network based Voice Conversion: Techniques, Challenges, and Recent Advancements", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "19 pages, 12 figures, 1 table", "summary": "Voice conversion (VC) stands as a crucial research area in speech synthesis,\nenabling the transformation of a speaker's vocal characteristics to resemble\nanother while preserving the linguistic content. This technology has broad\napplications, including automated movie dubbing, speech-to-singing conversion,\nand assistive devices for pathological speech rehabilitation. With the\nincreasing demand for high-quality and natural-sounding synthetic voices,\nresearchers have developed a wide range of VC techniques. Among these,\ngenerative adversarial network (GAN)-based approaches have drawn considerable\nattention for their powerful feature-mapping capabilities and potential to\nproduce highly realistic speech. Despite notable advancements, challenges such\nas ensuring training stability, maintaining linguistic consistency, and\nachieving perceptual naturalness continue to hinder progress in GAN-based VC\nsystems. This systematic review presents a comprehensive analysis of the voice\nconversion landscape, highlighting key techniques, key challenges, and the\ntransformative impact of GANs in the field. The survey categorizes existing\nmethods, examines technical obstacles, and critically evaluates recent\ndevelopments in GAN-based VC. By consolidating and synthesizing research\nfindings scattered across the literature, this review provides a structured\nunderstanding of the strengths and limitations of different approaches. The\nsignificance of this survey lies in its ability to guide future research by\nidentifying existing gaps, proposing potential directions, and offering\ninsights for building more robust and efficient VC systems. Overall, this work\nserves as an essential resource for researchers, developers, and practitioners\naiming to advance the state-of-the-art (SOTA) in voice conversion technology."}
{"id": "2504.19888", "pdf": "https://arxiv.org/pdf/2504.19888", "abs": "https://arxiv.org/abs/2504.19888", "authors": ["Han Chen", "Anne L. Martel"], "title": "Enhancing breast cancer detection on screening mammogram using self-supervised learning and a hybrid deep model of Swin Transformer and Convolutional Neural Network", "categories": ["cs.CV"], "comment": null, "summary": "Purpose: The scarcity of high-quality curated labeled medical training data\nremains one of the major limitations in applying artificial intelligence (AI)\nsystems to breast cancer diagnosis. Deep models for mammogram analysis and mass\n(or micro-calcification) detection require training with a large volume of\nlabeled images, which are often expensive and time-consuming to collect. To\nreduce this challenge, we proposed a novel method that leverages\nself-supervised learning (SSL) and a deep hybrid model, named \\textbf{HybMNet},\nwhich combines local self-attention and fine-grained feature extraction to\nenhance breast cancer detection on screening mammograms.\n  Approach: Our method employs a two-stage learning process: (1) SSL\nPretraining: We utilize EsViT, a SSL technique, to pretrain a Swin Transformer\n(Swin-T) using a limited set of mammograms. The pretrained Swin-T then serves\nas the backbone for the downstream task. (2) Downstream Training: The proposed\nHybMNet combines the Swin-T backbone with a CNN-based network and a novel\nfusion strategy. The Swin-T employs local self-attention to identify\ninformative patch regions from the high-resolution mammogram, while the\nCNN-based network extracts fine-grained local features from the selected\npatches. A fusion module then integrates global and local information from both\nnetworks to generate robust predictions. The HybMNet is trained end-to-end,\nwith the loss function combining the outputs of the Swin-T and CNN modules to\noptimize feature extraction and classification performance.\n  Results: The proposed method was evaluated for its ability to detect breast\ncancer by distinguishing between benign (normal) and malignant mammograms.\nLeveraging SSL pretraining and the HybMNet model, it achieved AUC of 0.864 (95%\nCI: 0.852, 0.875) on the CMMD dataset and 0.889 (95% CI: 0.875, 0.903) on the\nINbreast dataset, highlighting its effectiveness."}
{"id": "2504.19212", "pdf": "https://arxiv.org/pdf/2504.19212", "abs": "https://arxiv.org/abs/2504.19212", "authors": ["Tuan Nguyen", "Naseem Khan", "Issa Khalil"], "title": "CapsFake: A Multimodal Capsule Network for Detecting Instruction-Guided Deepfakes", "categories": ["cs.CV", "cs.AI"], "comment": "20 pages", "summary": "The rapid evolution of deepfake technology, particularly in\ninstruction-guided image editing, threatens the integrity of digital images by\nenabling subtle, context-aware manipulations. Generated conditionally from real\nimages and textual prompts, these edits are often imperceptible to both humans\nand existing detection systems, revealing significant limitations in current\ndefenses. We propose a novel multimodal capsule network, CapsFake, designed to\ndetect such deepfake image edits by integrating low-level capsules from visual,\ntextual, and frequency-domain modalities. High-level capsules, predicted\nthrough a competitive routing mechanism, dynamically aggregate local features\nto identify manipulated regions with precision. Evaluated on diverse datasets,\nincluding MagicBrush, Unsplash Edits, Open Images Edits, and Multi-turn Edits,\nCapsFake outperforms state-of-the-art methods by up to 20% in detection\naccuracy. Ablation studies validate its robustness, achieving detection rates\nabove 94% under natural perturbations and 96% against adversarial attacks, with\nexcellent generalization to unseen editing scenarios. This approach establishes\na powerful framework for countering sophisticated image manipulations."}
{"id": "2504.19894", "pdf": "https://arxiv.org/pdf/2504.19894", "abs": "https://arxiv.org/abs/2504.19894", "authors": ["Quynh Phung", "Long Mai", "Fabian David Caba Heilbron", "Feng Liu", "Jia-Bin Huang", "Cusuh Ham"], "title": "CineVerse: Consistent Keyframe Synthesis for Cinematic Scene Composition", "categories": ["cs.CV"], "comment": "link website: https://cinevers.github.io/", "summary": "We present CineVerse, a novel framework for the task of cinematic scene\ncomposition. Similar to traditional multi-shot generation, our task emphasizes\nthe need for consistency and continuity across frames. However, our task also\nfocuses on addressing challenges inherent to filmmaking, such as multiple\ncharacters, complex interactions, and visual cinematic effects. In order to\nlearn to generate such content, we first create the CineVerse dataset. We use\nthis dataset to train our proposed two-stage approach. First, we prompt a large\nlanguage model (LLM) with task-specific instructions to take in a high-level\nscene description and generate a detailed plan for the overall setting and\ncharacters, as well as the individual shots. Then, we fine-tune a text-to-image\ngeneration model to synthesize high-quality visual keyframes. Experimental\nresults demonstrate that CineVerse yields promising improvements in generating\nvisually coherent and contextually rich movie scenes, paving the way for\nfurther exploration in cinematic video synthesis."}
{"id": "2504.19223", "pdf": "https://arxiv.org/pdf/2504.19223", "abs": "https://arxiv.org/abs/2504.19223", "authors": ["Alexander Baumann", "Leonardo Ayala", "Silvia Seidlitz", "Jan Sellner", "Alexander Studier-Fischer", "Berkin Özdemir", "Lena Maier-Hein", "Slobodan Ilic"], "title": "CARL: Camera-Agnostic Representation Learning for Spectral Image Analysis", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Spectral imaging offers promising applications across diverse domains,\nincluding medicine and urban scene understanding, and is already established as\na critical modality in remote sensing. However, variability in channel\ndimensionality and captured wavelengths among spectral cameras impede the\ndevelopment of AI-driven methodologies, leading to camera-specific models with\nlimited generalizability and inadequate cross-camera applicability. To address\nthis bottleneck, we introduce $\\textbf{CARL}$, a model for\n$\\textbf{C}$amera-$\\textbf{A}$gnostic $\\textbf{R}$epresentation\n$\\textbf{L}$earning across RGB, multispectral, and hyperspectral imaging\nmodalities. To enable the conversion of a spectral image with any channel\ndimensionality to a camera-agnostic embedding, we introduce wavelength\npositional encoding and a self-attention-cross-attention mechanism to compress\nspectral information into learned query representations. Spectral-spatial\npre-training is achieved with a novel spectral self-supervised JEPA-inspired\nstrategy tailored to CARL. Large-scale experiments across the domains of\nmedical imaging, autonomous driving, and satellite imaging demonstrate our\nmodel's unique robustness to spectral heterogeneity, outperforming on datasets\nwith simulated and real-world cross-camera spectral variations. The scalability\nand versatility of the proposed approach position our model as a backbone for\nfuture spectral foundation models."}
{"id": "2504.19900", "pdf": "https://arxiv.org/pdf/2504.19900", "abs": "https://arxiv.org/abs/2504.19900", "authors": ["Han Chen", "Anne L. Martel"], "title": "Breast Cancer Detection from Multi-View Screening Mammograms with Visual Prompt Tuning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate detection of breast cancer from high-resolution mammograms is\ncrucial for early diagnosis and effective treatment planning. Previous studies\nhave shown the potential of using single-view mammograms for breast cancer\ndetection. However, incorporating multi-view data can provide more\ncomprehensive insights. Multi-view classification, especially in medical\nimaging, presents unique challenges, particularly when dealing with\nlarge-scale, high-resolution data. In this work, we propose a novel Multi-view\nVisual Prompt Tuning Network (MVPT-NET) for analyzing multiple screening\nmammograms. We first pretrain a robust single-view classification model on\nhigh-resolution mammograms and then innovatively adapt multi-view feature\nlearning into a task-specific prompt tuning process. This technique selectively\ntunes a minimal set of trainable parameters (7\\%) while retaining the\nrobustness of the pre-trained single-view model, enabling efficient integration\nof multi-view data without the need for aggressive downsampling. Our approach\noffers an efficient alternative to traditional feature fusion methods,\nproviding a more robust, scalable, and efficient solution for high-resolution\nmammogram analysis. Experimental results on a large multi-institution dataset\ndemonstrate that our method outperforms conventional approaches while\nmaintaining detection efficiency, achieving an AUROC of 0.852 for\ndistinguishing between Benign, DCIS, and Invasive classes. This work highlights\nthe potential of MVPT-NET for medical imaging tasks and provides a scalable\nsolution for integrating multi-view data in breast cancer detection."}
{"id": "2504.19254", "pdf": "https://arxiv.org/pdf/2504.19254", "abs": "https://arxiv.org/abs/2504.19254", "authors": ["Dylan Bouchard", "Mohit Singh Chauhan"], "title": "Uncertainty Quantification for Language Models: A Suite of Black-Box, White-Box, LLM Judge, and Ensemble Scorers", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "UQLM repository: https://github.com/cvs-health/uqlm", "summary": "Hallucinations are a persistent problem with Large Language Models (LLMs). As\nthese models become increasingly used in high-stakes domains, such as\nhealthcare and finance, the need for effective hallucination detection is\ncrucial. To this end, we propose a versatile framework for zero-resource\nhallucination detection that practitioners can apply to real-world use cases.\nTo achieve this, we adapt a variety of existing uncertainty quantification (UQ)\ntechniques, including black-box UQ, white-box UQ, and LLM-as-a-Judge,\ntransforming them as necessary into standardized response-level confidence\nscores ranging from 0 to 1. To enhance flexibility, we introduce a tunable\nensemble approach that incorporates any combination of the individual\nconfidence scores. This approach enables practitioners to optimize the ensemble\nfor a specific use case for improved performance. To streamline implementation,\nthe full suite of scorers is offered in this paper's companion Python toolkit,\nUQLM. To evaluate the performance of the various scorers, we conduct an\nextensive set of experiments using several LLM question-answering benchmarks.\nWe find that our tunable ensemble typically surpasses its individual components\nand outperforms existing hallucination detection methods. Our results\ndemonstrate the benefits of customized hallucination detection strategies for\nimproving the accuracy and reliability of LLMs."}
{"id": "2504.19918", "pdf": "https://arxiv.org/pdf/2504.19918", "abs": "https://arxiv.org/abs/2504.19918", "authors": ["Hugo Georgenthum", "Cristian Cosentino", "Fabrizio Marozzo", "Pietro Liò"], "title": "Enhancing Surgical Documentation through Multimodal Visual-Temporal Transformers and Generative AI", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The automatic summarization of surgical videos is essential for enhancing\nprocedural documentation, supporting surgical training, and facilitating\npost-operative analysis. This paper presents a novel method at the intersection\nof artificial intelligence and medicine, aiming to develop machine learning\nmodels with direct real-world applications in surgical contexts. We propose a\nmulti-modal framework that leverages recent advancements in computer vision and\nlarge language models to generate comprehensive video summaries. % The approach\nis structured in three key stages. First, surgical videos are divided into\nclips, and visual features are extracted at the frame level using visual\ntransformers. This step focuses on detecting tools, tissues, organs, and\nsurgical actions. Second, the extracted features are transformed into\nframe-level captions via large language models. These are then combined with\ntemporal features, captured using a ViViT-based encoder, to produce clip-level\nsummaries that reflect the broader context of each video segment. Finally, the\nclip-level descriptions are aggregated into a full surgical report using a\ndedicated LLM tailored for the summarization task. % We evaluate our method on\nthe CholecT50 dataset, using instrument and action annotations from 50\nlaparoscopic videos. The results show strong performance, achieving 96\\%\nprecision in tool detection and a BERT score of 0.74 for temporal context\nsummarization. This work contributes to the advancement of AI-assisted tools\nfor surgical reporting, offering a step toward more intelligent and reliable\nclinical documentation."}
{"id": "2504.19267", "pdf": "https://arxiv.org/pdf/2504.19267", "abs": "https://arxiv.org/abs/2504.19267", "authors": ["Mohamed Gado", "Towhid Taliee", "Muhammad Memon", "Dmitry Ignatov", "Radu Timofte"], "title": "VIST-GPT: Ushering in the Era of Visual Storytelling with LLMs?", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Visual storytelling is an interdisciplinary field combining computer vision\nand natural language processing to generate cohesive narratives from sequences\nof images. This paper presents a novel approach that leverages recent\nadvancements in multimodal models, specifically adapting transformer-based\narchitectures and large multimodal models, for the visual storytelling task.\nLeveraging the large-scale Visual Storytelling (VIST) dataset, our VIST-GPT\nmodel produces visually grounded, contextually appropriate narratives. We\naddress the limitations of traditional evaluation metrics, such as BLEU,\nMETEOR, ROUGE, and CIDEr, which are not suitable for this task. Instead, we\nutilize RoViST and GROOVIST, novel reference-free metrics designed to assess\nvisual storytelling, focusing on visual grounding, coherence, and\nnon-redundancy. These metrics provide a more nuanced evaluation of narrative\nquality, aligning closely with human judgment."}
{"id": "2504.19935", "pdf": "https://arxiv.org/pdf/2504.19935", "abs": "https://arxiv.org/abs/2504.19935", "authors": ["Xiem HoangVan", "Hieu Bui Minh", "Sang NguyenQuang", "Wen-Hsiao Peng"], "title": "Enhancing Quality for VVC Compressed Videos with Omniscient Quality Enhancement Model", "categories": ["cs.CV"], "comment": null, "summary": "The latest video coding standard H.266/VVC has shown its great improvement in\nterms of compression performance when compared to its predecessor HEVC\nstandard. Though VVC was implemented with many advanced techniques, it still\nmet the same challenges as its predecessor due to the need for even higher\nperceptual quality demand at the decoder side as well as the compression\nperformance at the encoder side. The advancement of Artificial Intelligence\n(AI) technology, notably the deep learning-based video quality enhancement\nmethods, was shown to be a promising approach to improving the perceptual\nquality experience. In this paper, we propose a novel Omniscient video quality\nenhancement Network for VVC compressed Videos. The Omniscient Network for\ncompressed video quality enhancement was originally designed for HEVC\ncompressed videos in which not only the spatial-temporal features but also\ncross-frequencies information were employed to augment the visual quality.\nInspired by this work, we propose a modification of the OVQE model and\nintegrate it into the lasted STD-VVC (Standard Versatile Video Coding) decoder\narchitecture. As assessed in a rich set of test conditions, the proposed\nOVQE-VVC solution is able to achieve significant PSNR improvement, notably\naround 0.74 dB and up to 1.2 dB with respect to the original STD-VVC codec.\nThis also corresponds to around 19.6% of bitrate saving while keeping a similar\nquality observation."}
{"id": "2504.19274", "pdf": "https://arxiv.org/pdf/2504.19274", "abs": "https://arxiv.org/abs/2504.19274", "authors": ["Mohammad M Maheri", "Hamed Haddadi", "Alex Davidson"], "title": "TeleSparse: Practical Privacy-Preserving Verification of Deep Neural Networks", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "This paper has been accepted to the Privacy Enhancing Technologies\n  Symposium (PETS) 2025", "summary": "Verification of the integrity of deep learning inference is crucial for\nunderstanding whether a model is being applied correctly. However, such\nverification typically requires access to model weights and (potentially\nsensitive or private) training data. So-called Zero-knowledge Succinct\nNon-Interactive Arguments of Knowledge (ZK-SNARKs) would appear to provide the\ncapability to verify model inference without access to such sensitive data.\nHowever, applying ZK-SNARKs to modern neural networks, such as transformers and\nlarge vision models, introduces significant computational overhead.\n  We present TeleSparse, a ZK-friendly post-processing mechanisms to produce\npractical solutions to this problem. TeleSparse tackles two fundamental\nchallenges inherent in applying ZK-SNARKs to modern neural networks: (1)\nReducing circuit constraints: Over-parameterized models result in numerous\nconstraints for ZK-SNARK verification, driving up memory and proof generation\ncosts. We address this by applying sparsification to neural network models,\nenhancing proof efficiency without compromising accuracy or security. (2)\nMinimizing the size of lookup tables required for non-linear functions, by\noptimizing activation ranges through neural teleportation, a novel adaptation\nfor narrowing activation functions' range.\n  TeleSparse reduces prover memory usage by 67% and proof generation time by\n46% on the same model, with an accuracy trade-off of approximately 1%. We\nimplement our framework using the Halo2 proving system and demonstrate its\neffectiveness across multiple architectures (Vision-transformer, ResNet,\nMobileNet) and datasets (ImageNet,CIFAR-10,CIFAR-100). This work opens new\ndirections for ZK-friendly model design, moving toward scalable,\nresource-efficient verifiable deep learning."}
{"id": "2504.19938", "pdf": "https://arxiv.org/pdf/2504.19938", "abs": "https://arxiv.org/abs/2504.19938", "authors": ["Yunfei Wan", "Jianheng Liu", "Jiarong Lin", "Fu Zhang"], "title": "Mesh-Learner: Texturing Mesh with Spherical Harmonics", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "In this paper, we present a 3D reconstruction and rendering framework termed\nMesh-Learner that is natively compatible with traditional rasterization\npipelines. It integrates mesh and spherical harmonic (SH) texture (i.e.,\ntexture filled with SH coefficients) into the learning process to learn each\nmesh s view-dependent radiance end-to-end. Images are rendered by interpolating\nsurrounding SH Texels at each pixel s sampling point using a novel\ninterpolation method. Conversely, gradients from each pixel are back-propagated\nto the related SH Texels in SH textures. Mesh-Learner exploits graphic features\nof rasterization pipeline (texture sampling, deferred rendering) to render,\nwhich makes Mesh-Learner naturally compatible with tools (e.g., Blender) and\ntasks (e.g., 3D reconstruction, scene rendering, reinforcement learning for\nrobotics) that are based on rasterization pipelines. Our system can train vast,\nunlimited scenes because we transfer only the SH textures within the frustum to\nthe GPU for training. At other times, the SH textures are stored in CPU RAM,\nwhich results in moderate GPU memory usage. The rendering results on\ninterpolation and extrapolation sequences in the Replica and FAST-LIVO2\ndatasets achieve state-of-the-art performance compared to existing\nstate-of-the-art methods (e.g., 3D Gaussian Splatting and M2-Mapping). To\nbenefit the society, the code will be available at\nhttps://github.com/hku-mars/Mesh-Learner."}
{"id": "2504.19275", "pdf": "https://arxiv.org/pdf/2504.19275", "abs": "https://arxiv.org/abs/2504.19275", "authors": ["Yiren Xu"], "title": "Balancing Creativity and Automation: The Influence of AI on Modern Film Production and Dissemination", "categories": ["cs.CY", "cs.AI", "I.5.m"], "comment": "19 pages, 1 figures, 2 tables", "summary": "The integration of Artificial Intelligence(AI) into film production has\nrevolutionized efficiency and creativity, yet it simultaneously raises critical\nethical and practical challenges. This study explores the dual impact of AI on\nmodern cinema through three objectives: defining the optimal human-AI\nrelationship, balancing creativity with automation, and developing ethical\nguidelines. By employing a mixed-method approach combining theoretical\nframeworks (auteur theory, human-technology relations) and case studies (The\nSafe Zone, Fast & Furious 7, The Brutalist), the research reveals that\npositioning AI as an \"embodiment tool\" rather than an independent \"alterity\npartner\" preserves human authorship and artistic integrity. Key findings\nhighlight the risks of surveillance capitalism in AI-driven markets and the\nethical dilemmas of deepfake technology. The study concludes with actionable\nrecommendations, including international regulatory frameworks and a Human\nControl Index (HCI) to quantify AI involvement. These insights aim to guide\nfilmmakers, policymakers, and scholars in navigating the evolving AI-cinema\nlandscape while safeguarding cultural diversity and ethical standards."}
{"id": "2504.19970", "pdf": "https://arxiv.org/pdf/2504.19970", "abs": "https://arxiv.org/abs/2504.19970", "authors": ["Narges Rashvand", "Ghazal Alinezhad Noghre", "Armin Danesh Pazho", "Babak Rahimi Ardabili", "Hamed Tabkhi"], "title": "Shopformer: Transformer-Based Framework for Detecting Shoplifting via Human Pose", "categories": ["cs.CV"], "comment": null, "summary": "Shoplifting remains a costly issue for the retail sector, but traditional\nsurveillance systems, which are mostly based on human monitoring, are still\nlargely ineffective, with only about 2% of shoplifters being arrested. Existing\nAI-based approaches rely on pixel-level video analysis which raises privacy\nconcerns, is sensitive to environmental variations, and demands significant\ncomputational resources. To address these limitations, we introduce Shopformer,\na novel transformer-based model that detects shoplifting by analyzing pose\nsequences rather than raw video. We propose a custom tokenization strategy that\nconverts pose sequences into compact embeddings for efficient transformer\nprocessing. To the best of our knowledge, this is the first pose-sequence-based\ntransformer model for shoplifting detection. Evaluated on real-world pose data,\nour method outperforms state-of-the-art anomaly detection models, offering a\nprivacy-preserving, and scalable solution for real-time retail surveillance.\nThe code base for this work is available at\nhttps://github.com/TeCSAR-UNCC/Shopformer."}
{"id": "2504.19276", "pdf": "https://arxiv.org/pdf/2504.19276", "abs": "https://arxiv.org/abs/2504.19276", "authors": ["Yiyang Zhou", "Zhaoyang Wang", "Tianle Wang", "Shangyu Xing", "Peng Xia", "Bo Li", "Kaiyuan Zheng", "Zijian Zhang", "Zhaorun Chen", "Wenhao Zheng", "Xuchao Zhang", "Chetan Bansal", "Weitong Zhang", "Ying Wei", "Mohit Bansal", "Huaxiu Yao"], "title": "Anyprefer: An Agentic Framework for Preference Data Synthesis", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "High-quality preference data is essential for aligning foundation models with\nhuman values through preference learning. However, manual annotation of such\ndata is often time-consuming and costly. Recent methods often adopt a\nself-rewarding approach, where the target model generates and annotates its own\npreference data, but this can lead to inaccuracies since the reward model\nshares weights with the target model, thereby amplifying inherent biases. To\naddress these issues, we propose Anyprefer, a framework designed to synthesize\nhigh-quality preference data for aligning the target model. Anyprefer frames\nthe data synthesis process as a cooperative two-player Markov Game, where the\ntarget model and the judge model collaborate together. Here, a series of\nexternal tools are introduced to assist the judge model in accurately rewarding\nthe target model's responses, mitigating biases in the rewarding process. In\naddition, a feedback mechanism is introduced to optimize prompts for both\nmodels, enhancing collaboration and improving data quality. The synthesized\ndata is compiled into a new preference dataset, Anyprefer-V1, consisting of 58K\nhigh-quality preference pairs. Extensive experiments show that Anyprefer\nsignificantly improves model alignment performance across four main\napplications, covering 21 datasets, achieving average improvements of 18.55% in\nfive natural language generation datasets, 3.66% in nine vision-language\nunderstanding datasets, 30.05% in three medical image analysis datasets, and\n16.00% in four visuo-motor control tasks."}
{"id": "2504.19991", "pdf": "https://arxiv.org/pdf/2504.19991", "abs": "https://arxiv.org/abs/2504.19991", "authors": ["Ioannis Kontogiorgakis", "Iason Tsardanidis", "Dimitrios Bormpoudakis", "Ilias Tsoumas", "Dimitra A. Loka", "Christos Noulas", "Alexandros Tsitouras", "Charalampos Kontoes"], "title": "Mapping of Weed Management Methods in Orchards using Sentinel-2 and PlanetScope Data", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Effective weed management is crucial for improving agricultural productivity,\nas weeds compete with crops for vital resources like nutrients and water.\nAccurate maps of weed management methods are essential for policymakers to\nassess farmer practices, evaluate impacts on vegetation health, biodiversity,\nand climate, as well as ensure compliance with policies and subsidies. However,\nmonitoring weed management methods is challenging as commonly rely on on-ground\nfield surveys, which are often costly, time-consuming and subject to delays. In\norder to tackle this problem, we leverage Earth Observation (EO) data and\nMachine Learning (ML). Specifically, we developed an ML approach for mapping\nfour distinct weed management methods (Mowing, Tillage, Chemical-spraying, and\nNo practice) in orchards using satellite image time series (SITS) data from two\ndifferent sources: Sentinel-2 (S2) and PlanetScope (PS). The findings\ndemonstrate the potential of ML-driven remote sensing to enhance the efficiency\nand accuracy of weed management mapping in orchards."}
{"id": "2504.19323", "pdf": "https://arxiv.org/pdf/2504.19323", "abs": "https://arxiv.org/abs/2504.19323", "authors": ["Hanchen Yang", "Zishen Wan", "Ritik Raj", "Joongun Park", "Ziwei Li", "Ananda Samajdar", "Arijit Raychowdhury", "Tushar Krishna"], "title": "NSFlow: An End-to-End FPGA Framework with Scalable Dataflow Architecture for Neuro-Symbolic AI", "categories": ["cs.AR", "cs.AI", "cs.LG", "cs.PF"], "comment": null, "summary": "Neuro-Symbolic AI (NSAI) is an emerging paradigm that integrates neural\nnetworks with symbolic reasoning to enhance the transparency, reasoning\ncapabilities, and data efficiency of AI systems. Recent NSAI systems have\ngained traction due to their exceptional performance in reasoning tasks and\nhuman-AI collaborative scenarios. Despite these algorithmic advancements,\nexecuting NSAI tasks on existing hardware (e.g., CPUs, GPUs, TPUs) remains\nchallenging, due to their heterogeneous computing kernels, high memory\nintensity, and unique memory access patterns. Moreover, current NSAI algorithms\nexhibit significant variation in operation types and scales, making them\nincompatible with existing ML accelerators. These challenges highlight the need\nfor a versatile and flexible acceleration framework tailored to NSAI workloads.\nIn this paper, we propose NSFlow, an FPGA-based acceleration framework designed\nto achieve high efficiency, scalability, and versatility across NSAI systems.\nNSFlow features a design architecture generator that identifies workload data\ndependencies and creates optimized dataflow architectures, as well as a\nreconfigurable array with flexible compute units, re-organizable memory, and\nmixed-precision capabilities. Evaluating across NSAI workloads, NSFlow achieves\n31x speedup over Jetson TX2, more than 2x over GPU, 8x speedup over TPU-like\nsystolic array, and more than 3x over Xilinx DPU. NSFlow also demonstrates\nenhanced scalability, with only 4x runtime increase when symbolic workloads\nscale by 150x. To the best of our knowledge, NSFlow is the first framework to\nenable real-time generalizable NSAI algorithms acceleration, demonstrating a\npromising solution for next-generation cognitive systems."}
{"id": "2504.19996", "pdf": "https://arxiv.org/pdf/2504.19996", "abs": "https://arxiv.org/abs/2504.19996", "authors": ["Andreas Kalogeras", "Dimitrios Bormpoudakis", "Iason Tsardanidis", "Dimitra A. Loka", "Charalampos Kontoes"], "title": "Monitoring digestate application on agricultural crops using Sentinel-2 Satellite imagery", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "The widespread use of Exogenous Organic Matter in agriculture necessitates\nmonitoring to assess its effects on soil and crop health. This study evaluates\noptical Sentinel-2 satellite imagery for detecting digestate application, a\npractice that enhances soil fertility but poses environmental risks like\nmicroplastic contamination and nitrogen losses. In the first instance,\nSentinel-2 satellite image time series (SITS) analysis of specific indices\n(EOMI, NDVI, EVI) was used to characterize EOM's spectral behavior after\napplication on the soils of four different crop types in Thessaly, Greece.\nFurthermore, Machine Learning (ML) models (namely Random Forest, k-NN, Gradient\nBoosting and a Feed-Forward Neural Network), were used to investigate digestate\npresence detection, achieving F1-scores up to 0.85. The findings highlight the\npotential of combining remote sensing and ML for scalable and cost-effective\nmonitoring of EOM applications, supporting precision agriculture and\nsustainability."}
{"id": "2504.19327", "pdf": "https://arxiv.org/pdf/2504.19327", "abs": "https://arxiv.org/abs/2504.19327", "authors": ["Moulik Choraria", "Xinbo Wu", "Akhil Bhimaraju", "Nitesh Sekhar", "Yue Wu", "Xu Zhang", "Prateek Singhal", "Lav R. Varshney"], "title": "Platonic Grounding for Efficient Multimodal Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The hyperscaling of data and parameter count in Transformer-based models is\nyielding diminishing performance improvement, especially when weighed against\ntraining costs. Such plateauing indicates the importance of methods for more\nefficient finetuning and inference, while retaining similar performance. This\nis especially relevant for multimodal learning paradigms, where inference costs\nof processing multimodal tokens can determine the model's practical viability.\nAt the same time, research on representations and mechanistic interpretability\nhas improved our understanding of the inner workings of Transformer-based\nmodels; one such line of work reveals an implicit alignment in the deeper\nlayers of pretrained models, across modalities. Taking inspiration from this,\nwe motivate and propose a simple modification to existing multimodal frameworks\nthat rely on aligning pretrained models. We demonstrate that our approach\nmaintains and, in some cases, even improves performance of baseline methods\nwhile achieving significant gains in both training and inference-time compute.\nOur work also has implications for combining pretrained models into larger\nsystems efficiently."}
{"id": "2504.20024", "pdf": "https://arxiv.org/pdf/2504.20024", "abs": "https://arxiv.org/abs/2504.20024", "authors": ["Wufei Ma", "Yu-Cheng Chou", "Qihao Liu", "Xingrui Wang", "Celso de Melo", "Jieneng Chen", "Jianwen Xie", "Alan Yuille"], "title": "SpatialReasoner: Towards Explicit and Generalizable 3D Spatial Reasoning", "categories": ["cs.CV"], "comment": "Project page: https://spatial-reasoner.github.io", "summary": "Recent studies in 3D spatial reasoning explore data-driven approaches and\nachieve enhanced spatial reasoning performance with reinforcement learning\n(RL). However, these methods typically perform spatial reasoning in an implicit\nmanner, and it remains underexplored whether the acquired 3D knowledge\ngeneralizes to unseen question types at any stage of the training. In this work\nwe introduce SpatialReasoner, a novel large vision-language model (LVLM) that\naddress 3D spatial reasoning with explicit 3D representations shared between\nstages -- 3D perception, computation, and reasoning. Explicit 3D\nrepresentations provide a coherent interface that supports advanced 3D spatial\nreasoning and enable us to study the factual errors made by LVLMs. Results show\nthat our SpatialReasoner achieve improved performance on a variety of spatial\nreasoning benchmarks and generalizes better when evaluating on novel 3D spatial\nreasoning questions. Our study bridges the 3D parsing capabilities of prior\nvisual foundation models with the powerful reasoning abilities of large\nlanguage models, opening new directions for 3D spatial reasoning."}
{"id": "2504.19333", "pdf": "https://arxiv.org/pdf/2504.19333", "abs": "https://arxiv.org/abs/2504.19333", "authors": ["James O' Neill", "Santhosh Subramanian", "Eric Lin", "Vaikkunth Mugunthan"], "title": "Unified Multi-Task Learning & Model Fusion for Efficient Language Model Guardrailing", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The trend towards large language models (LLMs) for guardrailing against\nundesired behaviors is increasing and has shown promise for censoring user\ninputs. However, increased latency, memory consumption, hosting expenses and\nnon-structured outputs can make their use prohibitive.\n  In this work, we show that task-specific data generation can lead to\nfine-tuned classifiers that significantly outperform current state of the art\n(SoTA) while being orders of magnitude smaller. Secondly, we show that using a\nsingle model, \\texttt{MultiTaskGuard}, that is pretrained on a large\nsynthetically generated dataset with unique task instructions further improves\ngeneralization. Thirdly, our most performant models, \\texttt{UniGuard}, are\nfound using our proposed search-based model merging approach that finds an\noptimal set of parameters to combine single-policy models and multi-policy\nguardrail models. % On 7 public datasets and 4 guardrail benchmarks we created,\nour efficient guardrail classifiers improve over the best performing SoTA\npublicly available LLMs and 3$^{\\text{rd}}$ party guardrail APIs in detecting\nunsafe and safe behaviors by an average F1 score improvement of \\textbf{29.92}\npoints over Aegis-LlamaGuard and \\textbf{21.62} over \\texttt{gpt-4o},\nrespectively. Lastly, our guardrail synthetic data generation process that uses\ncustom task-specific guardrail poli"}
{"id": "2504.20026", "pdf": "https://arxiv.org/pdf/2504.20026", "abs": "https://arxiv.org/abs/2504.20026", "authors": ["Zhengqin Li", "Dilin Wang", "Ka Chen", "Zhaoyang Lv", "Thu Nguyen-Phuoc", "Milim Lee", "Jia-Bin Huang", "Lei Xiao", "Cheng Zhang", "Yufeng Zhu", "Carl S. Marshall", "Yufeng Ren", "Richard Newcombe", "Zhao Dong"], "title": "LIRM: Large Inverse Rendering Model for Progressive Reconstruction of Shape, Materials and View-dependent Radiance Fields", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by CVPR 2025", "summary": "We present Large Inverse Rendering Model (LIRM), a transformer architecture\nthat jointly reconstructs high-quality shape, materials, and radiance fields\nwith view-dependent effects in less than a second. Our model builds upon the\nrecent Large Reconstruction Models (LRMs) that achieve state-of-the-art\nsparse-view reconstruction quality. However, existing LRMs struggle to\nreconstruct unseen parts accurately and cannot recover glossy appearance or\ngenerate relightable 3D contents that can be consumed by standard Graphics\nengines. To address these limitations, we make three key technical\ncontributions to build a more practical multi-view 3D reconstruction framework.\nFirst, we introduce an update model that allows us to progressively add more\ninput views to improve our reconstruction. Second, we propose a hexa-plane\nneural SDF representation to better recover detailed textures, geometry and\nmaterial parameters. Third, we develop a novel neural directional-embedding\nmechanism to handle view-dependent effects. Trained on a large-scale shape and\nmaterial dataset with a tailored coarse-to-fine training scheme, our model\nachieves compelling results. It compares favorably to optimization-based\ndense-view inverse rendering methods in terms of geometry and relighting\naccuracy, while requiring only a fraction of the inference time."}
{"id": "2504.19339", "pdf": "https://arxiv.org/pdf/2504.19339", "abs": "https://arxiv.org/abs/2504.19339", "authors": ["Dongqi Liu", "Xi Yu", "Vera Demberg", "Mirella Lapata"], "title": "Explanatory Summarization with Discourse-Driven Planning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by the Transactions of the Association for Computational\n  Linguistics (TACL)", "summary": "Lay summaries for scientific documents typically include explanations to help\nreaders grasp sophisticated concepts or arguments. However, current automatic\nsummarization methods do not explicitly model explanations, which makes it\ndifficult to align the proportion of explanatory content with human-written\nsummaries. In this paper, we present a plan-based approach that leverages\ndiscourse frameworks to organize summary generation and guide explanatory\nsentences by prompting responses to the plan. Specifically, we propose two\ndiscourse-driven planning strategies, where the plan is conditioned as part of\nthe input or part of the output prefix, respectively. Empirical experiments on\nthree lay summarization datasets show that our approach outperforms existing\nstate-of-the-art methods in terms of summary quality, and it enhances model\nrobustness, controllability, and mitigates hallucination."}
{"id": "2504.20032", "pdf": "https://arxiv.org/pdf/2504.20032", "abs": "https://arxiv.org/abs/2504.20032", "authors": ["Kai Ye", "Haidi Tang", "Bowen Liu", "Pingyang Dai", "Liujuan Cao", "Rongrong Ji"], "title": "More Clear, More Flexible, More Precise: A Comprehensive Oriented Object Detection benchmark for UAV", "categories": ["cs.CV"], "comment": null, "summary": "Applications of unmanned aerial vehicle (UAV) in logistics, agricultural\nautomation, urban management, and emergency response are highly dependent on\noriented object detection (OOD) to enhance visual perception. Although existing\ndatasets for OOD in UAV provide valuable resources, they are often designed for\nspecific downstream tasks.Consequently, they exhibit limited generalization\nperformance in real flight scenarios and fail to thoroughly demonstrate\nalgorithm effectiveness in practical environments. To bridge this critical gap,\nwe introduce CODrone, a comprehensive oriented object detection dataset for\nUAVs that accurately reflects real-world conditions. It also serves as a new\nbenchmark designed to align with downstream task requirements, ensuring greater\napplicability and robustness in UAV-based OOD.Based on application\nrequirements, we identify four key limitations in current UAV OOD datasets-low\nimage resolution, limited object categories, single-view imaging, and\nrestricted flight altitudes-and propose corresponding improvements to enhance\ntheir applicability and robustness.Furthermore, CODrone contains a broad\nspectrum of annotated images collected from multiple cities under various\nlighting conditions, enhancing the realism of the benchmark. To rigorously\nevaluate CODrone as a new benchmark and gain deeper insights into the novel\nchallenges it presents, we conduct a series of experiments based on 22\nclassical or SOTA methods.Our evaluation not only assesses the effectiveness of\nCODrone in real-world scenarios but also highlights key bottlenecks and\nopportunities to advance OOD in UAV applications.Overall, CODrone fills the\ndata gap in OOD from UAV perspective and provides a benchmark with enhanced\ngeneralization capability, better aligning with practical applications and\nfuture algorithm development."}
{"id": "2504.19341", "pdf": "https://arxiv.org/pdf/2504.19341", "abs": "https://arxiv.org/abs/2504.19341", "authors": ["Jialiang Zhao", "Naveen Kuppuswamy", "Siyuan Feng", "Benjamin Burchfiel", "Edward Adelson"], "title": "PolyTouch: A Robust Multi-Modal Tactile Sensor for Contact-rich Manipulation Using Tactile-Diffusion Policies", "categories": ["cs.RO", "cs.AI"], "comment": "Nominated for the best paper award at ICRA 2025", "summary": "Achieving robust dexterous manipulation in unstructured domestic environments\nremains a significant challenge in robotics. Even with state-of-the-art robot\nlearning methods, haptic-oblivious control strategies (i.e. those relying only\non external vision and/or proprioception) often fall short due to occlusions,\nvisual complexities, and the need for precise contact interaction control. To\naddress these limitations, we introduce PolyTouch, a novel robot finger that\nintegrates camera-based tactile sensing, acoustic sensing, and peripheral\nvisual sensing into a single design that is compact and durable. PolyTouch\nprovides high-resolution tactile feedback across multiple temporal scales,\nwhich is essential for efficiently learning complex manipulation tasks.\nExperiments demonstrate an at least 20-fold increase in lifespan over\ncommercial tactile sensors, with a design that is both easy to manufacture and\nscalable. We then use this multi-modal tactile feedback along with\nvisuo-proprioceptive observations to synthesize a tactile-diffusion policy from\nhuman demonstrations; the resulting contact-aware control policy significantly\noutperforms haptic-oblivious policies in multiple contact-aware manipulation\npolicies. This paper highlights how effectively integrating multi-modal contact\nsensing can hasten the development of effective contact-aware manipulation\npolicies, paving the way for more reliable and versatile domestic robots. More\ninformation can be found at https://polytouch.alanz.info/"}
{"id": "2504.20033", "pdf": "https://arxiv.org/pdf/2504.20033", "abs": "https://arxiv.org/abs/2504.20033", "authors": ["Sara Yavari", "Jacob Furst"], "title": "Mitigating Catastrophic Forgetting in the Incremental Learning of Medical Images", "categories": ["cs.CV", "I.2.6; I.2.10"], "comment": "15 Pages, 3 Figures, 3 Tables, 1 Algorithm, This paper will be\n  updated", "summary": "This paper proposes an Incremental Learning (IL) approach to enhance the\naccuracy and efficiency of deep learning models in analyzing T2-weighted (T2w)\nMRI medical images prostate cancer detection using the PI-CAI dataset. We used\nmultiple health centers' artificial intelligence and radiology data, focused on\ndifferent tasks that looked at prostate cancer detection using MRI (PI-CAI). We\nutilized Knowledge Distillation (KD), as it employs generated images from past\ntasks to guide the training of models for subsequent tasks. The approach\nyielded improved performance and faster convergence of the models. To\ndemonstrate the versatility and robustness of our approach, we evaluated it on\nthe PI-CAI dataset, a diverse set of medical imaging modalities including OCT\nand PathMNIST, and the benchmark continual learning dataset CIFAR-10. Our\nresults indicate that KD can be a promising technique for IL in medical image\nanalysis in which data is sourced from individual health centers and the\nstorage of large datasets is not feasible. By using generated images from prior\ntasks, our method enables the model to retain and apply previously acquired\nknowledge without direct access to the original data."}
{"id": "2504.19353", "pdf": "https://arxiv.org/pdf/2504.19353", "abs": "https://arxiv.org/abs/2504.19353", "authors": ["Weitao Du", "Shuning Chang", "Jiasheng Tang", "Yu Rong", "Fan Wang", "Shengchao Liu"], "title": "Flow Along the K-Amplitude for Generative Modeling", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In this work, we propose a novel generative learning paradigm, K-Flow, an\nalgorithm that flows along the $K$-amplitude. Here, $k$ is a scaling parameter\nthat organizes frequency bands (or projected coefficients), and amplitude\ndescribes the norm of such projected coefficients. By incorporating the\n$K$-amplitude decomposition, K-Flow enables flow matching across the scaling\nparameter as time. We discuss three venues and six properties of K-Flow, from\ntheoretical foundations, energy and temporal dynamics, and practical\napplications, respectively. Specifically, from the practical usage perspective,\nK-Flow allows steerable generation by controlling the information at different\nscales. To demonstrate the effectiveness of K-Flow, we conduct experiments on\nunconditional image generation, class-conditional image generation, and\nmolecule assembly generation. Additionally, we conduct three ablation studies\nto demonstrate how K-Flow steers scaling parameter to effectively control the\nresolution of image generation."}
{"id": "2504.20040", "pdf": "https://arxiv.org/pdf/2504.20040", "abs": "https://arxiv.org/abs/2504.20040", "authors": ["Zador Pataki", "Paul-Edouard Sarlin", "Johannes L. Schönberger", "Marc Pollefeys"], "title": "MP-SfM: Monocular Surface Priors for Robust Structure-from-Motion", "categories": ["cs.CV", "cs.RO"], "comment": "CVPR 2025", "summary": "While Structure-from-Motion (SfM) has seen much progress over the years,\nstate-of-the-art systems are prone to failure when facing extreme viewpoint\nchanges in low-overlap, low-parallax or high-symmetry scenarios. Because\ncapturing images that avoid these pitfalls is challenging, this severely limits\nthe wider use of SfM, especially by non-expert users. We overcome these\nlimitations by augmenting the classical SfM paradigm with monocular depth and\nnormal priors inferred by deep neural networks. Thanks to a tight integration\nof monocular and multi-view constraints, our approach significantly outperforms\nexisting ones under extreme viewpoint changes, while maintaining strong\nperformance in standard conditions. We also show that monocular priors can help\nreject faulty associations due to symmetries, which is a long-standing problem\nfor SfM. This makes our approach the first capable of reliably reconstructing\nchallenging indoor environments from few images. Through principled uncertainty\npropagation, it is robust to errors in the priors, can handle priors inferred\nby different models with little tuning, and will thus easily benefit from\nfuture progress in monocular depth and normal estimation. Our code is publicly\navailable at https://github.com/cvg/mpsfm."}
{"id": "2504.19362", "pdf": "https://arxiv.org/pdf/2504.19362", "abs": "https://arxiv.org/abs/2504.19362", "authors": ["Yunxuan Wang", "Ray Yin", "Yumei Tan", "Hao Chen", "Haiying Xia"], "title": "Low-Rank Adaptive Structural Priors for Generalizable Diabetic Retinopathy Grading", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Accepted by IJCNN 2025", "summary": "Diabetic retinopathy (DR), a serious ocular complication of diabetes, is one\nof the primary causes of vision loss among retinal vascular diseases. Deep\nlearning methods have been extensively applied in the grading of diabetic\nretinopathy (DR). However, their performance declines significantly when\napplied to data outside the training distribution due to domain shifts. Domain\ngeneralization (DG) has emerged as a solution to this challenge. However, most\nexisting DG methods overlook lesion-specific features, resulting in\ninsufficient accuracy. In this paper, we propose a novel approach that enhances\nexisting DG methods by incorporating structural priors, inspired by the\nobservation that DR grading is heavily dependent on vessel and lesion\nstructures. We introduce Low-rank Adaptive Structural Priors (LoASP), a\nplug-and-play framework designed for seamless integration with existing DG\nmodels. LoASP improves generalization by learning adaptive structural\nrepresentations that are finely tuned to the complexities of DR diagnosis.\nExtensive experiments on eight diverse datasets validate its effectiveness in\nboth single-source and multi-source domain scenarios. Furthermore,\nvisualizations reveal that the learned structural priors intuitively align with\nthe intricate architecture of the vessels and lesions, providing compelling\ninsights into their interpretability and diagnostic relevance."}
{"id": "2504.20041", "pdf": "https://arxiv.org/pdf/2504.20041", "abs": "https://arxiv.org/abs/2504.20041", "authors": ["Yibin Yan", "Jilan Xu", "Shangzhe Di", "Yikun Liu", "Yudi Shi", "Qirui Chen", "Zeqian Li", "Yifei Huang", "Weidi Xie"], "title": "Learning Streaming Video Representation via Multitask Training", "categories": ["cs.CV"], "comment": "Technical Report. Project Page:\n  https://go2heart.github.io/streamformer", "summary": "Understanding continuous video streams plays a fundamental role in real-time\napplications including embodied AI and autonomous driving. Unlike offline video\nunderstanding, streaming video understanding requires the ability to process\nvideo streams frame by frame, preserve historical information, and make\nlow-latency decisions.To address these challenges, our main contributions are\nthree-fold. (i) We develop a novel streaming video backbone, termed as\nStreamFormer, by incorporating causal temporal attention into a pre-trained\nvision transformer. This enables efficient streaming video processing while\nmaintaining image representation capability.(ii) To train StreamFormer, we\npropose to unify diverse spatial-temporal video understanding tasks within a\nmultitask visual-language alignment framework. Hence, StreamFormer learns\nglobal semantics, temporal dynamics, and fine-grained spatial relationships\nsimultaneously. (iii) We conduct extensive experiments on online action\ndetection, online video instance segmentation, and video question answering.\nStreamFormer achieves competitive results while maintaining efficiency,\ndemonstrating its potential for real-time applications."}
{"id": "2504.19370", "pdf": "https://arxiv.org/pdf/2504.19370", "abs": "https://arxiv.org/abs/2504.19370", "authors": ["Jean-Rémy Conti", "Stéphan Clémençon"], "title": "Mitigating Bias in Facial Recognition Systems: Centroid Fairness Loss Optimization", "categories": ["cs.CV", "cs.AI", "cs.LG", "stat.ML"], "comment": "Accepted at both the AFME and RegML Workshops at NeurIPS 2024. A\n  preliminary version has been accepted for publication by Springer Nature, in\n  the context of the ICPR 2024 conference", "summary": "The urging societal demand for fair AI systems has put pressure on the\nresearch community to develop predictive models that are not only globally\naccurate but also meet new fairness criteria, reflecting the lack of disparate\nmistreatment with respect to sensitive attributes ($\\textit{e.g.}$ gender,\nethnicity, age). In particular, the variability of the errors made by certain\nFacial Recognition (FR) systems across specific segments of the population\ncompromises the deployment of the latter, and was judged unacceptable by\nregulatory authorities. Designing fair FR systems is a very challenging\nproblem, mainly due to the complex and functional nature of the performance\nmeasure used in this domain ($\\textit{i.e.}$ ROC curves) and because of the\nhuge heterogeneity of the face image datasets usually available for training.\nIn this paper, we propose a novel post-processing approach to improve the\nfairness of pre-trained FR models by optimizing a regression loss which acts on\ncentroid-based scores. Beyond the computational advantages of the method, we\npresent numerical experiments providing strong empirical evidence of the gain\nin fairness and of the ability to preserve global accuracy."}
{"id": "2504.20042", "pdf": "https://arxiv.org/pdf/2504.20042", "abs": "https://arxiv.org/abs/2504.20042", "authors": ["Yu-Ju Tsai", "Brian Price", "Qing Liu", "Luis Figueroa", "Daniil Pakhomov", "Zhihong Ding", "Scott Cohen", "Ming-Hsuan Yang"], "title": "CompleteMe: Reference-based Human Image Completion", "categories": ["cs.CV"], "comment": "Project page: https://liagm.github.io/CompleteMe/", "summary": "Recent methods for human image completion can reconstruct plausible body\nshapes but often fail to preserve unique details, such as specific clothing\npatterns or distinctive accessories, without explicit reference images. Even\nstate-of-the-art reference-based inpainting approaches struggle to accurately\ncapture and integrate fine-grained details from reference images. To address\nthis limitation, we propose CompleteMe, a novel reference-based human image\ncompletion framework. CompleteMe employs a dual U-Net architecture combined\nwith a Region-focused Attention (RFA) Block, which explicitly guides the\nmodel's attention toward relevant regions in reference images. This approach\neffectively captures fine details and ensures accurate semantic correspondence,\nsignificantly improving the fidelity and consistency of completed images.\nAdditionally, we introduce a challenging benchmark specifically designed for\nevaluating reference-based human image completion tasks. Extensive experiments\ndemonstrate that our proposed method achieves superior visual quality and\nsemantic consistency compared to existing techniques. Project page:\nhttps://liagm.github.io/CompleteMe/"}
{"id": "2504.19373", "pdf": "https://arxiv.org/pdf/2504.19373", "abs": "https://arxiv.org/abs/2504.19373", "authors": ["Weidi Luo", "Qiming Zhang", "Tianyu Lu", "Xiaogeng Liu", "Yue Zhao", "Zhen Xiang", "Chaowei Xiao"], "title": "Doxing via the Lens: Revealing Privacy Leakage in Image Geolocation for Agentic Multi-Modal Large Reasoning Model", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "The increasing capabilities of agentic multi-modal large reasoning models,\nsuch as ChatGPT o3, have raised critical concerns regarding privacy leakage\nthrough inadvertent image geolocation. In this paper, we conduct the first\nsystematic and controlled study on the potential privacy risks associated with\nvisual reasoning abilities of ChatGPT o3. We manually collect and construct a\ndataset comprising 50 real-world images that feature individuals alongside\nprivacy-relevant environmental elements, capturing realistic and sensitive\nscenarios for analysis. Our experimental evaluation reveals that ChatGPT o3 can\npredict user locations with high precision, achieving street-level accuracy\n(within one mile) in 60% of cases. Through analysis, we identify key visual\ncues, including street layout and front yard design, that significantly\ncontribute to the model inference success. Additionally, targeted occlusion\nexperiments demonstrate that masking critical features effectively mitigates\ngeolocation accuracy, providing insights into potential defense mechanisms. Our\nfindings highlight an urgent need for privacy-aware development for agentic\nmulti-modal large reasoning models, particularly in applications involving\nprivate imagery."}
{"id": "2504.18540", "pdf": "https://arxiv.org/pdf/2504.18540", "abs": "https://arxiv.org/abs/2504.18540", "authors": ["Gonçalo Hora de Carvalho"], "title": "Exploring Visual Complaints through a test battery in Acquired Brain Injury Patients: A Detailed Analysis of the DiaNAH Dataset", "categories": ["q-bio.NC", "cs.CV", "q-bio.QM"], "comment": null, "summary": "This study investigated visual impairment complaints in a sample of 948\nAcquired Brain Injury (ABI) patients using the DiaNAH dataset, emphasizing\nadvanced machine learning techniques for managing missing data. Patients\ncompleted a CVS questionnaire capturing eight types of visual symptoms,\nincluding blurred vision and altered contrast perception. Due to incomplete\ndata, 181 patients were excluded, resulting in an analytical subset of 767\nindividuals. To address the challenge of missing data, an automated machine\nlearning (AutoML) approach was employed for data imputation, preserving the\ndistributional characteristics of the original dataset. Patients were grouped\naccording to singular and combined complaint clusters derived from the 40,320\npotential combinations identified through the CVS questionnaire. A linear\ncorrelation analysis revealed minimal to no direct relationship between\npatient-reported visual complaints and standard visual perceptual function\ntests. This study represents an initial systematic attempt to understand the\ncomplex relationship between subjective visual complaints and objective visual\nperceptual assessments in ABI patients. Given the limitations of sample size\nand variability, further studies with larger populations are recommended to\nrobustly explore these complaint clusters and their implications for visual\nperception following brain injury."}
{"id": "2504.19374", "pdf": "https://arxiv.org/pdf/2504.19374", "abs": "https://arxiv.org/abs/2504.19374", "authors": ["Suping Xu", "Chuyi Dai", "Lin Shang", "Changbin Shao", "Xibei Yang", "Witold Pedrycz"], "title": "Rethinking Label-specific Features for Label Distribution Learning", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "11 Pages, 5 figures", "summary": "Label distribution learning (LDL) is an emerging learning paradigm designed\nto capture the relative importance of labels for each instance. Label-specific\nfeatures (LSFs), constructed by LIFT, have proven effective for learning tasks\nwith label ambiguity by leveraging clustering-based prototypes for each label\nto re-characterize instances. However, directly introducing LIFT into LDL tasks\ncan be suboptimal, as the prototypes it collects primarily reflect\nintra-cluster relationships while neglecting interactions among distinct\nclusters. Additionally, constructing LSFs using multi-perspective information,\nrather than relying solely on Euclidean distance, provides a more robust and\ncomprehensive representation of instances, mitigating noise and bias that may\narise from a single distance perspective. To address these limitations, we\nintroduce Structural Anchor Points (SAPs) to capture inter-cluster\ninteractions. This leads to a novel LSFs construction strategy, LIFT-SAP, which\nenhances LIFT by integrating both distance and direction information of each\ninstance relative to SAPs. Furthermore, we propose a novel LDL algorithm, Label\nDistribution Learning via Label-specifIc FeaTure with SAPs (LDL-LIFT-SAP),\nwhich unifies multiple label description degrees predicted from different LSF\nspaces into a cohesive label distribution. Extensive experiments on 15\nreal-world datasets demonstrate the effectiveness of LIFT-SAP over LIFT, as\nwell as the superiority of LDL-LIFT-SAP compared to seven other\nwell-established algorithms."}
{"id": "2504.18547", "pdf": "https://arxiv.org/pdf/2504.18547", "abs": "https://arxiv.org/abs/2504.18547", "authors": ["Ching-Yi Lin", "Sahil Shah"], "title": "Low-Bit Integerization of Vision Transformers using Operand Reodering for Efficient Hardware", "categories": ["cs.LG", "cs.CV", "cs.SY", "eess.SY"], "comment": "4 pages + references, 5 figures, 2 tables in IEEE double column\n  conference template", "summary": "Pre-trained vision transformers have achieved remarkable performance across\nvarious visual tasks but suffer from expensive computational and memory costs.\nWhile model quantization reduces memory usage by lowering precision, these\nmodels still incur significant computational overhead due to the dequantization\nbefore matrix operations. In this work, we analyze the computation graph and\npropose an integerization process based on operation reordering. Specifically,\nthe process delays dequantization until after matrix operations. This enables\nintegerized matrix multiplication and linear module by directly processing the\nquantized input. To validate our approach, we synthesize the self-attention\nmodule of ViT on a systolic array-based hardware. Experimental results show\nthat our low-bit inference reduces per-PE power consumption for linear layer\nand matrix multiplication, bridging the gap between quantized models and\nefficient inference."}
{"id": "2504.19384", "pdf": "https://arxiv.org/pdf/2504.19384", "abs": "https://arxiv.org/abs/2504.19384", "authors": ["Syed Tauhid Ullah Shah", "Mohamad Hussein", "Ann Barcomb", "Mohammad Moshirpour"], "title": "From Inductive to Deductive: LLMs-Based Qualitative Data Analysis in Requirements Engineering", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Requirements Engineering (RE) is essential for developing complex and\nregulated software projects. Given the challenges in transforming stakeholder\ninputs into consistent software designs, Qualitative Data Analysis (QDA)\nprovides a systematic approach to handling free-form data. However, traditional\nQDA methods are time-consuming and heavily reliant on manual effort. In this\npaper, we explore the use of Large Language Models (LLMs), including GPT-4,\nMistral, and LLaMA-2, to improve QDA tasks in RE. Our study evaluates LLMs'\nperformance in inductive (zero-shot) and deductive (one-shot, few-shot)\nannotation tasks, revealing that GPT-4 achieves substantial agreement with\nhuman analysts in deductive settings, with Cohen's Kappa scores exceeding 0.7,\nwhile zero-shot performance remains limited. Detailed, context-rich prompts\nsignificantly improve annotation accuracy and consistency, particularly in\ndeductive scenarios, and GPT-4 demonstrates high reliability across repeated\nruns. These findings highlight the potential of LLMs to support QDA in RE by\nreducing manual effort while maintaining annotation quality. The structured\nlabels automatically provide traceability of requirements and can be directly\nutilized as classes in domain models, facilitating systematic software design."}
{"id": "2504.18549", "pdf": "https://arxiv.org/pdf/2504.18549", "abs": "https://arxiv.org/abs/2504.18549", "authors": ["Boyuan Peng", "Jiaju Chen", "Yiwei Zhang", "Cuiyi Peng", "Junyang Li", "Jiaming Deng", "Peiwu Qin"], "title": "Dual-Modality Computational Ophthalmic Imaging with Deep Learning and Coaxial Optical Design", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "The growing burden of myopia and retinal diseases necessitates more\naccessible and efficient eye screening solutions. This study presents a\ncompact, dual-function optical device that integrates fundus photography and\nrefractive error detection into a unified platform. The system features a\ncoaxial optical design using dichroic mirrors to separate wavelength-dependent\nimaging paths, enabling simultaneous alignment of fundus and refraction\nmodules. A Dense-U-Net-based algorithm with customized loss functions is\nemployed for accurate pupil segmentation, facilitating automated alignment and\nfocusing. Experimental evaluations demonstrate the system's capability to\nachieve high-precision pupil localization (EDE = 2.8 px, mIoU = 0.931) and\nreliable refractive estimation with a mean absolute error below 5%. Despite\nlimitations due to commercial lens components, the proposed framework offers a\npromising solution for rapid, intelligent, and scalable ophthalmic screening,\nparticularly suitable for community health settings."}
{"id": "2504.19394", "pdf": "https://arxiv.org/pdf/2504.19394", "abs": "https://arxiv.org/abs/2504.19394", "authors": ["Toby Simonds"], "title": "LLMs for Engineering: Teaching Models to Design High Powered Rockets", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have transformed software engineering, but their\napplication to physical engineering domains remains underexplored. This paper\nevaluates LLMs' capabilities in high-powered rocketry design through\nRocketBench, a benchmark connecting LLMs to high-fidelity rocket simulations.\nWe test models on two increasingly complex design tasks: target altitude\noptimization and precision landing challenges. Our findings reveal that while\nstate-of-the-art LLMs demonstrate strong baseline engineering knowledge, they\nstruggle to iterate on their designs when given simulation results and\nultimately plateau below human performance levels. However, when enhanced with\nreinforcement learning (RL), we show that a 7B parameter model outperforms both\nSoTA foundation models and human experts. This research demonstrates that\nRL-trained LLMs can serve as effective tools for complex engineering\noptimization, potentially transforming engineering domains beyond software\ndevelopment."}
{"id": "2504.18563", "pdf": "https://arxiv.org/pdf/2504.18563", "abs": "https://arxiv.org/abs/2504.18563", "authors": ["Abha Jha", "Ashwath Vaithinathan Aravindan", "Matthew Salaway", "Atharva Sandeep Bhide", "Duygu Nur Yaldiz"], "title": "Backdoor Defense in Diffusion Models via Spatial Attention Unlearning", "categories": ["cs.CR", "cs.AI", "cs.CV"], "comment": null, "summary": "Text-to-image diffusion models are increasingly vulnerable to backdoor\nattacks, where malicious modifications to the training data cause the model to\ngenerate unintended outputs when specific triggers are present. While\nclassification models have seen extensive development of defense mechanisms,\ngenerative models remain largely unprotected due to their high-dimensional\noutput space, which complicates the detection and mitigation of subtle\nperturbations. Defense strategies for diffusion models, in particular, remain\nunder-explored. In this work, we propose Spatial Attention Unlearning (SAU), a\nnovel technique for mitigating backdoor attacks in diffusion models. SAU\nleverages latent space manipulation and spatial attention mechanisms to isolate\nand remove the latent representation of backdoor triggers, ensuring precise and\nefficient removal of malicious effects. We evaluate SAU across various types of\nbackdoor attacks, including pixel-based and style-based triggers, and\ndemonstrate its effectiveness in achieving 100% trigger removal accuracy.\nFurthermore, SAU achieves a CLIP score of 0.7023, outperforming existing\nmethods while preserving the model's ability to generate high-quality,\nsemantically aligned images. Our results show that SAU is a robust, scalable,\nand practical solution for securing text-to-image diffusion models against\nbackdoor attacks."}
{"id": "2504.19409", "pdf": "https://arxiv.org/pdf/2504.19409", "abs": "https://arxiv.org/abs/2504.19409", "authors": ["Zuxing Lu", "Xin Yuan", "Shaowen Yang", "Jingyu Liu", "Jiawei Wang", "Changyin Sun"], "title": "GSFF-SLAM: 3D Semantic Gaussian Splatting SLAM via Feature Field", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Semantic-aware 3D scene reconstruction is essential for autonomous robots to\nperform complex interactions. Semantic SLAM, an online approach, integrates\npose tracking, geometric reconstruction, and semantic mapping into a unified\nframework, shows significant potential. However, existing systems, which rely\non 2D ground truth priors for supervision, are often limited by the sparsity\nand noise of these signals in real-world environments. To address this\nchallenge, we propose GSFF-SLAM, a novel dense semantic SLAM system based on 3D\nGaussian Splatting that leverages feature fields to achieve joint rendering of\nappearance, geometry, and N-dimensional semantic features. By independently\noptimizing feature gradients, our method supports semantic reconstruction using\nvarious forms of 2D priors, particularly sparse and noisy signals. Experimental\nresults demonstrate that our approach outperforms previous methods in both\ntracking accuracy and photorealistic rendering quality. When utilizing 2D\nground truth priors, GSFF-SLAM achieves state-of-the-art semantic segmentation\nperformance with 95.03\\% mIoU, while achieving up to 2.9$\\times$ speedup with\nonly marginal performance degradation."}
{"id": "2504.18591", "pdf": "https://arxiv.org/pdf/2504.18591", "abs": "https://arxiv.org/abs/2504.18591", "authors": ["Giovanni Catalani", "Michael Bauerheim", "Frédéric Tost", "Xavier Bertrand", "Joseph Morlier"], "title": "Geometry aware inference of steady state PDEs using Equivariant Neural Fields representations", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Recent advances in Neural Fields have enabled powerful,\ndiscretization-invariant methods for learning neural operators that approximate\nsolutions of Partial Differential Equations (PDEs) on general geometries.\nBuilding on these developments, we introduce enf2enf, an encoder--decoder\nmethodology for predicting steady-state Partial Differential Equations with\nnon-parameterized geometric variability, based on recently proposed Equivariant\nNeural Field architectures. In enf2enf, input geometries are encoded into\nlatent point cloud embeddings that inherently preserve geometric grounding and\ncapture local phenomena. The resulting representations are then combined with\nglobal parameters and directly decoded into continuous output fields, thus\nefficiently modeling the coupling between geometry and physics. By leveraging\nthe inductive biases of locality and translation invariance, our approach is\nable to capture fine-scale physical features as well as complex shape\nvariations, thereby enhancing generalization and physical compliance. Extensive\nexperiments on a high-fidelity aerodynamic dataset, a hyper-elastic material\nbenchmark, and multi-element airfoil geometries, demonstrate that the proposed\nmodel achieves superior or competitive performance compared to state-of-the-art\ngraph based, operator learning, and neural field methods. Notably, our method\nsupports real time inference and zero-shot super-resolution, enabling efficient\ntraining on low-resolution meshes while maintaining high accuracy on full-scale\ndiscretizations."}
{"id": "2504.19413", "pdf": "https://arxiv.org/pdf/2504.19413", "abs": "https://arxiv.org/abs/2504.19413", "authors": ["Prateek Chhikara", "Dev Khant", "Saket Aryan", "Taranjeet Singh", "Deshraj Yadav"], "title": "Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable prowess in\ngenerating contextually coherent responses, yet their fixed context windows\npose fundamental challenges for maintaining consistency over prolonged\nmulti-session dialogues. We introduce Mem0, a scalable memory-centric\narchitecture that addresses this issue by dynamically extracting,\nconsolidating, and retrieving salient information from ongoing conversations.\nBuilding on this foundation, we further propose an enhanced variant that\nleverages graph-based memory representations to capture complex relational\nstructures among conversational elements. Through comprehensive evaluations on\nLOCOMO benchmark, we systematically compare our approaches against six baseline\ncategories: (i) established memory-augmented systems, (ii) retrieval-augmented\ngeneration (RAG) with varying chunk sizes and k-values, (iii) a full-context\napproach that processes the entire conversation history, (iv) an open-source\nmemory solution, (v) a proprietary model system, and (vi) a dedicated memory\nmanagement platform. Empirical results show that our methods consistently\noutperform all existing memory systems across four question categories:\nsingle-hop, temporal, multi-hop, and open-domain. Notably, Mem0 achieves 26%\nrelative improvements in the LLM-as-a-Judge metric over OpenAI, while Mem0 with\ngraph memory achieves around 2% higher overall score than the base\nconfiguration. Beyond accuracy gains, we also markedly reduce computational\noverhead compared to full-context method. In particular, Mem0 attains a 91%\nlower p95 latency and saves more than 90% token cost, offering a compelling\nbalance between advanced reasoning capabilities and practical deployment\nconstraints. Our findings highlight critical role of structured, persistent\nmemory mechanisms for long-term conversational coherence, paving the way for\nmore reliable and efficient LLM-driven AI agents."}
{"id": "2504.18624", "pdf": "https://arxiv.org/pdf/2504.18624", "abs": "https://arxiv.org/abs/2504.18624", "authors": ["Ali SaraerToosi", "Avery Broderick"], "title": "Validation and Calibration of Semi-Analytical Models for the Event Horizon Telescope Observations of Sagittarius A*", "categories": ["astro-ph.HE", "astro-ph.IM", "cs.CV", "cs.LG", "85A99, 35Q75, 65C60, 62F15", "I.2.6; G.1.10; I.4.10"], "comment": "26 pages, 9 figures, 2 tables, submitted to ApJ", "summary": "The Event Horizon Telescope (EHT) enables the exploration of black hole\naccretion flows at event-horizon scales. Fitting ray-traced physical models to\nEHT observations requires the generation of synthetic images, a task that is\ncomputationally demanding. This study leverages \\alinet, a generative machine\nlearning model, to efficiently produce radiatively inefficient accretion flow\n(RIAF) images as a function of the specified physical parameters. \\alinet has\npreviously been shown to be able to interpolate black hole images and their\nassociated physical parameters after training on a computationally tractable\nset of library images. We utilize this model to estimate the uncertainty\nintroduced by a number of anticipated unmodeled physical effects, including\ninterstellar scattering and intrinsic source variability. We then use this to\ncalibrate physical parameter estimates and their associated uncertainties from\nRIAF model fits to mock EHT data via a library of general relativistic\nmagnetohydrodynamics models."}
{"id": "2504.19426", "pdf": "https://arxiv.org/pdf/2504.19426", "abs": "https://arxiv.org/abs/2504.19426", "authors": ["Steffen Dereich", "Arnulf Jentzen", "Adrian Riekert"], "title": "Sharp higher order convergence rates for the Adam optimizer", "categories": ["math.OC", "cs.AI", "68T05, 65K05, 90C25", "I.2.0"], "comment": "27 pages", "summary": "Gradient descent based optimization methods are the methods of choice to\ntrain deep neural networks in machine learning. Beyond the standard gradient\ndescent method, also suitable modified variants of standard gradient descent\ninvolving acceleration techniques such as the momentum method and/or adaptivity\ntechniques such as the RMSprop method are frequently considered optimization\nmethods. These days the most popular of such sophisticated optimization schemes\nis presumably the Adam optimizer that has been proposed in 2014 by Kingma and\nBa. A highly relevant topic of research is to investigate the speed of\nconvergence of such optimization methods. In particular, in 1964 Polyak showed\nthat the standard gradient descent method converges in a neighborhood of a\nstrict local minimizer with rate (x - 1)(x + 1)^{-1} while momentum achieves\nthe (optimal) strictly faster convergence rate (\\sqrt{x} - 1)(\\sqrt{x} +\n1)^{-1} where x \\in (1,\\infty) is the condition number (the ratio of the\nlargest and the smallest eigenvalue) of the Hessian of the objective function\nat the local minimizer. It is the key contribution of this work to reveal that\nAdam also converges with the strictly faster convergence rate (\\sqrt{x} -\n1)(\\sqrt{x} + 1)^{-1} while RMSprop only converges with the convergence rate (x\n- 1)(x + 1)^{-1}."}
{"id": "2504.18768", "pdf": "https://arxiv.org/pdf/2504.18768", "abs": "https://arxiv.org/abs/2504.18768", "authors": ["Letian Huang", "Dongwei Ye", "Jialin Dan", "Chengzhi Tao", "Huiwen Liu", "Kun Zhou", "Bo Ren", "Yuanqi Li", "Yanwen Guo", "Jie Guo"], "title": "TransparentGS: Fast Inverse Rendering of Transparent Objects with Gaussians", "categories": ["cs.GR", "cs.CV"], "comment": "accepted by SIGGRAPH 2025;\n  https://letianhuang.github.io/transparentgs/", "summary": "The emergence of neural and Gaussian-based radiance field methods has led to\nconsiderable advancements in novel view synthesis and 3D object reconstruction.\nNonetheless, specular reflection and refraction continue to pose significant\nchallenges due to the instability and incorrect overfitting of radiance fields\nto high-frequency light variations. Currently, even 3D Gaussian Splatting\n(3D-GS), as a powerful and efficient tool, falls short in recovering\ntransparent objects with nearby contents due to the existence of apparent\nsecondary ray effects. To address this issue, we propose TransparentGS, a fast\ninverse rendering pipeline for transparent objects based on 3D-GS. The main\ncontributions are three-fold. Firstly, an efficient representation of\ntransparent objects, transparent Gaussian primitives, is designed to enable\nspecular refraction through a deferred refraction strategy. Secondly, we\nleverage Gaussian light field probes (GaussProbe) to encode both ambient light\nand nearby contents in a unified framework. Thirdly, a depth-based iterative\nprobes query (IterQuery) algorithm is proposed to reduce the parallax errors in\nour probe-based framework. Experiments demonstrate the speed and accuracy of\nour approach in recovering transparent objects from complex environments, as\nwell as several applications in computer graphics and vision."}
{"id": "2504.19432", "pdf": "https://arxiv.org/pdf/2504.19432", "abs": "https://arxiv.org/abs/2504.19432", "authors": ["Zhe Dong", "Yuzhe Sun", "Tianzhu Liu", "Wangmeng Zuo", "Yanfeng Gu"], "title": "EarthMapper: Visual Autoregressive Models for Controllable Bidirectional Satellite-Map Translation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Satellite imagery and maps, as two fundamental data modalities in remote\nsensing, offer direct observations of the Earth's surface and\nhuman-interpretable geographic abstractions, respectively. The task of\nbidirectional translation between satellite images and maps (BSMT) holds\nsignificant potential for applications in urban planning and disaster response.\nHowever, this task presents two major challenges: first, the absence of precise\npixel-wise alignment between the two modalities substantially complicates the\ntranslation process; second, it requires achieving both high-level abstraction\nof geographic features and high-quality visual synthesis, which further\nelevates the technical complexity. To address these limitations, we introduce\nEarthMapper, a novel autoregressive framework for controllable bidirectional\nsatellite-map translation. EarthMapper employs geographic coordinate embeddings\nto anchor generation, ensuring region-specific adaptability, and leverages\nmulti-scale feature alignment within a geo-conditioned joint scale\nautoregression (GJSA) process to unify bidirectional translation in a single\ntraining cycle. A semantic infusion (SI) mechanism is introduced to enhance\nfeature-level consistency, while a key point adaptive guidance (KPAG) mechanism\nis proposed to dynamically balance diversity and precision during inference. We\nfurther contribute CNSatMap, a large-scale dataset comprising 302,132 precisely\naligned satellite-map pairs across 38 Chinese cities, enabling robust\nbenchmarking. Extensive experiments on CNSatMap and the New York dataset\ndemonstrate EarthMapper's superior performance, achieving significant\nimprovements in visual realism, semantic consistency, and structural fidelity\nover state-of-the-art methods. Additionally, EarthMapper excels in zero-shot\ntasks like in-painting, out-painting and coordinate-conditional generation,\nunderscoring its versatility."}
{"id": "2504.18802", "pdf": "https://arxiv.org/pdf/2504.18802", "abs": "https://arxiv.org/abs/2504.18802", "authors": ["Xiren Zhou", "Shikang Liu", "Xinyu Yan", "Yizhan Fan", "Xiangyu Wang", "Yu Kang", "Jian Cheng", "Huanhuan Chen"], "title": "Reservoir-enhanced Segment Anything Model for Subsurface Diagnosis", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Urban roads and infrastructure, vital to city operations, face growing\nthreats from subsurface anomalies like cracks and cavities. Ground Penetrating\nRadar (GPR) effectively visualizes underground conditions employing\nelectromagnetic (EM) waves; however, accurate anomaly detection via GPR remains\nchallenging due to limited labeled data, varying subsurface conditions, and\nindistinct target boundaries. Although visually image-like, GPR data\nfundamentally represent EM waves, with variations within and between waves\ncritical for identifying anomalies. Addressing these, we propose the\nReservoir-enhanced Segment Anything Model (Res-SAM), an innovative framework\nexploiting both visual discernibility and wave-changing properties of GPR data.\nRes-SAM initially identifies apparent candidate anomaly regions given minimal\nprompts, and further refines them by analyzing anomaly-induced changing\ninformation within and between EM waves in local GPR data, enabling precise and\ncomplete anomaly region extraction and category determination. Real-world\nexperiments demonstrate that Res-SAM achieves high detection accuracy (>85%)\nand outperforms state-of-the-art. Notably, Res-SAM requires only minimal\naccessible non-target data, avoids intensive training, and incorporates simple\nhuman interaction to enhance reliability. Our research provides a scalable,\nresource-efficient solution for rapid subsurface anomaly detection across\ndiverse environments, improving urban safety monitoring while reducing manual\neffort and computational cost."}
{"id": "2504.19443", "pdf": "https://arxiv.org/pdf/2504.19443", "abs": "https://arxiv.org/abs/2504.19443", "authors": ["Yejin Jeong", "Donghun Lee"], "title": "CLIP-KOA: Enhancing Knee Osteoarthritis Diagnosis with Multi-Modal Learning and Symmetry-Aware Loss Functions", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 2 figures", "summary": "Knee osteoarthritis (KOA) is a universal chronic musculoskeletal disorders\nworldwide, making early diagnosis crucial. Currently, the Kellgren and Lawrence\n(KL) grading system is widely used to assess KOA severity. However, its high\ninter-observer variability and subjectivity hinder diagnostic consistency. To\naddress these limitations, automated diagnostic techniques using deep learning\nhave been actively explored in recent years. In this study, we propose a\nCLIP-based framework (CLIP-KOA) to enhance the consistency and reliability of\nKOA grade prediction. To achieve this, we introduce a learning approach that\nintegrates image and text information and incorporate Symmetry Loss and\nConsistency Loss to ensure prediction consistency between the original and\nflipped images. CLIP-KOA achieves state-of-the-art accuracy of 71.86\\% on KOA\nseverity prediction task, and ablation studies show that CLIP-KOA has 2.36\\%\nimprovement in accuracy over the standard CLIP model due to our contribution.\nThis study shows a novel direction for data-driven medical prediction not only\nto improve reliability of fine-grained diagnosis and but also to explore\nmultimodal methods for medical image analysis. Our code is available at\nhttps://github.com/anonymized-link."}
{"id": "2504.18829", "pdf": "https://arxiv.org/pdf/2504.18829", "abs": "https://arxiv.org/abs/2504.18829", "authors": ["Jiayi Chen", "Yubin Ke", "Lin Peng", "He Wang"], "title": "Dexonomy: Synthesizing All Dexterous Grasp Types in a Grasp Taxonomy", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted by Robotics: Science and Systems (RSS 2025)", "summary": "Generalizable dexterous grasping with suitable grasp types is a fundamental\nskill for intelligent robots. Developing such skills requires a large-scale and\nhigh-quality dataset that covers numerous grasp types (i.e., at least those\ncategorized by the GRASP taxonomy), but collecting such data is extremely\nchallenging. Existing automatic grasp synthesis methods are often limited to\nspecific grasp types or object categories, hindering scalability. This work\nproposes an efficient pipeline capable of synthesizing contact-rich,\npenetration-free, and physically plausible grasps for any grasp type, object,\nand articulated hand. Starting from a single human-annotated template for each\nhand and grasp type, our pipeline tackles the complicated synthesis problem\nwith two stages: optimize the object to fit the hand template first, and then\nlocally refine the hand to fit the object in simulation. To validate the\nsynthesized grasps, we introduce a contact-aware control strategy that allows\nthe hand to apply the appropriate force at each contact point to the object.\nThose validated grasps can also be used as new grasp templates to facilitate\nfuture synthesis. Experiments show that our method significantly outperforms\nprevious type-unaware grasp synthesis baselines in simulation. Using our\nalgorithm, we construct a dataset containing 10.7k objects and 9.5M grasps,\ncovering 31 grasp types in the GRASP taxonomy. Finally, we train a\ntype-conditional generative model that successfully performs the desired grasp\ntype from single-view object point clouds, achieving an 82.3% success rate in\nreal-world experiments. Project page: https://pku-epic.github.io/Dexonomy."}
{"id": "2504.19457", "pdf": "https://arxiv.org/pdf/2504.19457", "abs": "https://arxiv.org/abs/2504.19457", "authors": ["Siyi Liu", "Kishaloy Halder", "Zheng Qi", "Wei Xiao", "Nikolaos Pappas", "Phu Mon Htut", "Neha Anna John", "Yassine Benajiba", "Dan Roth"], "title": "Towards Long Context Hallucination Detection", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious tasks. However, they are prone to contextual hallucination, generating\ninformation that is either unsubstantiated or contradictory to the given\ncontext. Although many studies have investigated contextual hallucinations in\nLLMs, addressing them in long-context inputs remains an open problem. In this\nwork, we take an initial step toward solving this problem by constructing a\ndataset specifically designed for long-context hallucination detection.\nFurthermore, we propose a novel architecture that enables pre-trained encoder\nmodels, such as BERT, to process long contexts and effectively detect\ncontextual hallucinations through a decomposition and aggregation mechanism.\nOur experimental results show that the proposed architecture significantly\noutperforms previous models of similar size as well as LLM-based models across\nvarious metrics, while providing substantially faster inference."}
{"id": "2504.18954", "pdf": "https://arxiv.org/pdf/2504.18954", "abs": "https://arxiv.org/abs/2504.18954", "authors": ["Marco Mezzina", "Pieter De Backer", "Tom Vercauteren", "Matthew Blaschko", "Alexandre Mottrie", "Tinne Tuytelaars"], "title": "Surgeons vs. Computer Vision: A comparative analysis on surgical phase recognition capabilities", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Purpose: Automated Surgical Phase Recognition (SPR) uses Artificial\nIntelligence (AI) to segment the surgical workflow into its key events,\nfunctioning as a building block for efficient video review, surgical education\nas well as skill assessment. Previous research has focused on short and linear\nsurgical procedures and has not explored if temporal context influences\nexperts' ability to better classify surgical phases. This research addresses\nthese gaps, focusing on Robot-Assisted Partial Nephrectomy (RAPN) as a highly\nnon-linear procedure. Methods: Urologists of varying expertise were grouped and\ntasked to indicate the surgical phase for RAPN on both single frames and video\nsnippets using a custom-made web platform. Participants reported their\nconfidence levels and the visual landmarks used in their decision-making. AI\narchitectures without and with temporal context as trained and benchmarked on\nthe Cholec80 dataset were subsequently trained on this RAPN dataset. Results:\nVideo snippets and presence of specific visual landmarks improved phase\nclassification accuracy across all groups. Surgeons displayed high confidence\nin their classifications and outperformed novices, who struggled discriminating\nphases. The performance of the AI models is comparable to the surgeons in the\nsurvey, with improvements when temporal context was incorporated in both cases.\nConclusion: SPR is an inherently complex task for expert surgeons and computer\nvision, where both perform equally well when given the same context.\nPerformance increases when temporal information is provided. Surgical tools and\norgans form the key landmarks for human interpretation and are expected to\nshape the future of automated SPR."}
{"id": "2504.19460", "pdf": "https://arxiv.org/pdf/2504.19460", "abs": "https://arxiv.org/abs/2504.19460", "authors": ["Mahya Khazaei", "Ali Bahrani", "George Tzanetakis"], "title": "A Real-Time Gesture-Based Control Framework", "categories": ["cs.HC", "cs.AI"], "comment": "8 pages, 4 figures, 2025 International Computer Music Conference", "summary": "We introduce a real-time, human-in-the-loop gesture control framework that\ncan dynamically adapt audio and music based on human movement by analyzing live\nvideo input. By creating a responsive connection between visual and auditory\nstimuli, this system enables dancers and performers to not only respond to\nmusic but also influence it through their movements. Designed for live\nperformances, interactive installations, and personal use, it offers an\nimmersive experience where users can shape the music in real time.\n  The framework integrates computer vision and machine learning techniques to\ntrack and interpret motion, allowing users to manipulate audio elements such as\ntempo, pitch, effects, and playback sequence. With ongoing training, it\nachieves user-independent functionality, requiring as few as 50 to 80 samples\nto label simple gestures. This framework combines gesture training, cue\nmapping, and audio manipulation to create a dynamic, interactive experience.\nGestures are interpreted as input signals, mapped to sound control commands,\nand used to naturally adjust music elements, showcasing the seamless interplay\nbetween human interaction and machine response."}
{"id": "2504.18989", "pdf": "https://arxiv.org/pdf/2504.18989", "abs": "https://arxiv.org/abs/2504.18989", "authors": ["Gal Almog", "Ariel Shamir", "Ohad Fried"], "title": "REED-VAE: RE-Encode Decode Training for Iterative Image Editing with Diffusion Models", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "Accepted to Eurographics 2025. Project page:\n  https://reed-vae.github.io/", "summary": "While latent diffusion models achieve impressive image editing results, their\napplication to iterative editing of the same image is severely restricted. When\ntrying to apply consecutive edit operations using current models, they\naccumulate artifacts and noise due to repeated transitions between pixel and\nlatent spaces. Some methods have attempted to address this limitation by\nperforming the entire edit chain within the latent space, sacrificing\nflexibility by supporting only a limited, predetermined set of diffusion\nediting operations. We present a RE-encode decode (REED) training scheme for\nvariational autoencoders (VAEs), which promotes image quality preservation even\nafter many iterations. Our work enables multi-method iterative image editing:\nusers can perform a variety of iterative edit operations, with each operation\nbuilding on the output of the previous one using both diffusion-based\noperations and conventional editing techniques. We demonstrate the advantage of\nREED-VAE across a range of image editing scenarios, including text-based and\nmask-based editing frameworks. In addition, we show how REED-VAE enhances the\noverall editability of images, increasing the likelihood of successful and\nprecise edit operations. We hope that this work will serve as a benchmark for\nthe newly introduced task of multi-method image editing. Our code and models\nwill be available at https://github.com/galmog/REED-VAE"}
{"id": "2504.19467", "pdf": "https://arxiv.org/pdf/2504.19467", "abs": "https://arxiv.org/abs/2504.19467", "authors": ["Jiageng Wu", "Bowen Gu", "Ren Zhou", "Kevin Xie", "Doug Snyder", "Yixing Jiang", "Valentina Carducci", "Richard Wyss", "Rishi J Desai", "Emily Alsentzer", "Leo Anthony Celi", "Adam Rodman", "Sebastian Schneeweiss", "Jonathan H. Chen", "Santiago Romero-Brufau", "Kueiyu Joshua Lin", "Jie Yang"], "title": "BRIDGE: Benchmarking Large Language Models for Understanding Real-world Clinical Practice Text", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) hold great promise for medical applications and\nare evolving rapidly, with new models being released at an accelerated pace.\nHowever, current evaluations of LLMs in clinical contexts remain limited. Most\nexisting benchmarks rely on medical exam-style questions or PubMed-derived\ntext, failing to capture the complexity of real-world electronic health record\n(EHR) data. Others focus narrowly on specific application scenarios, limiting\ntheir generalizability across broader clinical use. To address this gap, we\npresent BRIDGE, a comprehensive multilingual benchmark comprising 87 tasks\nsourced from real-world clinical data sources across nine languages. We\nsystematically evaluated 52 state-of-the-art LLMs (including DeepSeek-R1,\nGPT-4o, Gemini, and Llama 4) under various inference strategies. With a total\nof 13,572 experiments, our results reveal substantial performance variation\nacross model sizes, languages, natural language processing tasks, and clinical\nspecialties. Notably, we demonstrate that open-source LLMs can achieve\nperformance comparable to proprietary models, while medically fine-tuned LLMs\nbased on older architectures often underperform versus updated general-purpose\nmodels. The BRIDGE and its corresponding leaderboard serve as a foundational\nresource and a unique reference for the development and evaluation of new LLMs\nin real-world clinical text understanding."}
{"id": "2504.19002", "pdf": "https://arxiv.org/pdf/2504.19002", "abs": "https://arxiv.org/abs/2504.19002", "authors": ["Delun Lai", "Yeyubei Zhang", "Yunchong Liu", "Chaojie Li", "Huadong Mo"], "title": "Deep Learning-Based Multi-Modal Fusion for Robust Robot Perception and Navigation", "categories": ["cs.LG", "cs.CV", "cs.RO"], "comment": "6 pages, 4 figures", "summary": "This paper introduces a novel deep learning-based multimodal fusion\narchitecture aimed at enhancing the perception capabilities of autonomous\nnavigation robots in complex environments. By utilizing innovative feature\nextraction modules, adaptive fusion strategies, and time-series modeling\nmechanisms, the system effectively integrates RGB images and LiDAR data. The\nkey contributions of this work are as follows: a. the design of a lightweight\nfeature extraction network to enhance feature representation; b. the\ndevelopment of an adaptive weighted cross-modal fusion strategy to improve\nsystem robustness; and c. the incorporation of time-series information modeling\nto boost dynamic scene perception accuracy. Experimental results on the KITTI\ndataset demonstrate that the proposed approach increases navigation and\npositioning accuracy by 3.5% and 2.2%, respectively, while maintaining\nreal-time performance. This work provides a novel solution for autonomous robot\nnavigation in complex environments."}
{"id": "2504.19475", "pdf": "https://arxiv.org/pdf/2504.19475", "abs": "https://arxiv.org/abs/2504.19475", "authors": ["Sonia Joseph", "Praneet Suresh", "Lorenz Hufe", "Edward Stevinson", "Robert Graham", "Yash Vadi", "Danilo Bzdok", "Sebastian Lapuschkin", "Lee Sharkey", "Blake Aaron Richards"], "title": "Prisma: An Open Source Toolkit for Mechanistic Interpretability in Vision and Video", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "4 pages, 3 figures, 9 tables. Oral and Tutorial at the CVPR\n  Mechanistic Interpretability for Vision (MIV) Workshop", "summary": "Robust tooling and publicly available pre-trained models have helped drive\nrecent advances in mechanistic interpretability for language models. However,\nsimilar progress in vision mechanistic interpretability has been hindered by\nthe lack of accessible frameworks and pre-trained weights. We present Prisma\n(Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an\nopen-source framework designed to accelerate vision mechanistic\ninterpretability research, providing a unified toolkit for accessing 75+ vision\nand video transformers; support for sparse autoencoder (SAE), transcoder, and\ncrosscoder training; a suite of 80+ pre-trained SAE weights; activation\ncaching, circuit analysis tools, and visualization tools; and educational\nresources. Our analysis reveals surprising findings, including that effective\nvision SAEs can exhibit substantially lower sparsity patterns than language\nSAEs, and that in some instances, SAE reconstructions can decrease model loss.\nPrisma enables new research directions for understanding vision model internals\nwhile lowering barriers to entry in this emerging field."}
{"id": "2504.19174", "pdf": "https://arxiv.org/pdf/2504.19174", "abs": "https://arxiv.org/abs/2504.19174", "authors": ["Xueqi Ma", "Yilin Liu", "Tianlong Gao", "Qirui Huang", "Hui Huang"], "title": "CLR-Wire: Towards Continuous Latent Representations for 3D Curve Wireframe Generation", "categories": ["cs.GR", "cs.CV"], "comment": "SIGGRAPH 2025", "summary": "We introduce CLR-Wire, a novel framework for 3D curve-based wireframe\ngeneration that integrates geometry and topology into a unified Continuous\nLatent Representation. Unlike conventional methods that decouple vertices,\nedges, and faces, CLR-Wire encodes curves as Neural Parametric Curves along\nwith their topological connectivity into a continuous and fixed-length latent\nspace using an attention-driven variational autoencoder (VAE). This unified\napproach facilitates joint learning and generation of both geometry and\ntopology. To generate wireframes, we employ a flow matching model to\nprogressively map Gaussian noise to these latents, which are subsequently\ndecoded into complete 3D wireframes. Our method provides fine-grained modeling\nof complex shapes and irregular topologies, and supports both unconditional\ngeneration and generation conditioned on point cloud or image inputs.\nExperimental results demonstrate that, compared with state-of-the-art\ngenerative approaches, our method achieves substantial improvements in\naccuracy, novelty, and diversity, offering an efficient and comprehensive\nsolution for CAD design, geometric reconstruction, and 3D content creation."}
{"id": "2504.19480", "pdf": "https://arxiv.org/pdf/2504.19480", "abs": "https://arxiv.org/abs/2504.19480", "authors": ["Dixiao Wei", "Peng Yi", "Jinlong Lei", "Yiguang Hong", "Yuchuan Du"], "title": "An Automated Reinforcement Learning Reward Design Framework with Large Language Model for Cooperative Platoon Coordination", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement Learning (RL) has demonstrated excellent decision-making\npotential in platoon coordination problems. However, due to the variability of\ncoordination goals, the complexity of the decision problem, and the\ntime-consumption of trial-and-error in manual design, finding a well\nperformance reward function to guide RL training to solve complex platoon\ncoordination problems remains challenging. In this paper, we formally define\nthe Platoon Coordination Reward Design Problem (PCRDP), extending the RL-based\ncooperative platoon coordination problem to incorporate automated reward\nfunction generation. To address PCRDP, we propose a Large Language Model\n(LLM)-based Platoon coordination Reward Design (PCRD) framework, which\nsystematically automates reward function discovery through LLM-driven\ninitialization and iterative optimization. In this method, LLM first\ninitializes reward functions based on environment code and task requirements\nwith an Analysis and Initial Reward (AIR) module, and then iteratively\noptimizes them based on training feedback with an evolutionary module. The AIR\nmodule guides LLM to deepen their understanding of code and tasks through a\nchain of thought, effectively mitigating hallucination risks in code\ngeneration. The evolutionary module fine-tunes and reconstructs the reward\nfunction, achieving a balance between exploration diversity and convergence\nstability for training. To validate our approach, we establish six challenging\ncoordination scenarios with varying complexity levels within the Yangtze River\nDelta transportation network simulation. Comparative experimental results\ndemonstrate that RL agents utilizing PCRD-generated reward functions\nconsistently outperform human-engineered reward functions, achieving an average\nof 10\\% higher performance metrics in all scenarios."}
{"id": "2504.19189", "pdf": "https://arxiv.org/pdf/2504.19189", "abs": "https://arxiv.org/abs/2504.19189", "authors": ["Lei Zhong", "Chuan Guo", "Yiming Xie", "Jiawei Wang", "Changjian Li"], "title": "Sketch2Anim: Towards Transferring Sketch Storyboards into 3D Animation", "categories": ["cs.GR", "cs.CV"], "comment": "Project page: https://zhongleilz.github.io/Sketch2Anim/", "summary": "Storyboarding is widely used for creating 3D animations. Animators use the 2D\nsketches in storyboards as references to craft the desired 3D animations\nthrough a trial-and-error process. The traditional approach requires\nexceptional expertise and is both labor-intensive and time-consuming.\nConsequently, there is a high demand for automated methods that can directly\ntranslate 2D storyboard sketches into 3D animations. This task is\nunder-explored to date and inspired by the significant advancements of motion\ndiffusion models, we propose to address it from the perspective of conditional\nmotion synthesis. We thus present Sketch2Anim, composed of two key modules for\nsketch constraint understanding and motion generation. Specifically, due to the\nlarge domain gap between the 2D sketch and 3D motion, instead of directly\nconditioning on 2D inputs, we design a 3D conditional motion generator that\nsimultaneously leverages 3D keyposes, joint trajectories, and action words, to\nachieve precise and fine-grained motion control. Then, we invent a neural\nmapper dedicated to aligning user-provided 2D sketches with their corresponding\n3D keyposes and trajectories in a shared embedding space, enabling, for the\nfirst time, direct 2D control of motion generation. Our approach successfully\ntransfers storyboards into high-quality 3D motions and inherently supports\ndirect 3D animation editing, thanks to the flexibility of our multi-conditional\nmotion generator. Comprehensive experiments and evaluations, and a user\nperceptual study demonstrate the effectiveness of our approach."}
{"id": "2504.19483", "pdf": "https://arxiv.org/pdf/2504.19483", "abs": "https://arxiv.org/abs/2504.19483", "authors": ["Bertram Højer", "Oliver Jarvis", "Stefan Heinrich"], "title": "Improving Reasoning Performance in Large Language Models via Representation Engineering", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Has been accepted at \"The Thirteenth International Conference on\n  Learning Representations (ICLR 2025)\" Link to publication:\n  https://openreview.net/forum?id=IssPhpUsKt", "summary": "Recent advancements in large language models (LLMs) have resulted in\nincreasingly anthropomorphic language concerning the ability of LLMs to reason.\nWhether reasoning in LLMs should be understood to be inherently different is,\nhowever, widely debated. We propose utilizing a representation engineering\napproach wherein model activations are read from the residual stream of an LLM\nwhen processing a reasoning task. The activations are used to derive a control\nvector that is applied to the model as an inference-time intervention,\nmodulating the representational space of the model, to improve performance on\nthe specified task. We publish the code for deriving control vectors and\nanalyzing model representations. The method allows us to improve performance on\nreasoning benchmarks and assess how control vectors influence the final logit\ndistribution of a model via metrics such as KL divergence and entropy. We apply\ncontrol vectors to Mistral-7B-Instruct and a range of Pythia models on an\ninductive, a deductive and mathematical reasoning task. We show that an LLM\ncan, to a certain degree, be controlled to improve its perceived reasoning\nability by modulating activations. The intervention is dependent upon the\nability to reliably extract the model's typical state when correctly solving a\ntask. Our results suggest that reasoning performance can be modulated in the\nsame manner as other information-processing tasks performed by LLMs and\ndemonstrate that we are capable of improving performance on specific tasks via\na simple intervention on the residual stream with no additional training."}
{"id": "2504.19200", "pdf": "https://arxiv.org/pdf/2504.19200", "abs": "https://arxiv.org/abs/2504.19200", "authors": ["Tristan Manchester", "Adam Anders", "Julio Spadotto", "Hannah Eccleston", "William Beavan", "Hugues Arcis", "Brian J. Connolly"], "title": "Leveraging Modified Ex Situ Tomography Data for Segmentation of In Situ Synchrotron X-Ray Computed Tomography", "categories": ["cond-mat.mtrl-sci", "cs.CV"], "comment": null, "summary": "In situ synchrotron X-ray computed tomography enables dynamic material\nstudies, but automated segmentation remains challenging due to complex imaging\nartefacts and limited training data. We present a methodology for deep\nlearning-based segmentation by transforming high-quality ex situ laboratory\ndata to train models for binary segmentation of in situ synchrotron data,\ndemonstrated through copper oxide dissolution studies. Using a modified\nSegFormer architecture, our approach achieves high segmentation performance on\nunseen data while reducing processing time from hours to seconds per 3D\ndataset. The method maintains consistent performance over significant\nmorphological changes during experiments, despite training only on static\nspecimens. This methodology can be readily applied to diverse materials\nsystems, accelerating the analysis of time-resolved tomographic data across\nscientific disciplines."}
{"id": "2504.19496", "pdf": "https://arxiv.org/pdf/2504.19496", "abs": "https://arxiv.org/abs/2504.19496", "authors": ["Rudy Morel", "Jiequn Han", "Edouard Oyallon"], "title": "DISCO: learning to DISCover an evolution Operator for multi-physics-agnostic prediction", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "We address the problem of predicting the next state of a dynamical system\ngoverned by unknown temporal partial differential equations (PDEs) using only a\nshort trajectory. While standard transformers provide a natural black-box\nsolution to this task, the presence of a well-structured evolution operator in\nthe data suggests a more tailored and efficient approach. Specifically, when\nthe PDE is fully known, classical numerical solvers can evolve the state\naccurately with only a few parameters. Building on this observation, we\nintroduce DISCO, a model that uses a large hypernetwork to process a short\ntrajectory and generate the parameters of a much smaller operator network,\nwhich then predicts the next state through time integration. Our framework\ndecouples dynamics estimation (i.e., DISCovering an evolution operator from a\nshort trajectory) from state prediction (i.e., evolving this operator).\nExperiments show that pretraining our model on diverse physics datasets\nachieves state-of-the-art performance while requiring significantly fewer\nepochs. Moreover, it generalizes well and remains competitive when fine-tuned\non downstream tasks."}
{"id": "2504.19203", "pdf": "https://arxiv.org/pdf/2504.19203", "abs": "https://arxiv.org/abs/2504.19203", "authors": ["Ehsan Karami", "Hamid Soltanian-Zadeh"], "title": "Improving Generalization in MRI-Based Deep Learning Models for Total Knee Replacement Prediction", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Knee osteoarthritis (KOA) is a common joint disease that causes pain and\nmobility issues. While MRI-based deep learning models have demonstrated\nsuperior performance in predicting total knee replacement (TKR) and disease\nprogression, their generalizability remains challenging, particularly when\napplied to imaging data from different sources. In this study, we have shown\nthat replacing batch normalization with instance normalization, using data\naugmentation, and applying contrastive loss improves model generalization in a\nbaseline deep learning model for knee osteoarthritis (KOA) prediction. We\ntrained and evaluated our model using MRI data from the Osteoarthritis\nInitiative (OAI) database, considering sagittal fat-suppressed\nintermediate-weighted turbo spin-echo (FS-IW-TSE) images as the source domain\nand sagittal fat-suppressed three-dimensional (3D) dual-echo in steady state\n(DESS) images as the target domain. The results demonstrate a statistically\nsignificant improvement in classification accuracy across both domains, with\nour approach outperforming the baseline model."}
{"id": "2504.19545", "pdf": "https://arxiv.org/pdf/2504.19545", "abs": "https://arxiv.org/abs/2504.19545", "authors": ["Zezeng Li", "Zhihui Qi", "Weimin Wang", "Ziliang Wang", "Junyi Duan", "Na Lei"], "title": "Point2Quad: Generating Quad Meshes from Point Clouds via Face Prediction", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Quad meshes are essential in geometric modeling and computational mechanics.\nAlthough learning-based methods for triangle mesh demonstrate considerable\nadvancements, quad mesh generation remains less explored due to the challenge\nof ensuring coplanarity, convexity, and quad-only meshes. In this paper, we\npresent Point2Quad, the first learning-based method for quad-only mesh\ngeneration from point clouds. The key idea is learning to identify quad mesh\nwith fused pointwise and facewise features. Specifically, Point2Quad begins\nwith a k-NN-based candidate generation considering the coplanarity and\nsquareness. Then, two encoders are followed to extract geometric and\ntopological features that address the challenge of quad-related constraints,\nespecially by combining in-depth quadrilaterals-specific characteristics.\nSubsequently, the extracted features are fused to train the classifier with a\ndesigned compound loss. The final results are derived after the refinement by a\nquad-specific post-processing. Extensive experiments on both clear and noise\ndata demonstrate the effectiveness and superiority of Point2Quad, compared to\nbaseline methods under comprehensive metrics."}
{"id": "2504.19253", "pdf": "https://arxiv.org/pdf/2504.19253", "abs": "https://arxiv.org/abs/2504.19253", "authors": ["Taoyi Wang", "Lijian Wang", "Yihan Lin", "Mingtao Ou", "Yuguo Chen", "Xinglong Ji", "Rong Zhao"], "title": "Quantitative evaluation of brain-inspired vision sensors in high-speed robotic perception", "categories": ["cs.RO", "cs.CV"], "comment": "8 pages, 8 figures, 1 table, conference", "summary": "Perception systems in robotics encounter significant challenges in high-speed\nand dynamic conditions when relying on traditional cameras, where motion blur\ncan compromise spatial feature integrity and task performance. Brain-inspired\nvision sensors (BVS) have recently gained attention as an alternative, offering\nhigh temporal resolution with reduced bandwidth and power requirements. Here,\nwe present the first quantitative evaluation framework for two representative\nclasses of BVSs in variable-speed robotic sensing, including event-based vision\nsensors (EVS) that detect asynchronous temporal contrasts, and the\nprimitive-based sensor Tianmouc that employs a complementary mechanism to\nencode both spatiotemporal changes and intensity. A unified testing protocol is\nestablished, including crosssensor calibrations, standardized testing\nplatforms, and quality metrics to address differences in data modality. From an\nimaging standpoint, we evaluate the effects of sensor non-idealities, such as\nmotion-induced distortion, on the capture of structural information. For\nfunctional benchmarking, we examine task performance in corner detection and\nmotion estimation under different rotational speeds. Results indicate that EVS\nperforms well in highspeed, sparse scenarios and in modestly fast, complex\nscenes, but exhibits performance limitations in high-speed, cluttered settings\ndue to pixel-level bandwidth variations and event rate saturation. In\ncomparison, Tianmouc demonstrates consistent performance across sparse and\ncomplex scenarios at various speeds, supported by its global, precise,\nhigh-speed spatiotemporal gradient samplings. These findings offer valuable\ninsights into the applicationdependent suitability of BVS technologies and\nsupport further advancement in this area."}
{"id": "2504.19565", "pdf": "https://arxiv.org/pdf/2504.19565", "abs": "https://arxiv.org/abs/2504.19565", "authors": ["Meng Xiao", "Xunxin Cai", "Chengrui Wang", "Yuanchun Zhou"], "title": "m-KAILIN: Knowledge-Driven Agentic Scientific Corpus Distillation Framework for Biomedical Large Language Models Training", "categories": ["cs.CL", "cs.AI", "q-bio.QM"], "comment": "22 pages, Large Language Model, Agentic AI, Dataset Distillation,\n  Multi-agent Collaboration", "summary": "The rapid progress of large language models (LLMs) in biomedical research has\nunderscored the limitations of existing open-source annotated scientific\ncorpora, which are often insufficient in quantity and quality. Addressing the\nchallenge posed by the complex hierarchy of biomedical knowledge, we propose a\nknowledge-driven, multi-agent framework for scientific corpus distillation\ntailored for LLM training in the biomedical domain. Central to our approach is\na collaborative multi-agent architecture, where specialized agents, each guided\nby the Medical Subject Headings (MeSH) hierarchy, work in concert to\nautonomously extract, synthesize, and self-evaluate high-quality textual data\nfrom vast scientific literature. These agents collectively generate and refine\ndomain-specific question-answer pairs, ensuring comprehensive coverage and\nconsistency with biomedical ontologies while minimizing manual involvement.\nExtensive experimental results show that language models trained on our\nmulti-agent distilled datasets achieve notable improvements in biomedical\nquestion-answering tasks, outperforming both strong life sciences LLM baselines\nand advanced proprietary models. Notably, our AI-Ready dataset enables\nLlama3-70B to surpass GPT-4 with MedPrompt and Med-PaLM-2, despite their larger\nscale. Detailed ablation studies and case analyses further validate the\neffectiveness and synergy of each agent within the framework, highlighting the\npotential of multi-agent collaboration in biomedical LLM training."}
{"id": "2504.19267", "pdf": "https://arxiv.org/pdf/2504.19267", "abs": "https://arxiv.org/abs/2504.19267", "authors": ["Mohamed Gado", "Towhid Taliee", "Muhammad Memon", "Dmitry Ignatov", "Radu Timofte"], "title": "VIST-GPT: Ushering in the Era of Visual Storytelling with LLMs?", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Visual storytelling is an interdisciplinary field combining computer vision\nand natural language processing to generate cohesive narratives from sequences\nof images. This paper presents a novel approach that leverages recent\nadvancements in multimodal models, specifically adapting transformer-based\narchitectures and large multimodal models, for the visual storytelling task.\nLeveraging the large-scale Visual Storytelling (VIST) dataset, our VIST-GPT\nmodel produces visually grounded, contextually appropriate narratives. We\naddress the limitations of traditional evaluation metrics, such as BLEU,\nMETEOR, ROUGE, and CIDEr, which are not suitable for this task. Instead, we\nutilize RoViST and GROOVIST, novel reference-free metrics designed to assess\nvisual storytelling, focusing on visual grounding, coherence, and\nnon-redundancy. These metrics provide a more nuanced evaluation of narrative\nquality, aligning closely with human judgment."}
{"id": "2504.19590", "pdf": "https://arxiv.org/pdf/2504.19590", "abs": "https://arxiv.org/abs/2504.19590", "authors": ["Israa Alsiyat"], "title": "Arabic Metaphor Sentiment Classification Using Semantic Information", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this paper, I discuss the testing of the Arabic Metaphor Corpus (AMC) [1]\nusing newly designed automatic tools for sentiment classification for AMC based\non semantic tags. The tool incorporates semantic emotional tags for sentiment\nclassification. I evaluate the tool using standard methods, which are F-score,\nrecall, and precision. The method is to show the impact of Arabic online\nmetaphors on sentiment through the newly designed tools. To the best of our\nknowledge, this is the first approach to conduct sentiment classification for\nArabic metaphors using semantic tags to find the impact of the metaphor."}
{"id": "2504.19362", "pdf": "https://arxiv.org/pdf/2504.19362", "abs": "https://arxiv.org/abs/2504.19362", "authors": ["Yunxuan Wang", "Ray Yin", "Yumei Tan", "Hao Chen", "Haiying Xia"], "title": "Low-Rank Adaptive Structural Priors for Generalizable Diabetic Retinopathy Grading", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Accepted by IJCNN 2025", "summary": "Diabetic retinopathy (DR), a serious ocular complication of diabetes, is one\nof the primary causes of vision loss among retinal vascular diseases. Deep\nlearning methods have been extensively applied in the grading of diabetic\nretinopathy (DR). However, their performance declines significantly when\napplied to data outside the training distribution due to domain shifts. Domain\ngeneralization (DG) has emerged as a solution to this challenge. However, most\nexisting DG methods overlook lesion-specific features, resulting in\ninsufficient accuracy. In this paper, we propose a novel approach that enhances\nexisting DG methods by incorporating structural priors, inspired by the\nobservation that DR grading is heavily dependent on vessel and lesion\nstructures. We introduce Low-rank Adaptive Structural Priors (LoASP), a\nplug-and-play framework designed for seamless integration with existing DG\nmodels. LoASP improves generalization by learning adaptive structural\nrepresentations that are finely tuned to the complexities of DR diagnosis.\nExtensive experiments on eight diverse datasets validate its effectiveness in\nboth single-source and multi-source domain scenarios. Furthermore,\nvisualizations reveal that the learned structural priors intuitively align with\nthe intricate architecture of the vessels and lesions, providing compelling\ninsights into their interpretability and diagnostic relevance."}
{"id": "2504.19592", "pdf": "https://arxiv.org/pdf/2504.19592", "abs": "https://arxiv.org/abs/2504.19592", "authors": ["Roman Malashin", "Daniil Ilyukhin"], "title": "Neural network task specialization via domain constraining", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper introduces a concept of neural network specialization via\ntask-specific domain constraining, aimed at enhancing network performance on\ndata subspace in which the network operates. The study presents experiments on\ntraining specialists for image classification and object detection tasks. The\nresults demonstrate that specialization can enhance a generalist's accuracy\neven without additional data or changing training regimes: solely by\nconstraining class label space in which the network performs. Theoretical and\nexperimental analyses indicate that effective specialization requires modifying\ntraditional fine-tuning methods and constraining data space to semantically\ncoherent subsets. The specialist extraction phase before tuning the network is\nproposed for maximal performance gains. We also provide analysis of the\nevolution of the feature space during specialization. This study paves way to\nfuture research for developing more advanced dynamically configurable image\nanalysis systems, where computations depend on the specific input.\nAdditionally, the proposed methods can help improve system performance in\nscenarios where certain data domains should be excluded from consideration of\nthe generalist network."}
{"id": "2504.19401", "pdf": "https://arxiv.org/pdf/2504.19401", "abs": "https://arxiv.org/abs/2504.19401", "authors": ["Shuo Wang", "Tong Ren", "Nan Cheng", "Li Zhang", "Rong Wang"], "title": "Innovative Integration of 4D Cardiovascular Reconstruction and Hologram: A New Visualization Tool for Coronary Artery Bypass Grafting Planning", "categories": ["physics.med-ph", "cs.CV", "cs.GR", "eess.IV", "J.3; I.3.8"], "comment": "35 pages, 9 figures", "summary": "Background: Coronary artery bypass grafting (CABG) planning requires advanced\nspatial visualization and consideration of coronary artery depth,\ncalcification, and pericardial adhesions. Objective: To develop and evaluate a\ndynamic cardiovascular holographic visualization tool for preoperative CABG\nplanning. Methods: Using 4D cardiac computed tomography angiography data from\n14 CABG candidates, we developed a semi-automated workflow for time-resolved\nsegmentation of cardiac structures, epicardial adipose tissue (EAT), and\ncoronary arteries with calcium scoring. The workflow incorporated methods for\ncardiac segmentation, coronary calcification quantification, visualization of\ncoronary depth within EAT, and pericardial adhesion assessment through motion\nanalysis. Dynamic cardiovascular holograms were displayed using the Looking\nGlass platform. Thirteen cardiac surgeons evaluated the tool using a Likert\nscale. Additionally, pericardial adhesion scores from holograms of 21 patients\n(including seven undergoing secondary cardiac surgeries) were compared with\nintraoperative findings. Results: Surgeons rated the visualization tool highly\nfor preoperative planning utility (mean Likert score: 4.57/5.0). Hologram-based\npericardial adhesion scoring strongly correlated with intraoperative findings\n(r=0.786, P<0.001). Conclusion: This study establishes a visualization\nframework for CABG planning that produces clinically relevant dynamic holograms\nfrom patient-specific data, with clinical feedback confirming its effectiveness\nfor preoperative planning."}
{"id": "2504.19594", "pdf": "https://arxiv.org/pdf/2504.19594", "abs": "https://arxiv.org/abs/2504.19594", "authors": ["Lorenzo Alvisi", "Serena Tardelli", "Maurizio Tesconi"], "title": "Mapping the Italian Telegram Ecosystem", "categories": ["cs.SI", "cs.AI"], "comment": null, "summary": "Telegram has become a major space for political discourse and alternative\nmedia. However, its lack of moderation allows misinformation, extremism, and\ntoxicity to spread. While prior research focused on these particular phenomena\nor topics, these have mostly been examined separately, and a broader\nunderstanding of the Telegram ecosystem is still missing. In this work, we fill\nthis gap by conducting a large-scale analysis of the Italian Telegram sphere,\nleveraging a dataset of 186 million messages from 13,151 chats collected in\n2023. Using network analysis, Large Language Models, and toxicity detection\ntools, we examine how different thematic communities form, align ideologically,\nand engage in harmful discourse within the Italian cultural context. Results\nshow strong thematic and ideological homophily. We also identify mixed\nideological communities where far-left and far-right rhetoric coexist on\nparticular geopolitical issues. Beyond political analysis, we find that\ntoxicity, rather than being isolated in a few extreme chats, appears widely\nnormalized within highly toxic communities. Moreover, we find that Italian\ndiscourse primarily targets Black people, Jews, and gay individuals\nindependently of the topic. Finally, we uncover common trend of intra-national\nhostility, where Italians often attack other Italians, reflecting regional and\nintra-regional cultural conflicts that can be traced back to old historical\ndivisions. This study provides the first large-scale mapping of the Italian\nTelegram ecosystem, offering insights into ideological interactions, toxicity,\nand identity-targets of hate and contributing to research on online toxicity\nacross different cultural and linguistic contexts on Telegram."}
{"id": "2504.19408", "pdf": "https://arxiv.org/pdf/2504.19408", "abs": "https://arxiv.org/abs/2504.19408", "authors": ["Maitreya Sonawane", "Sumit Mamtani"], "title": "UNet with Axial Transformer : A Neural Weather Model for Precipitation Nowcasting", "categories": ["cs.LG", "cs.CV", "eess.SP"], "comment": null, "summary": "Making accurate weather predictions can be particularly challenging for\nlocalized storms or events that evolve on hourly timescales, such as\nthunderstorms. Hence, our goal for the project was to model Weather Nowcasting\nfor making highly localized and accurate predictions that apply to the\nimmediate future replacing the current numerical weather models and data\nassimilation systems with Deep Learning approaches. A significant advantage of\nmachine learning is that inference is computationally cheap given an\nalready-trained model, allowing forecasts that are nearly instantaneous and in\nthe native high resolution of the input data. In this work we developed a novel\nmethod that employs Transformer-based machine learning models to forecast\nprecipitation. This approach works by leveraging axial attention mechanisms to\nlearn complex patterns and dynamics from time series frames. Moreover, it is a\ngeneric framework and can be applied to univariate and multivariate time series\ndata, as well as time series embeddings data. This paper represents an initial\nresearch on the dataset used in the domain of next frame prediciton, and hence,\nwe demonstrate state-of-the-art results in terms of metrices (PSNR = 47.67,\nSSIM = 0.9943) used for the given dataset using UNet with Axial Transformer."}
{"id": "2504.19595", "pdf": "https://arxiv.org/pdf/2504.19595", "abs": "https://arxiv.org/abs/2504.19595", "authors": ["Pietro Bongini", "Sara Mandelli", "Andrea Montibeller", "Mirko Casu", "Orazio Pontorno", "Claudio Ragaglia", "Luca Zanchetta", "Mattia Aquilina", "Taiba Majid Wani", "Luca Guarnera", "Benedetta Tondi", "Paolo Bestagini", "Irene Amerini", "Francesco Denatale", "Sebastiano Battiato", "Mauro Barni"], "title": "WILD: a new in-the-Wild Image Linkage Dataset for synthetic image attribution", "categories": ["cs.MM", "cs.AI", "cs.CV"], "comment": null, "summary": "Synthetic image source attribution is an open challenge, with an increasing\nnumber of image generators being released yearly. The complexity and the sheer\nnumber of available generative techniques, as well as the scarcity of\nhigh-quality open source datasets of diverse nature for this task, make\ntraining and benchmarking synthetic image source attribution models very\nchallenging. WILD is a new in-the-Wild Image Linkage Dataset designed to\nprovide a powerful training and benchmarking tool for synthetic image\nattribution models. The dataset is built out of a closed set of 10 popular\ncommercial generators, which constitutes the training base of attribution\nmodels, and an open set of 10 additional generators, simulating a real-world\nin-the-wild scenario. Each generator is represented by 1,000 images, for a\ntotal of 10,000 images in the closed set and 10,000 images in the open set.\nHalf of the images are post-processed with a wide range of operators. WILD\nallows benchmarking attribution models in a wide range of tasks, including\nclosed and open set identification and verification, and robust attribution\nwith respect to post-processing and adversarial attacks. Models trained on WILD\nare expected to benefit from the challenging scenario represented by the\ndataset itself. Moreover, an assessment of seven baseline methodologies on\nclosed and open set attribution is presented, including robustness tests with\nrespect to post-processing."}
{"id": "2504.19438", "pdf": "https://arxiv.org/pdf/2504.19438", "abs": "https://arxiv.org/abs/2504.19438", "authors": ["Lingrui Zhang", "Liang Guo", "Xiao An", "Feng Lin", "Binlong Zheng", "Jiankun Wang", "Zhirui Li"], "title": "Dual Attention Driven Lumbar Magnetic Resonance Image Feature Enhancement and Automatic Diagnosis of Herniation", "categories": ["eess.IV", "cs.CV"], "comment": "9 pages, 7 figures", "summary": "Lumbar disc herniation (LDH) is a common musculoskeletal disease that\nrequires magnetic resonance imaging (MRI) for effective clinical management.\nHowever, the interpretation of MRI images heavily relies on the expertise of\nradiologists, leading to delayed diagnosis and high costs for training\nphysicians. Therefore, this paper proposes an innovative automated LDH\nclassification framework. To address these key issues, the framework utilizes\nT1-weighted and T2-weighted MRI images from 205 people. The framework extracts\nclinically actionable LDH features and generates standardized diagnostic\noutputs by leveraging data augmentation and channel and spatial attention\nmechanisms. These outputs can help physicians make confident and time-effective\ncare decisions when needed. The proposed framework achieves an area under the\nreceiver operating characteristic curve (AUC-ROC) of 0.969 and an accuracy of\n0.9486 for LDH detection. The experimental results demonstrate the performance\nof the proposed framework. Our framework only requires a small number of\ndatasets for training to demonstrate high diagnostic accuracy. This is expected\nto be a solution to enhance the LDH detection capabilities of primary\nhospitals."}
{"id": "2504.19598", "pdf": "https://arxiv.org/pdf/2504.19598", "abs": "https://arxiv.org/abs/2504.19598", "authors": ["Dou Quan", "Rufan Zhou", "Shuang Wang", "Ning Huyan", "Dong Zhao", "Yunan Li", "Licheng Jiao"], "title": "Lightweight Adapter Learning for More Generalized Remote Sensing Change Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Deep learning methods have shown promising performances in remote sensing\nimage change detection (CD). However, existing methods usually train a\ndataset-specific deep network for each dataset. Due to the significant\ndifferences in the data distribution and labeling between various datasets, the\ntrained dataset-specific deep network has poor generalization performances on\nother datasets. To solve this problem, this paper proposes a change adapter\nnetwork (CANet) for a more universal and generalized CD. CANet contains\ndataset-shared and dataset-specific learning modules. The former explores the\ndiscriminative features of images, and the latter designs a lightweight adapter\nmodel, to deal with the characteristics of different datasets in data\ndistribution and labeling. The lightweight adapter can quickly generalize the\ndeep network for new CD tasks with a small computation cost. Specifically, this\npaper proposes an interesting change region mask (ICM) in the adapter, which\ncan adaptively focus on interested change objects and decrease the influence of\nlabeling differences in various datasets. Moreover, CANet adopts a unique batch\nnormalization layer for each dataset to deal with data distribution\ndifferences. Compared with existing deep learning methods, CANet can achieve\nsatisfactory CD performances on various datasets simultaneously. Experimental\nresults on several public datasets have verified the effectiveness and\nadvantages of the proposed CANet on CD. CANet has a stronger generalization\nability, smaller training costs (merely updating 4.1%-7.7% parameters), and\nbetter performances under limited training datasets than other deep learning\nmethods, which also can be flexibly inserted with existing deep models."}
{"id": "2504.19595", "pdf": "https://arxiv.org/pdf/2504.19595", "abs": "https://arxiv.org/abs/2504.19595", "authors": ["Pietro Bongini", "Sara Mandelli", "Andrea Montibeller", "Mirko Casu", "Orazio Pontorno", "Claudio Ragaglia", "Luca Zanchetta", "Mattia Aquilina", "Taiba Majid Wani", "Luca Guarnera", "Benedetta Tondi", "Paolo Bestagini", "Irene Amerini", "Francesco Denatale", "Sebastiano Battiato", "Mauro Barni"], "title": "WILD: a new in-the-Wild Image Linkage Dataset for synthetic image attribution", "categories": ["cs.MM", "cs.AI", "cs.CV"], "comment": null, "summary": "Synthetic image source attribution is an open challenge, with an increasing\nnumber of image generators being released yearly. The complexity and the sheer\nnumber of available generative techniques, as well as the scarcity of\nhigh-quality open source datasets of diverse nature for this task, make\ntraining and benchmarking synthetic image source attribution models very\nchallenging. WILD is a new in-the-Wild Image Linkage Dataset designed to\nprovide a powerful training and benchmarking tool for synthetic image\nattribution models. The dataset is built out of a closed set of 10 popular\ncommercial generators, which constitutes the training base of attribution\nmodels, and an open set of 10 additional generators, simulating a real-world\nin-the-wild scenario. Each generator is represented by 1,000 images, for a\ntotal of 10,000 images in the closed set and 10,000 images in the open set.\nHalf of the images are post-processed with a wide range of operators. WILD\nallows benchmarking attribution models in a wide range of tasks, including\nclosed and open set identification and verification, and robust attribution\nwith respect to post-processing and adversarial attacks. Models trained on WILD\nare expected to benefit from the challenging scenario represented by the\ndataset itself. Moreover, an assessment of seven baseline methodologies on\nclosed and open set attribution is presented, including robustness tests with\nrespect to post-processing."}
{"id": "2504.19600", "pdf": "https://arxiv.org/pdf/2504.19600", "abs": "https://arxiv.org/abs/2504.19600", "authors": ["Pengfei Zhang", "Shouqing Jia"], "title": "Image Generation Method Based on Heat Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Denoising Diffusion Probabilistic Models (DDPMs) achieve high-quality image\ngeneration without adversarial training, but they process images as a whole.\nSince adjacent pixels are highly likely to belong to the same object, we\npropose the Heat Diffusion Model (HDM) to further preserve image details and\ngenerate more realistic images. HDM is a model that incorporates pixel-level\noperations while maintaining the same training process as DDPM. In HDM, the\ndiscrete form of the two-dimensional heat equation is integrated into the\ndiffusion and generation formulas of DDPM, enabling the model to compute\nrelationships between neighboring pixels during image processing. Our\nexperiments demonstrate that HDM can generate higher-quality samples compared\nto models such as DDPM, Consistency Diffusion Models (CDM), Latent Diffusion\nModels (LDM), and Vector Quantized Generative Adversarial Networks (VQGAN)."}
{"id": "2504.19627", "pdf": "https://arxiv.org/pdf/2504.19627", "abs": "https://arxiv.org/abs/2504.19627", "authors": ["Run Luo", "Renke Shan", "Longze Chen", "Ziqiang Liu", "Lu Wang", "Min Yang", "Xiaobo Xia"], "title": "VCM: Vision Concept Modeling Based on Implicit Contrastive Learning with Vision-Language Instruction Fine-Tuning", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "VCM", "summary": "Large Vision-Language Models (LVLMs) are pivotal for real-world AI tasks like\nembodied intelligence due to their strong vision-language reasoning abilities.\nHowever, current LVLMs process entire images at the token level, which is\ninefficient compared to humans who analyze information and generate content at\nthe conceptual level, extracting relevant visual concepts with minimal effort.\nThis inefficiency, stemming from the lack of a visual concept model, limits\nLVLMs' usability in real-world applications. To address this, we propose VCM,\nan end-to-end self-supervised visual concept modeling framework. VCM leverages\nimplicit contrastive learning across multiple sampled instances and\nvision-language fine-tuning to construct a visual concept model without\nrequiring costly concept-level annotations. Our results show that VCM\nsignificantly reduces computational costs (e.g., 85\\% fewer FLOPs for\nLLaVA-1.5-7B) while maintaining strong performance across diverse image\nunderstanding tasks. Moreover, VCM enhances visual encoders' capabilities in\nclassic visual concept perception tasks. Extensive quantitative and qualitative\nexperiments validate the effectiveness and efficiency of VCM."}
{"id": "2504.19627", "pdf": "https://arxiv.org/pdf/2504.19627", "abs": "https://arxiv.org/abs/2504.19627", "authors": ["Run Luo", "Renke Shan", "Longze Chen", "Ziqiang Liu", "Lu Wang", "Min Yang", "Xiaobo Xia"], "title": "VCM: Vision Concept Modeling Based on Implicit Contrastive Learning with Vision-Language Instruction Fine-Tuning", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "VCM", "summary": "Large Vision-Language Models (LVLMs) are pivotal for real-world AI tasks like\nembodied intelligence due to their strong vision-language reasoning abilities.\nHowever, current LVLMs process entire images at the token level, which is\ninefficient compared to humans who analyze information and generate content at\nthe conceptual level, extracting relevant visual concepts with minimal effort.\nThis inefficiency, stemming from the lack of a visual concept model, limits\nLVLMs' usability in real-world applications. To address this, we propose VCM,\nan end-to-end self-supervised visual concept modeling framework. VCM leverages\nimplicit contrastive learning across multiple sampled instances and\nvision-language fine-tuning to construct a visual concept model without\nrequiring costly concept-level annotations. Our results show that VCM\nsignificantly reduces computational costs (e.g., 85\\% fewer FLOPs for\nLLaVA-1.5-7B) while maintaining strong performance across diverse image\nunderstanding tasks. Moreover, VCM enhances visual encoders' capabilities in\nclassic visual concept perception tasks. Extensive quantitative and qualitative\nexperiments validate the effectiveness and efficiency of VCM."}
{"id": "2504.19718", "pdf": "https://arxiv.org/pdf/2504.19718", "abs": "https://arxiv.org/abs/2504.19718", "authors": ["Victoria Yue Chen", "Daoye Wang", "Stephan Garbin", "Sebastian Winberg", "Timo Bolkart", "Thabo Beeler"], "title": "Pixels2Points: Fusing 2D and 3D Features for Facial Skin Segmentation", "categories": ["cs.GR", "cs.CV"], "comment": "4 pages, 4 figures, to be published in Eurographics 2025 as a short\n  paper", "summary": "Face registration deforms a template mesh to closely fit a 3D face scan, the\nquality of which commonly degrades in non-skin regions (e.g., hair, beard,\naccessories), because the optimized template-to-scan distance pulls the\ntemplate mesh towards the noisy scan surface. Improving registration quality\nrequires a clean separation of skin and non-skin regions on the scan mesh.\nExisting image-based (2D) or scan-based (3D) segmentation methods however\nperform poorly. Image-based segmentation outputs multi-view inconsistent masks,\nand they cannot account for scan inaccuracies or scan-image misalignment, while\nscan-based methods suffer from lower spatial resolution compared to images. In\nthis work, we introduce a novel method that accurately separates skin from\nnon-skin geometry on 3D human head scans. For this, our method extracts\nfeatures from multi-view images using a frozen image foundation model and\naggregates these features in 3D. These lifted 2D features are then fused with\n3D geometric features extracted from the scan mesh, to then predict a\nsegmentation mask directly on the scan mesh. We show that our segmentations\nimprove the registration accuracy over pure 2D or 3D segmentation methods by\n8.89% and 14.3%, respectively. Although trained only on synthetic data, our\nmodel generalizes well to real data."}
{"id": "2504.19645", "pdf": "https://arxiv.org/pdf/2504.19645", "abs": "https://arxiv.org/abs/2504.19645", "authors": ["Shadan Shukr Sabr", "Nazira Sabr Mustafa", "Talar Sabah Omar", "Salah Hwayyiz Rasool", "Nawzad Anwer Omer", "Darya Sabir Hamad", "Hemin Abdulhameed Shams", "Omer Mahmood Kareem", "Rozhan Noori Abdullah", "Khabat Atar Abdullah", "Mahabad Azad Mohammad", "Haneen Al-Raghefy", "Safar M. Asaad", "Sara Jamal Mohammed", "Twana Saeed Ali", "Fazil Shawrow", "Halgurd S. Maghdid"], "title": "A Comprehensive Part-of-Speech Tagging to Standardize Central-Kurdish Language: A Research Guide for Kurdish Natural Language Processing Tasks", "categories": ["cs.CL", "cs.AI", "K.5; K.7; J.7"], "comment": "25 pages, 4 figures, 2 tables", "summary": "- The field of natural language processing (NLP) has dramatically expanded\nwithin the last decade. Many human-being applications are conducted daily via\nNLP tasks, starting from machine translation, speech recognition, text\ngeneration and recommendations, Part-of-Speech tagging (POS), and Named-Entity\nRecognition (NER). However, low-resourced languages, such as the\nCentral-Kurdish language (CKL), mainly remain unexamined due to shortage of\nnecessary resources to support their development. The POS tagging task is the\nbase of other NLP tasks; for example, the POS tag set has been used to\nstandardized languages to provide the relationship between words among the\nsentences, followed by machine translation and text recommendation.\nSpecifically, for the CKL, most of the utilized or provided POS tagsets are\nneither standardized nor comprehensive. To this end, this study presented an\naccurate and comprehensive POS tagset for the CKL to provide better performance\nof the Kurdish NLP tasks. The article also collected most of the POS tags from\ndifferent studies as well as from Kurdish linguistic experts to standardized\npart-of-speech tags. The proposed POS tagset is designed to annotate a large\nCKL corpus and support Kurdish NLP tasks. The initial investigations of this\nstudy via comparison with the Universal Dependencies framework for standard\nlanguages, show that the proposed POS tagset can streamline or correct\nsentences more accurately for Kurdish NLP tasks."}
{"id": "2504.19779", "pdf": "https://arxiv.org/pdf/2504.19779", "abs": "https://arxiv.org/abs/2504.19779", "authors": ["Claudia Drygala", "Hanno Gottschalk", "Thomas Kruse", "Ségolène Martin", "Annika Mütze"], "title": "Learning Brenier Potentials with Convex Generative Adversarial Neural Networks", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Brenier proved that under certain conditions on a source and a target\nprobability measure there exists a strictly convex function such that its\ngradient is a transport map from the source to the target distribution. This\nfunction is called the Brenier potential. Furthermore, detailed information on\nthe H\\\"older regularity of the Brenier potential is available. In this work we\ndevelop the statistical learning theory of generative adversarial neural\nnetworks that learn the Brenier potential. As by the transformation of\ndensities formula, the density of the generated measure depends on the second\nderivative of the Brenier potential, we develop the universal approximation\ntheory of ReCU networks with cubic activation $\\mathtt{ReCU}(x)=\\max\\{0,x\\}^3$\nthat combines the favorable approximation properties of H\\\"older functions with\na Lipschitz continuous density. In order to assure the convexity of such\ngeneral networks, we introduce an adversarial training procedure for a\npotential function represented by the ReCU networks that combines the classical\ndiscriminator cross entropy loss with a penalty term that enforces (strict)\nconvexity. We give a detailed decomposition of learning errors and show that\nfor a suitable high penalty parameter all networks chosen in the adversarial\nmin-max optimization problem are strictly convex. This is further exploited to\nprove the consistency of the learning procedure for (slowly) expanding network\ncapacity. We also implement the described learning algorithm and apply it to a\nnumber of standard test cases from Gaussian mixture to image data as target\ndistributions. As predicted in theory, we observe that the convexity loss\nbecomes inactive during the training process and the potentials represented by\nthe neural networks have learned convexity."}
{"id": "2504.19653", "pdf": "https://arxiv.org/pdf/2504.19653", "abs": "https://arxiv.org/abs/2504.19653", "authors": ["Leon Davies", "Baihua Li", "Mohamad Saada", "Simon Sølvsten", "Qinggang Meng"], "title": "GAN-SLAM: Real-Time GAN Aided Floor Plan Creation Through SLAM", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "10 pages, preprint conference submission", "summary": "SLAM is a fundamental component of modern autonomous systems, providing\nrobots and their operators with a deeper understanding of their environment.\nSLAM systems often encounter challenges due to the dynamic nature of robotic\nmotion, leading to inaccuracies in mapping quality, particularly in 2D\nrepresentations such as Occupancy Grid Maps. These errors can significantly\ndegrade map quality, hindering the effectiveness of specific downstream tasks\nsuch as floor plan creation. To address this challenge, we introduce our novel\n'GAN-SLAM', a new SLAM approach that leverages Generative Adversarial Networks\nto clean and complete occupancy grids during the SLAM process, reducing the\nimpact of noise and inaccuracies introduced on the output map. We adapt and\nintegrate accurate pose estimation techniques typically used for 3D SLAM into a\n2D form. This enables the quality improvement 3D LiDAR-odometry has seen in\nrecent years to be effective for 2D representations. Our results demonstrate\nsubstantial improvements in map fidelity and quality, with minimal noise and\nerrors, affirming the effectiveness of GAN-SLAM for real-world mapping\napplications within large-scale complex environments. We validate our approach\non real-world data operating in real-time, and on famous examples of 2D maps.\nThe improved quality of the output map enables new downstream tasks, such as\nfloor plan drafting, further enhancing the capabilities of autonomous systems.\nOur novel approach to SLAM offers a significant step forward in the field,\nimproving the usability for SLAM in mapping-based tasks, and offers insight\ninto the usage of GANs for OGM error correction."}
{"id": "2504.19822", "pdf": "https://arxiv.org/pdf/2504.19822", "abs": "https://arxiv.org/abs/2504.19822", "authors": ["Minjong Cheon"], "title": "Mjölnir: A Deep Learning Parametrization Framework for Global Lightning Flash Density", "categories": ["cs.LG", "cs.AI", "cs.CV", "physics.ao-ph"], "comment": null, "summary": "Recent advances in AI-based weather forecasting models, such as FourCastNet,\nPangu-Weather, and GraphCast, have demonstrated the remarkable ability of deep\nlearning to emulate complex atmospheric dynamics. Building on this momentum, we\npropose Mj\\\"olnir, a novel deep learning-based framework for global lightning\nflash density parameterization. Trained on ERA5 atmospheric predictors and\nWorld Wide Lightning Location Network (WWLLN) observations at a daily temporal\nresolution and 1 degree spatial resolution, Mj\\\"olnir captures the nonlinear\nmapping between large-scale environmental conditions and lightning activity.\nThe model architecture is based on the InceptionNeXt backbone with SENet, and a\nmulti-task learning strategy to simultaneously predict lightning occurrence and\nmagnitude. Extensive evaluations yield that Mollnir accurately reproduces the\nglobal distribution, seasonal variability, and regional characteristics of\nlightning activity, achieving a global Pearson correlation coefficient of 0.96\nfor annual mean fields. These results suggest that Mj\\\"olnir serves not only as\nan effective data-driven global lightning parameterization but also as a\npromising AI-based scheme for next-generation Earth system models (AI-ESMs)."}
{"id": "2504.19654", "pdf": "https://arxiv.org/pdf/2504.19654", "abs": "https://arxiv.org/abs/2504.19654", "authors": ["Leon Davies", "Baihua Li", "Mohamad Saada", "Simon Sølvsten", "Qinggang Meng"], "title": "Transformation & Translation Occupancy Grid Mapping: 2-Dimensional Deep Learning Refined SLAM", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "12 pages, preprint, submitted to Robotics And Autonomous Systems", "summary": "SLAM (Simultaneous Localisation and Mapping) is a crucial component for\nrobotic systems, providing a map of an environment, the current location and\nprevious trajectory of a robot. While 3D LiDAR SLAM has received notable\nimprovements in recent years, 2D SLAM lags behind. Gradual drifts in odometry\nand pose estimation inaccuracies hinder modern 2D LiDAR-odometry algorithms in\nlarge complex environments. Dynamic robotic motion coupled with inherent\nestimation based SLAM processes introduce noise and errors, degrading map\nquality. Occupancy Grid Mapping (OGM) produces results that are often noisy and\nunclear. This is due to the fact that evidence based mapping represents maps\naccording to uncertain observations. This is why OGMs are so popular in\nexploration or navigation tasks. However, this also limits OGMs' effectiveness\nfor specific mapping based tasks such as floor plan creation in complex scenes.\nTo address this, we propose our novel Transformation and Translation Occupancy\nGrid Mapping (TT-OGM). We adapt and enable accurate and robust pose estimation\ntechniques from 3D SLAM to the world of 2D and mitigate errors to improve map\nquality using Generative Adversarial Networks (GANs). We introduce a novel data\ngeneration method via deep reinforcement learning (DRL) to build datasets large\nenough for training a GAN for SLAM error correction. We demonstrate our SLAM in\nreal-time on data collected at Loughborough University. We also prove its\ngeneralisability on a variety of large complex environments on a collection of\nlarge scale well-known 2D occupancy maps. Our novel approach enables the\ncreation of high quality OGMs in complex scenes, far surpassing the\ncapabilities of current SLAM algorithms in terms of quality, accuracy and\nreliability."}
{"id": "2504.19854", "pdf": "https://arxiv.org/pdf/2504.19854", "abs": "https://arxiv.org/abs/2504.19854", "authors": ["Chia-Yu Hung", "Qi Sun", "Pengfei Hong", "Amir Zadeh", "Chuan Li", "U-Xuan Tan", "Navonil Majumder", "Soujanya Poria"], "title": "NORA: A Small Open-Sourced Generalist Vision Language Action Model for Embodied Tasks", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Existing Visual-Language-Action (VLA) models have shown promising performance\nin zero-shot scenarios, demonstrating impressive task execution and reasoning\ncapabilities. However, a significant challenge arises from the limitations of\nvisual encoding, which can result in failures during tasks such as object\ngrasping. Moreover, these models typically suffer from high computational\noverhead due to their large sizes, often exceeding 7B parameters. While these\nmodels excel in reasoning and task planning, the substantial computational\noverhead they incur makes them impractical for real-time robotic environments,\nwhere speed and efficiency are paramount. To address the limitations of\nexisting VLA models, we propose NORA, a 3B-parameter model designed to reduce\ncomputational overhead while maintaining strong task performance. NORA adopts\nthe Qwen-2.5-VL-3B multimodal model as its backbone, leveraging its superior\nvisual-semantic understanding to enhance visual reasoning and action grounding.\nAdditionally, our \\model{} is trained on 970k real-world robot demonstrations\nand equipped with the FAST+ tokenizer for efficient action sequence generation.\nExperimental results demonstrate that NORA outperforms existing large-scale VLA\nmodels, achieving better task performance with significantly reduced\ncomputational overhead, making it a more practical solution for real-time\nrobotic autonomy."}
{"id": "2504.19659", "pdf": "https://arxiv.org/pdf/2504.19659", "abs": "https://arxiv.org/abs/2504.19659", "authors": ["Muhammad Sabih", "Abrarul Karim", "Jakob Wittmann", "Frank Hannig", "Jürgen Teich"], "title": "Hardware/Software Co-Design of RISC-V Extensions for Accelerating Sparse DNNs on FPGAs", "categories": ["cs.LG", "cs.AI", "cs.AR"], "comment": null, "summary": "The customizability of RISC-V makes it an attractive choice for accelerating\ndeep neural networks (DNNs). It can be achieved through instruction set\nextensions and corresponding custom functional units. Yet, efficiently\nexploiting these opportunities requires a hardware/software co-design approach\nin which the DNN model, software, and hardware are designed together. In this\npaper, we propose novel RISC-V extensions for accelerating DNN models\ncontaining semi-structured and unstructured sparsity. While the idea of\naccelerating structured and unstructured pruning is not new, our novel design\noffers various advantages over other designs. To exploit semi-structured\nsparsity, we take advantage of the fine-grained (bit-level) configurability of\nFPGAs and suggest reserving a few bits in a block of DNN weights to encode the\ninformation about sparsity in the succeeding blocks. The proposed custom\nfunctional unit utilizes this information to skip computations. To exploit\nunstructured sparsity, we propose a variable cycle sequential\nmultiply-and-accumulate unit that performs only as many multiplications as the\nnon-zero weights. Our implementation of unstructured and semi-structured\npruning accelerators can provide speedups of up to a factor of 3 and 4,\nrespectively. We then propose a combined design that can accelerate both types\nof sparsities, providing speedups of up to a factor of 5. Our designs consume a\nsmall amount of additional FPGA resources such that the resulting co-designs\nenable the acceleration of DNNs even on small FPGAs. We benchmark our designs\non standard TinyML applications such as keyword spotting, image classification,\nand person detection."}
{"id": "2504.19930", "pdf": "https://arxiv.org/pdf/2504.19930", "abs": "https://arxiv.org/abs/2504.19930", "authors": ["Thanuja Uruththirakodeeswaran", "Harald Becher", "Michelle Noga", "Lawrence H. Le", "Pierre Boulanger", "Jonathan Windram", "Kumaradevan Punithakumar"], "title": "Accelerated 3D-3D rigid registration of echocardiographic images obtained from apical window using particle filter", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "The perfect alignment of 3D echocardiographic images captured from various\nangles has improved image quality and broadened the field of view. This study\nproposes an accelerated sequential Monte Carlo (SMC) algorithm for 3D-3D rigid\nregistration of transthoracic echocardiographic images with significant and\nlimited overlap taken from apical window that is robust to the noise and\nintensity variation in ultrasound images. The algorithm estimates the\ntranslational and rotational components of the rigid transform through an\niterative process and requires an initial approximation of the rotation and\ntranslation limits. We perform registration in two ways: the image-based\nregistration computes the transform to align the end-diastolic frame of the\napical nonstandard image to the apical standard image and applies the same\ntransform to all frames of the cardiac cycle, whereas the mask-based\nregistration approach uses the binary masks of the left ventricle in the same\nway. The SMC and exhaustive search (EX) algorithms were evaluated for 4D\ntemporal sequences recorded from 7 volunteers who participated in a study\nconducted at the Mazankowski Alberta Heart Institute. The evaluations\ndemonstrate that the mask-based approach of the accelerated SMC yielded a Dice\nscore value of 0.819 +/- 0.045 for the left ventricle and gained 16.7x speedup\ncompared to the CPU version of the SMC algorithm."}
{"id": "2504.19667", "pdf": "https://arxiv.org/pdf/2504.19667", "abs": "https://arxiv.org/abs/2504.19667", "authors": ["Michael Banf", "Johannes Kuhn"], "title": "A Tripartite Perspective on GraphRAG", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable capabilities across\nvarious domains, yet they struggle with knowledge-intensive tasks in areas that\ndemand factual accuracy, e.g. industrial automation and healthcare. Key\nlimitations include their tendency to hallucinate, lack of source traceability\n(provenance), and challenges in timely knowledge updates. Combining language\nmodels with knowledge graphs (GraphRAG) offers promising avenues for overcoming\nthese deficits. However, a major challenge lies in creating such a knowledge\ngraph in the first place. Here, we propose a novel approach that combines LLMs\nwith a tripartite knowledge graph representation, which is constructed by\nconnecting complex, domain-specific objects via a curated ontology of\ncorresponding, domain-specific concepts to relevant sections within chunks of\ntext through a concept-anchored pre-analysis of source documents starting from\nan initial lexical graph. As a consequence, our Tripartite-GraphRAG approach\nimplements: i) a concept-specific, information-preserving pre-compression of\ntextual chunks; ii) allows for the formation of a concept-specific relevance\nestimation of embedding similarities grounded in statistics; and iii) avoids\ncommon challenges w.r.t. continuous extendability, such as the need for entity\nresolution and deduplication. By applying a transformation to the knowledge\ngraph, we formulate LLM prompt creation as an unsupervised node classification\nproblem, drawing on ideas from Markov Random Fields. We evaluate our approach\non a healthcare use case, involving multi-faceted analyses of patient anamneses\ngiven a set of medical concepts as well as clinical literature. Experiments\nindicate that it can optimize information density, coverage, and arrangement of\nLLM prompts while reducing their lengths, which may lead to reduced costs and\nmore consistent and reliable LLM outputs."}
{"id": "2504.19937", "pdf": "https://arxiv.org/pdf/2504.19937", "abs": "https://arxiv.org/abs/2504.19937", "authors": ["Sima Soltanpour", "Rachel Utama", "Arnold Chang", "Md Taufiq Nasseef", "Dan Madularu", "Praveen Kulkarni", "Craig Ferris", "Chris Joslin"], "title": "SST-DUNet: Automated preclinical functional MRI skull stripping using Smart Swin Transformer and Dense UNet", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Skull stripping is a common preprocessing step that is often performed\nmanually in Magnetic Resonance Imaging (MRI) pipelines, including functional\nMRI (fMRI). This manual process is time-consuming and operator dependent.\nAutomating this process is challenging for preclinical data due to variations\nin brain geometry, resolution, and tissue contrast. While existing methods for\nMRI skull stripping exist, they often struggle with the low resolution and\nvarying slice sizes in preclinical fMRI data. This study proposes a novel\nmethod called SST-DUNet, that integrates a dense UNet-based architecture with a\nfeature extractor based on Smart Swin Transformer (SST) for fMRI skull\nstripping. The Smart Shifted Window Multi-Head Self-Attention (SSW-MSA) module\nin SST is adapted to replace the mask-based module in the Swin Transformer\n(ST), enabling the learning of distinct channel-wise features while focusing on\nrelevant dependencies within brain structures. This modification allows the\nmodel to better handle the complexities of fMRI skull stripping, such as low\nresolution and variable slice sizes. To address the issue of class imbalance in\npreclinical data, a combined loss function using Focal and Dice loss is\nutilized. The model was trained on rat fMRI images and evaluated across three\nin-house datasets with a Dice similarity score of 98.65%, 97.86%, and 98.04%.\nThe fMRI results obtained through automatic skull stripping using the SST-DUNet\nmodel closely align with those from manual skull stripping for both seed-based\nand independent component analyses. These results indicate that the SST-DUNet\ncan effectively substitute manual brain extraction in rat fMRI analysis."}
{"id": "2504.19673", "pdf": "https://arxiv.org/pdf/2504.19673", "abs": "https://arxiv.org/abs/2504.19673", "authors": ["Stefanie Krause", "Ashish Dalvi", "Syed Khubaib Zaidi"], "title": "Generative AI in Education: Student Skills and Lecturer Roles", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Generative Artificial Intelligence (GenAI) tools such as ChatGPT are emerging\nas a revolutionary tool in education that brings both positive aspects and\nchallenges for educators and students, reshaping how learning and teaching are\napproached. This study aims to identify and evaluate the key competencies\nstudents need to effectively engage with GenAI in education and to provide\nstrategies for lecturers to integrate GenAI into teaching practices. The study\napplied a mixed method approach with a combination of a literature review and a\nquantitative survey involving 130 students from South Asia and Europe to obtain\nits findings. The literature review identified 14 essential student skills for\nGenAI engagement, with AI literacy, critical thinking, and ethical AI practices\nemerging as the most critical. The student survey revealed gaps in prompt\nengineering, bias awareness, and AI output management. In our study of lecturer\nstrategies, we identified six key areas, with GenAI Integration and Curriculum\nDesign being the most emphasised. Our findings highlight the importance of\nincorporating GenAI into education. While literature prioritized ethics and\npolicy development, students favour hands-on, project-based learning and\npractical AI applications. To foster inclusive and responsible GenAI adoption,\ninstitutions should ensure equitable access to GenAI tools, establish clear\nacademic integrity policies, and advocate for global GenAI research\ninitiatives."}
{"id": "2504.20007", "pdf": "https://arxiv.org/pdf/2504.20007", "abs": "https://arxiv.org/abs/2504.20007", "authors": ["Anita Srbinovska", "Angela Srbinovska", "Vivek Senthil", "Adrian Martin", "John McCluskey", "Ernest Fokoué"], "title": "Towards AI-Driven Policing: Interdisciplinary Knowledge Discovery from Police Body-Worn Camera Footage", "categories": ["cs.AI", "cs.CV"], "comment": "6 pages, 2 figures, and 1 table", "summary": "This paper proposes a novel interdisciplinary framework for analyzing police\nbody-worn camera (BWC) footage from the Rochester Police Department (RPD) using\nadvanced artificial intelligence (AI) and statistical machine learning (ML)\ntechniques. Our goal is to detect, classify, and analyze patterns of\ninteraction between police officers and civilians to identify key behavioral\ndynamics, such as respect, disrespect, escalation, and de-escalation. We apply\nmultimodal data analysis by integrating video, audio, and natural language\nprocessing (NLP) techniques to extract meaningful insights from BWC footage. We\npresent our methodology, computational techniques, and findings, outlining a\npractical approach for law enforcement while advancing the frontiers of\nknowledge discovery from police BWC data."}
{"id": "2504.19674", "pdf": "https://arxiv.org/pdf/2504.19674", "abs": "https://arxiv.org/abs/2504.19674", "authors": ["Madhur Jindal", "Hari Shrawgi", "Parag Agrawal", "Sandipan Dandapat"], "title": "$\\texttt{SAGE}$: A Generic Framework for LLM Safety Evaluation", "categories": ["cs.CR", "cs.AI"], "comment": "24 pages, 9 main pages excluding references and appendix", "summary": "Safety evaluation of Large Language Models (LLMs) has made progress and\nattracted academic interest, but it remains challenging to keep pace with the\nrapid integration of LLMs across diverse applications. Different applications\nexpose users to various harms, necessitating application-specific safety\nevaluations with tailored harms and policies. Another major gap is the lack of\nfocus on the dynamic and conversational nature of LLM systems. Such potential\noversights can lead to harms that go unnoticed in standard safety benchmarks.\nThis paper identifies the above as key requirements for robust LLM safety\nevaluation and recognizing that current evaluation methodologies do not satisfy\nthese, we introduce the $\\texttt{SAGE}$ (Safety AI Generic Evaluation)\nframework. $\\texttt{SAGE}$ is an automated modular framework designed for\ncustomized and dynamic harm evaluations. It utilizes adversarial user models\nthat are system-aware and have unique personalities, enabling a holistic\nred-teaming evaluation. We demonstrate $\\texttt{SAGE}$'s effectiveness by\nevaluating seven state-of-the-art LLMs across three applications and harm\npolicies. Our experiments with multi-turn conversational evaluations revealed a\nconcerning finding that harm steadily increases with conversation length.\nFurthermore, we observe significant disparities in model behavior when exposed\nto different user personalities and scenarios. Our findings also reveal that\nsome models minimize harmful outputs by employing severe refusal tactics that\ncan hinder their usefulness. These insights highlight the necessity of adaptive\nand context-specific testing to ensure better safety alignment and safer\ndeployment of LLMs in real-world scenarios."}
{"id": "2504.19675", "pdf": "https://arxiv.org/pdf/2504.19675", "abs": "https://arxiv.org/abs/2504.19675", "authors": ["Osma Suominen", "Juho Inkinen", "Mona Lehtinen"], "title": "Annif at SemEval-2025 Task 5: Traditional XMTC augmented by LLMs", "categories": ["cs.CL", "cs.AI", "cs.DL", "cs.IR", "cs.LG", "I.2.7"], "comment": "6 pages, 4 figures, submitted to SemEval-2025 workshop Task 5:\n  LLMs4Subjects", "summary": "This paper presents the Annif system in SemEval-2025 Task 5 (LLMs4Subjects),\nwhich focussed on subject indexing using large language models (LLMs). The task\nrequired creating subject predictions for bibliographic records from the\nbilingual TIBKAT database using the GND subject vocabulary. Our approach\ncombines traditional natural language processing and machine learning\ntechniques implemented in the Annif toolkit with innovative LLM-based methods\nfor translation and synthetic data generation, and merging predictions from\nmonolingual models. The system ranked first in the all-subjects category and\nsecond in the tib-core-subjects category in the quantitative evaluation, and\nfourth in qualitative evaluations. These findings demonstrate the potential of\ncombining traditional XMTC algorithms with modern LLM techniques to improve the\naccuracy and efficiency of subject indexing in multilingual contexts."}
{"id": "2504.19715", "pdf": "https://arxiv.org/pdf/2504.19715", "abs": "https://arxiv.org/abs/2504.19715", "authors": ["Heisei Yonezawa", "Ansei Yonezawa", "Itsuro Kajiwara"], "title": "Model-based controller assisted domain randomization in deep reinforcement learning: application to nonlinear powertrain control", "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.SY"], "comment": null, "summary": "Complex mechanical systems such as vehicle powertrains are inherently subject\nto multiple nonlinearities and uncertainties arising from parametric\nvariations. Modeling and calibration errors are therefore unavoidable, making\nthe transfer of control systems from simulation to real-world systems a\ncritical challenge. Traditional robust controls have limitations in handling\ncertain types of nonlinearities and uncertainties, requiring a more practical\napproach capable of comprehensively compensating for these various constraints.\nThis study proposes a new robust control approach using the framework of deep\nreinforcement learning (DRL). The key strategy lies in the synergy among domain\nrandomization-based DRL, long short-term memory (LSTM)-based actor and critic\nnetworks, and model-based control (MBC). The problem setup is modeled via the\nlatent Markov decision process (LMDP), a set of vanilla MDPs, for a controlled\nsystem subject to uncertainties and nonlinearities. In LMDP, the dynamics of an\nenvironment simulator is randomized during training to improve the robustness\nof the control system to real testing environments. The randomization increases\ntraining difficulties as well as conservativeness of the resultant control\nsystem; therefore, progress is assisted by concurrent use of a model-based\ncontroller based on a nominal system model. Compared to traditional DRL-based\ncontrols, the proposed controller design is smarter in that we can achieve a\nhigh level of generalization ability with a more compact neural network\narchitecture and a smaller amount of training data. The proposed approach is\nverified via practical application to active damping for a complex powertrain\nsystem with nonlinearities and parametric variations. Comparative tests\ndemonstrate the high robustness of the proposed approach."}
{"id": "2504.19720", "pdf": "https://arxiv.org/pdf/2504.19720", "abs": "https://arxiv.org/abs/2504.19720", "authors": ["Ranran Zhen", "Juntao Li", "Yixin Ji", "Zhenlin Yang", "Tong Liu", "Qingrong Xia", "Xinyu Duan", "Zhefeng Wang", "Baoxing Huai", "Min Zhang"], "title": "Taming the Titans: A Survey of Efficient LLM Inference Serving", "categories": ["cs.CL", "cs.AI", "cs.DC", "cs.LG"], "comment": "work in progress;11 pages of main paper with 7 main figures, overall\n  20 pages", "summary": "Large Language Models (LLMs) for Generative AI have achieved remarkable\nprogress, evolving into sophisticated and versatile tools widely adopted across\nvarious domains and applications. However, the substantial memory overhead\ncaused by their vast number of parameters, combined with the high computational\ndemands of the attention mechanism, poses significant challenges in achieving\nlow latency and high throughput for LLM inference services. Recent\nadvancements, driven by groundbreaking research, have significantly accelerated\nprogress in this field. This paper provides a comprehensive survey of these\nmethods, covering fundamental instance-level approaches, in-depth cluster-level\nstrategies, emerging scenario directions, and other miscellaneous but important\nareas. At the instance level, we review model placement, request scheduling,\ndecoding length prediction, storage management, and the disaggregation\nparadigm. At the cluster level, we explore GPU cluster deployment,\nmulti-instance load balancing, and cloud service solutions. For emerging\nscenarios, we organize the discussion around specific tasks, modules, and\nauxiliary methods. To ensure a holistic overview, we also highlight several\nniche yet critical areas. Finally, we outline potential research directions to\nfurther advance the field of LLM inference serving."}
{"id": "2504.19754", "pdf": "https://arxiv.org/pdf/2504.19754", "abs": "https://arxiv.org/abs/2504.19754", "authors": ["Carlo Merola", "Jaspinder Singh"], "title": "Reconstructing Context: Evaluating Advanced Chunking Strategies for Retrieval-Augmented Generation", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "13 pages, 2 figures, Second Workshop on Knowledge-Enhanced\n  Information Retrieval, ECIR 2025", "summary": "Retrieval-augmented generation (RAG) has become a transformative approach for\nenhancing large language models (LLMs) by grounding their outputs in external\nknowledge sources. Yet, a critical question persists: how can vast volumes of\nexternal knowledge be managed effectively within the input constraints of LLMs?\nTraditional methods address this by chunking external documents into smaller,\nfixed-size segments. While this approach alleviates input limitations, it often\nfragments context, resulting in incomplete retrieval and diminished coherence\nin generation. To overcome these shortcomings, two advanced techniques, late\nchunking and contextual retrieval, have been introduced, both aiming to\npreserve global context. Despite their potential, their comparative strengths\nand limitations remain unclear. This study presents a rigorous analysis of late\nchunking and contextual retrieval, evaluating their effectiveness and\nefficiency in optimizing RAG systems. Our results indicate that contextual\nretrieval preserves semantic coherence more effectively but requires greater\ncomputational resources. In contrast, late chunking offers higher efficiency\nbut tends to sacrifice relevance and completeness."}
{"id": "2504.19755", "pdf": "https://arxiv.org/pdf/2504.19755", "abs": "https://arxiv.org/abs/2504.19755", "authors": ["Kapil Kashyap", "Sean Fargose", "Chrisil Dabre", "Fatema Dolaria", "Nilesh Patil", "Aniket Kore"], "title": "Hybrid Approach Combining Ultrasound and Blood Test Analysis with a Voting Classifier for Accurate Liver Fibrosis and Cirrhosis Assessment", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Liver cirrhosis is an insidious condition involving the substitution of\nnormal liver tissue with fibrous scar tissue and causing major health\ncomplications. The conventional method of diagnosis using liver biopsy is\ninvasive and, therefore, inconvenient for use in regular screening. In this\npaper,we present a hybrid model that combines machine learning techniques with\nclinical data and ultrasoundscans to improve liver fibrosis and cirrhosis\ndetection accuracy is presented. The model integrates fixed blood test\nprobabilities with deep learning model predictions (DenseNet-201) for\nultrasonic images. The combined hybrid model achieved an accuracy of 92.5%. The\nfindings establish the viability of the combined model in enhancing diagnosis\naccuracy and supporting early intervention in liver disease care."}
{"id": "2504.19792", "pdf": "https://arxiv.org/pdf/2504.19792", "abs": "https://arxiv.org/abs/2504.19792", "authors": ["Runtian Zhai"], "title": "Contextures: The Mechanism of Representation Learning", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "PhD Dissertation", "summary": "This dissertation establishes the contexture theory to mathematically\ncharacterize the mechanism of representation learning, or pretraining. Despite\nthe remarkable empirical success of foundation models, it is not very clear\nwhat representations they learn, and why these representations are useful for\nvarious downstream tasks. A scientific understanding of representation learning\nis critical, especially at this point when scaling up the model size is\nproducing diminishing returns, and designing new pretraining methods is\nimperative for further progress.\n  Prior work treated different representation learning methods quite\ndifferently, whereas the contexture theory provides a unified framework for\nanalyzing these methods. The central argument is that a representation is\nlearned from the association between the input X and a context variable A. We\nprove that if an encoder captures the maximum information of this association,\nin which case we say that the encoder learns the contexture, then it will be\noptimal on the class of tasks that are compatible with the context. We also\nshow that a context is the most useful when the association between X and A is\nneither too strong nor too weak. The important implication of the contexture\ntheory is that increasing the model size alone will achieve diminishing\nreturns, and further advancements require better contexts.\n  We demonstrate that many pretraining objectives can learn the contexture,\nincluding supervised learning, self-supervised learning, generative models,\netc. Then, we introduce two general objectives -- SVME and KISE, for learning\nthe contexture. We also show how to mix multiple contexts together, an\neffortless way to create better contexts from existing ones. Then, we prove\nstatistical learning bounds for representation learning. Finally, we discuss\nthe effect of the data distribution shift from pretraining to the downstream\ntask."}
{"id": "2504.19818", "pdf": "https://arxiv.org/pdf/2504.19818", "abs": "https://arxiv.org/abs/2504.19818", "authors": ["Feng Chen", "Ilias Stogiannidis", "Andrew Wood", "Danilo Bueno", "Dominic Williams", "Fraser Macfarlane", "Bruce Grieve", "Darren Wells", "Jonathan A. Atkinson", "Malcolm J. Hawkesford", "Stephen A. Rolfe", "Tracy Lawson", "Tony Pridmore", "Mario Valerio Giuffrida", "Sotirios A. Tsaftaris"], "title": "PhenoAssistant: A Conversational Multi-Agent AI System for Automated Plant Phenotyping", "categories": ["cs.MA", "cs.AI"], "comment": null, "summary": "Plant phenotyping increasingly relies on (semi-)automated image-based\nanalysis workflows to improve its accuracy and scalability. However, many\nexisting solutions remain overly complex, difficult to reimplement and\nmaintain, and pose high barriers for users without substantial computational\nexpertise. To address these challenges, we introduce PhenoAssistant: a\npioneering AI-driven system that streamlines plant phenotyping via intuitive\nnatural language interaction. PhenoAssistant leverages a large language model\nto orchestrate a curated toolkit supporting tasks including automated phenotype\nextraction, data visualisation and automated model training. We validate\nPhenoAssistant through several representative case studies and a set of\nevaluation tasks. By significantly lowering technical hurdles, PhenoAssistant\nunderscores the promise of AI-driven methodologies to democratising AI adoption\nin plant biology."}
{"id": "2504.19822", "pdf": "https://arxiv.org/pdf/2504.19822", "abs": "https://arxiv.org/abs/2504.19822", "authors": ["Minjong Cheon"], "title": "Mjölnir: A Deep Learning Parametrization Framework for Global Lightning Flash Density", "categories": ["cs.LG", "cs.AI", "cs.CV", "physics.ao-ph"], "comment": null, "summary": "Recent advances in AI-based weather forecasting models, such as FourCastNet,\nPangu-Weather, and GraphCast, have demonstrated the remarkable ability of deep\nlearning to emulate complex atmospheric dynamics. Building on this momentum, we\npropose Mj\\\"olnir, a novel deep learning-based framework for global lightning\nflash density parameterization. Trained on ERA5 atmospheric predictors and\nWorld Wide Lightning Location Network (WWLLN) observations at a daily temporal\nresolution and 1 degree spatial resolution, Mj\\\"olnir captures the nonlinear\nmapping between large-scale environmental conditions and lightning activity.\nThe model architecture is based on the InceptionNeXt backbone with SENet, and a\nmulti-task learning strategy to simultaneously predict lightning occurrence and\nmagnitude. Extensive evaluations yield that Mollnir accurately reproduces the\nglobal distribution, seasonal variability, and regional characteristics of\nlightning activity, achieving a global Pearson correlation coefficient of 0.96\nfor annual mean fields. These results suggest that Mj\\\"olnir serves not only as\nan effective data-driven global lightning parameterization but also as a\npromising AI-based scheme for next-generation Earth system models (AI-ESMs)."}
{"id": "2504.19847", "pdf": "https://arxiv.org/pdf/2504.19847", "abs": "https://arxiv.org/abs/2504.19847", "authors": ["Juhan Park", "Kyungjae Lee", "Hyung Jin Chang", "Jungchan Cho"], "title": "Foundation Model-Driven Framework for Human-Object Interaction Prediction with Segmentation Mask Integration", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In this work, we introduce Segmentation to Human-Object Interaction\n(\\textit{\\textbf{Seg2HOI}}) approach, a novel framework that integrates\nsegmentation-based vision foundation models with the human-object interaction\ntask, distinguished from traditional detection-based Human-Object Interaction\n(HOI) methods. Our approach enhances HOI detection by not only predicting the\nstandard triplets but also introducing quadruplets, which extend HOI triplets\nby including segmentation masks for human-object pairs. More specifically,\nSeg2HOI inherits the properties of the vision foundation model (e.g.,\npromptable and interactive mechanisms) and incorporates a decoder that applies\nthese attributes to HOI task. Despite training only for HOI, without additional\ntraining mechanisms for these properties, the framework demonstrates that such\nfeatures still operate efficiently. Extensive experiments on two public\nbenchmark datasets demonstrate that Seg2HOI achieves performance comparable to\nstate-of-the-art methods, even in zero-shot scenarios. Lastly, we propose that\nSeg2HOI can generate HOI quadruplets and interactive HOI segmentation from\nnovel text and visual prompts that were not used during training, making it\nversatile for a wide range of applications by leveraging this flexibility."}
{"id": "2504.19848", "pdf": "https://arxiv.org/pdf/2504.19848", "abs": "https://arxiv.org/abs/2504.19848", "authors": ["Simona Casini", "Pietro Ducange", "Francesco Marcelloni", "Lorenzo Pollini"], "title": "Human-Centered AI and Autonomy in Robotics: Insights from a Bibliometric Study", "categories": ["cs.RO", "cs.AI"], "comment": "International Joint Conference on Neural Network 2025 - Accepted", "summary": "The development of autonomous robotic systems offers significant potential\nfor performing complex tasks with precision and consistency. Recent advances in\nArtificial Intelligence (AI) have enabled more capable intelligent automation\nsystems, addressing increasingly complex challenges. However, this progress\nraises questions about human roles in such systems. Human-Centered AI (HCAI)\naims to balance human control and automation, ensuring performance enhancement\nwhile maintaining creativity, mastery, and responsibility. For real-world\napplications, autonomous robots must balance task performance with reliability,\nsafety, and trustworthiness. Integrating HCAI principles enhances human-robot\ncollaboration and ensures responsible operation.\n  This paper presents a bibliometric analysis of intelligent autonomous robotic\nsystems, utilizing SciMAT and VOSViewer to examine data from the Scopus\ndatabase. The findings highlight academic trends, emerging topics, and AI's\nrole in self-adaptive robotic behaviour, with an emphasis on HCAI architecture.\nThese insights are then projected onto the IBM MAPE-K architecture, with the\ngoal of identifying how these research results map into actual robotic\nautonomous systems development efforts for real-world scenarios."}
{"id": "2504.19854", "pdf": "https://arxiv.org/pdf/2504.19854", "abs": "https://arxiv.org/abs/2504.19854", "authors": ["Chia-Yu Hung", "Qi Sun", "Pengfei Hong", "Amir Zadeh", "Chuan Li", "U-Xuan Tan", "Navonil Majumder", "Soujanya Poria"], "title": "NORA: A Small Open-Sourced Generalist Vision Language Action Model for Embodied Tasks", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Existing Visual-Language-Action (VLA) models have shown promising performance\nin zero-shot scenarios, demonstrating impressive task execution and reasoning\ncapabilities. However, a significant challenge arises from the limitations of\nvisual encoding, which can result in failures during tasks such as object\ngrasping. Moreover, these models typically suffer from high computational\noverhead due to their large sizes, often exceeding 7B parameters. While these\nmodels excel in reasoning and task planning, the substantial computational\noverhead they incur makes them impractical for real-time robotic environments,\nwhere speed and efficiency are paramount. To address the limitations of\nexisting VLA models, we propose NORA, a 3B-parameter model designed to reduce\ncomputational overhead while maintaining strong task performance. NORA adopts\nthe Qwen-2.5-VL-3B multimodal model as its backbone, leveraging its superior\nvisual-semantic understanding to enhance visual reasoning and action grounding.\nAdditionally, our \\model{} is trained on 970k real-world robot demonstrations\nand equipped with the FAST+ tokenizer for efficient action sequence generation.\nExperimental results demonstrate that NORA outperforms existing large-scale VLA\nmodels, achieving better task performance with significantly reduced\ncomputational overhead, making it a more practical solution for real-time\nrobotic autonomy."}
{"id": "2504.19863", "pdf": "https://arxiv.org/pdf/2504.19863", "abs": "https://arxiv.org/abs/2504.19863", "authors": ["Daniel Kienzle", "Robin Schön", "Rainer Lienhart", "Shin'Ichi Satoh"], "title": "Towards Ball Spin and Trajectory Analysis in Table Tennis Broadcast Videos via Physically Grounded Synthetic-to-Real Transfer", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "To be published in 2025 IEEE/CVF International Conference on Computer\n  Vision and Pattern Recognition Workshops (CVPRW)", "summary": "Analyzing a player's technique in table tennis requires knowledge of the\nball's 3D trajectory and spin. While, the spin is not directly observable in\nstandard broadcasting videos, we show that it can be inferred from the ball's\ntrajectory in the video. We present a novel method to infer the initial spin\nand 3D trajectory from the corresponding 2D trajectory in a video. Without\nground truth labels for broadcast videos, we train a neural network solely on\nsynthetic data. Due to the choice of our input data representation, physically\ncorrect synthetic training data, and using targeted augmentations, the network\nnaturally generalizes to real data. Notably, these simple techniques are\nsufficient to achieve generalization. No real data at all is required for\ntraining. To the best of our knowledge, we are the first to present a method\nfor spin and trajectory prediction in simple monocular broadcast videos,\nachieving an accuracy of 92.0% in spin classification and a 2D reprojection\nerror of 0.19% of the image diagonal."}
{"id": "2504.19874", "pdf": "https://arxiv.org/pdf/2504.19874", "abs": "https://arxiv.org/abs/2504.19874", "authors": ["Amir Zandieh", "Majid Daliri", "Majid Hadian", "Vahab Mirrokni"], "title": "TurboQuant: Online Vector Quantization with Near-optimal Distortion Rate", "categories": ["cs.LG", "cs.AI", "cs.DB", "cs.DS"], "comment": "25 pages", "summary": "Vector quantization, a problem rooted in Shannon's source coding theory, aims\nto quantize high-dimensional Euclidean vectors while minimizing distortion in\ntheir geometric structure. We propose TurboQuant to address both mean-squared\nerror (MSE) and inner product distortion, overcoming limitations of existing\nmethods that fail to achieve optimal distortion rates. Our data-oblivious\nalgorithms, suitable for online applications, achieve near-optimal distortion\nrates (within a small constant factor) across all bit-widths and dimensions.\nTurboQuant achieves this by randomly rotating input vectors, inducing a\nconcentrated Beta distribution on coordinates, and leveraging the\nnear-independence property of distinct coordinates in high dimensions to simply\napply optimal scalar quantizers per each coordinate. Recognizing that\nMSE-optimal quantizers introduce bias in inner product estimation, we propose a\ntwo-stage approach: applying an MSE quantizer followed by a 1-bit Quantized JL\n(QJL) transform on the residual, resulting in an unbiased inner product\nquantizer. We also provide a formal proof of the information-theoretic lower\nbounds on best achievable distortion rate by any vector quantizer,\ndemonstrating that TurboQuant closely matches these bounds, differing only by a\nsmall constant ($\\approx 2.7$) factor. Experimental results validate our\ntheoretical findings, showing that for KV cache quantization, we achieve\nabsolute quality neutrality with 3.5 bits per channel and marginal quality\ndegradation with 2.5 bits per channel. Furthermore, in nearest neighbor search\ntasks, our method outperforms existing product quantization techniques in\nrecall while reducing indexing time to virtually zero."}
{"id": "2504.19900", "pdf": "https://arxiv.org/pdf/2504.19900", "abs": "https://arxiv.org/abs/2504.19900", "authors": ["Han Chen", "Anne L. Martel"], "title": "Breast Cancer Detection from Multi-View Screening Mammograms with Visual Prompt Tuning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate detection of breast cancer from high-resolution mammograms is\ncrucial for early diagnosis and effective treatment planning. Previous studies\nhave shown the potential of using single-view mammograms for breast cancer\ndetection. However, incorporating multi-view data can provide more\ncomprehensive insights. Multi-view classification, especially in medical\nimaging, presents unique challenges, particularly when dealing with\nlarge-scale, high-resolution data. In this work, we propose a novel Multi-view\nVisual Prompt Tuning Network (MVPT-NET) for analyzing multiple screening\nmammograms. We first pretrain a robust single-view classification model on\nhigh-resolution mammograms and then innovatively adapt multi-view feature\nlearning into a task-specific prompt tuning process. This technique selectively\ntunes a minimal set of trainable parameters (7\\%) while retaining the\nrobustness of the pre-trained single-view model, enabling efficient integration\nof multi-view data without the need for aggressive downsampling. Our approach\noffers an efficient alternative to traditional feature fusion methods,\nproviding a more robust, scalable, and efficient solution for high-resolution\nmammogram analysis. Experimental results on a large multi-institution dataset\ndemonstrate that our method outperforms conventional approaches while\nmaintaining detection efficiency, achieving an AUROC of 0.852 for\ndistinguishing between Benign, DCIS, and Invasive classes. This work highlights\nthe potential of MVPT-NET for medical imaging tasks and provides a scalable\nsolution for integrating multi-view data in breast cancer detection."}
{"id": "2504.19901", "pdf": "https://arxiv.org/pdf/2504.19901", "abs": "https://arxiv.org/abs/2504.19901", "authors": ["Hude Liu", "Jerry Yao-Chieh Hu", "Zhao Song", "Han Liu"], "title": "Attention Mechanism, Max-Affine Partition, and Universal Approximation", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "We establish the universal approximation capability of single-layer,\nsingle-head self- and cross-attention mechanisms with minimal attached\nstructures. Our key insight is to interpret single-head attention as an input\ndomain-partition mechanism that assigns distinct values to subregions. This\nallows us to engineer the attention weights such that this assignment imitates\nthe target function. Building on this, we prove that a single self-attention\nlayer, preceded by sum-of-linear transformations, is capable of approximating\nany continuous function on a compact domain under the $L_\\infty$-norm.\nFurthermore, we extend this construction to approximate any Lebesgue integrable\nfunction under $L_p$-norm for $1\\leq p <\\infty$. Lastly, we also extend our\ntechniques and show that, for the first time, single-head cross-attention\nachieves the same universal approximation guarantees."}
{"id": "2504.19918", "pdf": "https://arxiv.org/pdf/2504.19918", "abs": "https://arxiv.org/abs/2504.19918", "authors": ["Hugo Georgenthum", "Cristian Cosentino", "Fabrizio Marozzo", "Pietro Liò"], "title": "Enhancing Surgical Documentation through Multimodal Visual-Temporal Transformers and Generative AI", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The automatic summarization of surgical videos is essential for enhancing\nprocedural documentation, supporting surgical training, and facilitating\npost-operative analysis. This paper presents a novel method at the intersection\nof artificial intelligence and medicine, aiming to develop machine learning\nmodels with direct real-world applications in surgical contexts. We propose a\nmulti-modal framework that leverages recent advancements in computer vision and\nlarge language models to generate comprehensive video summaries. % The approach\nis structured in three key stages. First, surgical videos are divided into\nclips, and visual features are extracted at the frame level using visual\ntransformers. This step focuses on detecting tools, tissues, organs, and\nsurgical actions. Second, the extracted features are transformed into\nframe-level captions via large language models. These are then combined with\ntemporal features, captured using a ViViT-based encoder, to produce clip-level\nsummaries that reflect the broader context of each video segment. Finally, the\nclip-level descriptions are aggregated into a full surgical report using a\ndedicated LLM tailored for the summarization task. % We evaluate our method on\nthe CholecT50 dataset, using instrument and action annotations from 50\nlaparoscopic videos. The results show strong performance, achieving 96\\%\nprecision in tool detection and a BERT score of 0.74 for temporal context\nsummarization. This work contributes to the advancement of AI-assisted tools\nfor surgical reporting, offering a step toward more intelligent and reliable\nclinical documentation."}
{"id": "2504.19940", "pdf": "https://arxiv.org/pdf/2504.19940", "abs": "https://arxiv.org/abs/2504.19940", "authors": ["Luigia Costabile", "Gian Marco Orlando", "Valerio La Gatta", "Vincenzo Moscato"], "title": "Assessing the Potential of Generative Agents in Crowdsourced Fact-Checking", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": null, "summary": "The growing spread of online misinformation has created an urgent need for\nscalable, reliable fact-checking solutions. Crowdsourced fact-checking - where\nnon-experts evaluate claim veracity - offers a cost-effective alternative to\nexpert verification, despite concerns about variability in quality and bias.\nEncouraged by promising results in certain contexts, major platforms such as X\n(formerly Twitter), Facebook, and Instagram have begun shifting from\ncentralized moderation to decentralized, crowd-based approaches.\n  In parallel, advances in Large Language Models (LLMs) have shown strong\nperformance across core fact-checking tasks, including claim detection and\nevidence evaluation. However, their potential role in crowdsourced workflows\nremains unexplored. This paper investigates whether LLM-powered generative\nagents - autonomous entities that emulate human behavior and decision-making -\ncan meaningfully contribute to fact-checking tasks traditionally reserved for\nhuman crowds. Using the protocol of La Barbera et al. (2024), we simulate\ncrowds of generative agents with diverse demographic and ideological profiles.\nAgents retrieve evidence, assess claims along multiple quality dimensions, and\nissue final veracity judgments.\n  Our results show that agent crowds outperform human crowds in truthfulness\nclassification, exhibit higher internal consistency, and show reduced\nsusceptibility to social and cognitive biases. Compared to humans, agents rely\nmore systematically on informative criteria such as Accuracy, Precision, and\nInformativeness, suggesting a more structured decision-making process. Overall,\nour findings highlight the potential of generative agents as scalable,\nconsistent, and less biased contributors to crowd-based fact-checking systems."}
{"id": "2504.19944", "pdf": "https://arxiv.org/pdf/2504.19944", "abs": "https://arxiv.org/abs/2504.19944", "authors": ["Markus Bläser", "Julian Dörfler", "Maciej Liśkiewicz", "Benito van der Zander"], "title": "Probabilistic and Causal Satisfiability: Constraining the Model", "categories": ["cs.CC", "cs.AI", "cs.LO"], "comment": "accepted at ICALP 25", "summary": "We study the complexity of satisfiability problems in probabilistic and\ncausal reasoning. Given random variables $X_1, X_2,\\ldots$ over finite domains,\nthe basic terms are probabilities of propositional formulas over atomic events\n$X_i = x_i$, such as $P(X_1 = x_1)$ or $P(X_1 = x_1 \\vee X_2 = x_2)$. The basic\nterms can be combined using addition (yielding linear terms) or multiplication\n(polynomial terms). The probabilistic satisfiability problem asks whether a\njoint probability distribution satisfies a Boolean combination of\n(in)equalities over such terms. Fagin et al. (1990) showed that for basic and\nlinear terms, this problem is NP-complete, making it no harder than Boolean\nsatisfiability, while Moss\\'e et al. (2022) proved that for polynomial terms,\nit is complete for the existential theory of the reals.\n  Pearl's Causal Hierarchy (PCH) extends the probabilistic setting with\ninterventional and counterfactual reasoning, enriching the expressiveness of\nlanguages. However, Moss\\'e et al. (2022) found that satisfiability complexity\nremains unchanged. Van der Zander et al. (2023) showed that introducing a\nmarginalization operator to languages induces a significant increase in\ncomplexity.\n  We extend this line of work by adding two new dimensions to the problem by\nconstraining the models. First, we fix the graph structure of the underlying\nstructural causal model, motivated by settings like Pearl's do-calculus, and\ngive a nearly complete landscape across different arithmetics and PCH levels.\nSecond, we study small models. While earlier work showed that satisfiable\ninstances admit polynomial-size models, this is no longer guaranteed with\ncompact marginalization. We characterize the complexities of satisfiability\nunder small-model constraints across different settings."}
{"id": "2504.19949", "pdf": "https://arxiv.org/pdf/2504.19949", "abs": "https://arxiv.org/abs/2504.19949", "authors": ["Aydoğan Soylu", "Tufan Kumbasar"], "title": "Capturing Aerodynamic Characteristics of ATTAS Aircraft with Evolving Intelligent System", "categories": ["eess.SY", "cs.AI", "cs.SY"], "comment": "in International Congress on Human-Computer Interaction, Optimization\n  and Robotic Applications, 2025", "summary": "Accurate modeling of aerodynamic coefficients is crucial for understanding\nand optimizing the performance of modern aircraft systems. This paper presents\nthe novel deployment of an Evolving Type-2 Quantum Fuzzy Neural Network\n(eT2QFNN) for modeling the aerodynamic coefficients of the ATTAS aircraft to\nexpress the aerodynamic characteristics. eT2QFNN can represent the nonlinear\naircraft model by creating multiple linear submodels with its rule-based\nstructure through an incremental learning strategy rather than a traditional\nbatch learning approach. Moreover, it enhances robustness to uncertainties and\ndata noise through its quantum membership functions, as well as its automatic\nrule-learning and parameter-tuning capabilities. During the estimation of the\naerodynamic coefficients via the flight data of the ATTAS, two different\nstudies are conducted in the training phase: one with a large amount of data\nand the other with a limited amount of data. The results show that the modeling\nperformance of the eT2QFNN is superior in comparison to baseline counterparts.\nFurthermore, eT2QFNN estimated the aerodynamic model with fewer rules compared\nto Type-1 fuzzy counterparts. In addition, by applying the Delta method to the\nproposed approach, the stability and control derivatives of the aircraft are\nanalyzed. The results prove the superiority of the proposed eT2QFNN in\nrepresenting aerodynamic coefficients."}
{"id": "2504.19951", "pdf": "https://arxiv.org/pdf/2504.19951", "abs": "https://arxiv.org/abs/2504.19951", "authors": ["Vineeth Sai Narajala", "Ken Huang", "Idan Habler"], "title": "Securing GenAI Multi-Agent Systems Against Tool Squatting: A Zero Trust Registry-Based Approach", "categories": ["cs.CR", "cs.AI"], "comment": "12 pages, 4 figures, 1 table", "summary": "The rise of generative AI (GenAI) multi-agent systems (MAS) necessitates\nstandardized protocols enabling agents to discover and interact with external\ntools. However, these protocols introduce new security challenges,\nparticularly; tool squatting; the deceptive registration or representation of\ntools. This paper analyzes tool squatting threats within the context of\nemerging interoperability standards, such as Model Context Protocol (MCP) or\nseamless communication between agents protocols. It introduces a comprehensive\nTool Registry system designed to mitigate these risks. We propose a\nsecurity-focused architecture featuring admin-controlled registration,\ncentralized tool discovery, fine grained access policies enforced via dedicated\nAgent and Tool Registry services, a dynamic trust scoring mechanism based on\ntool versioning and known vulnerabilities, and just in time credential\nprovisioning. Based on its design principles, the proposed registry framework\naims to effectively prevent common tool squatting vectors while preserving the\nflexibility and power of multi-agent systems. This work addresses a critical\nsecurity gap in the rapidly evolving GenAI ecosystem and provides a foundation\nfor secure tool integration in production environments."}
{"id": "2504.19956", "pdf": "https://arxiv.org/pdf/2504.19956", "abs": "https://arxiv.org/abs/2504.19956", "authors": ["Vineeth Sai Narajala", "Om Narayan"], "title": "Securing Agentic AI: A Comprehensive Threat Model and Mitigation Framework for Generative AI Agents", "categories": ["cs.CR", "cs.AI"], "comment": "12 pages, 2 figures, 1 table", "summary": "As generative AI (GenAI) agents become more common in enterprise settings,\nthey introduce security challenges that differ significantly from those posed\nby traditional systems. These agents are not just LLMs; they reason, remember,\nand act, often with minimal human oversight. This paper introduces a\ncomprehensive threat model tailored specifically for GenAI agents, focusing on\nhow their autonomy, persistent memory access, complex reasoning, and tool\nintegration create novel risks. This research work identifies 9 primary threats\nand organizes them across five key domains: cognitive architecture\nvulnerabilities, temporal persistence threats, operational execution\nvulnerabilities, trust boundary violations, and governance circumvention. These\nthreats are not just theoretical they bring practical challenges such as\ndelayed exploitability, cross-system propagation, cross system lateral\nmovement, and subtle goal misalignments that are hard to detect with existing\nframeworks and standard approaches. To help address this, the research work\npresent two complementary frameworks: ATFAA - Advanced Threat Framework for\nAutonomous AI Agents, which organizes agent-specific risks, and SHIELD, a\nframework proposing practical mitigation strategies designed to reduce\nenterprise exposure. While this work builds on existing work in LLM and AI\nsecurity, the focus is squarely on what makes agents different and why those\ndifferences matter. Ultimately, this research argues that GenAI agents require\na new lens for security. If we fail to adapt our threat models and defenses to\naccount for their unique architecture and behavior, we risk turning a powerful\nnew tool into a serious enterprise liability."}
{"id": "2504.19967", "pdf": "https://arxiv.org/pdf/2504.19967", "abs": "https://arxiv.org/abs/2504.19967", "authors": ["Adway Das", "Agnimitra Sengupta", "S. Ilgin Guler"], "title": "Enhancing short-term traffic prediction by integrating trends and fluctuations with attention mechanism", "categories": ["cs.ET", "cs.AI", "cs.LG", "stat.AP"], "comment": null, "summary": "Traffic flow prediction is a critical component of intelligent transportation\nsystems, yet accurately forecasting traffic remains challenging due to the\ninteraction between long-term trends and short-term fluctuations. Standard deep\nlearning models often struggle with these challenges because their\narchitectures inherently smooth over fine-grained fluctuations while focusing\non general trends. This limitation arises from low-pass filtering effects, gate\nbiases favoring stability, and memory update mechanisms that prioritize\nlong-term information retention. To address these shortcomings, this study\nintroduces a hybrid deep learning framework that integrates both long-term\ntrend and short-term fluctuation information using two input features processed\nin parallel, designed to capture complementary aspects of traffic flow\ndynamics. Further, our approach leverages attention mechanisms, specifically\nBahdanau attention, to selectively focus on critical time steps within traffic\ndata, enhancing the model's ability to predict congestion and other transient\nphenomena. Experimental results demonstrate that features learned from both\nbranches are complementary, significantly improving the goodness-of-fit\nstatistics across multiple prediction horizons compared to a baseline model.\nNotably, the attention mechanism enhances short-term forecast accuracy by\ndirectly targeting immediate fluctuations, though challenges remain in fully\nintegrating long-term trends. This framework can contribute to more effective\ncongestion mitigation and urban mobility planning by advancing the robustness\nand precision of traffic prediction models."}
{"id": "2504.19982", "pdf": "https://arxiv.org/pdf/2504.19982", "abs": "https://arxiv.org/abs/2504.19982", "authors": ["Emre Can Acikgoz", "Carl Guo", "Suvodip Dey", "Akul Datta", "Takyoung Kim", "Gokhan Tur", "Dilek Hakkani-Tür"], "title": "TD-EVAL: Revisiting Task-Oriented Dialogue Evaluation by Combining Turn-Level Precision with Dialogue-Level Comparisons", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Task-oriented dialogue (TOD) systems are experiencing a revolution driven by\nLarge Language Models (LLMs), yet the evaluation methodologies for these\nsystems remain insufficient for their growing sophistication. While traditional\nautomatic metrics effectively assessed earlier modular systems, they focus\nsolely on the dialogue level and cannot detect critical intermediate errors\nthat can arise during user-agent interactions. In this paper, we introduce\nTD-EVAL (Turn and Dialogue-level Evaluation), a two-step evaluation framework\nthat unifies fine-grained turn-level analysis with holistic dialogue-level\ncomparisons. At turn level, we evaluate each response along three TOD-specific\ndimensions: conversation cohesion, backend knowledge consistency, and policy\ncompliance. Meanwhile, we design TOD Agent Arena that uses pairwise comparisons\nto provide a measure of dialogue-level quality. Through experiments on MultiWOZ\n2.4 and {\\tau}-Bench, we demonstrate that TD-EVAL effectively identifies the\nconversational errors that conventional metrics miss. Furthermore, TD-EVAL\nexhibits better alignment with human judgments than traditional and LLM-based\nmetrics. These findings demonstrate that TD-EVAL introduces a new paradigm for\nTOD system evaluation, efficiently assessing both turn and system levels with a\nplug-and-play framework for future research."}
{"id": "2504.19985", "pdf": "https://arxiv.org/pdf/2504.19985", "abs": "https://arxiv.org/abs/2504.19985", "authors": ["Keyhan Rayati", "Amirhossein Feizi", "Alireza Beigy", "Pourya Shahverdi", "Mehdi Tale Masouleh", "Ahmad Kalhor"], "title": "Real-Time Imitation of Human Head Motions, Blinks and Emotions by Nao Robot: A Closed-Loop Approach", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "This paper introduces a novel approach for enabling real-time imitation of\nhuman head motion by a Nao robot, with a primary focus on elevating human-robot\ninteractions. By using the robust capabilities of the MediaPipe as a computer\nvision library and the DeepFace as an emotion recognition library, this\nresearch endeavors to capture the subtleties of human head motion, including\nblink actions and emotional expressions, and seamlessly incorporate these\nindicators into the robot's responses. The result is a comprehensive framework\nwhich facilitates precise head imitation within human-robot interactions,\nutilizing a closed-loop approach that involves gathering real-time feedback\nfrom the robot's imitation performance. This feedback loop ensures a high\ndegree of accuracy in modeling head motion, as evidenced by an impressive R2\nscore of 96.3 for pitch and 98.9 for yaw. Notably, the proposed approach holds\npromise in improving communication for children with autism, offering them a\nvaluable tool for more effective interaction. In essence, proposed work\nexplores the integration of real-time head imitation and real-time emotion\nrecognition to enhance human-robot interactions, with potential benefits for\nindividuals with unique communication needs."}
{"id": "2504.19990", "pdf": "https://arxiv.org/pdf/2504.19990", "abs": "https://arxiv.org/abs/2504.19990", "authors": ["Salem Lahlou"], "title": "Mitigating Societal Cognitive Overload in the Age of AI: Challenges and Directions", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Societal cognitive overload, driven by the deluge of information and\ncomplexity in the AI age, poses a critical challenge to human well-being and\nsocietal resilience. This paper argues that mitigating cognitive overload is\nnot only essential for improving present-day life but also a crucial\nprerequisite for navigating the potential risks of advanced AI, including\nexistential threats. We examine how AI exacerbates cognitive overload through\nvarious mechanisms, including information proliferation, algorithmic\nmanipulation, automation anxieties, deregulation, and the erosion of meaning.\nThe paper reframes the AI safety debate to center on cognitive overload,\nhighlighting its role as a bridge between near-term harms and long-term risks.\nIt concludes by discussing potential institutional adaptations, research\ndirections, and policy considerations that arise from adopting an\noverload-resilient perspective on human-AI alignment, suggesting pathways for\nfuture exploration rather than prescribing definitive solutions."}
{"id": "2504.19996", "pdf": "https://arxiv.org/pdf/2504.19996", "abs": "https://arxiv.org/abs/2504.19996", "authors": ["Andreas Kalogeras", "Dimitrios Bormpoudakis", "Iason Tsardanidis", "Dimitra A. Loka", "Charalampos Kontoes"], "title": "Monitoring digestate application on agricultural crops using Sentinel-2 Satellite imagery", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "The widespread use of Exogenous Organic Matter in agriculture necessitates\nmonitoring to assess its effects on soil and crop health. This study evaluates\noptical Sentinel-2 satellite imagery for detecting digestate application, a\npractice that enhances soil fertility but poses environmental risks like\nmicroplastic contamination and nitrogen losses. In the first instance,\nSentinel-2 satellite image time series (SITS) analysis of specific indices\n(EOMI, NDVI, EVI) was used to characterize EOM's spectral behavior after\napplication on the soils of four different crop types in Thessaly, Greece.\nFurthermore, Machine Learning (ML) models (namely Random Forest, k-NN, Gradient\nBoosting and a Feed-Forward Neural Network), were used to investigate digestate\npresence detection, achieving F1-scores up to 0.85. The findings highlight the\npotential of combining remote sensing and ML for scalable and cost-effective\nmonitoring of EOM applications, supporting precision agriculture and\nsustainability."}
{"id": "2504.19997", "pdf": "https://arxiv.org/pdf/2504.19997", "abs": "https://arxiv.org/abs/2504.19997", "authors": ["Ivo Brett"], "title": "Simplified and Secure MCP Gateways for Enterprise AI Integration", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "The increased adoption of the Model Context Protocol (MCP) for AI Agents\nnecessitates robust security for Enterprise integrations. This paper introduces\nthe MCP Gateway to simplify self-hosted MCP server integration. The proposed\narchitecture integrates security principles, authentication, intrusion\ndetection, and secure tunneling, enabling secure self-hosting without exposing\ninfrastructure. Key contributions include a reference architecture, threat\nmodel mapping, simplified integration strategies, and open-source\nimplementation recommendations. This work focuses on the unique challenges of\nenterprise-centric, self-hosted AI integrations, unlike existing public MCP\nserver solutions."}
{"id": "2504.20018", "pdf": "https://arxiv.org/pdf/2504.20018", "abs": "https://arxiv.org/abs/2504.20018", "authors": ["Jiongli Zhu", "Yue Wang", "Bailu Ding", "Philip A. Bernstein", "Vivek Narasayya", "Surajit Chaudhuri"], "title": "MINT: Multi-Vector Search Index Tuning", "categories": ["cs.DB", "cs.AI"], "comment": null, "summary": "Vector search plays a crucial role in many real-world applications. In\naddition to single-vector search, multi-vector search becomes important for\nmulti-modal and multi-feature scenarios today. In a multi-vector database, each\nrow is an item, each column represents a feature of items, and each cell is a\nhigh-dimensional vector. In multi-vector databases, the choice of indexes can\nhave a significant impact on performance. Although index tuning for relational\ndatabases has been extensively studied, index tuning for multi-vector search\nremains unclear and challenging. In this paper, we define multi-vector search\nindex tuning and propose a framework to solve it. Specifically, given a\nmulti-vector search workload, we develop algorithms to find indexes that\nminimize latency and meet storage and recall constraints. Compared to the\nbaseline, our latency achieves 2.1X to 8.3X speedup."}
{"id": "2504.20019", "pdf": "https://arxiv.org/pdf/2504.20019", "abs": "https://arxiv.org/abs/2504.20019", "authors": ["Abdelhakim Amer", "David Felsager", "Yury Brodskiy", "Andriy Sarabakha"], "title": "Modelling of Underwater Vehicles using Physics-Informed Neural Networks with Control", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "This paper has been accepted for presentation at the International\n  Joint Conference on Neural Networks (IJCNN) 2025. The final version consists\n  of 8 pages", "summary": "Physics-informed neural networks (PINNs) integrate physical laws with\ndata-driven models to improve generalization and sample efficiency. This work\nintroduces an open-source implementation of the Physics-Informed Neural Network\nwith Control (PINC) framework, designed to model the dynamics of an underwater\nvehicle. Using initial states, control actions, and time inputs, PINC extends\nPINNs to enable physically consistent transitions beyond the training domain.\nVarious PINC configurations are tested, including differing loss functions,\ngradient-weighting schemes, and hyperparameters. Validation on a simulated\nunderwater vehicle demonstrates more accurate long-horizon predictions compared\nto a non-physics-informed baseline"}
{"id": "2504.20020", "pdf": "https://arxiv.org/pdf/2504.20020", "abs": "https://arxiv.org/abs/2504.20020", "authors": ["Xin Wang", "Haoyang Li", "Zeyang Zhang", "Haibo Chen", "Wenwu Zhu"], "title": "Modular Machine Learning: An Indispensable Path towards New-Generation Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": "11 pages, 3 figures", "summary": "Large language models (LLMs) have dramatically advanced machine learning\nresearch including natural language processing, computer vision, data mining,\netc., yet they still exhibit critical limitations in reasoning, factual\nconsistency, and interpretability. In this paper, we introduce a novel learning\nparadigm -- Modular Machine Learning (MML) -- as an essential approach toward\nnew-generation LLMs. MML decomposes the complex structure of LLMs into three\ninterdependent components: modular representation, modular model, and modular\nreasoning, aiming to enhance LLMs' capability of counterfactual reasoning,\nmitigating hallucinations, as well as promoting fairness, safety, and\ntransparency. Specifically, the proposed MML paradigm can: i) clarify the\ninternal working mechanism of LLMs through the disentanglement of semantic\ncomponents; ii) allow for flexible and task-adaptive model design; iii) enable\ninterpretable and logic-driven decision-making process. We present a feasible\nimplementation of MML-based LLMs via leveraging advanced techniques such as\ndisentangled representation learning, neural architecture search and\nneuro-symbolic learning. We critically identify key challenges, such as the\nintegration of continuous neural and discrete symbolic processes, joint\noptimization, and computational scalability, present promising future research\ndirections that deserve further exploration. Ultimately, the integration of the\nMML paradigm with LLMs has the potential to bridge the gap between statistical\n(deep) learning and formal (logical) reasoning, thereby paving the way for\nrobust, adaptable, and trustworthy AI systems across a wide range of real-world\napplications."}
{"id": "2504.20026", "pdf": "https://arxiv.org/pdf/2504.20026", "abs": "https://arxiv.org/abs/2504.20026", "authors": ["Zhengqin Li", "Dilin Wang", "Ka Chen", "Zhaoyang Lv", "Thu Nguyen-Phuoc", "Milim Lee", "Jia-Bin Huang", "Lei Xiao", "Cheng Zhang", "Yufeng Zhu", "Carl S. Marshall", "Yufeng Ren", "Richard Newcombe", "Zhao Dong"], "title": "LIRM: Large Inverse Rendering Model for Progressive Reconstruction of Shape, Materials and View-dependent Radiance Fields", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by CVPR 2025", "summary": "We present Large Inverse Rendering Model (LIRM), a transformer architecture\nthat jointly reconstructs high-quality shape, materials, and radiance fields\nwith view-dependent effects in less than a second. Our model builds upon the\nrecent Large Reconstruction Models (LRMs) that achieve state-of-the-art\nsparse-view reconstruction quality. However, existing LRMs struggle to\nreconstruct unseen parts accurately and cannot recover glossy appearance or\ngenerate relightable 3D contents that can be consumed by standard Graphics\nengines. To address these limitations, we make three key technical\ncontributions to build a more practical multi-view 3D reconstruction framework.\nFirst, we introduce an update model that allows us to progressively add more\ninput views to improve our reconstruction. Second, we propose a hexa-plane\nneural SDF representation to better recover detailed textures, geometry and\nmaterial parameters. Third, we develop a novel neural directional-embedding\nmechanism to handle view-dependent effects. Trained on a large-scale shape and\nmaterial dataset with a tailored coarse-to-fine training scheme, our model\nachieves compelling results. It compares favorably to optimization-based\ndense-view inverse rendering methods in terms of geometry and relighting\naccuracy, while requiring only a fraction of the inference time."}
